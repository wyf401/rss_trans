<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 22 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于极端多标签知识图链接预测的检索增强语言模型</title>
      <link>https://arxiv.org/abs/2405.12656</link>
      <description><![CDATA[arXiv:2405.12656v1 公告类型：新
摘要：用于开放式探究的大型语言模型（LLM）外推遇到两个关键问题：（1）幻觉和（2）昂贵的培训成本。这些问题给专业领域和个性化数据的法学硕士带来了挑战，需要真实的回应和较低的微调成本。现有的工作试图通过使用知识图（KG）中的信息来增强较小语言模型的输入来解决该问题。然而，它们有两个局限性：（1）无法从知识图谱中的大型一跳邻域中提取相关信息；（2）对具有不同特征的知识图谱应用相同的增强策略，这可能会导致性能低下。此外，开放式询问通常会产生多种答复，使推断进一步复杂化。我们提出了一项新任务，即极限多标签 KG 链接预测任务，使模型能够使用结构化的现实世界知识对多个响应进行外推。我们的检索器通过一起考虑实体、关系和文本数据来识别相关的一跳邻居。我们的实验表明，（1）具有不同特征的知识图谱需要不同的增强策略，（2）用文本数据增强语言模型的输入可以显着提高任务性能。通过将检索增强框架与知识图谱相结合，我们的框架能够以较小的参数大小根据给定的知识图谱进行推断。代码可以在GitHub上获取：https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git]]></description>
      <guid>https://arxiv.org/abs/2405.12656</guid>
      <pubDate>Wed, 22 May 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>MentalQA：带注释的阿拉伯语心理健康问答语料库</title>
      <link>https://arxiv.org/abs/2405.12619</link>
      <description><![CDATA[arXiv:2405.12619v1 公告类型：新
摘要：无论背景、教育程度或社会经济地位如何，精神健康障碍都会对全球人民产生重大影响。然而，获得足够的护理仍然是一个挑战，特别是对于资源有限的服务不足的社区。文本挖掘工具通过协助专业人员诊断和治疗患者，为支持心理保健提供了巨大的潜力。这项研究解决了用于开发此类工具的阿拉伯心理健康资源的稀缺问题。我们介绍 MentalQA，这是一个新颖的阿拉伯语数据集，具有对话式问答 (QA) 交互功能。为了确保数据质量，我们使用定义良好的模式和质量控制措施进行了严格的注释过程。数据是从问答医疗平台收集的。心理健康问题和相应答案的注释方案借鉴了现有的分类方案并进行了一些修改。问题类型包括六个不同的类别：诊断、治疗、解剖学和生理学、流行病学、健康生活方式和提供者选择。回答策略包括信息提供、直接指导和情感支持。三位经验丰富的注释者协作注释数据以确保一致性。我们的研究结果表明，注释者之间的一致性很高，Fleiss 的问题类型 Kappa 为 0.61 美元，答案策略为 0.98 美元。深入的分析揭示了富有洞察力的模式，包括不同年龄组的问题偏好的差异以及问题类型和回答策略之间的强相关性。 MentalQA 为开发能够支持心理健康专业人员和个人寻求信息的阿拉伯文本挖掘工具提供了宝贵的基础。]]></description>
      <guid>https://arxiv.org/abs/2405.12619</guid>
      <pubDate>Wed, 22 May 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>用于文本生成的屏蔽和因果语言模型的探索</title>
      <link>https://arxiv.org/abs/2405.12630</link>
      <description><![CDATA[arXiv:2405.12630v1 公告类型：新
摘要：大型语言模型 (LLM) 彻底改变了自然语言处理 (NLP) 领域，并在该领域的几乎所有任务中都取得了最先进的性能。然而，文本生成中普遍使用的方法，即因果语言建模 (CLM)，从左到右按顺序生成文本，本质上限制了模型的自由度，模型无法决定每个标记的生成时间和位置。相比之下，主要用于语言理解任务的掩码语言建模 (MLM) 可以在文本中的任何位置以任何顺序生成标记。本文对文本生成任务的 MLM 和 CLM 方法进行了广泛的比较。为此，我们在三个不同的数据集上预先训练了几个大小相当的语言模型，即 1) 医疗出院摘要、2) 电影情节概要和 3) 作者验证数据集。为了评估生成的文本的质量，我们首先采用定量指标，然后进行定性人工评估，以分析连贯性和语法正确性。此外，我们通过在三个不同的下游任务中使用生成的文本来评估其实用性：1）实体识别，2）文本分类，3）作者验证。结果表明，MLM 在所有数据集的文本生成中始终优于 CLM，生成的文本具有更高的定量分数和更好的连贯性。研究还发现生成的文本的质量与下游任务中的模型性能之间没有很强的相关性。通过这项研究，我们表明 MLM 用于文本生成具有巨大的未来研究潜力，并为该领域的未来研究提供了方向。]]></description>
      <guid>https://arxiv.org/abs/2405.12630</guid>
      <pubDate>Wed, 22 May 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>Tagengo：多语言聊天数据集</title>
      <link>https://arxiv.org/abs/2405.12612</link>
      <description><![CDATA[arXiv:2405.12612v1 公告类型：新
摘要：开源大语言模型（LLM）近年来已经显示出巨大的进步。然而，其中许多模型仅关注流行的口语。我们提供了包含 74 种语言的超过 70k 个提示-响应对的高质量数据集，其中包括人类生成的提示和合成响应。我们使用这个数据集来训练最先进的开源英语法学硕士进行多语言聊天。我们在 6 种语言的 MT-Bench 聊天基准上评估了我们的模型，发现我们的多语言模型在每种语言上都优于以前最先进的开源 LLM。我们进一步发现，与仅对特定语言的数据进行训练相比，对更多多语言数据进行训练有利于所选目标语言（日语）的性能。这些结果表明有必要对大量高质量的多语言数据进行培训，以使法学硕士更容易获得。]]></description>
      <guid>https://arxiv.org/abs/2405.12612</guid>
      <pubDate>Wed, 22 May 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>量化大型语言模型中的出现</title>
      <link>https://arxiv.org/abs/2405.12617</link>
      <description><![CDATA[arXiv:2405.12617v1 公告类型：新
摘要：涌现，广泛概念化为法学硕士的“智能”行为，最近经过研究并证明由于缺乏可测量的定义而难以量化。最常见的是，它是通过跨大量数据集和任务的模型性能进行统计估计的，这会消耗大量资源。此外，这种估计很难解释，并且可能无法准确反映模型的内在出现。在这项工作中，我们提出了一种用于估计出现率的量化解决方案。受动力学中涌现论的启发，我们通过比较宏观（语义）级别与微观（令牌）级别的熵减少来量化涌现的强度，这两者都源自 Transformer 块内的表示。使用低成本估计器，我们的量化方法在上下文学习 (ICL) 和自然句子下展示了一系列 LM（GPT-2、GEMMA 等）的一致行为。实证结果表明（1）我们的方法给出了一致的测量结果，与基于性能指标的现有观察结果相一致，验证了我们的涌现量化的有效性； (2) 我们提出的指标揭示了新的出现模式，例如我们的指标方差与 ICL 中“镜头”数量之间的相关性，这进一步提出了一种解释法学硕士中幻觉的新方法； (3) 我们提供了一个潜在的解决方案，通过 GPT-2 等较小的 LM 来估计更大且封闭资源的 LM 的出现。我们的代码位于：https://github.com/Zodiark-ch/Emergence-of-LLMs/。]]></description>
      <guid>https://arxiv.org/abs/2405.12617</guid>
      <pubDate>Wed, 22 May 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>通过 KV 缓存压缩的矩阵分解解锁无数据低位量化</title>
      <link>https://arxiv.org/abs/2405.12591</link>
      <description><![CDATA[arXiv:2405.12591v1 公告类型：新
摘要：键值（KV）缓存是加速大型语言模型（LLM）推理的重要技术，但会产生大量内存开销。为了压缩 KV 缓存的大小，现有方法通常会牺牲精度或需要额外的数据进行校准，从而限制了它们在 LLM 部署中的实用性。在本文中，我们引入了 \textbf{DecoQuant}，一种基于张量分解方法的新型无数据低位量化技术，以有效压缩 KV 缓存。我们的核心思想是通过进行张量分解来调整原始矩阵的离群分布，从而将量化困难从矩阵迁移到分解的局部张量。特别地，我们发现异常值主要集中在小的局部张量上，而大的张量往往具有较窄的值范围。基于这一发现，我们建议对大张量应用低位量化，同时保持小张量的高精度表示。此外，我们利用所提出的量化方法来压缩 LLM 的 KV 缓存，以加速推理并开发专为 DecoQuant 定制的高效反量化内核。通过大量的实验，DecoQuant 展示了显着的效率提升，内存占用量减少了 $\sim$75\%，同时保持了相当的生成质量。]]></description>
      <guid>https://arxiv.org/abs/2405.12591</guid>
      <pubDate>Wed, 22 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>微小的改进带来了弹性：针对 LLM 红队的高效前缀模型</title>
      <link>https://arxiv.org/abs/2405.12604</link>
      <description><![CDATA[arXiv:2405.12604v1 公告类型：新
摘要：随着大型语言模型（LLM）红队策略的激增，有关提高 LLM 防御策略的安全性和鲁棒性的文献中的缺陷变得越来越明显。本文介绍了基于 LLM 的 \textbf{sentinel} 模型作为即插即用的前缀模块，旨在仅使用少量（$&lt;30$）额外令牌即可重建输入提示，从而有效减少目标 LLM 响应中的毒性。哨兵模型自然地克服了用于微调大型目标模型的 \textit{参数效率低下} 和 \textit{有限的模型可访问性}。我们采用近端策略优化（PPO）的交错训练方案来动态优化红队和哨兵模型，并结合受多智能体集中批评家启发的价值头共享机制来管理智能体之间复杂的相互作用。我们在文本到文本和文本到图像方面的广泛实验证明了我们的方法在减轻有毒输出方面的有效性，即使在处理 \texttt{Llama-2}、\texttt{GPT-3.5} 和 \ texttt{Stable-Diffusion}，强调了我们的框架在增强各种应用程序的安全性和鲁棒性方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.12604</guid>
      <pubDate>Wed, 22 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>PyramidInfer：用于高吞吐量 LLM 推理的 Pyramid KV 缓存压缩</title>
      <link>https://arxiv.org/abs/2405.12532</link>
      <description><![CDATA[arXiv:2405.12532v1 公告类型：新
摘要：大型语言模型 (LLM) 表现出了卓越的理解能力，但在推理过程中面临 GPU 内存使用的挑战，阻碍了它们在聊天机器人等实时应用程序中的可扩展性。为了加速推理，我们将计算出的键和值（KV 缓存）存储在 GPU 内存中。现有方法研究 KV 缓存压缩，通过修剪预先计算的 KV 缓存来减少内存。然而，他们忽略了层与层之间的层间依赖以及预计算中巨大的内存消耗。为了探索这些缺陷，我们发现影响后代的关键键和值的数量逐层减少，我们可以通过注意力权重的一致性来提取它们。基于这些发现，我们提出了 PyramidInfer，一种通过分层保留关键上下文来压缩 KV 缓存的方法。 PyramidInfer 通过计算更少的键和值来节省大量内存，而不会牺牲性能。实验结果表明，与 Accelerate 相比，PyramidInfer 的吞吐量提高了 2.2 倍，KV 缓存中的 GPU 内存减少了 54% 以上。]]></description>
      <guid>https://arxiv.org/abs/2405.12532</guid>
      <pubDate>Wed, 22 May 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>挖掘可解释性和泛化性：基于自学的事实验证</title>
      <link>https://arxiv.org/abs/2405.12579</link>
      <description><![CDATA[arXiv:2405.12579v1 公告类型：新
摘要：基于商业法学硕士的事实核查已成为主流。尽管这些方法具有很高的可解释性，但与传统的微调方法相比，其准确性较差，而且数据安全也是一个重要问题。在本文中，我们提出了一种基于自我指令的微调方法，用于平衡准确性和可解释性的事实检查。我们的方法包括数据增强和改进的 DPO 微调。前者首先指示模型根据主张证据对和标签生成正面和负面解释，然后根据我们定制的难度标准对数据集进行采样。后者采用我们提出的改进的 DPO，使用生成的样本对模型进行微调。我们对最小规模的 LLaMA-7B 模型进行微调，并在具有挑战性的事实检查数据集 FEVEROUS 和 HOVER 上对其进行评估，利用四种微调方法和三种小样本学习方法进行比较。实验表明，我们的方法不仅保持了与传统微调方法相当甚至超越的准确性，而且还生成了流畅的解释文本。此外，它还表现出很高的泛化性能。正如实验所示，我们的方法是第一个利用自我监督学习进行事实检查的方法，并创新性地结合对比学习和改进的 DPO 来微调法学硕士。]]></description>
      <guid>https://arxiv.org/abs/2405.12579</guid>
      <pubDate>Wed, 22 May 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>利用多样化的数据生成进行适应性零样本对话状态跟踪</title>
      <link>https://arxiv.org/abs/2405.12468</link>
      <description><![CDATA[arXiv:2405.12468v1 公告类型：新
摘要：这项工作表明，通过使用合成数据生成技术增加训练数据的多样性，可以显着提高零样本对话状态跟踪（DST）的准确性。由于数据收集成本高昂，当前的DST培训资源在应用领域数量和覆盖的时隙类型上受到严重限制，导致对新领域的适应性有限。所提出的工作克服了这一挑战，使用一种新颖的全自动数据生成方法来创建合成的零样本 DST 训练资源。与以前生成 DST 数据的方法不同，所提出的方法生成全新的应用程序域来生成对话，并配有银色对话状态注释和槽描述。该方法用于创建用于训练零样本 DST 模型的 D0T 数据集，该数据集涵盖了前所未有的 1,000 多个领域。在 MultiWOZ 基准上进行的实验表明，基于不同合成数据的训练模型可将联合目标准确度提高 6.7%，取得与更大模型相媲美的结果。]]></description>
      <guid>https://arxiv.org/abs/2405.12468</guid>
      <pubDate>Wed, 22 May 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>稀疏自动编码器在语言模型中实现可扩展且可靠的电路识别</title>
      <link>https://arxiv.org/abs/2405.12522</link>
      <description><![CDATA[arXiv:2405.12522v1 公告类型：新
摘要：本文介绍了一种使用离散稀疏自动编码器在大型语言模型中发现可解释电路的高效且鲁棒的方法。我们的方法解决了现有技术的关键限制，即计算复杂性和对超参数的敏感性。我们建议在精心设计的正例和负例上训练稀疏自动编码器，其中模型只能正确预测正例的下一个标记。我们假设，学习到的注意力头输出表示会在头进行特定计算时发出信号。通过将学习到的表示离散化为整数代码，并测量每个头的正例所特有的代码之间的重叠，我们可以直接识别电路中涉及的注意力头，而不需要昂贵的消融或架构修改。在三个经过充分研究的任务上——间接对象识别、大于比较和文档字符串完成——与最先进的基线相比，所提出的方法在恢复地面实况电路方面实现了更高的精度和召回率，同时将运行时间缩短了数小时至秒。值得注意的是，每项任务我们只需要 5-10 个文本示例即可学习稳健的表示。我们的研究结果强调了离散稀疏自动编码器在可扩展和高效的机械可解释性方面的前景，为分析大型语言模型的内部工作原理提供了新的方向。]]></description>
      <guid>https://arxiv.org/abs/2405.12522</guid>
      <pubDate>Wed, 22 May 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>SirLLM：流媒体无限保留法学硕士</title>
      <link>https://arxiv.org/abs/2405.12528</link>
      <description><![CDATA[arXiv:2405.12528v1 公告类型：新
摘要：随着大型语言模型（LLM）在各个领域变得越来越普遍，它们处理任意长度的输入并保持一定程度的记忆的能力变得至关重要。然而，过长文本的一次性输入是有限的，研究表明，当输入长度超过LLM预先训练的文本长度时，文本生成能力会急剧下降。此外，简单地延长预训练文本的长度是不切实际的，因为获取长文本数据很困难，而且这会给法学硕士带来大量的内存消耗成本。最近的努力采用了流式输入来减轻过长文本输入的压力，但这种方法会严重损害模型的长期记忆能力。
  受这一挑战的推动，我们引入了 Streaming Infinite Retentive LLM (SirLLM)，它允许 LLM 在无限长度的对话期间保持更长的内存，而无需进行微调。 SirLLM利用Token Entropy指标和记忆衰减机制来过滤关键短语，赋予LLM持久且灵活的记忆。我们设计了三个不同的任务并构建了三个数据集来从不同角度衡量 SirLLM 的有效性：（1）DailyDialog； (2) 杂货购物； （3）剪刀石头布。我们的实验结果有力地证明了 SirLLM 可以在不同的 LLM 和任务中实现稳定且显着的改进，有力地证明了其有效性。谈话时，“先生可能会忘记自己”，但 SirLLM 却绝不会这样做！我们的代码可在 https://github.com/Zoeyao27/SirLLM 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2405.12528</guid>
      <pubDate>Wed, 22 May 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>针对低资源语言家庭的有针对性的多语言适应</title>
      <link>https://arxiv.org/abs/2405.12413</link>
      <description><![CDATA[arXiv:2405.12413v1 公告类型：新
摘要：众所周知，多语言模型的“大规模多语言”训练限制了它们在任何一种语言中的实用性，并且它们在资源匮乏的语言上表现尤其差。然而，有证据表明，资源匮乏的语言可以从有针对性的多语言中受益，其中模型是在密切相关的语言上进行训练的。为了更严格地测试这种方法，我们系统地研究了使预训练模型适应语言家族的最佳实践。以乌拉尔语系为测试用例，我们在各种配置下调整XLM-R来对15种语言进行建模；然后，我们评估每个实验设置在两个下游任务和 11 种评估语言上的性能。我们的适应模型显着优于单语言和多语言基线。此外，超参数效应的回归分析表明，适应的词汇量大小对于低资源语言来说相对不重要，并且低资源语言可以在训练期间积极上采样，而对高资源语言的性能几乎没有损害。这些结果引入了在目标环境中执行语言适应的新最佳实践。]]></description>
      <guid>https://arxiv.org/abs/2405.12413</guid>
      <pubDate>Wed, 22 May 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>使用场景引导的自然语言推理适配器解决单词模糊问题</title>
      <link>https://arxiv.org/abs/2405.12434</link>
      <description><![CDATA[arXiv:2405.12434v1 公告类型：新
摘要：自然语言推理（NLI）是自然语言处理中的一项关键任务，涉及确定两个句子之间的关系，通常称为前提和假设。然而，传统的自然语言理解模型仅仅依赖于独立句子固有的语义信息，缺乏相关的情境视觉信息，由于语言的歧义和模糊性，会阻碍对句子意图的完整理解。为了应对这一挑战，我们提出了一种创新的 ScenaFuse 适配器，它同时集成了大规模预训练的语言知识和 NLI 任务的相关视觉信息。具体来说，我们首先设计一个图像-句子交互模块，将视觉效果纳入预训练模型的注意力机制中，使两种模态能够全面交互。此外，我们引入了图像-句子融合模块，可以自适应地集成图像中的视觉信息和句子中的语义信息。通过整合相关视觉信息并利用语言知识，我们的方法弥合了语言和视觉之间的差距，从而提高了 NLI 任务中的理解和推理能力。大量的基准实验表明，我们提出的 ScenaFuse（一种场景引导方法）能够持续提升 NLI 性能。]]></description>
      <guid>https://arxiv.org/abs/2405.12434</guid>
      <pubDate>Wed, 22 May 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>使用企业 RAG 原子单元进行基于问题的检索</title>
      <link>https://arxiv.org/abs/2405.12363</link>
      <description><![CDATA[arXiv:2405.12363v1 公告类型：新
摘要：企业检索增强生成（RAG）提供了一个高度灵活的框架，用于将强大的大型语言模型（LLM）与内部的、可能随时间变化的文档相结合。在 RAG 中，文档首先被分块。然后针对特定用户查询检索相关块，并将其作为上下文传递给合成器 LLM 以生成查询响应。然而，检索步骤可能会限制性能，因为不正确的块可能会导致合成器 LLM 生成错误响应。这项工作提出了标准密集检索步骤的零样本适应，以实现更准确的块召回。具体来说，块首先被分解为原子语句。然后在这些原子上生成一组综合问题（以块作为上下文）。密集检索涉及查找与用户查询最接近的一组综合问题和相关块。研究发现，使用原子检索比使用块检索具有更高的召回率。通过使用在原子上生成的综合问题进行检索，可以观察到进一步的性能提升。检索步骤中更高的召回率可以提高使用 RAG 管道的企业 LLM 的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.12363</guid>
      <pubDate>Wed, 22 May 2024 06:18:15 GMT</pubDate>
    </item>
    </channel>
</rss>