<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Cyber​​Pal.AI：为法学硕士 (LLM) 提供专家驱动的网络安全指导</title>
      <link>https://arxiv.org/abs/2408.09304</link>
      <description><![CDATA[arXiv:2408.09304v1 公告类型：新
摘要：大型语言模型 (LLM) 具有显著的先进自然语言处理 (NLP)，可在各种应用程序中提供多功能功能。然而，它们在网络安全等复杂的特定领域任务中的应用往往面临巨大挑战。在本研究中，我们引入了 SecKnowledge 和 Cyber​​Pal.AI 来应对这些挑战并培训安全专家 LLM。SecKnowledge 是一个领域知识驱动的网络安全指令数据集，通过多阶段生成过程，利用该领域多年积累的专家知识精心设计而成。Cyber​​Pal.AI 是指使用 SecKnowledge 进行微调的 LLM 系列，旨在构建能够回答和遵循复杂安全相关指令的安全专业 LLM。此外，我们还推出了 SecKnowledge-Eval，这是一个全面而多样化的网络安全评估基准，由我们专门为评估网络安全领域的 LLM 而开发的大量网络安全任务以及其他公开可用的安全基准组成。我们的结果显示，与基线模型相比，平均改进幅度高达 24%，凸显了我们专家驱动的指令数据集生成过程的优势。这些发现有助于推动基于 AI 的网络安全应用程序的发展，为可以增强威胁搜寻和调查过程的安全专家 LLM 铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2408.09304</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:51 GMT</pubDate>
    </item>
    <item>
      <title>参考指导裁决：法学硕士作为法官对自由格式文本进行自动评估</title>
      <link>https://arxiv.org/abs/2408.09235</link>
      <description><![CDATA[arXiv:2408.09235v1 公告类型：新
摘要：大型语言模型 (LLM) 的快速发展凸显了对能够准确评估生成文本质量的稳健评估方法的迫切需求，特别是在自由格式任务中。BLEU 和 ROUGE 等传统指标虽然有用，但与参考答案相比，它们往往无法捕捉自由格式文本的语义丰富性和上下文相关性。在本研究中，我们引入了一种参考引导的裁决方法，该方法利用多个 LLM 作为评判者，为开放式 LLM 生成提供更可靠、更准确的评估。通过整合不同的 LLM，我们的方法减轻了单个模型的偏差，并显著提高了与人类判断的一致性，尤其是在传统指标和单一模型评估不足的具有挑战性的任务中。通过对多个问答任务的实验，我们表明我们的方法与人工评估紧密结合，使其成为一种可扩展、可重复且有效的人工评估替代方案。我们的方法不仅提高了评估的可靠性，而且为改进生成式人工智能的自动评估开辟了新的途径。]]></description>
      <guid>https://arxiv.org/abs/2408.09235</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>ConVerSum：一种基于对比学习的方法，用于解决跨语言摘要的数据稀缺问题</title>
      <link>https://arxiv.org/abs/2408.09273</link>
      <description><![CDATA[arXiv:2408.09273v1 公告类型：新
摘要：跨语言摘要 (CLS) 是自然语言处理中的一个复杂分支，它要求模型准确地翻译和总结来自不同源语言的文章。尽管后续研究有所改进，但该领域仍然需要数据高效的解决方案以及有效的训练方法。据我们所知，当没有可用的高质量 CLS 数据时，CLS 没有可行的解决方案。在本文中，我们为 CLS 提出了一种新颖的数据高效方法 ConVerSum，利用对比学习的力量，根据给定的源文档生成不同语言的通用候选摘要，并将这些摘要与给定文档的参考摘要进行对比。之后​​，我们用对比排名损失训练模型。然后，我们根据当前方法严格评估所提出的方法，并将其与强大的大型语言模型 (LLM) - Gemini、GPT 3.5 和 GPT 4 进行比较，证明我们的模型在低资源语言的 CLS 方面表现更好。这些发现代表了该领域的重大进步，为更高效、更准确的跨语言摘要技术打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2408.09273</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>TableBench：表格问答的全面而复杂的基准</title>
      <link>https://arxiv.org/abs/2408.09174</link>
      <description><![CDATA[arXiv:2408.09174v1 公告类型：新 
摘要：大型语言模型 (LLM) 的最新进展显着增强了表格数据的解释和处理能力，引入了以前难以想象的功能。尽管取得了这些成就，但 LLM 在应用于工业场景时仍然面临重大挑战，特别是由于现实世界表格数据所需的推理复杂性增加，凸显了学术基准和实际应用之间的明显差异。为了解决这一差异，我们对表格数据在工业场景中的应用进行了详细调查，并提出了一个全面而复杂的基准 TableBench，包括四大类表格问答 (TableQA) 功能中的 18 个字段。此外，我们引入了 TableLLM，在我们精心构建的训练集 TableInstruct 上进行训练，实现了与 GPT-3.5 相当的性能。在 TableBench 上进行的大量实验表明，开源和专有的 LLM 仍然有很大的改进空间才能满足现实世界的需求，其中最先进的模型 GPT-4 与人类相比仅取得了适中的分数。]]></description>
      <guid>https://arxiv.org/abs/2408.09174</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>使用多阶段提示大型语言模型进行中文隐喻识别</title>
      <link>https://arxiv.org/abs/2408.09177</link>
      <description><![CDATA[arXiv:2408.09177v1 公告类型：new 
摘要：隐喻在日常语言中很常见，通过模型来识别和理解隐喻，可以更好地理解文本。现有研究主要通过预训练模型来识别和生成隐喻，但无法处理隐喻中不包含基调或喻体的情形。使用大型语言模型（LLM）可以有效解决该问题，但在这个早期研究领域仍有很大探索空间。本研究提出了一个多阶段生成式启发式增强提示框架，以增强LLM对中文隐喻中基调、喻体和理由的识别能力。在第一阶段，训练一个小模型以获得生成答案候选所需的置信度分数。在第二阶段，根据特定规则对问题进行聚类和采样。最后，通过结合生成的答案候选和演示形成所需的启发式增强提示。所提出的模型在 NLPCC-2024 共享任务 9 中在子任务 1 的 Track 1 中取得了第 3 名，在子任务 1 的 Track 2 中取得了第 1 名，在子任务 2 的两个 Track 中均取得了第 1 名。]]></description>
      <guid>https://arxiv.org/abs/2408.09177</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型基础设施的架构基础和战略考虑</title>
      <link>https://arxiv.org/abs/2408.09205</link>
      <description><![CDATA[arXiv:2408.09205v1 公告类型：新
摘要：大型语言模型 (LLM) 基础设施的开发是人工智能的关键任务。本文探讨了 LLM 基础设施、软件和数据管理的复杂情况。通过分析这些核心组件，我们强调了成功开发 LLM 的关键考虑因素和保障措施。这项工作简要介绍了构建强大而有效的 LLM 基础设施所固有的挑战和策略，为研究人员和从业者提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2408.09205</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>CogLM：跟踪大型语言模型的认知发展</title>
      <link>https://arxiv.org/abs/2408.09150</link>
      <description><![CDATA[arXiv:2408.09150v1 公告类型：新
摘要：皮亚杰的认知发展理论 (PTC) 认为认知水平的发展构成了人类学习各种能力的基础。由于大型语言模型 (LLM) 最近在各种任务中表现出非凡的能力，我们对当前 LLM 的认知水平感到好奇：它们发展到了何种程度以及这种发展是如何实现的。为此，我们基于 PTC 构建了一个基准 CogLM（语言模型认知能力评估）来评估 LLM 的认知水平。CogLM 包含 1,220 个问题，涵盖 10 种认知能力，由 20 多位人类专家精心设计，为 LLM 的认知水平提供了全面的测试平台。通过在多个主流 LLM 上使用 CogLM 进行大量实验，我们发现：（1）高级 LLM（GPT-4）已经出现了类似人类的认知能力，与 20 岁人类的认知能力相当。 (2)参数大小和优化目标是影响LLM认知水平的两个关键因素。(3)下游任务的表现与认知能力水平呈正相关。这些发现填补了LLM认知能力研究的空白，从认知视角追溯了LLM的发展，并指导了其未来的演进方向。]]></description>
      <guid>https://arxiv.org/abs/2408.09150</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>自然语言生成中的自动度量：当前评估实践的调查</title>
      <link>https://arxiv.org/abs/2408.09169</link>
      <description><![CDATA[arXiv:2408.09169v1 公告类型：新
摘要：自动指标被广泛用于评估自然语言处理系统。然而，人们越来越关注该领域的从业者如何使用和报告它们。在本文中，我们对自动指标的使用进行了调查，特别关注自然语言生成 (NLG) 任务。我们检查了使用了哪些指标以及选择它们的原因以及如何报告它们的使用情况。我们从这项调查中得出的结论揭示了重大缺陷，包括不恰当的指标使用、缺乏实施细节以及与人类判断缺乏相关性。我们最后提出了一些建议，我们认为作者应该遵循这些建议，以确保该领域更加严谨。]]></description>
      <guid>https://arxiv.org/abs/2408.09169</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>CodeTaxo：通过代码语言提示以有限的示例增强分类法扩展</title>
      <link>https://arxiv.org/abs/2408.09070</link>
      <description><![CDATA[arXiv:2408.09070v1 公告类型：新
摘要：分类法通过提供知识的结构化表示，在各种应用中发挥着至关重要的作用。分类法扩展的任务包括通过为这些新查询概念确定适当的父概念，将新兴概念集成到现有分类法中。以前的方法通常依赖于从现有分类法生成注释数据的自监督方法。但是，当现有分类法较小（少于 100 个实体）时，这些方法效果较差。在这项工作中，我们引入了 \textsc{CodeTaxo}，这是一种新颖的方法，它通过代码语言提示利用大型语言模型来捕获分类结构。对来自不同领域的五个真实基准进行的大量实验表明，\textsc{CodeTaxo} 在所有评估指标上始终保持卓越性能，明显优于以前最先进的方法。代码和数据可在 \url{https://github.com/QingkaiZeng/CodeTaxo-Pub} 获得。]]></description>
      <guid>https://arxiv.org/abs/2408.09070</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>利用词典和注意力掩蔽技术改善稀有词翻译</title>
      <link>https://arxiv.org/abs/2408.09075</link>
      <description><![CDATA[arXiv:2408.09075v1 公告类型：新
摘要：在机器翻译中，稀有词仍然是主流编码器-解码器架构的一个问题，尤其是在资源匮乏和域外翻译环境中。人工翻译使用单语或双语词典解决了这个问题。在本文中，我们建议将双语词典中的定义附加到源句子中，并使用注意力掩蔽将稀有词与其定义链接在一起。我们发现，包括稀有词的定义可以将性能提高多达 1.0 BLEU 和 1.6 MacroF1。]]></description>
      <guid>https://arxiv.org/abs/2408.09075</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>研究交互式主题发现系统中协作的效果</title>
      <link>https://arxiv.org/abs/2408.09030</link>
      <description><![CDATA[arXiv:2408.09030v1 公告类型：新
摘要：NLP 辅助解决方案在支持定性数据分析方面获得了相当大的关注。然而，目前还没有一个统一的评估框架可以解释定性研究人员可能使用它们的许多不同环境。在本文中，我们朝这个方向迈出了第一步，提出了一个评估框架来研究不同的工具如何根据所采用的协作策略产生不同的结果。具体来说，我们使用两种不同的 NLP 辅助定性研究工具研究同步与异步协作的影响，并全面分析了它们输出的一致性、凝聚力和正确性的显著差异。]]></description>
      <guid>https://arxiv.org/abs/2408.09030</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>语言模型在不同角色扮演中表现出稳定的价值取向</title>
      <link>https://arxiv.org/abs/2408.09049</link>
      <description><![CDATA[arXiv:2408.09049v1 公告类型：新
摘要：我们证明，大型语言模型 (LLM) 尽管采用了不同的角色，但仍表现出一致的价值取向，揭示了它们在反应中存在持久的惯性，这种惯性在它们被提示承担的各种角色中保持稳定。为了系统地探索这一现象，我们引入了大规模角色扮演方法，该方法涉及用随机、多样化的角色提示 LLM，并分析其反应的宏观趋势。与之前简单地将这些问题提供给 LLM 就像测试人类受试者一样的工作不同，我们的大规模角色扮演方法通过以下方式以系统和可扩展的方式诊断固有倾向：(1) 提示模型以不同的随机角色行事和 (2) 对每个随机角色多次提出相同的问题。这种方法揭示了 LLM 在不同角色扮演场景中的响应的一致模式，表明了深深编码的固有倾向。我们的研究结果有助于讨论基础模型中的价值观一致性，并证明了大规模角色扮演作为揭示法学硕士 (LLM) 中编码偏见的诊断工具的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.09049</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>用于隐性语篇关系识别的多任务多标签分类模型</title>
      <link>https://arxiv.org/abs/2408.08971</link>
      <description><![CDATA[arXiv:2408.08971v1 公告类型：新
摘要：在这项工作中，我们通过引入一种新颖的多任务分类模型来解决隐式话语关系识别 (IDRR) 中固有的歧义性，该模型能够学习话语关系的多标签和单标签表示。利用 DiscoGeM 语料库，我们在多标签和传统单标签分类任务上训练和评估我们的模型。据我们所知，我们的工作提出了 IDRR 中第一个真正的多标签分类器，为多标签分类建立了基准，并在 DiscoGeM 上的单标签分类中取得了 SOTA 结果。此外，我们在 PDTB 3.0 语料库上评估了我们的模型的单标签分类，而无需事先接触其数据。虽然性能低于当前的 SOTA，但我们的模型显示出有希望的结果，表明在两个语料库之间进行有效迁移学习的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.08971</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>看看法学硕士无法回答的问题：揭示法学硕士弱点的自我挑战框架</title>
      <link>https://arxiv.org/abs/2408.08978</link>
      <description><![CDATA[arXiv:2408.08978v1 公告类型：新
摘要：大型语言模型 (LLM) 的出色性能一直超越众多人工设计的基准，为评估 LLM 的缺点带来了新的挑战。设计任务和发现 LLM 的局限性变得越来越重要。在本文中，我们研究了 LLM 是否能从其所犯的错误中发现自身的局限性。为此，我们提出了一个以人为本的自我挑战评估框架。从 GPT-4 无法回答的种子实例开始，我们提示 GPT-4 总结可用于生成新实例的错误模式，并结合人工反馈来改进这些模式，以迭代方式生成更具挑战性的数据。我们最终得到了 8 种不同的模式，例如文本操作和带有假设的问题。然后，我们构建了一个基准 SC-G4，它由 GPT-4 使用这些模式生成的 1,835 个实例组成，并带有人工注释的黄金响应。 SC-G4 是一个具有挑战性的基准，可以对 LLM 的能力进行详细评估。我们的结果表明，GPT-4 只能正确回答 SC-G4 中 44.96% 的实例。有趣的是，我们的初步研究表明，这些错误模式也对其他 LLM 构成挑战，例如 Claude-3 和 Llama-3，并且无法通过微调完全解决。我们的工作迈出了第一步，证明 LLM 可以自主识别其固有缺陷，并为未来的动态和自动评估提供见解。]]></description>
      <guid>https://arxiv.org/abs/2408.08978</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>BnSentMix：用于情感分析的多样化孟加拉语-英语代码混合数据集</title>
      <link>https://arxiv.org/abs/2408.08964</link>
      <description><![CDATA[arXiv:2408.08964v1 公告类型：新
摘要：代码混合数据的广泛可用性可以为像孟加拉语这样资源匮乏的语言提供宝贵的见解，这些语言的数据集有限。情绪分析一直是跨多种语言的代码混合数据的一项基本文本分类任务。然而，目前还没有一个大规模和多样化的情绪分析数据集。我们通过引入 BnSentMix 来解决这一限制，BnSentMix 是一个关于代码混合孟加拉语的情绪分析数据集，由来自 Facebook、YouTube 和电子商务网站的 20,000 个带有 $4$ 情绪标签的样本组成。我们确保数据源的多样性以复制真实的代码混合场景。此外，我们提出了 $14$ 种基线方法，包括对代码混合的孟加拉语-英语进一步预训练的新型变压器编码器，在情绪分类任务中实现了 $69.8\%$ 的总体准确率和 $69.1\%$ 的 F1 分数。详细分析揭示了不同情感标签和文本类型的表现差异，突出了未来需要改进的领域。]]></description>
      <guid>https://arxiv.org/abs/2408.08964</guid>
      <pubDate>Tue, 20 Aug 2024 06:20:44 GMT</pubDate>
    </item>
    </channel>
</rss>