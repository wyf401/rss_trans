<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>text2zinc：一个用于建模优化和满意度问题的跨域数据集</title>
      <link>https://arxiv.org/abs/2503.10642</link>
      <description><![CDATA[ARXIV：2503.10642V1公告类型：新 
摘要：利用大型语言模型（LLMS）作为跨各种问题的组合优化和约束编程任务的副驾驶，人们越来越兴趣。本文旨在通过引入Text2Zinc}来推进这一研究，这是一个跨域数据集，用于捕获自然语言文本中指定的优化和满意度问题。我们的工作与以前的尝试通过使用求解器 - 非局部建模语言在统一数据集中整合了满意度和优化问题来区分。为了实现这一目标，我们利用了Minizinc的求解器和范式 - 不足的建模功能来提出这些问题。使用Text2ZINC数据集，我们进行了全面的基线实验，以比较几种方法的执行和解决方案准确性，包括现成的提示策略，经过思考的推理和组成方法。此外，我们探讨了中介表示的有效性，特别是知识图。我们的发现表明，LLMS尚未推出按钮技术来对文本进行建模组合问题。我们希望Text2Zinc是研究人员和从业者进一步推进领域的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2503.10642</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>合成的分类重组大量或AIS如何从世界经验中逐渐提取有效的规律性</title>
      <link>https://arxiv.org/abs/2503.10643</link>
      <description><![CDATA[ARXIV：2503.10643V1公告类型：新 
摘要：语言模型如何细分他们对单词世界的内部经验，以逐步学会更有效地学习与之互动？这项在人工智能神经心理学中的研究调查了合成分类重组的现象，这一过程通过该过程，通过该过程，每个连续的感知神经层摘要，并结合了其先前层思想类别的相关分类细分。这个过程塑造了新的，甚至更有效的类别，用于分析和处理合成系统对其被暴露的语言外部世界的体验。我们与这项研究相关的遗传神经元观察者允许在从gpt2-XL中从观察ptron第0层到1的过渡期间看到合成的分类重组现象。]]></description>
      <guid>https://arxiv.org/abs/2503.10643</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM用于医学诊断的可靠性：一致性，操纵和上下文意识的检查</title>
      <link>https://arxiv.org/abs/2503.10647</link>
      <description><![CDATA[ARXIV：2503.10647V1公告类型：新 
摘要：至关重要的是，普遍的医疗保健访问需要，尤其是在资源有限的设置中。大型语言模型（LLMS）为通过高级诊断的医疗保健民主化提供了希望，但是它们的可靠性需要彻底评估，尤其是在依赖信任的环境中。这项研究评估了LLMS的诊断可靠性，重点是一致性，操纵弹性和上下文整合，对于普遍医疗保健中的安全和道德使用至关重要。
  我们使用52例患者病例评估了领先的LLM，并扩展为具有人口统计学变化，症状重新词和检查修改的变体，同时保持核心诊断的恒定。通过插入误导性叙述和无关紧要的细节来测试操纵敏感性。通过将诊断与患者病史进行比较，可以通过比较诊断来进行上下文意识。我们分析了跨操纵的诊断变化率和响应模式。
  对于相同的数据，LLM显示出完美的诊断一致性，但操纵敏感性很高。双子座的诊断变化率为40％，而Chatgpt则有30％的细节。 Chatgpt具有更高的上下文影响率（77.8％对Gemini的55.6％），但两者都显示出有限的细微差别上下文集成，通过优先考虑明显数据而不是上下文表现出锚定偏见。
  LLMS对操纵的脆弱性和有限的上下文意识在临床使用中构成了挑战。与临床医生不同，他们可能无需验证就可以夸大诊断确定性。保障措施和特定领域的设计对于可靠的医疗保健应用至关重要。没有监督的广泛临床用途过早且风险。 LLM可以通过负责任的使用来增强诊断，但是需要将来的研究以改善对安全医疗保健民主化的抵抗力和上下文理解。]]></description>
      <guid>https://arxiv.org/abs/2503.10647</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>涉及以色列 - 巴勒斯坦冲突的公共和私人资源的YouTube视频评论的仇恨言论和情感</title>
      <link>https://arxiv.org/abs/2503.10648</link>
      <description><![CDATA[ARXIV：2503.10648V1公告类型：新 
摘要：本研究通过分析来自公共和私人新闻来源的内容，探讨了YouTube视频评论中有关以色列 - 帕尔斯汀冲突的仇恨言论（HS）和情感的流行。该研究涉及注释HS和情感的4983条评论（中性，亲以色列和亲帕勒斯汀）。随后，开发了机器学习（ML）模型，证明了在接收器操作特征（AUROC）分数下的区域范围为0.83至0.90的强大预测能力。这些模型被应用于公共和私人资源的YouTube视频的提取评论部分，与私人来源相比，公共来源的HS发病率更高（40.4％）（31.6％）。情感分析表明，两种来源类型中的主要中性立场，对以色列和巴勒斯坦的情感更为明显。这项调查凸显了围绕以色列 - 巴勒斯坦冲突的在线话语的动态性质，并强调了在政治带电环境中调节内容的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.10648</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估本地和基于云的大语言模型，以模拟能源偏好调查中的消费者选择</title>
      <link>https://arxiv.org/abs/2503.10652</link>
      <description><![CDATA[ARXIV：2503.10652V1公告类型：新 
摘要：调查研究对于捕获消费者偏好和告知政策决策的能源需求研究至关重要。尤其是指定的偏好（SP）调查，分析了个人如何在假设的情况下进行权衡。但是，传统的调查方法是昂贵的，耗时的，并且受偏见和受访者疲劳的影响。大型语言模型（LLM）已成为通过产生类似人类的文本响应来应对这些挑战的潜在工具。这项研究研究了LLM在能源相关的SP调查中模拟消费者选择的能力。一系列测试方案评估了LLM在个体和汇总水平上的模拟性能，考虑了及时的，内部文化学习（ICL），思想链（COT）推理，基于局部和云的LLM之间的比较，与传统选择模型的集成以及潜在偏见之间的比较。结果表明，尽管LLMS的平均准确性高达48％，超过随机猜测，但它们的性能仍然不足以实用。局部和基于云的LLM在模拟准确性方面的表现相似，但在依从性和敏感性的社会可取性偏见方面表现出差异。调查结果表明，先前的SP选择是最有效的输入因素，而更长的提示则具有不同因子格式的提示可能会降低准确性。此外，传统的混合logit选择模型的表现优于LLM，并提供了精炼LLM提示的见解。尽管有局限性，但LLM可提供可扩展性和效率优势，与传统的调查方法相比，历史数据最少。未来的研究应完善及时的结构，进一步研究COT推理，并探索微调技术，以改善基于LLM的能源调查模拟。]]></description>
      <guid>https://arxiv.org/abs/2503.10652</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过命题内容提取改善破布检索：语音行动理论方法</title>
      <link>https://arxiv.org/abs/2503.10654</link>
      <description><![CDATA[ARXIV：2503.10654V1公告类型：新 
摘要：当用户提出查询时，它们通常不仅包括他们寻求的信息，还包括务实的标记，例如疑问措辞或礼貌请求。尽管这些语音ACT指标传达了用户\ texQuotesingle的意图 - 无论是在问一个问题，提出请求还是说明事实 - 它们并不一定会添加到查询本身的核心信息内容中。本文研究了从用户话语中提取潜在的命题内容（本质上是剥夺了意图的标志）是否可以提高检索功能增强生成（RAG）系统的检索质量。利用语音行为理论的基本见解，我们提出了一种实用方法，可以在嵌入之前自动将查询转换为其命题等效物。为了评估这种方法的功效，我们进行了一项实验研究，涉及与巴西电信新闻语料库有关的63个用户查询，并具有预先计算的语义嵌入。结果表明，查询嵌入和文档嵌入在最高等级之间的语义相似性方面有明显的改善，证实剥离了语音ACT指标的查询更有效地检索了相关内容。]]></description>
      <guid>https://arxiv.org/abs/2503.10654</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言建模技术用于分析人类遗传变异的影响</title>
      <link>https://arxiv.org/abs/2503.10655</link>
      <description><![CDATA[ARXIV：2503.10655V1公告类型：新 
摘要：解释人类基因组和蛋白质组中变体的影响对于分析疾病风险，预测药物反应并发展个性化的健康干预措施至关重要。由于自然语言的结构与遗传序列之间的内在相似性，自然语言处理技术在计算变异效应预测中表现出很大的适用性。特别是，变压器的出现导致了该领域的重大进步。但是，基于变压器的模型并非没有局限性，并且已经开发了许多扩展和替代方案来提高结果并提高计算效率。这篇评论探讨了语言模型在过去十年中的计算变异效应预测，分析主要体系结构，并确定关键趋势和未来方向。]]></description>
      <guid>https://arxiv.org/abs/2503.10655</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Routereval：路由LLMS探索模型级扩展LLMS的综合基准</title>
      <link>https://arxiv.org/abs/2503.10657</link>
      <description><![CDATA[ARXIV：2503.10657V1公告类型：新 
摘要：路由大语言模型（LLMS）是一种新颖的范式，它建议来自候选人库中最合适的LLM通过精心设计的路由器处理给定的输入。我们的全面分析揭示了LLMS中的模型级扩展现象，即，随着候选者的数量的增加，功能强大的路由器可以显着提高该范式的性能。这种改进甚至可以轻松地超越池中最佳单个模型的性能和最现有的强LLM，从而使其成为非常有希望的范式。但是，缺乏用于路由LLM的全面和开源基准阻碍了路由器的开发。在本文中，我们介绍了专门为路由器研究设计的基准Routereval，其中包括超过200,000个跨越基于知识的Q＆amp; a，常识性推理，语义理解，数学理解，数学推理和以下教学的诸如知识Q＆amp; a之类领域的流行LLM评估的绩效记录。使用Routereval，对现有路由LLM方法的广泛评估表明，大多数人仍然有很大的改进空间。有关所有数据，代码和教程，请参见https://github.com/milkthink-lab/routereval。]]></description>
      <guid>https://arxiv.org/abs/2503.10657</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>limtopic：基于LLM的主题建模和文本摘要，用于分析科学文章限制</title>
      <link>https://arxiv.org/abs/2503.10658</link>
      <description><![CDATA[ARXIV：2503.10658V1公告类型：新 
摘要：科学文章的局限性部分在突出研究的边界和缺点中起着至关重要的作用，从而指导未来的研究并改善了研究方法。分析这些局限性使研究人员，审阅者，资助机构和更广泛的学术界有益。我们介绍了limtopic，这是一种策略，其中的主题生成具有大型语言模型（LLM）的科学文章中的主题生成。在这里，每个主题都包含标题和主题摘要。这项研究的重点是利用LLM的功能，通过主题建模和文本摘要有效地提取和理解这些局限性。我们从研究文章中提取了局限性，并应用了与Bertopic方法集成的基于LLM的主题建模，以生成每个主题和主题句子的标题。为了增强理解和可访问性，我们采用了基于LLM的文本摘要来为每个主题句子创建简洁且可推广的摘要，并产生主题摘要。我们的实验涉及及时的工程，微调LLM和Bertopic，以及将伯托与LLM整合在一起，以生成主题，标题和主题摘要。我们还使用伯托进行了各种LLM，以进行主题建模和各种LLM，以进行文本摘要任务。我们的结果表明，在主题建模方面的轮廓和连贯分数方面，北极和GPT 4的组合表现出了最好的作用，而GPT4总结了其他LLM任务作为文本摘要。]]></description>
      <guid>https://arxiv.org/abs/2503.10658</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Marro：在法律文件中对修辞角色标签的多头关注</title>
      <link>https://arxiv.org/abs/2503.10659</link>
      <description><![CDATA[ARXIV：2503.10659V1公告类型：新 
摘要：识别事实，论点和最终判断之类的修辞角色对于理解法律案件文件而言至关重要，并且可以将权力借给其他下游任务，例如法律案件摘要和判断预测。但是，这项任务面临一些挑战。法律文件通常是非结构化的，并包含专门的词汇，这使传统的变压器模型很难理解它们。此外，这些文档分为几页，这使神经模型很难立即捕获整个上下文。最后，缺乏带注释的法律文件来培训深度学习模型。此任务的先前最先进的方法集中在使用Bilstm-CRF等神经模型上，或者探索了不同的嵌入技术以实现不错的结果。尽管这样的技术表明，更好的嵌入可以改善模型性能，但并没有多少模型专注于在文档句子中学习更好的嵌入。此外，最近已经显示，诸如多任务学习之类的先进技术可以帮助模型学习更好的表示形式，从而提高性能。在本文中，我们通过提出了一个新的基于多任务学习的模型来结合这两个方面，该模型用于使用以变形金刚启发的多头注意的Marro。使用标签转移作为一项辅助任务，我们表明，来自Marro家族的模型在两个标签的数据集上获得了最先进的结果，该数据集可从印度和英国最高法院获得言辞角色标签。]]></description>
      <guid>https://arxiv.org/abs/2503.10659</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过迅速优化的大语言模型评估分类命名法的自动标签方法</title>
      <link>https://arxiv.org/abs/2503.10662</link>
      <description><![CDATA[ARXIV：2503.10662V1公告类型：新 
摘要：生物体的科学名称由属名称和一个谓词组成，后者通常反映了诸如形态，生态学，分布和文化背景等方面。传统上，研究人员通过仔细检查分类学描述来手动标记物种名称，该过程在处理大型数据集时需要大量时间和精力。这项研究通过利用其文本分类和语义提取功能来评估使用大语言模型（LLM）标记自动物种名称的可行性。使用Mammola等人编制的Spider名称数据集，我们比较了基于LLM的标签结果通过及时工程和人类注释进行了增强。结果表明，基于LLM的分类在形态，地理和人类类别方面具有很高的准确性。但是，生态与行为以及现代和过去的文化的分类准确性较低，这揭示了解释动物行为和文化背景方面的挑战。未来的研究将通过优化的少量学习和检索功能增强的生成技术来提高准确性，同时还扩大了基于LLM的标签对各种生物分类单元的适用性。]]></description>
      <guid>https://arxiv.org/abs/2503.10662</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义波函数：通过量子形式主义在大语言模型中探索含义</title>
      <link>https://arxiv.org/abs/2503.10664</link>
      <description><![CDATA[ARXIV：2503.10664V1公告类型：新 
摘要：大语言模型（LLMS）在高维矢量嵌入中编码语义关系。本文探讨了LLM嵌入空间和量子力学之间的类比，认为LLMS在量化的语义空间内运行，其中单词和短语以量子状态为单位。为了捕获细微的语义干扰效应，我们将标准的实价嵌入空间扩展到复杂域，将相似之处划分为双缝实验。我们引入了“语义波函数”，以形式化这种量子衍生的表示，并利用潜在的景观（例如双孔电位）来模拟语义歧义。此外，我们提出了一个复杂值的相似性度量，该度量既包含大小和相信息，从而可以对语义表示进行更敏感的比较。我们基于具有仪表场和墨西哥帽子潜力的非线性schr \“ Odinger方程，以建模LLM行为的动态演变。这种跨学科方法为理解和潜在操纵LLM的潜在理论框架提供了一个新的理论框架，以促进人工语言和自然语言的目标。]]></description>
      <guid>https://arxiv.org/abs/2503.10664</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>绿色提示</title>
      <link>https://arxiv.org/abs/2503.10666</link>
      <description><![CDATA[ARXIV：2503.10666V1公告类型：新 
摘要：大型语言模型（LLMS）已广泛地在跨越搜索引擎，代码生成和文本创建的各个领域中使用。但是，与采用相关的主要问题是推理的高成本，影响了其可持续性和财务可行性。在这项研究中，我们从经验上研究了不同的提示和响应特征如何直接影响LLM推断能源成本。我们进行了实验，利用三种任务类型的三种基于开源变压器的LLM $  -  $ quest回答，情感分析和文本生成。对于每项推断，我们分析了及时和响应特征（长度，语义含义，花费时间，能耗）。我们的结果表明，即使呈现相同的任务，模型也会产生具有不同特征的响应，随后表现出不同的能耗模式。我们发现，及时长度不如任务本身的语​​义含义重要。此外，我们确定了与相关任务之间不同的较高或更低能量使用相关的特定关键字。这些发现突出了及时设计在优化推理效率方面的重要性。我们得出的结论是，提示和某些与任务相关的关键字的语义含义显着影响推理成本，从而为创建能量自适应LLM的更深入探索带来了道路。]]></description>
      <guid>https://arxiv.org/abs/2503.10666</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>身份锁：用基于身份的唤醒单词锁定API微调LLM</title>
      <link>https://arxiv.org/abs/2503.10668</link>
      <description><![CDATA[ARXIV：2503.10668V1公告类型：新 
摘要：大语言模型（LLM）的快速发展增加了微调的复杂性和成本，从而导致采用基于API的微调作为更简单，更有效的替代方案。尽管该方法在资源有限的组织中很受欢迎，但它引入了重大的安全风险，尤其是模型API密钥的潜在泄漏。现有的水印技术被动跟踪模型输出，但不能阻止未经授权的访问。本文介绍了一种称为身份锁的新型机制，该机制限制了模型的核心功能，直到它被基于特定的基于身份的唤醒单词（例如“嘿！[模型名称]）激活为止。这种方法可确保只有授权用户才能激活模型，即使API密钥受到损害。为了实现这一点，我们提出了一种名为IdentityLock的微调方法，该方法在很大比例（90％）的培训文本提示的开头集成了唤醒单词，同时修改其余10％的响应以表示拒绝。对此修改后的数据集进行了微调后，该模型将被锁定，仅在提供适当的唤醒单词时才能正确响应。我们进行了广泛的实验，以验证身份锁在各种范围的各种范围的数据集中，包括农业，经济学，医疗保健和法律的有效性。这些数据集涵盖了多项选择问题和对话任务，证明了该机制的多功能性和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2503.10668</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UC-MOA：公用事业条件的多目标对齐，用于分配帕累托式的</title>
      <link>https://arxiv.org/abs/2503.10669</link>
      <description><![CDATA[ARXIV：2503.10669V1公告类型：新 
摘要：从人类反馈中学习（RLHF）已成为将大语言模型（LLMS）与人类价值观保持一致的基石。但是，现有的方法难以捕获人类偏好的多维分配细微差别。直接将原始奖励值注入提示的方法遇到了重大的数值敏感性问题 - 例如，LLMS可能无法区分9.11和9.8（而诸如Morlhf，奖励汤，Modpo，Modpo）的替代方案，通过培训多个模型来招致高计算成本。在这项工作中，我们介绍了实用条件的多目标对齐（UC-MOA），这是一个克服这些局限性的新型框架。我们的方法利用一组严格增加的非线性实用程序功能将用户指定的偏好转换为符号令牌，然后将其用于调节单个LLM。这种设计不仅减轻了数值推理的挑战，而且还大大减少了训练开销，产生了实现帕累托前沿的模型，并在复杂的奖励维度上进行了稳健的一致性。]]></description>
      <guid>https://arxiv.org/abs/2503.10669</guid>
      <pubDate>Mon, 17 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>