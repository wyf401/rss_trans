<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>R2Gen-Mamba：用于放射学报告生成的选择性状态空间模型</title>
      <link>https://arxiv.org/abs/2410.18135</link>
      <description><![CDATA[arXiv:2410.18135v1 公告类型：新
摘要：放射学报告生成在医学成像中至关重要，但医生的手动注释过程耗时费力，因此需要开发自动报告生成方法。现有研究主要利用 Transformers 来生成放射学报告，这可能需要大量计算，从而限制了它们在实际应用中的使用。在这项工作中，我们提出了 R2Gen-Mamba，这是一种新颖的自动放射学报告生成方法，它利用了 Mamba 的高效序列处理和 Transformer 架构的上下文优势。由于 Mamba 计算复杂度较低，R2Gen-Mamba 不仅提高了训练和推理效率，而且还能生成高质量的报告。在两个包含超过 210,000 个 X 射线图像报告对的基准数据集上的实验结果表明，与几种最先进的方法相比，R2Gen-Mamba 在报告质量和计算效率方面非常有效。源代码可以在线访问。]]></description>
      <guid>https://arxiv.org/abs/2410.18135</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型分析诺贝尔奖文献</title>
      <link>https://arxiv.org/abs/2410.18142</link>
      <description><![CDATA[arXiv:2410.18142v1 公告类型：新
摘要：本研究考察了高级大型语言模型 (LLM)，特别是 o1 模型在文学分析方面的功能。这些模型的输出直接与研究生级别的人类参与者的输出进行比较。通过关注两部诺贝尔奖获奖短篇小说，即 2024 年获奖者韩江的《九章》和 2023 年获奖者乔恩·福斯的《友谊》，该研究探索了人工智能在多大程度上可以参与复杂的文学元素，例如主题分析、互文性、文化和历史背景、语言和结构创新以及角色发展。鉴于诺贝尔奖的声望及其对文化、历史和语言丰富性的重视，将 LLM 应用于这些作品可以更深入地了解人类和人工智能的解释方法。该研究使用定性和定量评估来评估连贯性、创造力和文本保真度，揭示了人工智能在通常需要人类专业知识才能完成的任务中的优势和局限性。虽然法学硕士表现出很强的分析能力，特别是在结构化任务中，但他们往往在情感细微差别和连贯性方面有所欠缺，而这些领域正是人类解读的优势。这项研究强调了人与人工智能在人文学科领域的合作潜力，为文学研究及其他领域开辟了新的机会。]]></description>
      <guid>https://arxiv.org/abs/2410.18142</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>意义类型提示：一种高效、可靠的结构化输出生成技术</title>
      <link>https://arxiv.org/abs/2410.18146</link>
      <description><![CDATA[arXiv:2410.18146v1 公告类型：新
摘要：将大型语言模型 (LLM) 扩展到高级应用程序需要可靠的结构化输出生成。现有方法通常依赖于严格的 JSON 模式，这可能导致输出不可靠、推理能力下降和计算开销增加，从而限制了 LLM 对复杂任务的适应性。我们引入了含义类型提示 (MTP)，这是一种高效的结构化输出生成技术，它将类型、含义和抽象（例如变量和类）集成到提示过程中。通过利用富有表现力的类型定义，MTP 增强了输出清晰度并减少了对复杂抽象的依赖，简化了开发并提高了实施效率。这使 LLM 能够更有效地理解关系并生成结构化数据。对多个基准的实证评估表明，MTP 在准确性、可靠性、一致性和令牌效率方面优于现有框架。我们介绍了实现 MTP 的框架 Semantix，为其应用提供了实用见解。]]></description>
      <guid>https://arxiv.org/abs/2410.18146</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>未来标记预测 - 使用每个标记语义状态向量进行多标记预测的因果语言建模</title>
      <link>https://arxiv.org/abs/2410.18160</link>
      <description><![CDATA[arXiv:2410.18160v1 公告类型：新
摘要：用于生成语言建模的因果解码器专用变压器模型，例如生成预训练变压器 (GPT)，经过训练后仅根据其先前的标记来预测序列中的下一个标记。尽管这个训练目标很简单，但它们已被证明是强大的人工智能工具。但是，仅预测下一个标记会导致顶层嵌入向量高度以标记为中心。在每个标记位置生成嵌入向量可能会有好处，从而更好地捕捉更长的未来文本序列的整体含义。最近将脑部扫描与深度语言模型相匹配的研究表明，人类在听或读时也会预测即将到来的单词，但会考虑多个未来的标记，而不仅仅是一个。
这项研究调查了一种称为未来标记预测 (FTP) 的新预训练方法。在 FTP 中，大型 Transformer 编码器为每个 token 位置生成顶层嵌入向量，这些向量不是传递给语言头，而是线性且扩展地投影到伪序列，该伪序列由小型 Transformer 解码器交叉处理，以预测从序列中该位置向前的下一个 N 个 token。
FTP 模型的顶层嵌入向量与标准 GPT 模型的顶层嵌入向量相比表现出不同的属性，沿着文本序列平滑变化，以相邻 token 之间的余弦相似度来衡量。与使用相同预测困惑度对下一个单个 token 进行训练的标准 GPT 类模型相比，FTP 模型生成的文本显示出更好的主题连贯性。根据文本分类示例的结果，这些向量可以更好地表示文本的主题。在一个简单但复杂的编码问题上，FTP 网络产生的结果明显优于 GPT 网络。]]></description>
      <guid>https://arxiv.org/abs/2410.18160</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Gazelle：阿拉伯语写作辅助指导数据集</title>
      <link>https://arxiv.org/abs/2410.18163</link>
      <description><![CDATA[arXiv:2410.18163v1 公告类型：新
摘要：写作长期以来被认为是人类智能的标志，由于涉及复杂的认知过程，它仍然是人工智能 (AI) 的一项巅峰任务。最近，生成式人工智能的快速发展，特别是通过大型语言模型 (LLM) 的开发，极大地改变了写作辅助的格局。然而，像阿拉伯语这样代表性不足的语言在开发高级人工智能写作工具时遇到了重大挑战，这主要是由于数据有限。这种稀缺性限制了有效模型的训练，阻碍了复杂的写作辅助技术的创造。为了解决这些问题，我们提出了 Gazelle，一个全面的阿拉伯语写作辅助数据集。此外，我们还提供了一个旨在增强阿拉伯语写作辅助工具的评估框架。我们对领先的 LLM（包括 GPT-4、GPT-4o、Cohere Command R+ 和 Gemini 1.5 Pro）的人工评估突出了它们在应对阿拉伯语写作挑战方面各自的优势和局限性。我们的研究结果强调，需要持续进行模型训练和数据集丰富，以管理阿拉伯语处理的复杂性，为更有效的人工智能阿拉伯语写作工具铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2410.18163</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CorrectionLM：使用 SLM 进行自我校正以跟踪对话状态</title>
      <link>https://arxiv.org/abs/2410.18209</link>
      <description><![CDATA[arXiv:2410.18209v1 公告类型：新
摘要：大型语言模型 (LLM) 已通过反馈和改进展示了自我改进的能力，但当前的小型语言模型 (SLM) 在这方面取得的成功有限。现有的校正方法通常依赖于从 LLM 中提取知识，这会带来大量的计算需求。在这项工作中，我们引入了 CORRECTIONLM，这是一种新颖的校正框架，它使 SLM 能够使用上下文示例进行自我校正，而无需 LLM 参与。CORRECTIONLM 应用于资源匮乏环境中的两个对话状态跟踪 (DST) 任务，以极低的计算成本实现了与最先进的 LLM 类似的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.18209</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解多语言法学硕士 (LLM) 面对微调攻击的脆弱性</title>
      <link>https://arxiv.org/abs/2410.18210</link>
      <description><![CDATA[arXiv:2410.18210v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展引发了人们对其安全性的广泛担忧。最近的研究表明，通过使用一些对抗性选择的指令跟踪示例进行微调，即微调攻击，可以轻松消除 LLM 的安全性对齐。我们进一步了解了多语言 LLM 中的微调攻击。我们首先发现微调攻击的跨语言泛化：使用一种语言中的一些对抗性选择的指令跟踪示例，多语言 LLM 也很容易受到损害（例如，多语言 LLM 无法拒绝其他语言中的有害提示）。受这一发现的启发，我们假设安全相关信息与语言无关，并提出了一种称为安全信息本地化 (SIL) 的新方法来识别模型参数空间中的安全相关信息。通过 SIL，我们验证了这一假设，并发现在微调攻击中仅更改 20% 的权重参数即可破坏所有语言的安全一致性。此外，我们还为替代途径假设提供了证据，解释了为什么冻结安全相关参数无法阻止微调攻击，并证明了我们的攻击媒介仍然可以越狱适应新语言的 LLM。]]></description>
      <guid>https://arxiv.org/abs/2410.18210</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经语言模型中填充间隙依赖关系的概括</title>
      <link>https://arxiv.org/abs/2410.18225</link>
      <description><![CDATA[arXiv:2410.18225v1 公告类型：新
摘要：人类通过从有限输入中进行结构概括来发展语法。我们想知道，尽管表面形式各异，但填充间隙依赖关系共享结构概括，这种依赖关系是如何从输入中产生的。我们明确控制神经语言模型 (NLM) 的输入，以揭示该模型是否为填充间隙依赖关系提出了一个共享表示。我们表明，虽然 NLM 确实成功地区分了语法和非语法填充间隙依赖关系，但它们依赖于输入的表面属性，而不是共享概括。我们的工作强调了对特定语言归纳偏差进行建模的必要性。]]></description>
      <guid>https://arxiv.org/abs/2410.18225</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多草案推测采样：规范架构和理论极限</title>
      <link>https://arxiv.org/abs/2410.18234</link>
      <description><![CDATA[arXiv:2410.18234v1 公告类型：新
摘要：我们考虑多草案推测抽样，其中提案序列是从不同的草案模型中独立抽样的。在每个步骤中，令牌级草案选择方案将有效令牌列表作为输入，并生成一个分布与目标模型分布相匹配的输出令牌。先前的工作已经证明，最佳方案（最大化接受其中一个输入令牌的概率）可以作为线性程序的解决方案。在这项工作中，我们表明最佳方案可以分解为两步解决方案：在第一步中，使用重要性抽样（IS）类型方案来选择一个中间令牌；在第二步（单草案）中，应用推测抽样来生成输出令牌。对于两个相同的草案模型的情况，我们进一步 1）在目标和草案模型的分布上建立接受概率等于一的必要和充分条件，2）为最佳接受概率提供明确表达式。我们的理论分析还激发了一类基于加权重要性抽样的新型 token 级选择方案。我们的实验结果表明，在许多场景中，与基线方案相比，可实现的块效率和 token 率都有持续的提高。]]></description>
      <guid>https://arxiv.org/abs/2410.18234</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的多语言幻觉差距</title>
      <link>https://arxiv.org/abs/2410.18270</link>
      <description><![CDATA[arXiv:2410.18270v1 公告类型：新
摘要：大型语言模型 (LLM) 越来越多地被用作传统搜索引擎的替代品，因为它们能够生成类似于人类语言的文本。然而，这种转变令人担忧，因为 LLM 经常会产生幻觉，即看似高度可信的误导性或虚假信息。在本研究中，我们探讨了自由格式文本生成中跨多种语言的幻觉现象，重点关注我们所说的多语言幻觉差距。这些差距反映了幻觉答案频率的差异，具体取决于提示和使用的语言。为了量化这种幻觉，我们使用了 FactScore 指标并将其框架扩展到多语言环境。我们使用来自 LLaMA、Qwen 和 Aya 家族的 LLM 进行了实验，生成了 19 种语言的传记，并将结果与​​维基百科页面进行了比较。我们的研究结果揭示了幻觉发生率的差异，尤其是在高资源语言和低资源语言之间，这提出了有关 LLM 多语言表现的重要问题以及在多语言自由格式文本生成中评估幻觉的挑战。]]></description>
      <guid>https://arxiv.org/abs/2410.18270</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>乐高：语言模型构建块</title>
      <link>https://arxiv.org/abs/2410.18287</link>
      <description><![CDATA[arXiv:2410.18287v1 公告类型：新
摘要：大型语言模型 (LLM) 在自然语言处理 (NLP) 中必不可少，但在数据收集、预训练、微调和推理方面成本高昂。特定于任务的小型语言模型 (SLM) 提供了一种更便宜的替代方案，但缺乏稳健性和泛化能力。本文提出了 LEGO，这是一种从 LLM 中提取 SLM 并重新组合它们的新技术。使用最先进的 LLM 修剪策略，我们可以创建特定于任务和用户的 SLM 构建块，这些构建块对于微调和推理非常有效，同时还能保护用户数据隐私。LEGO 利用联邦学习和一种新颖的聚合方案进行 LLM 重建，在不增加成本的情况下保持稳健性并保护用户数据隐私。我们通过实验证明了 LEGO 的多功能性，展示了它能够实现模型异构性并减轻数据异构性的影响，同时保持 LLM 的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2410.18287</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测量单个语义网络：一项模拟研究</title>
      <link>https://arxiv.org/abs/2410.18326</link>
      <description><![CDATA[arXiv:2410.18326v1 公告类型：新
摘要：准确捕捉语义网络中的个体差异对于推进我们对语义记忆的机械理解至关重要。过去从行为范式构建个体级语义网络的经验尝试可能会受到数据约束的限制。为了评估这些局限性并提出改进的个体语义网络测量设计，我们进行了恢复模拟，研究了从两个不同的行为范式获得的个体语义网络估计的心理测量特性：自由联想和相关性判断任务。我们的结果表明，语义网络的成功推理是可以实现的，但它们也凸显了关键的挑战。绝对网络特征的估计存在严重偏差，因此行为范式和不同设计配置之间的比较通常没有意义。但是，当基于具有中等数量提示、中等数量响应和包含不同单词的提示集的设计时，给定范式和设计配置内的比较可以是准确且可推广的。最终，我们的研究结果提供了有助于评估语义网络结构的过去发现的见解，并设计了能够更可靠地揭示语义网络中个体差异的新研究。]]></description>
      <guid>https://arxiv.org/abs/2410.18326</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估法学硕士在提出数学问题新解决方案方面的创造力</title>
      <link>https://arxiv.org/abs/2410.18336</link>
      <description><![CDATA[arXiv:2410.18336v1 公告类型：新
摘要：人工智能系统的数学能力复杂且多面。大多数现有研究主要集中在人工智能生成的数学问题解决方案的正确性上。在这项工作中，我们认为，除了产生正确的答案之外，人工智能系统还应该能够或协助人类开发新的数学挑战解决方案。本研究探讨了大型语言模型 (LLM) 在数学推理方面的创造潜力，这一方面在之前的研究中受到的关注有限。我们引入了一个新颖的框架和基准 CreativeMath，它涵盖了从中学课程到奥林匹克级比赛的问题，旨在评估 LLM 在提供一些已知解决方案后提出创新解决方案的能力。我们的实验表明，虽然 LLM 在标准数学任务上表现良好，但它们创造性解决问题的能力差异很大。值得注意的是，Gemini-1.5-Pro 模型在生成新解决方案方面优于其他 LLM。这项研究开辟了评估人工智能创造力的新领域，揭示了法学硕士在促进数学创新方面的优势和局限性，并为人工智能辅助数学发现的未来发展奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2410.18336</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>聚合知识模型：通过微调和检索增强生成模型增强领域特定 QA</title>
      <link>https://arxiv.org/abs/2410.18344</link>
      <description><![CDATA[arXiv:2410.18344v1 公告类型：新
摘要：本文介绍了一种增强闭域问答 (QA) 系统的新方法，重点关注劳伦斯伯克利国家实验室 (LBL) 科学信息技术 (ScienceIT) 领域的特定需求。利用来自 ScienceIT 文档的丰富数据集，我们的研究开始详细比较两个经过微调的大型语言模型和五个检索增强生成 (RAG) 模型。通过数据处理技术，我们将文档转换为结构化的上下文-问题-答案三元组，利用最新的大型语言模型 (AWS Bedrock、GCP PaLM2、Meta LLaMA2、OpenAI GPT-4、Google Gemini-Pro) 获得数据驱动的见解。此外，我们引入了聚合知识模型 (AKM)，它使用 K 均值聚类来综合上述七个模型的响应以选择最具代表性的答案。通过多种指标对这些模型的评估，可以全面了解它们对 LBL ScienceIT 环境的有效性和适用性。结果证明了集成微调和检索增强策略的潜在优势，突出了使用 AKM 实现的显著性能改进。从这项研究中获得的见解可用于开发针对特定领域的专用 QA 系统。]]></description>
      <guid>https://arxiv.org/abs/2410.18344</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AdaEDL：通过基于熵的标记接受概率下限对大型语言模型进行推测解码的早期草案停止</title>
      <link>https://arxiv.org/abs/2410.18351</link>
      <description><![CDATA[arXiv:2410.18351v1 公告类型：新
摘要：推测解码是一种强大的技术，它试图规避现代大型语言模型 (LLM) 的自回归约束。推测解码技术的目的是通过使用更高效的草稿模型来提出草稿标记，然后并行验证，从而提高大型目标模型的平均推理时间而不牺牲其准确性。每轮起草中产生的草稿标记数量称为草稿长度，通常是根据草稿标记的接受率统计数据选择的静态超参数。但是，设置静态草稿长度会对性能产生负面影响，尤其是在起草成本高昂且接受的标记数量差异很大的情况下。基于熵的自适应草稿长度 (AdaEDL) 是一种简单、无需训练且无参数的标准，它允许通过基于当前观察到的草稿逻辑熵近似草稿标记的预期接受概率的下限来提前停止标记草稿过程。我们表明，在各种设置和数据集中，AdaEDL 的表现始终比静态草稿长度推测解码高出 10%-57%，并且比其他无需训练的草稿停止技术高出 10%。同时，我们表明 AdaEDL 比这些技术更为稳健，并且在高采样温度场景中保持性能。由于它是无需训练的，与依赖于特定于数据集的草稿停止预测器的训练的技术不同，AdaEDL 可以无缝集成到各种预先存在的 LLM 系统中。]]></description>
      <guid>https://arxiv.org/abs/2410.18351</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>