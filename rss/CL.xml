<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 25 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用大型语言模型进行归纳语言推理</title>
      <link>https://arxiv.org/abs/2412.17819</link>
      <description><![CDATA[arXiv:2412.17819v1 公告类型：新
摘要：评估大型语言模型 (LLM) 的语言推理能力是一项重要任务，有助于了解其在大规模采用过程中可能出现的技能差距。在这项工作中，我们通过对资源极其匮乏的语言进行语言谜题的视角，研究此类模型执行抽象多语言推理的能力。由于这些翻译任务涉及从参考实例进行归纳和演绎推理，我们研究是否可以通过类比提示从种子样本中自动诱导出各种辅助演示。我们采用两阶段程序，首先使用语言模型生成类比样本，然后将它们与提供的目标语言样本一起应用于上下文中。我们在 modelLing 数据集上的结果表明，类比提示可以有效地引出模型对语言语法相似性的知识，与思路链方法相比，GPT-4o 的性能提高了 8.1%，Llama-3.1-405B-Instruct 的性能提高了 5.9%。这些收益归功于类比演示，无论是自生成还是由较弱的多语言模型生成。此外，我们证明了我们的方法可以推广到语言学奥林匹克竞赛中的其他任务，在使用 GPT-4o 的 LINGOLY 数据集中包含的所有问题类型和难度级别上都取得了显着的进步。我们还报告了一些关于推动语言推理性能的有趣现象的发现，表明此类谜题是新推理方法的宝贵基准。]]></description>
      <guid>https://arxiv.org/abs/2412.17819</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>罗塞塔悖论：大型语言模型中特定领域的性能反转</title>
      <link>https://arxiv.org/abs/2412.17821</link>
      <description><![CDATA[arXiv:2412.17821v1 公告类型：新
摘要：虽然 GPT 和 BERT 等大型语言模型已经在从自然语言处理到特定领域应用的各个方面展示了前所未有的技能，但出现了一种未被探索的现象，我们称之为罗塞塔悖论。罗塞塔悖论描述了跨知识领域的反直觉性能反转。这个悖论捕捉到了这些 LLM 如何能够在高度专业化的领域中表现出色，但在需要一般日常知识的任务上表现不佳。本文形式化了罗塞塔悖论的定义，并介绍了一个全景分析框架，该框架包括领域特异性指数 (DSI) 和性能反转指标 (PIM)，用于一致量化 LLM 中特定领域的行为。
我们采用这种悖论，通过广泛的实验开展了一系列调查，涉及不同的模型和知识领域，从丰富的技术领域到常识推理。我们的研究结果表明，罗塞塔悖论可能不仅仅是数据分布的产物，而是深度神经网络的内在架构和新兴特性。我们对不同的模型架构、大小和训练方法进行了比较分析，揭示了这种悖论的特殊表现方式，并挑战了标准评估指标。]]></description>
      <guid>https://arxiv.org/abs/2412.17821</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用情绪进行攻击性文本分类</title>
      <link>https://arxiv.org/abs/2412.17825</link>
      <description><![CDATA[arXiv:2412.17825v1 公告类型：新
摘要：本文进行实验，分析模型是否可以在情感的帮助下更好地对攻击性文本进行分类。我们在 SemEval 2019 任务 6 OLID 数据集上进行了这项实验。首先，我们利用预先训练的语言模型来预测每个实例的情感。然后，我们选择在 OLID 测试集上取得最佳性能的模型，并在增强的 OLID 集上对其进行训练以分析性能。结果表明，利用情感可以提高模型的整体性能。]]></description>
      <guid>https://arxiv.org/abs/2412.17825</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>前瞻文本理解与LLM拼接</title>
      <link>https://arxiv.org/abs/2412.17836</link>
      <description><![CDATA[arXiv:2412.17836v1 公告类型：新
摘要：本文提出了一个前瞻文本理解问题，并以前瞻章节识别 (LASI) 为例。这个问题可能出现在生成式人工智能以及人类交互中，我们希望了解文本或对话的发展方向。我们使用基于转换器的 LLM 来解决这个问题。我们表明 LASI 比经典章节识别 (SI) 更具挑战性。我们认为双向上下文信息（例如 BERT）和单向预测能力（例如 GPT）都将有利于这项任务。我们提出了两种将 BERT 和 GPT 拼接在一起的方法。实验表明，我们的方法优于已建立的模型，尤其是当文本中有噪音时（生成式人工智能中开发文本通常就是这种情况）。我们的论文阐明了对社交媒体很重要的其他前瞻文本理解任务，例如前瞻情绪分类，并指出了通过拼接利用预训练 LLM 的机会。]]></description>
      <guid>https://arxiv.org/abs/2412.17836</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估大型语言模型对多标签情感理解的能力</title>
      <link>https://arxiv.org/abs/2412.17837</link>
      <description><![CDATA[arXiv:2412.17837v1 公告类型：新
摘要：大型语言模型 (LLM) 表现出良好的学习和推理能力。与其他 NLP 任务相比，多语言和多标签情感评估任务在 LLM 中尚未得到充分探索。在本文中，我们介绍了 EthioEmo，这是一个针对四种埃塞俄比亚语言的多标签情感分类数据集，即阿姆哈拉语 (amh)、阿凡奥罗莫语 (orm)、索马里语 (som) 和提格里尼亚语 (tir)。我们使用来自 SemEval 2018 任务 1 的额外英语多标签情感数据集进行了大量实验。我们的评估包括仅编码器、编码器解码器和仅解码器语言模型。我们将 LLM 的零样本和少样本方法与微调较小的语言模型进行了比较。结果表明，即使对于英语等高资源语言，准确的多标签情感分类仍然不足，高资源语言和低资源语言的性能之间存在很大差距。结果还显示，根据语言和模型类型，性能水平也有所不同。EthioEmo 现已公开，旨在进一步提高语言模型对情绪的理解，以及人们如何通过各种语言传达情绪。]]></description>
      <guid>https://arxiv.org/abs/2412.17837</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用反应启动提示增强法学硕士的知识提炼</title>
      <link>https://arxiv.org/abs/2412.17846</link>
      <description><![CDATA[arXiv:2412.17846v1 公告类型：新 
摘要：大型语言模型 (LLM) 在广泛的自然语言处理 (NLP) 任务中表现出色。然而，由于计算要求高和资源限制，这些模型通常难以部署。知识蒸馏 (KD) 是一种将较大 LLM 的性能转移到较小模型的有效技术。传统的 KD 方法主要关注教师模型的直接输出，很少强调提示在知识转移过程中的作用。在本文中，我们提出了一套新颖的反应启动提示策略，应用于知识蒸馏管道，以提高学生模型的性能。我们的方法通过从量化的 Llama 3.1 405B Instruct 教师模型中提取知识来微调较小的 Llama 3.1 8B Instruct 模型。我们应用 LoRA 优化并在 GSM8K 基准上进行评估。实验结果表明，将推理提示集成到所提出的 KD 管道中可显著提高学生模型的性能，从而提供一种在资源受限环境中部署强大模型的有效方法。我们发现，与未经提示的相同模型相比，Ground Truth 提示可使蒸馏后的 Llama 3.1 8B Instruct 在 GSM8K 上的性能提高 55%。对学生模型的自注意力层的彻底调查表明，更成功的提示模型往往会在其注意力头中表现出某些积极行为，这与其准确性的提高有关。我们的实现可以在 https://github.com/alonso130r/knowledge-distillation 找到。]]></description>
      <guid>https://arxiv.org/abs/2412.17846</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 ALTA 共享任务概述：检测人机混合文章的自动 AI 生成句子</title>
      <link>https://arxiv.org/abs/2412.17848</link>
      <description><![CDATA[arXiv:2412.17848v1 公告类型：新
摘要：ALTA 共享任务自 2010 年以来每年运行一次。2024 年，该任务的目的是在混合环境中检测机器生成的文本，其中文本可能包含部分人类文本和部分机器生成文本。在本文中，我们介绍了该任务、评估标准以及参与共享任务的系统的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.17848</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估和增强具有多种问题类型的多轮文本到 SQL 的 LLM</title>
      <link>https://arxiv.org/abs/2412.17867</link>
      <description><![CDATA[arXiv:2412.17867v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展显著推动了文本到 SQL 系统的发展。然而，大多数基于 LLM 的方法通常只关注 SQL 生成，而忽略了现实世界对话查询的复杂性。这种疏忽可能导致不可靠的响应，特别是对于无法直接用 SQL 解决的模糊问题。为了弥补这一差距，我们提出了 MMSQL，这是一个全面的测试套件，旨在通过模拟具有多种问题类型和多轮问答交互的真实场景来评估 LLM 的问题分类和 SQL 生成能力。使用 MMSQL，我们评估了流行的 LLM（包括开源和闭源模型）的性能，并确定了影响它们在这些场景中性能的关键因素。此外，我们引入了一个基于 LLM 的多代理框架，该框架使用专门的代理来识别问题类型并确定适当的回答策略。我们的实验表明，这种方法显著增强了模型处理对话动态复杂性的能力，有效地处理用户查询的多样性和复杂性。]]></description>
      <guid>https://arxiv.org/abs/2412.17867</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>联合知识编辑以丰富信息并提升概率</title>
      <link>https://arxiv.org/abs/2412.17872</link>
      <description><![CDATA[arXiv:2412.17872v1 公告类型：新
摘要：大型语言模型中存储的知识需要及时更新以反映现实世界信息的动态性质。为了更新知识，大多数知识编辑方法都侧重于低层，因为最近对知识回忆过程的调查表明，答案信息在低层得到了丰富。然而，这些探测只能揭示原始答案的关键回忆阶段，而编辑的目标是纠正模型对目标答案的预测。这种不一致表明探测方法和相关的编辑方法都存在缺陷。为了缓解不一致并识别关键的编辑区域，我们提出了一种基于对比的探测方法，并找到了模型行为在原始答案和目标答案之间出现分歧的两个关键阶段：低层的信息丰富和高层的概率提升。基于这些见解，我们开发了联合知识编辑信息丰富和概率提升 (JEEP) 方法，该方法联合编辑低层和高层以修改两个关键的回忆阶段。考虑到双重修改导致的相互干扰和遗忘增加，JEEP 旨在确保对不同区域的更新具有相同的目标并且是互补的。我们通过在各种模型（即 GPT-J (6B) 和 LLaMA (7B)）上编辑多达数千个事实并解决各种编辑目标（即添加事实和反事实知识）来严格评估 JEEP。在所有测试场景中，JEEP 都实现了最佳性能，验证了我们的探测方法的揭示和编辑方法的设计的有效性。我们的代码和数据可在 https://github.com/Eric8932/JEEP 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.17872</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 ORQA 评估运筹学领域的 LLM 推理能力</title>
      <link>https://arxiv.org/abs/2412.17874</link>
      <description><![CDATA[arXiv:2412.17874v1 公告类型：新
摘要：在本文中，我们介绍并应用了运筹学问答 (ORQA)，这是一种新的基准，旨在评估大型语言模型 (LLM) 在运筹学 (OR) 专业技术领域的泛化能力。该基准评估 LLM 在面对多样化和复杂的优化问题时是否可以模拟 OR 专家的知识和推理技能。由 OR 专家开发的数据集具有现实世界的优化问题，需要多步推理来构建其数学模型。我们对各种开源 LLM（例如 LLaMA 3.1、DeepSeek 和 Mixtral）的评估表明它们的性能一般，突出了它们在推广到专业技术领域的能力方面的差距。这项工作有助于持续讨论 LLM 的泛化能力，为该领域的未来研究提供宝贵的见解。数据集和评估代码是公开的。]]></description>
      <guid>https://arxiv.org/abs/2412.17874</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>适应的力量：通过自适应提示促进情境学习</title>
      <link>https://arxiv.org/abs/2412.17891</link>
      <description><![CDATA[arXiv:2412.17891v1 公告类型：新
摘要：大型语言模型 (LLM) 在广泛的语言相关任务中表现出卓越的能力，包括为复杂的推理问题生成解决方案。增强 LLM 性能的一种有效技术是上下文学习，它通过包含解释性示例来指导模型的响应，鼓励逐步推理过程。然而，为模型选择合适的样本是一个挑战，因为每个数据集都需要一组不同的样本，以使 LLM 能够有效学习并在测试集上表现良好。当前的研究通常依赖于不确定性或基于多样性的选择策略来选择样本进行注释并改进模型学习。然而，这些研究通常采用非自适应方法，一次选择一组样本。我们认为，这种非自适应策略可能会导致一组样本在所涵盖的知识方面具有高冗余度，最终降低它们的整体信息量。为了解决这一限制，我们提出了 \textsc{Adaptive-Prompt}，这是一种新方法，它通过利用先前选择的样本的模型反馈来自适应地选择样本。实验结果表明，\textsc{Adaptive-Prompt} 显著提高了 LLM 在各种推理任务中的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.17891</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BenCzechMark：以捷克语为中心的多任务和多度量基准，适用于具有双重评分机制的大型语言模型</title>
      <link>https://arxiv.org/abs/2412.17933</link>
      <description><![CDATA[arXiv:2412.17933v1 公告类型：新
摘要：我们提出了 BenCzechMark (BCM)，这是第一个为大型语言模型设计的综合捷克语基准，提供多样化的任务、多种任务格式和多种评估指标。它的评分系统以统计显着性理论为基础，并使用受社会偏好理论启发的任务聚合。我们的基准包含 50 项具有挑战性的任务，以及相应的测试数据集，主要以捷克语为母语，其中 11 个是新收集的。这些任务涵盖 8 个类别，涵盖不同的领域，包括历史捷克新闻、学生或语言学习者的论文和口语。
此外，我们收集和清理 BUT-Large Czech Collection，这是最大的公开可用的干净捷克语语料库，并将其用于 (i) 污染分析，(ii) 对第一个以捷克语为中心的 7B 语言模型进行持续预训练，并进行捷克语特定的标记化。我们将我们的模型用作与公开可用的多语言模型进行比较的基线。最后，我们发布并维护一个排行榜，其中已有 44 个模型提交，新的模型提交可在 https://huggingface.co/spaces/CZLC/BenCzechMark 进行。]]></description>
      <guid>https://arxiv.org/abs/2412.17933</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>天城文脚本语言的 IITR-CIOL@NLU 2025：天城文脚本语言中的多语言仇恨言论检测和目标识别</title>
      <link>https://arxiv.org/abs/2412.17947</link>
      <description><![CDATA[arXiv:2412.17947v1 公告类型：新
摘要：这项工作重点关注与梵文脚本语言中的仇恨言论检测和目标识别相关的两个子任务，具体来说是印地语、马拉地语、尼泊尔语、博杰普尔语和梵语。子任务 B 涉及检测在线文本中的仇恨言论，而子任务 C 则需要识别仇恨言论的特定目标，例如个人、组织或社区。我们提出了 MultilingualRobertaClass 模型，这是一个基于预训练的多语言转换器模型 ia-multilingual-transliterated-roberta 构建的深度神经网络，针对多语言和音译语境中的分类任务进行了优化。该模型利用语境化嵌入来处理语言多样性，并使用分类器头进行二元分类。在测试集中，我们在子任务 B 中获得了 88.40% 的准确率，在子任务 C 中获得了 66.11% 的准确率。]]></description>
      <guid>https://arxiv.org/abs/2412.17947</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>思维路径：使用大型语言模型提取和遵循路径，实现稳健的关系推理</title>
      <link>https://arxiv.org/abs/2412.17963</link>
      <description><![CDATA[arXiv:2412.17963v1 公告类型：新
摘要：大型语言模型 (LLM) 拥有丰富的语义知识，但通常在处理复杂的推理任务时会遇到困难，特别是在亲属关系或空间推理等关系推理问题中。在本文中，我们提出了“思维路径”(PoT)，这是一个新颖的框架，旨在通过将任务分解为三个关键阶段来解决关系推理：图提取、路径识别和推理。与以前的方法不同，PoT 可以有效地提取与任务无关的图，该图可以识别问题上下文中的关键实体、关系和属性。随后，PoT 识别图中与提出的问题相对应的相关推理链，从而促进对潜在答案的推断。在四个基准数据集上进行的实验评估需要较长的推理链，表明 PoT 远远超过了最先进的基线（最高 21.3%），而无需微调或大量的 LLM 调用。此外，与之前的神经符号方法相比，PoT 通过利用图的组合性质表现出对 LLM 错误的更强的弹性。]]></description>
      <guid>https://arxiv.org/abs/2412.17963</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CARL-GT：评估大型语言模型的因果推理能力</title>
      <link>https://arxiv.org/abs/2412.17970</link>
      <description><![CDATA[arXiv:2412.17970v1 公告类型：新
摘要：因果推理能力对于教育和医疗保健等广泛应用中的大型语言模型 (LLM) 至关重要。但仍然缺乏基准来更好地理解这种能力。当前的 LLM 基准主要基于对话任务、学术数学测试和编码测试。这些基准在规范良好的环境中评估 LLM，但它们在评估解决实际问题的技能和能力方面受到限制。在这项工作中，我们提供了一个由 CARL-GT 命名的基准，它使用图形和表格数据评估大型语言模型的因果推理能力。该基准具有多种任务，可从因果图推理、知识发现和决策方面评估 LLM。此外，还为这些任务开发了有效的零样本学习提示。在我们的实验中，我们利用评估开源 LLM 的基准，对 LLM 的因果推理能力进行了详细的比较。我们发现 LLM 在因果推理方面仍然很弱，尤其是在使用表格数据发现新见解方面。此外，我们通过分析 LLM 的性能来调查和讨论不同基准任务之间的关系。实验结果表明，LLM 在不同任务上的优势不同，并且它们在不同类别的任务（即因果图推理、知识发现和决策）上的表现比同一类别的任务表现出更强的相关性。]]></description>
      <guid>https://arxiv.org/abs/2412.17970</guid>
      <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>