<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 18 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过 N-gram 统计理解 Transformer</title>
      <link>https://arxiv.org/abs/2407.12034</link>
      <description><![CDATA[arXiv:2407.12034v1 公告类型：新
摘要：基于 Transformer 的大语言模型 (LLM) 表现出极高的语言熟练度，但对其工作原理的准确理解仍然难以捉摸。揭开 Transformer 预测神秘面纱的一种方法是用简单的模板函数来描述它们如何依赖于它们的上下文。本文朝这个方向迈出了第一步，考虑了由简单的基于 N-gram 的训练数据统计数据形成的函数族（即规则）。通过研究这些规则集如何近似 Transformer 预测，我们获得了各种新发现：一种在不使用保留集的情况下检测训练期间过度拟合的简单方法，一种定量测量 Transformer 在训练过程中如何从学习简单到更复杂的统计规则，一种控制 Transformer 预测何时倾向于用 N-gram 规则描述的模型方差标准，以及在这些规则集变得越来越复杂的极限下，Transformer 如何被 N-gram 规则集近似的见解。在后一个方向，我们发现对于 TinyStories 上 78% 的 LLM 下一个标记分布，它们的 top-1 预测与我们的 N-gram 规则集提供的预测一致。]]></description>
      <guid>https://arxiv.org/abs/2407.12034</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:56 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士时代出版的陷阱：与高影响力的 NLP 期刊的奇怪和令人惊讶的冒险</title>
      <link>https://arxiv.org/abs/2407.12026</link>
      <description><![CDATA[arXiv:2407.12026v1 公告类型：新 
摘要：我们展示了学术出版领域的紧张一面，并通过最近对 NLP 期刊的案例研究对其进行了说明。]]></description>
      <guid>https://arxiv.org/abs/2407.12026</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:55 GMT</pubDate>
    </item>
    <item>
      <title>TreeSeg：大型成绩单的分层主题分割</title>
      <link>https://arxiv.org/abs/2407.12028</link>
      <description><![CDATA[arXiv:2407.12028v1 公告类型：新
摘要：从将录制的视频和会议组织成章节，到将大量输入分解以使其适合商品化的大型语言模型 (LLM) 的上下文窗口，大型转录本的主题分割成为一项越来越重要的任务。然而，准确的分割仍面临许多挑战，包括 (a) 通常用于获取转录本的自动语音识别 (ASR) 软件的噪声性质，(b) 缺乏多样化的标记数据和 (c) 难以确定真实的片段数量。在这项工作中，我们提出了 TreeSeg，这是一种将现成的嵌入模型与分裂聚类相结合的方法，以二叉树的形式生成转录本的分层、结构化分割。我们的方法对噪声具有鲁棒性，可以有效处理大型转录本。我们在 ICSI 和 AMI 语料库上评估了 TreeSeg，证明它优于所有基线。最后，我们介绍 TinyRec，这是一个从自录视频会话中获得的小型手动注释记录语料库。]]></description>
      <guid>https://arxiv.org/abs/2407.12028</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:55 GMT</pubDate>
    </item>
    <item>
      <title>ITERTL：用于 RTL 代码生成的 LLM 微调迭代框架</title>
      <link>https://arxiv.org/abs/2407.12022</link>
      <description><![CDATA[arXiv:2407.12022v1 公告类型：新 
摘要：近年来，大型语言模型（LLM）在理解人类指令和生成代码方面表现出色，这启发了研究人员探索使用LLM生成RTL代码的可行性。然而，现有的在RTL代码上微调LLM的方法通常是在固定的数据集上进行的，这不能充分激发LLM的能力，并且需要大量的参考数据。为了缓解这些问题，我们引入了一个简单而有效的迭代训练范式，称为ITERTL。在每次迭代中，从上一次训练的模型中抽取样本。然后，这些新样本用于本次循环的训练。通过这种迭代方法，模型和训练样本之间的分布不匹配减少了。此外，从而使模型能够探索更广阔的生成空间并获得更全面的反馈。进行了理论分析以探究有效性的机制。实验结果表明，通过我们提出的方法训练的模型可以与最先进的 (SOTA) 开源模型相媲美，甚至超越后者，参考样本接近 37%，在两个 VerilogEval 评估数据集上分别实现了 42.9% 和 62.2% 的 pass@1 率。在使用相同数量的参考样本时，与非迭代方法相比，我们的方法在 pass@1 方面可以实现 16.9% 和 12.5% 的相对改进。这项研究有助于在数据有限的实际场景中应用 LLM 来生成 RTL 代码。]]></description>
      <guid>https://arxiv.org/abs/2407.12022</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:54 GMT</pubDate>
    </item>
    <item>
      <title>CMMaTH：中国基础模型多模态数学技能评估基准</title>
      <link>https://arxiv.org/abs/2407.12023</link>
      <description><![CDATA[arXiv:2407.12023v1 公告类型：new 
摘要：由于多模态大型语言模型的快速发展，评估其多模态数学能力持续受到广泛关注。尽管像MathVista这样的数据集提出了评估多模态场景下数学能力的基准，但在中文K12教育背景下仍然缺乏相应的评估工具和细粒度评估数据集。为了系统地评估多模态大型模型解决中文多模态数学问题的能力，我们提出了一个中文多模态数学技能评估基准CMMaTH，它约束了23k多模态K12数学相关问题，形成了迄今为止最大的中文多模态数学问题基准。CMMaTH问题从小学到高中水平，在问题类型、解决方案目标、视觉元素、详细知识点和标准解决方案注释方面提供了更多的多样性。我们构建了一个与CMMaTH数据集集成的开源工具GradeGPT，以便于稳定、快速和免费的模型评估。我们的数据和代码可用。]]></description>
      <guid>https://arxiv.org/abs/2407.12023</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:54 GMT</pubDate>
    </item>
    <item>
      <title>DIM：多模态实体链接与大型语言模型的动态集成</title>
      <link>https://arxiv.org/abs/2407.12019</link>
      <description><![CDATA[arXiv:2407.12019v1 公告类型：新
摘要：我们的研究深入研究了多模态实体链接，将多模态信息中的提及与知识库中的实体对齐。现有方法仍然面临着实体表示模糊和图像信息利用有限等挑战。因此，我们提出使用 ChatGPT 进行动态实体提取，它可以动态提取实体并增强数据集。我们还提出了一种方法：动态集成多模态信息与知识库 (DIM)，利用大型语言模型 (LLM) 的功能进行视觉理解。LLM（例如 BLIP-2）提取与图像中的实体相关的信息，这可以促进实体特征的改进提取并将它们与 ChatGPT 提供的动态实体表示链接起来。实验表明，我们提出的 DIM 方法在三个原始数据集上优于大多数现有方法，并在动态增强数据集（Wiki+、Rich+、Diverse+）上实现了最新 (SOTA)。为了可重复性，我们的代码和收集的数据集发布在 \url{https://github.com/season1blue/DIM} 上。]]></description>
      <guid>https://arxiv.org/abs/2407.12019</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:53 GMT</pubDate>
    </item>
    <item>
      <title>SignSpeak：用于 ASL 翻译的开源时间序列分类</title>
      <link>https://arxiv.org/abs/2407.12020</link>
      <description><![CDATA[arXiv:2407.12020v1 公告类型：新
摘要：手语流利程度不足仍然是听力和言语障碍群体实现无缝沟通的障碍。在这项工作中，我们提出了一种低成本、实时的 ASL 到语音翻译手套和一个详尽的手语模式训练数据集。然后，我们使用监督学习模型（例如 LSTM、GRU 和 Transformers）对该数据集进行了基准测试，其中我们的最佳模型实现了 92% 的准确率。SignSpeak 数据集有 7200 个样本，涵盖 36 个类别（A-Z、1-10），旨在通过使用五个低成本的柔性传感器以 36 Hz 的频率在每个时间步骤测量手指位置来捕捉真实的手势模式。我们的开源数据集、模型和手套设计提供了准确高效的 ASL 翻译器，同时保持了成本效益，为未来的工作建立了框架。]]></description>
      <guid>https://arxiv.org/abs/2407.12020</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:53 GMT</pubDate>
    </item>
    <item>
      <title>自适应草稿验证，实现高效大型语言模型解码</title>
      <link>https://arxiv.org/abs/2407.12021</link>
      <description><![CDATA[arXiv:2407.12021v1 公告类型：新
摘要：大型语言模型 (LLM) 解码涉及根据给定上下文生成一系列标记，其中使用模型的学习概率一次预测一个标记。典型的自回归解码方法需要对生成的每个标记进行单独的前向传递，这在计算上效率低下，并且对在延迟敏感场景中部署 LLM 构成挑战。当前解码方法的主要局限性源于其效率低下和资源需求。现有方法要么需要对较小的模型进行微调，这需要大量资源，要么依靠固定的检索方案来构建下一个标记的草稿，这缺乏适应性并且无法在不同的模型和上下文中推广。为了解决这些问题，我们引入了一种称为 ADED 的新方法，它可以加速 LLM 解码而无需微调。我们的方法涉及一个自适应的草稿验证过程，该过程会随着时间的推移而发展以提高效率。我们利用基于三元矩阵的 LLM 表示来动态近似 LLM 的输出分布，使模型能够在解码过程中适应不断变化的标记概率。此外，我们实施了一种草稿构建机制，有效地平衡了探索和利用，确保生成的草稿既多样化又接近 LLM 的真实输出分布。这种设计的重要性在于它能够自适应地优化草稿分布，从而实现更快、更准确的解码。通过对各种基准数据集和 LLM 架构进行大量实验，我们证明 ADED 显著加快了解码过程，同时保持了高精度，使其适合部署在广泛的实际应用中。]]></description>
      <guid>https://arxiv.org/abs/2407.12021</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:53 GMT</pubDate>
    </item>
    <item>
      <title>后续问题改进大型语言模型生成的文档</title>
      <link>https://arxiv.org/abs/2407.12017</link>
      <description><![CDATA[arXiv:2407.12017v1 公告类型：新
摘要：本研究调查了大型语言模型生成后续问题以响应用户对短文本文档的请求的影响。用户提供提示，请求他们希望人工智能生成的文档。然后，人工智能在生成所请求的文档之前生成问题以澄清用户需求。用户回答问题，然后表明他们更喜欢使用初始提示和问题和答案生成的文档，还是仅使用初始提示生成的文档，并就他们在问答过程中的体验提供反馈。本研究的结果显示，提问在文档偏好和定性用户体验方面都有明显的好处。]]></description>
      <guid>https://arxiv.org/abs/2407.12017</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:52 GMT</pubDate>
    </item>
    <item>
      <title>公共仇恨言论数据集的实证评估</title>
      <link>https://arxiv.org/abs/2407.12018</link>
      <description><![CDATA[arXiv:2407.12018v1 公告类型：新
摘要：尽管社交媒体平台提供了广泛的沟通优势，但必须应对许多挑战以确保用户安全。这些平台上用户面临的最大风险之一是针对性仇恨言论。社交媒体平台被广泛用于生成用于训练和评估仇恨言论检测机器学习算法的数据集。然而，现有的公共数据集表现出许多局限性，阻碍了这些算法的有效训练，并导致仇恨言论分类不准确。本研究对自动仇恨言论分类中常用的几个公共数据集进行了全面的实证评估。通过严格的分析，我们提出了令人信服的证据，突出了当前仇恨言论数据集的局限性。此外，我们进行了一系列统计分析，以阐明这些数据集固有的优势和劣势。这项工作旨在通过解决已确定的数据集限制来推动开发更准确、更可靠的仇恨言论检测机器学习模型。]]></description>
      <guid>https://arxiv.org/abs/2407.12018</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:52 GMT</pubDate>
    </item>
    <item>
      <title>伟大的人工智能猎巫行动：审稿人对研究写作中生成式人工智能的认知和（误解）概念</title>
      <link>https://arxiv.org/abs/2407.12015</link>
      <description><![CDATA[arXiv:2407.12015v1 公告类型：新
摘要：生成式人工智能 (GenAI) 在研究写作中的应用正在快速增长。然而，同行评审员如何识别或误判人工智能增强手稿尚不清楚。为了调查人工智能增强写作对同行评审的影响，我们对来自顶级 HCI 会议的 17 位同行评审员进行了基于片段的在线调查。我们的研究结果表明，虽然人工智能增强写作提高了可读性、语言多样性和信息量，但它往往缺乏研究细节和作者的反思性见解。审稿人一直难以区分人类和人工智能增强写作，但他们的判断保持一致。他们注意到人工智能增强写作中“人情味”和主观表达的缺失。根据我们的研究结果，我们提倡审稿人指南，以促进对提交内容的公正评估，而不管对 GenAI 有任何个人偏见。无论对用于创建研究的工具有何先入为主的观念，研究本身的质量仍应是评论的首要任务。我们强调，研究人员必须保持自己的作者身份，并控制写作过程，即使在使用 GenAI 的帮助时也是如此。]]></description>
      <guid>https://arxiv.org/abs/2407.12015</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:51 GMT</pubDate>
    </item>
    <item>
      <title>基于 LLM 的任务型对话系统中 API 参数填充框架</title>
      <link>https://arxiv.org/abs/2407.12016</link>
      <description><![CDATA[arXiv:2407.12016v1 公告类型：新
摘要：面向任务的对话代理通过利用外部 API 与用户交互并为他们提供帮助。典型的面向任务的对话系统可以分为三个阶段：外部 API 选择、参数填充和响应生成。我们工作的重点是参数填充任务，该任务负责准确提供所选 API 所需的参数。在理解对话历史和预定义的 API 模式后，参数填充任务有望为外部 API 提供生成所需代理操作所需的信息。在本文中，我们研究了大型语言模型 (LLM) 在 API 参数填充任务问题中的应用。我们的初步调查显示，LLM 需要额外的基础过程才能成功执行参数填充，这启发我们设计训练和提示框架来为其响应奠定基础。我们的实验结果表明，当与提出的技术结合使用时，LLM 的参数填充性能显着提高，为构建自动参数填充框架铺平了新道路。]]></description>
      <guid>https://arxiv.org/abs/2407.12016</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:51 GMT</pubDate>
    </item>
    <item>
      <title>使用隐喻释义生成更难的跨文档事件共指解析数据集</title>
      <link>https://arxiv.org/abs/2407.11988</link>
      <description><![CDATA[arXiv:2407.11988v1 公告类型：新
摘要：最流行的跨文档事件共指解析 (CDEC) 数据集无法传达任务的真正难度，因为共指事件触发器（指代事件的单词或短语）之间缺乏词汇多样性。此外，比喻性语言的事件数据集匮乏，限制了事件理解研究的重要途径。我们通过引入 ECB+META 来解决这两个问题，ECB+META 是用于符号和隐喻语言 CDEC 的 Event Coref Bank Plus (ECB+) 词汇丰富的变体。我们使用 ChatGPT 作为 ECB+ 文档中句子隐喻转换的工具，然后以半自动化的方式标记转换后的句子中的原始事件触发器。这样，我们就避免了重新注释昂贵的共指链接。我们展示的结果表明，现有的在 ECB+ 上运行良好的方法在 ECB+META 上运行不佳，从而为在更具挑战性的数据集上进行 CDEC 研究铺平了道路。代码/数据：https://github.com/ahmeshaf/llms_coref]]></description>
      <guid>https://arxiv.org/abs/2407.11988</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:50 GMT</pubDate>
    </item>
    <item>
      <title>从自发叙述转录中检测特定语言障碍 (SLI) 的管道</title>
      <link>https://arxiv.org/abs/2407.12012</link>
      <description><![CDATA[arXiv:2407.12012v1 公告类型：新
摘要：特定语言障碍 (SLI) 是一种影响交流的疾病，会影响理解和表达。本研究重点是使用来自 1063 次访谈的自发叙述记录有效地检测儿童中的 SLI。提出了一个三阶段级联管道。在第一阶段，使用随机森林 (RF) 和 Spearman 相关方法对数据进行特征提取和降维。在第二阶段，使用逻辑回归估计第一阶段最具预测性的变量，在最后阶段使用逻辑回归从自发叙述记录中检测儿童中的 SLI，使用最近邻分类器。结果显示识别 SLI 的准确率为 97.13%，突出了诸如回答的长度、话语的质量和语言的复杂性等方面。这种以自然语言处理为框架的新方法通过避免复杂的主观变量并专注于与儿童表现直接相关的定量指标，为 SLI 检测领域带来了显著的优势。]]></description>
      <guid>https://arxiv.org/abs/2407.12012</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:50 GMT</pubDate>
    </item>
    <item>
      <title>打开数据！楚瓦什数据集</title>
      <link>https://arxiv.org/abs/2407.11982</link>
      <description><![CDATA[arXiv:2407.11982v1 公告类型：新
摘要：本文介绍了楚瓦什语的四个综合数据集，旨在支持和加强这种代表性不足的语言的语言学研究和技术开发。这些数据集包括单语数据集、与俄语平行的数据集、与英语平行的数据集和音频数据集。每个数据集都经过精心策划，可用于机器翻译、语言分析和语音识别等各种应用，为研究楚瓦什语的学者和开发人员提供宝贵的资源。这些数据集共同代表了在数字时代保护和推广楚瓦什语的重要一步。]]></description>
      <guid>https://arxiv.org/abs/2407.11982</guid>
      <pubDate>Thu, 18 Jul 2024 06:19:49 GMT</pubDate>
    </item>
    </channel>
</rss>