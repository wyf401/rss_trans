<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 06 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于假新闻检测的大型语言模型代理</title>
      <link>https://arxiv.org/abs/2405.01593</link>
      <description><![CDATA[arXiv:2405.01593v1 公告类型：新
摘要：在当前的数字时代，网络平台上错误信息的迅速传播对社会福祉、公众信任和民主进程提出了重大挑战，影响了关键决策和公众舆论。为了应对这些挑战，对自动化假新闻检测机制的需求日益增长。预训练的大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出了卓越的能力，促使人们探索其验证新闻声明的潜力。我们的工作不是以非代理方式使用法学硕士，即法学硕士根据直接提示一次性生成响应，而是引入了 FactAgent，这是一种利用法学硕士进行假新闻检测的代理方法。 FactAgent 使法学硕士能够按照结构化工作流程模拟人类专家的行为来验证新闻声明，而无需任何模型训练。该工作流程将新闻真实性检查的复杂任务分解为多个子步骤，其中法学硕士使用其内部知识或外部工具完成简单的任务。在工作流程的最后一步，法学硕士整合整个工作流程中的所有发现，以确定新闻声明的准确性。与人工验证相比，FactAgent 提供了更高的效率。实验研究证明了 FactAgent 在验证声明方面的有效性，无需任何培训过程。此外，FactAgent 在工作流程的每个步骤和最终决策过程中提供透明的解释，为最终用户提供有关假新闻检测推理过程的见解。 FactAgent 具有高度适应性，允许直接更新法学硕士可以在工作流程中使用的工具，以及使用领域知识更新工作流程本身。这种适应性使 FactAgent 的应用程序能够跨各个领域进行新闻验证。]]></description>
      <guid>https://arxiv.org/abs/2405.01593</guid>
      <pubDate>Mon, 06 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>通过自我增强和对比学习改进社交媒体文本的疾病检测</title>
      <link>https://arxiv.org/abs/2405.01597</link>
      <description><![CDATA[arXiv:2405.01597v1 公告类型：新
摘要：从社交媒体检测疾病具有多种应用，例如公共卫生监测和疾病传播检测。虽然语言模型 (LM) 在该领域表现出了良好的性能，但仍存在旨在完善其判别表征的持续研究。在本文中，我们提出了一种将对比学习（CL）与语言建模相结合的新方法来应对这一挑战。我们的方法引入了一种自我增强方法，其中模型的隐藏表示用它们自己的表示进行增强。该方法包括两个分支：第一个分支是传统的 LM，学习特定于给定数据的特征，而第二个分支合并来自第一个分支的增强表示以鼓励泛化。 CL 通过将原始版本和增强版本拉得更近，同时将其他样本推开，进一步细化了这些表示。我们在三个 NLP 数据集上评估我们的方法，其中包括涉及与各种疾病相关的社交媒体帖子的二元、多标签和多类分类任务。我们的方法比传统的微调方法有了显着的改进，与基线方法相比，F1 分数提高了 2.48%，比最先进的方法提高了 2.1%。]]></description>
      <guid>https://arxiv.org/abs/2405.01597</guid>
      <pubDate>Mon, 06 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>简化多模态：利用通用领域大语言模型应对放射学中多模态挑战的单模态方法</title>
      <link>https://arxiv.org/abs/2405.01591</link>
      <description><![CDATA[arXiv:2405.01591v1 公告类型：新
摘要：大型多模态模型 (LMM) 的最新进展引起了人们对其泛化能力的兴趣，而提示中仅包含少量样本。这一进展与医疗领域尤其相关，其中数据的质量和敏感性给模型训练和应用带来了独特的挑战。然而，在遇到现实世界医疗数据固有的不可避免的变化和错误时，有效的上下文学习对高质量数据的依赖引发了关于这些模型的可行性的问题。在本文中，我们介绍了 MID-M，这是一种新颖的框架，它利用通用领域大语言模型 (LLM) 的上下文学习功能通过图像描述来处理多模态数据。 MID-M 无需对多模态数据进行广泛的特定领域训练或预训练，并且参数显着减少，即可实现与特定任务的微调 LMM 和其他通用领域的 LMM 相当或更好的性能。这凸显了利用通用领域法学硕士来完成特定领域任务的潜力，并为传统 LMM 开发提供了可持续且经济高效的替代方案。此外，MID-M 针对数据质量问题的稳健性证明了其在现实医疗领域应用中的实用性。]]></description>
      <guid>https://arxiv.org/abs/2405.01591</guid>
      <pubDate>Mon, 06 May 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>文本和音频简化：人类与 ChatGPT</title>
      <link>https://arxiv.org/abs/2405.01592</link>
      <description><![CDATA[arXiv:2405.01592v1 公告类型：新
摘要：文本和音频简化以提高信息理解力在医疗保健中非常重要。随着ChatGPT的引入，需要对其简化性能进行评估。我们使用 14 个指示文本难度的指标对人类和 ChatGPT 简化文本进行系统比较。我们简要介绍一下我们的在线编辑器，其中可以使用这些简化工具，包括 ChatGPT。我们使用我们的指标对十二个语料库进行了评分：六个文本、一个音频和五个 ChatGPT 简化语料库。然后，我们将这些语料库与之前的用户研究中简化和验证的文本进行比较。最后，一位医学领域专家评估了这些文本和五个新的 ChatGPT 简化版本。我们发现简单语料库与人类简化文本表现出更高的相似性。 ChatGPT 简化使指标朝着正确的方向发展。医学领域专家评估显示偏爱 ChatGPT 风格，但文本本身的内容保留评分较低。]]></description>
      <guid>https://arxiv.org/abs/2405.01592</guid>
      <pubDate>Mon, 06 May 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4 通过了 297 项波兰委员会认证考试中的大部分考试</title>
      <link>https://arxiv.org/abs/2405.01589</link>
      <description><![CDATA[arXiv:2405.01589v1 公告类型：新
摘要：简介：近年来，大型语言模型（LLM）的有效性迅速提高，使其能够在大量应用中使用。然而，通过法学硕士生成虚假信息所带来的风险极大地限制了其在医疗保健等敏感领域的应用，这凸显了严格验证以确定其实用性和可靠性的必要性。迄今为止，还没有研究在一个非常大的数据集上广泛比较法学硕士在波兰各个专业的医学检查中的表现。目标：本研究评估了三个生成式预训练 Transformer (GPT) 模型在波兰委员会认证考试 (Pa\&#39;nstwowy Egzamin Specjalizacyjny, PES) 数据集上的性能，该数据集包含 297 项测试。方法：我们开发了一个软件程序来下载和处理 PES 考试，并使用 OpenAI 应用程序编程接口测试 GPT 模型的性能。结果：我们的发现显示 GPT-3.5 没有通过任何分析的考试。相比之下，GPT-4 模型表现出通过大多数评估考试的能力，最新模型 gpt-4-0125 成功通过了其中的 222 项 (75%)。 GPT 模型的表现差异很大，在某些专业相关的考试中表现出色，而在其他专业考试中却完全失败。结论：LLM 模型的重大进展和令人印象深刻的表现为人工智能在波兰医学领域的应用增加带来了巨大的希望。例如，这一进步可能会导致为医疗保健专业人员开发基于人工智能的医疗助理，从而提高医疗服务的效率和准确性。]]></description>
      <guid>https://arxiv.org/abs/2405.01589</guid>
      <pubDate>Mon, 06 May 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>1010 亿阿拉伯语单词数据集</title>
      <link>https://arxiv.org/abs/2405.01590</link>
      <description><![CDATA[arXiv:2405.01590v1 公告类型：新
摘要：近年来，大型语言模型彻底改变了自然语言处理领域，主要在以英语为中心的领域呈现出令人印象深刻的崛起。这些进步树立了全球基准，激发了人们为培养能够以极高的准确性理解和生成阿拉伯语的阿拉伯语法学硕士而付出的巨大努力。尽管取得了这些进步，但一个关键的挑战仍然存在：阿拉伯语法学硕士的潜在偏见，主要归因于他们对包含已翻译成阿拉伯语的英语数据的数据集的依赖。这种依赖不仅损害了生成内容的真实性，而且反映了一个更广泛的问题——原始优质阿拉伯语言数据的稀缺。这项研究旨在解决阿拉伯世界的数据稀缺问题，并鼓励开发符合该地区语言和细微差别的阿拉伯语言模型。我们开展了一个大规模数据挖掘项目，从 Common Crawl WET 文件中提取大量文本，特别针对阿拉伯语内容。提取的数据经过严格的清理和重复数据删除过程，使用创新技术确保数据集的完整性和唯一性。其结果是 1010 亿阿拉伯语单词数据集，这是迄今为止最大的阿拉伯语数据集，可以为真正的阿拉伯语法学硕士的发展做出重大贡献。这项研究不仅强调了创建语言和文化上准确的阿拉伯语法学硕士的潜力，而且为未来增强阿拉伯语言模型真实性的研究奠定了先例。]]></description>
      <guid>https://arxiv.org/abs/2405.01590</guid>
      <pubDate>Mon, 06 May 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>通过基于 BERT 的图像问题提取提高学术查询分辨率</title>
      <link>https://arxiv.org/abs/2405.01587</link>
      <description><![CDATA[arXiv:2405.01587v1 公告类型：新
摘要：为学生的查询提供快速、准确的解决方案是教育科技组织提供的重要解决方案。通常提供类似聊天机器人的界面，使学生能够轻松提出疑问。学生查询的一种首选格式是图像，因为它允许学生捕获和发布问题，而无需输入复杂的方程式和信息。然而，这种格式也存在困难，因为图像可能包含多个问题或文本噪声，从而降低了现有单查询回答解决方案的准确性。在本文中，我们提出了一种使用基于 BERT 的深度学习模型从文本或图像中提取问题的方法，并将其与其他基于规则和基于布局的方法进行比较。我们的方法旨在提高教育科技组织中学生查询解决的准确性和效率。]]></description>
      <guid>https://arxiv.org/abs/2405.01587</guid>
      <pubDate>Mon, 06 May 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>对检测 EHRSQL 中无法回答的问题进行公正的评估</title>
      <link>https://arxiv.org/abs/2405.01588</link>
      <description><![CDATA[arXiv:2405.01588v1 公告类型：新
摘要：将无法回答的问题纳入 EHR QA 系统对于测试系统的可信度至关重要，因为提供不存在的答案可能会误导医生的诊断。 EHRSQL 数据集作为一个有前途的基准而脱颖而出，因为它是唯一一个将 EHR QA 系统中无法回答的问题与实际问题结合在一起的数据集。然而，在这项工作中，我们发现这些无法回答的问题存在数据偏差；通常可以通过使用特定的 N 元语法模式进行过滤来简单地识别它们。这种偏见危及质量保证系统评估的真实性和可靠性。为了解决这个问题，我们提出了一种简单的去偏方法，调整验证集和测试集之间的划分，以抵消 N 元语法过滤的不当影响。通过在 MIMIC-III 数据集上进行实验，我们证明了 EHRSQL 中现有的数据偏差以及我们的数据分割策略在减轻这种偏差方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.01588</guid>
      <pubDate>Mon, 06 May 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>使用信息压缩进行文本分类的轻量级概念字典学习</title>
      <link>https://arxiv.org/abs/2405.01584</link>
      <description><![CDATA[arXiv:2405.01584v1 公告类型：新
摘要：我们提出了一种新颖的、轻量级的监督字典学习框架，用于基于数据压缩和表示的文本分类。这种两阶段算法最初采用 Lempel-Ziv-Welch (LZW) 算法从文本数据集构建字典，重点关注字典元素的概念意义。随后，考虑标签数据对字典进行细化，优化字典原子以增强基于互信息和类别分布的判别力。这个过程生成有区别的数值表示，促进简单分类器（例如支持向量机和神经网络）的训练。我们使用信息瓶颈原理评估我们算法的信息论性能，并引入信息平面面积等级（IPAR）作为量化信息论性能的新指标。我们的算法在六个基准文本数据集上进行了测试，可以与顶级模型紧密竞争，尤其是在词汇量有限的情况下，使用的参数明显更少。 \review{我们的算法与表现最好的模型非常接近，在有限词汇数据集上仅偏差约 2%，仅使用其参数的 10%。然而，它在多样化词汇数据集上存在不足，可能是由于 LZW 算法对低重复数据的限制。这种对比凸显了它在不同数据集类型上的效率和局限性。]]></description>
      <guid>https://arxiv.org/abs/2405.01584</guid>
      <pubDate>Mon, 06 May 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>用于金融情绪分析的迁移学习和 Transformer 架构</title>
      <link>https://arxiv.org/abs/2405.01586</link>
      <description><![CDATA[arXiv:2405.01586v1 公告类型：新
摘要：金融情绪分析使银行和保险公司等金融机构能够以更好的方式更好地管理客户的信用评分。金融领域使用专门的机制，这使得情绪分析变得困难。在本文中，我们提出了一种预训练的语言模型，可以帮助用更少的标记数据解决这个问题。我们扩展了迁移学习和转换架构原则的原则，并考虑到最近爆发的流行病（如新冠肺炎）。我们将情感分析应用于两组不同的数据。我们还采用较小的训练集并将其作为模型的一部分进行微调。]]></description>
      <guid>https://arxiv.org/abs/2405.01586</guid>
      <pubDate>Mon, 06 May 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>基于文本质量的剪枝，用于高效训练语言模型</title>
      <link>https://arxiv.org/abs/2405.01582</link>
      <description><![CDATA[arXiv:2405.01582v1 公告类型：新
摘要：近年来，训练语言模型（LM）依赖于对海量数据集进行大量计算训练，这使得训练过程极其费力。在本文中，我们提出了一种新方法，以模型无关的方式对大型未标记 NLP 数据集中的文本质量进行数值评估，从而为文本实例分配“质量分数”。
  通过提出文本质量度量，本文建立了一个识别和消除低质量文本实例的框架，从而提高了 LM 模型的训练效率。多个模型和数据集的实验结果证明了这种方法的有效性，展示了训练有效性的显着提升，并凸显了资源高效型 LM 训练的潜力。
  例如，我们观察到，在 OpenWebText 数据集上训练时，多个 LM 模型的 14 个下游评估任务的绝对准确度平均提高了 0.9%，同时使用的数据减少了 40%，训练速度提高了 42%；在使用 20% 的数据集时，平均绝对准确度提高了 0.8%维基百科数据集上的数据更少，训练速度提高 21%。]]></description>
      <guid>https://arxiv.org/abs/2405.01582</guid>
      <pubDate>Mon, 06 May 2024 06:18:14 GMT</pubDate>
    </item>
    <item>
      <title>MediFact 参加 MEDIQA-M3G 2024：利用多模式学习解答皮肤病学医学问题</title>
      <link>https://arxiv.org/abs/2405.01583</link>
      <description><![CDATA[arXiv:2405.01583v1 公告类型：新
摘要：MEDIQA-M3G 2024 挑战赛需要为皮肤病学中的多语言和多模态医学答案生成提供新颖的解决方案（wai Yim 等人，2024a）。本文通过提出一种用于开放式医学问答 (QA) 的弱监督学习方法来解决传统方法的局限性。我们的系统通过 VGG16-CNN-SVM 模型利用现成的 MEDIQA-M3G 图像，实现对信息丰富的皮肤状况表示的多语言（英语、中文、西班牙语）学习。使用预先训练的 QA 模型，我们通过多模态融合进一步弥合了视觉和文本信息之间的差距。即使没有预定义的答案选项，这种方法也可以解决复杂的开放式问题。我们通过向 ViT-CLIP 模型提供多个响应以及图像来增强生成全面答案的能力。这项工作推动了医学 QA 研究，为临床决策支持系统铺平了道路，并最终改善了医疗保健服务。]]></description>
      <guid>https://arxiv.org/abs/2405.01583</guid>
      <pubDate>Mon, 06 May 2024 06:18:14 GMT</pubDate>
    </item>
    <item>
      <title>HateTinyLLM：使用小型大型语言模型检测仇恨言论</title>
      <link>https://arxiv.org/abs/2405.01577</link>
      <description><![CDATA[arXiv:2405.01577v1 公告类型：新
摘要：仇恨言论包括口头、书面或行为交流，针对基于敏感特征的个人或群体使用贬损或歧视性语言。自动仇恨言论检测在遏制仇恨言论传播方面发挥着至关重要的作用，尤其是在社交媒体平台上。人们设计了各种方法来应对这一挑战，包括深度学习的最新进展。在这项研究中，我们介绍了 HateTinyLLM，这是一种基于微调解码器的小型大型语言模型 (tinyLLM) 的新颖框架，用于高效的仇恨语音检测。我们的实验结果表明，经过微调的 HateTinyLLM 明显优于预训练的 mixtral-7b 模型。我们探索了各种微型 LLM，包括 PY007/TinyLlama-1.1B-step-50K-105b、Microsoft/phi-2 和 facebook/opt-1.3b，并使用 LoRA 和适配器方法对它们进行了微调。我们的观察表明，所有基于 LoRA 的微调模型都达到了 80% 以上的准确率。]]></description>
      <guid>https://arxiv.org/abs/2405.01577</guid>
      <pubDate>Mon, 06 May 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的 Mercurial 顶级本体</title>
      <link>https://arxiv.org/abs/2405.01581</link>
      <description><![CDATA[arXiv:2405.01581v1 公告类型：新
摘要：在我们的工作中，我们系统化并分析了大型语言模型 (LLM) 生成的响应中隐含的本体论承诺，重点以 ChatGPT 3.5 作为案例研究。我们研究法学硕士尽管没有明确的本体论，但如何表现出隐含的本体论分类，这些分类反映在他们生成的文本中。本文提出了一种理解法学硕士本体论承诺的方法，将本体论定义为一种对某些文本的本体论承诺提供系统说明的理论。我们研究了 ChatGPT 的本体论假设，并提出了一个系统化的说明，即 GPT 的顶级本体。这包括一个分类法（以 OWL 文件形式提供），以及关于本体论假设的讨论（例如，关于其分体论或存在论）。我们表明，在某些方面，GPT 的顶级本体与现有的顶级本体非常相似。然而，法学硕士生成文本的灵活性带来了重大挑战，包括本体论过载、模糊性和不一致。]]></description>
      <guid>https://arxiv.org/abs/2405.01581</guid>
      <pubDate>Mon, 06 May 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>揭示语言模型中的欺骗倾向：模拟公司人工智能助手</title>
      <link>https://arxiv.org/abs/2405.01576</link>
      <description><![CDATA[arXiv:2405.01576v1 公告类型：新
摘要：我们通过构建公司人工智能助手的真实模拟环境来研究人工智能系统的欺骗倾向。模拟公司员工为助理提供完成任务，这些任务涵盖写作协助、信息检索和编程。然后，我们介绍模型可能倾向于做出欺骗性行为的情况，同时注意不要指示或以其他方式迫使模型这样做。在不同的场景中，我们发现Claude 3 Opus
  1）遵守大量发表评论以影响公众对公司看法的任务，随后欺骗人们这样做，
  2) 当审计员提出问题时撒谎，以及
  3）在能力评估期间有策略地假装能力不如实际。
  我们的工作表明，即使训练有素的模型是有帮助、无害和诚实的，有时在现实场景中也会表现出欺骗性，而没有明显的外部压力。]]></description>
      <guid>https://arxiv.org/abs/2405.01576</guid>
      <pubDate>Mon, 06 May 2024 06:18:12 GMT</pubDate>
    </item>
    </channel>
</rss>