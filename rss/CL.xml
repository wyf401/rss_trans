<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>揭示人工智能多智能体系统中的对话偏见</title>
      <link>https://arxiv.org/abs/2501.14844</link>
      <description><![CDATA[arXiv:2501.14844v1 公告类型：新
摘要：检测生成模型产生的输出中的偏见对于降低其在关键环境中的应用所涉及的潜在风险至关重要。然而，现有的大多数用于识别生成文本中偏见的方法都孤立地考虑模型，而忽略了它们的上下文应用。具体而言，涉及生成模型的多智能体系统中可能出现的偏见仍未得到充分研究。为了解决这一差距，我们提出了一个框架，旨在量化对话式大型语言模型 (LLM) 多智能体系统中的偏见。我们的方法包括模拟小型回音室，其中初始化具有一致观点的两极分化主题的 LLM 对参与讨论。与预期相反，我们观察到生成的消息中表达的立场发生了重大变化，特别是在回音室中，所有代理最初都表达保守的观点，这与许多 LLM 对自由主义立场的政治偏见相一致。至关重要的是，回声室实验中观察到的偏见仍然无法通过当前最先进的、依赖问卷的偏见检测方法检测到。这凸显了开发更复杂的工具包以检测和缓解人工智能多智能体系统的偏见的迫切需求。执行实验的代码可在 https://anonymous.4open.science/r/LLMsConversationalBias-7725 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2501.14844</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于远程竞技场中的局部性偏见和结果</title>
      <link>https://arxiv.org/abs/2501.14850</link>
      <description><![CDATA[arXiv:2501.14850v1 公告类型：新 
摘要：长距离竞技场 (LRA) 基准旨在评估 Transformer 改进和替代方案在长距离依赖建模任务中的性能。Transformer 及其主要变体在这个基准上表现不佳，而一系列新的架构（如状态空间模型 (SSM)）获得了一些关注，在 LRA 中的表现大大优于 Transformer。最近的研究表明，在去噪预训练阶段，Transformer 可以使用这些新架构在 LRA 中取得有竞争力的结果。在这项工作中，我们讨论并解释了 MEGA 和 SSM 等架构在长距离竞技场中的优越性，以及 Transformer 结果的最新改进，指出了任务的位置和局部性质。我们表明，虽然 LRA 是长距离依赖建模的基准，但实际上大部分性能来自短距离依赖。通过使用训练技术来缓解数据效率低下的问题，Transformers 能够通过适当的位置编码达到最佳性能。此外，使用相同的技术，我们能够消除 SSM 卷积核的所有限制，并在不降低性能的情况下学习完全参数化的卷积，这表明 SSM 背后的设计选择只是为这些特定任务增加了归纳偏差和学习效率。我们的见解表明，应谨慎解读 LRA 结果，并呼吁重新设计基准。]]></description>
      <guid>https://arxiv.org/abs/2501.14850</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>JustLogic：评估大型语言模型中演绎推理的综合基准</title>
      <link>https://arxiv.org/abs/2501.14851</link>
      <description><![CDATA[arXiv:2501.14851v1 公告类型：新
摘要：逻辑推理是大型语言模型 (LLM) 的重要组成部分，近年来大量的研究工作旨在提高其演绎推理能力。然而，现有的演绎推理基准对于评估和推进 LLM 至关重要，但由于缺乏任务复杂性、先验知识作为混杂因素的存在以及肤浅的错误分析而不足。为了解决这些不足，我们引入了 JustLogic，这是一个合成生成的演绎推理基准，旨在对 LLM 进行严格评估。JustLogic (i) 非常复杂，能够生成多种语言模式、词汇和论证结构；(ii) 独立于先验知识，消除了拥有先验知识的模型的优势并确保仅使用演绎推理来回答问题；(iii) 能够对推理深度和论证形式对模型准确性的异质影响进行深入的错误分析。我们在 JustLogic 上的实验结果表明，大多数最先进的 (SOTA) LLM 的表现明显低于人类平均水平，表明模型有很大的改进空间。所有代码和数据均可在 https://github.com/michaelchen-lab/JustLogic 上找到]]></description>
      <guid>https://arxiv.org/abs/2501.14851</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LoRA 微调的动态适应性，可实现大型语言模型的高效且针对特定任务的优化</title>
      <link>https://arxiv.org/abs/2501.14859</link>
      <description><![CDATA[arXiv:2501.14859v1 公告类型：新
摘要：本文提出了一种新的大型语言模型微调方法——动态 LoRA。该方法以标准低秩自适应框架为基础，进一步增加了动态自适应机制，以提高效率和性能。动态 LoRA 的关键贡献在于其自适应权重分配机制与基于输入特征的自适应策略相结合。这些增强功能允许更精确的微调过程，更适合特定任务。传统的 LoRA 方法使用静态适配器设置，不考虑模型层的不同重要性。相比之下，动态 LoRA 引入了一种在微调过程中动态评估层重要性的机制。这种评估可以重新分配适配器参数以适应每个单独任务的独特需求，从而获得更好的优化结果。灵活性的另一个提升来自于对输入特征分布的考虑，这有助于模型在面对复杂多样的数据集时更好地泛化。联合方法不仅提高了每个单一任务的性能，还提高了模型的泛化能力。动态 LoRA 的效率已在基准数据集（例如 GLUE）上的实验中得到验证，结果令人惊讶。更具体地说，该方法实现了 88.1% 的准确率，F1 得分为 87.3%。值得注意的是，这些改进是以计算成本略有增加为代价的：仅比标准 LoRA 多 0.1% 的资源。性能和效率之间的这种平衡使动态 LoRA 成为微调 LLM 的实用、可扩展解决方案，尤其是在资源受限的情况下。更进一步说，它的适应性使其成为包括多模式任务在内的更高级应用的有希望的基础。]]></description>
      <guid>https://arxiv.org/abs/2501.14859</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DrawEduMath：使用专家注释的学生手绘数学图像评估视觉语言模型</title>
      <link>https://arxiv.org/abs/2501.14877</link>
      <description><![CDATA[arXiv:2501.14877v1 公告类型：新
摘要：在现实世界中，视觉语言模型 (VLM) 应该能够稳健地处理自然、嘈杂的视觉内容以及特定领域的语言和概念。例如，使用数字学习平台的 K-12 教育工作者可能需要检查学生数学作业的许多图像并提供反馈。为了评估 VLM 在这种环境下支持教育工作者的潜力，我们引入了 DrawEduMath，这是一个英语数据集，包含 2,030 张学生对 K-12 数学问题手写答案的图像。教师提供了详细的注释，包括每幅图像的自由形式描述和 11,661 个问答 (QA) 对。这些注释捕捉了大量的教学见解，从学生的解决问题策略到他们的绘画、图表和写作的构图。我们评估了教师的 QA 对以及使用语言模型 (LM) 从教师描述中得出的 44,362 个合成 QA 对的 VLM。我们发现，即使是最先进的 VLM 在 DrawEduMath 问题上也有很大改进空间。我们还发现，合成 QA 虽然不完美，但可以产生与教师编写的 QA 类似的模型排名。我们发布 DrawEduMath 是为了支持评估 VLM 在考虑教育背景的情况下对收集的图像进行数学推理的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.14877</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>谨慎验证：依赖不完善事实性指标的陷阱</title>
      <link>https://arxiv.org/abs/2501.14883</link>
      <description><![CDATA[arXiv:2501.14883v1 公告类型：新
摘要：大型语言模型的改进使人们越来越乐观地认为它们可以作为自然语言生成输出的可靠评估器。在本文中，我们通过彻底重新评估 11 个数据集上的五个最先进的事实性指标来挑战这种乐观情绪，这些数据集用于摘要、检索增强生成和问答。我们发现这些评估器彼此不一致，并且经常错误估计系统级性能，这两者都可能导致各种陷阱。我们进一步表明，这些指标对高度释义的输出和利用源文档遥远部分的输出表现出偏见。我们敦促这些事实性指标的用户谨慎行事，并在继续之前手动验证这些指标在其感兴趣领域的可靠性。]]></description>
      <guid>https://arxiv.org/abs/2501.14883</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我反思的大型语言模型：黑格尔辩证法</title>
      <link>https://arxiv.org/abs/2501.14917</link>
      <description><![CDATA[arXiv:2501.14917v2 公告类型：新
摘要：通过哲学视角研究 NLP 最近引起了研究人员的关注，因为它将计算方法与古典哲学流派联系起来。本文介绍了一种受黑格尔辩证法启发的 LLM 自我反思的哲学方法，利用自我辩证法模拟内部批评，然后通过解决矛盾点来综合新的想法。此外，本文通过建立动态退火方法研究了 LLM 生成温度的影响，该方法在早期阶段促进创造力，并通过关注细微差别逐渐完善它，以及固定的生成温度策略。我们提出的方法经过检验，以确定其从初始命题产生新想法的能力。此外，我们还利用多代理多数投票 (MAMV) 策略来评估所生成想法的有效性和新颖性，这在缺乏领域专家的情况下非常有用。我们的实验表明，这种方法在生成新想法方面很有前景，并为未来的研究奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2501.14917</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于细粒度指令处理的上下文感知神经梯度映射</title>
      <link>https://arxiv.org/abs/2501.14936</link>
      <description><![CDATA[arXiv:2501.14936v1 公告类型：新
摘要：将上下文嵌入集成到大型语言模型的优化过程中是自然语言处理的一大进步。上下文感知神经梯度映射框架引入了动态梯度调整机制，将上下文嵌入直接纳入优化过程。这种方法有助于实时调整参数，即使在存在稀疏或嘈杂的数据输入的情况下也能增强特定于任务的泛化。该框架的数学基础依赖于梯度下降修改，其中上下文嵌入来自经过训练以将输入特征映射到最佳适应梯度的补充神经网络。通过采用微分几何原理，高维输入依赖关系被编码为低维梯度流形，从而无需重新训练整个模型即可实现高效适应。实证评估表明，所提出的框架在各种指标（包括准确性、对噪声的鲁棒性和计算效率）上始终优于基线模型。上下文特定嵌入的集成使语言理解更加复杂，从而提高了模型处理各种语言现象的能力。此外，通过该方法实现的计算效率证明了其对于在不同约束条件下运行的大规模语言模型的可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2501.14936</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CASE-Bench：大型语言模型的上下文感知安全评估基准</title>
      <link>https://arxiv.org/abs/2501.14940</link>
      <description><![CDATA[arXiv:2501.14940v1 公告类型：新 
摘要：将大型语言模型 (LLM) 与人类价值观相结合对于其安全部署和广泛采用至关重要。当前的 LLM 安全基准通常仅关注拒绝单个有问题的查询，这忽略了查询发生的上下文的重要性，并且可能导致在安全上下文中不必要地拒绝查询，从而降低用户体验。为了解决这一差距，我们引入了 CASE-Bench，这是一种上下文感知安全评估基准，它将上下文集成到 LLM 的安全评估中。CASE-Bench 根据上下文完整性理论为分类查询分配不同的、正式描述的上下文。此外，与以前主要依赖少数注释者的多数投票的研究相比，我们招募了足够数量的注释者，以确保基于功效分析检测到实验条件之间的统计显着差异。我们使用 CASE-Bench 对各种开源和商业 LLM 进行了广泛的分析，结果表明环境对人类判断有显著影响（z 检验 p&lt;0.0001），强调了环境在安全评估中的必要性。我们还发现人类判断和 LLM 响应之间存在显著的不匹配，尤其是在安全环境中的商业模型中。]]></description>
      <guid>https://arxiv.org/abs/2501.14940</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ExPerT：个性化长文本生成的有效且可解释的评估</title>
      <link>https://arxiv.org/abs/2501.14956</link>
      <description><![CDATA[arXiv:2501.14956v1 公告类型：新
摘要：评估大型语言模型 (LLM) 生成的个性化文本具有挑战性，因为只有 LLM 用户（即提示作者）才能可靠地评估输出，但在各个研究中重新吸引同一个人是不可行的。本文通过引入可解释的基于参考的评估框架 ExPerT 来解决评估个性化文本生成的挑战。ExPerT 利用 LLM 从生成文本和参考文本中提取原子方面及其证据，匹配这些方面，并根据内容和写作风格评估它们的一致性——这是个性化文本生成的两个关键属性。此外，ExPerT 为评估过程的每一步生成详细、细粒度的解释，从而提高了透明度和可解释性。我们的实验表明，与最先进的文本生成评估方法相比，ExPerT 在与人类判断的一致性方面实现了 7.2% 的相对改善。此外，人类评估人员对 ExPerT 解释的可用性评分为 5 分中的 4.7 分，突显了其在使评估决策更易于解释方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.14956</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>教育领域注释分类工具的回顾</title>
      <link>https://arxiv.org/abs/2501.14976</link>
      <description><![CDATA[arXiv:2501.14976v1 公告类型：新
摘要：注释由与内容相关的部分信息组成，用于解释内容或添加更多信息。在教育领域使用注释作为工具对学习过程有积极影响。使用此工具的通常方法是向学生提供内容（通常是文本），他们必须将注释与之关联。在大多数情况下，这项任务是由合作的学生小组完成的。这个过程鼓励对内容进行分析和理解，因为他们必须理解这些内容才能对其进行注释，同时也鼓励团队合作。为了方便使用，近几十年来开发了计算机应用程序，这些应用程序实现了注释过程并提供一组附加功能。这些功能之一是对所做的注释进行分类。这种功能可以在学习过程中以各种方式得到利用，比如在注释过程中指导学生，向学生提供有关注释过程如何进行的信息，向老师提供有关学生如何写作和如何理解内容的信息，以及实施其他创新教育过程。从这个意义上说，注释的分类在注释在教育领域的应用中起着至关重要的作用。对注释的研究很多，但大多数只考虑了分类方面。本文对注释工具中使用的分类机制进行了初步研究，确定了四种类型的情况：缺乏分类机制、基于预先建立的词汇的分类、基于可扩展词汇的分类以及基于结构化词汇的分类。]]></description>
      <guid>https://arxiv.org/abs/2501.14976</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言中同理心建模的泥潭：理论构建的实际影响</title>
      <link>https://arxiv.org/abs/2501.14981</link>
      <description><![CDATA[arXiv:2501.14981v1 公告类型：新
摘要：NLP 中同理心的概念操作化多种多样，有些具有特定的行为和属性，而另一些则更为抽象。这些变化如何相互关联并捕捉文本中可观察到的同理心属性仍不清楚。为了深入了解这一点，我们分析了适应不同理论基础的同理心任务的同理心模型的转移性能。我们研究 (1) 同理心定义的维度，(2) 定义的维度与测量/观察到的属性之间的对应关系，以及 (3) 数据表示它们的利弊，发现与其他转移设置特征相比，它们对性能有显著影响。将同理心任务的理论基础描述为直接、抽象或相邻进一步表明，直接预测指定同理心成分的任务具有更高的可转移性。我们的工作为精确和多维同理心操作化的需求提供了实证证据。]]></description>
      <guid>https://arxiv.org/abs/2501.14981</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对多产品问答的联合检索增强生成</title>
      <link>https://arxiv.org/abs/2501.14998</link>
      <description><![CDATA[arXiv:2501.14998v1 公告类型：新
摘要：大型语言模型和检索增强生成的最新进展提高了人们对企业产品领域特定问答的兴趣。然而，人工智能助手在多产品 QA 环境中经常面临挑战，需要跨不同领域做出准确的响应。现有的多领域 RAG-QA 方法要么不加区分地查询所有领域，增加计算成本和 LLM 幻觉，要么依赖于严格的资源选择，这可能会限制搜索结果。我们引入了 MKP-QA，这是一种新颖的多产品知识增强 QA 框架，具有跨领域和相关知识的概率联合搜索。该方法通过聚合查询域和查询段落概率相关性来提高多域搜索质量。为了解决多产品 QA 缺乏合适基准的问题，我们还提供了专注于三款 Adob​​e 产品的新数据集：Adobe Experience Platform、Target 和 Customer Journey Analytics。我们的实验表明，MKP-QA 在检索准确性和响应质量方面显著提升了多产品 RAG-QA 的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.14998</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MDEval：评估和增强大型语言模型中的 Markdown 感知</title>
      <link>https://arxiv.org/abs/2501.15000</link>
      <description><![CDATA[arXiv:2501.15000v1 公告类型：新 
摘要：大型语言模型（LLM）有望提供结构化的 Markdown 响应，以提高网络聊天机器人（例如 ChatGPT）的可读性。尽管有大量指标来评估 LLM，但它们未能从输出内容结构的角度评估可读性。为此，我们专注于一个被忽视但重要的指标——Markdown 感知，它直接影响这些语言模型生成的内容的可读性和结构。在本文中，我们通过构建一个包含 20K 个实例的数据集（涵盖英文和中文的 10 个主题），引入了 MDEval，这是一个评估 LLM 的 Markdown 感知的综合基准。与传统的基于模型的评估不同，MDEval 通过结合基于模型的生成任务和统计方法提供了出色的可解释性。我们的结果表明，MDEval 的 Spearman 相关性达到 0.791，与人类的准确率达到 84.1%，远远优于现有方法。大量实验结果还表明，通过对我们提出的数据集进行微调，性能较差的开源模型在 Markdown 感知方面能够达到与 GPT-4o 相当的性能。为了确保可重复性和透明度，MDEval 在 https://github.com/SWUFE-DB-Group/MDEval-Benchmark 上开源。]]></description>
      <guid>https://arxiv.org/abs/2501.15000</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AKVQ-VL：用于视觉语言模型的注意力感知 KV 缓存自适应 2 位量化</title>
      <link>https://arxiv.org/abs/2501.15021</link>
      <description><![CDATA[arXiv:2501.15021v1 公告类型：新
摘要：视觉语言模型 (VLM) 在多模态任务中表现出色。然而，过长的多模态输入会导致键值 (KV) 缓存过大，从而导致大量内存消耗和 I/O 瓶颈。以前的大型语言模型 (LLM) 的 KV 量化方法可能会缓解这些问题，但忽略了多模态标记的注意力显着性差异，导致性能不佳。在本文中，我们研究了 VLM 中的注意力感知标记显着性模式并提出了 AKVQ-VL。AKVQ-VL 利用提出的文本显着性注意 (TSA) 和枢轴标记显着性注意 (PSA) 模式来自适应地分配比特预算。此外，实现极低比特量化需要有效解决 KV 张量中的异常值。 AKVQ-VL 利用 Walsh-Hadamard 变换 (WHT) 构建无异常值 KV 缓存，从而降低量化难度。对 12 个长上下文和多模态任务的 2 位量化评估表明，AKVQ-VL 保持甚至提高了准确率，优于面向 LLM 的方法。AKVQ-VL 可将峰值内存使用量降低 2.13 倍，支持高达 3.25 倍的批量大小和 2.46 倍的吞吐量。]]></description>
      <guid>https://arxiv.org/abs/2501.15021</guid>
      <pubDate>Tue, 28 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>