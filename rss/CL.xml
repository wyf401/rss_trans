<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 12 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>循序渐进地翻译：分解翻译流程，提高长篇文本的翻译质量</title>
      <link>https://arxiv.org/abs/2409.06790</link>
      <description><![CDATA[arXiv:2409.06790v1 公告类型：新
摘要：在本文中，我们借鉴翻译研究中既定的流程，提出了一种长篇文本翻译的分步方法。我们不是将机器翻译视为单一的整体任务，而是提出了一个框架，让语言模型参与多轮交互，包括翻译前研究、起草、改进和校对，从而逐步改进翻译。使用 Gemini 1.5 Pro 对十种语言对进行广泛的自动评估表明，与传统的零样本提示方法和早期的类人基线策略相比，分步翻译可以大大提高翻译质量，从而在 WMT2024 上取得最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2409.06790</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>惊奇分解：语言处理中 ERP 组件的统一计算模型</title>
      <link>https://arxiv.org/abs/2409.06803</link>
      <description><![CDATA[arXiv:2409.06803v1 公告类型：新
摘要：几十年来，语言相关 ERP 成分的功能解释一直是心理语言学争论的焦点。我们提出了一种大脑中人类语言处理的信息论模型，其中传入的语言输入首先被浅层处理，然后被更深入地处理，这两种信息处理对应于不同的脑电图特征。正式地，我们表明上下文中单词的信息内容（惊讶）可以分解为两个量：（A）启发式惊讶，表示单词的浅层处理难度，与 N400 信号相对应；（B）差异信号，反映浅层和深层解释之间的差异，与 P600 信号相对应。这两个量都可以使用现代 NLP 模型直接估计。我们通过成功模拟由先前报告的六个实验中的各种语言操作引起的 ERP 模式来验证我们的理论，并成功做出了新的定性和定量预测。我们的理论与假设“足够好”启发式解释阶段的传统认知理论兼容，但具有精确的信息论表述。该模型提供了基于认知过程的 ERP 组件的信息论模型，并使我们更接近完全指定的语言处理神经计算模型。]]></description>
      <guid>https://arxiv.org/abs/2409.06803</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PingPong：具有用户模拟和多模型评估的角色扮演语言模型基准</title>
      <link>https://arxiv.org/abs/2409.06820</link>
      <description><![CDATA[arXiv:2409.06820v1 公告类型：新
摘要：我们引入了一种用于评估语言模型角色扮演能力的新基准。我们的方法利用语言模型本身来模拟动态、多轮对话中的用户并评估由此产生的对话。该框架由三个主要组件组成：假设特定角色的玩家模型、模拟用户行为的询问者模型和评估对话质量的判断模型。我们进行了比较自动评估和人工注释的实验以验证我们的方法，证明了跨多个标准的强相关性。这项工作为在交互场景中对模型能力进行稳健和动态评估奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2409.06820</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>小型模型在 LLM 时代的作用是什么：一项调查</title>
      <link>https://arxiv.org/abs/2409.06857</link>
      <description><![CDATA[arXiv:2409.06857v2 公告类型：新
摘要：大型语言模型 (LLM) 在推进通用人工智能 (AGI) 方面取得了重大进展，从而开发出越来越大的模型，例如 GPT-4 和 LLaMA-405B。然而，扩大模型规模会导致计算成本和能耗呈指数级增长，使得这些模型对于资源有限的学术研究人员和企业来说不切实际。同时，小型模型 (SM) 经常用于实际环境，尽管它们的重要性目前被低估了。这引发了关于小型模型在 LLM 时代的作用的重要问题，这一主题在先前的研究中受到的关注有限。在这项工作中，我们从两个关键角度系统地研究了 LLM 和 SM 之间的关系：协作和竞争。我们希望这项调查能为从业者提供有价值的见解，加深对小型模型贡献的理解，并促进更有效地利用计算资源。代码可在 https://github.com/tigerchen52/role_of_small_models 获取。]]></description>
      <guid>https://arxiv.org/abs/2409.06857</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于评估基于 LLM 的研究问题提取任务评估函数的数据集</title>
      <link>https://arxiv.org/abs/2409.06883</link>
      <description><![CDATA[arXiv:2409.06883v1 公告类型：新
摘要：文本摘要技术取得了显著进展。然而，从研究论文等高度专业化的文档中准确提取和总结必要信息的任务尚未得到充分研究。我们专注于从研究论文中提取研究问题 (RQ) 的任务，并构建一个新的数据集，该数据集由机器学习论文、GPT-4 从这些论文中提取的 RQ 以及从多个角度对提取的 RQ 的人工评估组成。利用该数据集，我们系统地比较了最近提出的基于 LLM 的摘要评估函数，发现没有一个函数与人工评估显示出足够高的相关性。我们希望我们的数据集为进一步研究开发针对 RQ 提取任务的更好的评估函数奠定基础，并有助于提高任务的性能。数据集可在 https://github.com/auto-res/PaperRQ-HumanAnno-Dataset 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.06883</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你有十三个小时来解决迷宫：通过函数调用增强 AI 游戏大师的能力</title>
      <link>https://arxiv.org/abs/2409.06949</link>
      <description><![CDATA[arXiv:2409.06949v1 公告类型：新
摘要：由于大型语言模型 (LLM) 的局限性和游戏大师角色的复杂性，为基于文本的游戏开发一致且可靠的 AI 游戏大师是一项具有挑战性的任务。本文提出了一种通过利用桌面角色扮演游戏“吉姆汉森的迷宫：冒险游戏”中的函数调用来增强 AI 游戏大师的新方法。我们的方法涉及通过函数集成特定于游戏的控件，我们表明这可以提高 AI 游戏大师的叙事质量和状态更新一致性。基于人工评估和单元测试的实验结果证明了我们的方法在增强游戏体验和保持与游戏状态的一致性方面的有效性。这项工作有助于游戏 AI 和交互式故事叙述的进步，为设计更具吸引力和一致性的 AI 驱动的游戏大师提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2409.06949</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越IID：从指令交互和依赖的角度优化指令学习</title>
      <link>https://arxiv.org/abs/2409.07045</link>
      <description><![CDATA[arXiv:2409.07045v1 公告类型：新
摘要：随着各种指令数据集的出现，一个关键的挑战是如何有效地选择和集成这些指令来微调大型语言模型 (LLM)。以前的研究主要侧重于选择单个高质量指令。然而，这些工作忽视了不同类别指令之间的联合相互作用和依赖关系，导致选择策略不理想。此外，这些交互模式的性质在很大程度上仍未得到探索，更不用说优化与它们相关的指令集了。为了填补这些空白，在本文中，我们：(1) 系统地研究不同类别指令之间的交互和依赖模式，(2) 使用基于线性规划的方法设法优化与交互模式有关的指令集，并使用指令依赖性分类法指导的课程学习优化 SFT 的学习方案。跨不同 LLM 的实验结果表明，在广泛采用的基准上，性能优于强基线。]]></description>
      <guid>https://arxiv.org/abs/2409.07045</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>母语提示与非母语提示：比较分析</title>
      <link>https://arxiv.org/abs/2409.07054</link>
      <description><![CDATA[arXiv:2409.07054v1 公告类型：新
摘要：大型语言模型 (LLM) 在不同领域表现出非凡的能力，包括标准自然语言处理 (NLP) 任务。为了从 LLM 中获取知识，提示起着关键作用，包括自然语言指令。大多数开源和闭源 LLM 都是在可用的标记和未标记资源（如文本、图像、音频和视频等数字内容）上进行训练的。因此，这些模型对高资源语言有更好的了解，但对低资源语言却很吃力。由于提示在理解其功能方面起着至关重要的作用，因此用于提示的语言仍然是一个重要的研究问题。虽然在这方面已经有大量研究，但仍然有限，对中低资源语言的探索较少。在本研究中，我们研究了与 12 个不同的阿拉伯语数据集（9.7K 个数据点）相关的 11 个不同 NLP 任务的不同提示策略（母语与非母语）。我们总共进行了 197 项实验，涉及 3 个 LLM、12 个数据集和 3 种提示策略。我们的研究结果表明，平均而言，非母语提示表现最佳，其次是混合提示和母语提示。]]></description>
      <guid>https://arxiv.org/abs/2409.07054</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法律事实预测：任务定义与数据集构建</title>
      <link>https://arxiv.org/abs/2409.07055</link>
      <description><![CDATA[arXiv:2409.07055v1 公告类型：新
摘要：法律事实是指在审判中可以通过公认证据证明的事实。它们构成了法院判决的基础。本文介绍了一种新颖的NLP任务：法律事实预测，旨在根据证据清单预测法律事实。预测的事实可以指导参与审判的当事人及其律师在审判期间加强陈述并优化策略。此外，由于真实的法律事实在最终判决前难以获得，预测的事实也是法律判决预测的重要依据。我们为真实的民事贷款案件LFPLoan构建了一个由证据清单和真实法律事实组成的基准数据集。我们在这个数据集上的实验表明，这项任务并不简单，需要进一步进行大量的研究工作。]]></description>
      <guid>https://arxiv.org/abs/2409.07055</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用新颖的基于图形的口语响应连贯性建模对对话测试进行自动口语评估</title>
      <link>https://arxiv.org/abs/2409.07064</link>
      <description><![CDATA[arXiv:2409.07064v1 公告类型：新
摘要：对话测试中的自动口语评估 (ASAC) 旨在评估 L2（第二语言）说话者在对话者与一个或多个候选人互动的环境中的整体口语能力。尽管之前的 ASAC 方法在各自的数据集上表现出色，但仍然缺乏专门关注将对话中逻辑流程的连贯性纳入评分模型的研究。为了应对这一关键挑战，我们提出了一个分层图模型，该模型恰当地结合了广泛的响应间交互（例如话语关系）和细微的语义信息（例如语义词和说话者意图），随后将其与上下文信息融合以进行最终预测。在 NICT-JLE 基准数据集上的大量实验结果表明，与一些强大的基线相比，我们提出的建模方法可以在各种评估指标方面显著提高预测准确性。这也阐明了研究 ASAC 中口头反应的连贯性相关方面的重要性。]]></description>
      <guid>https://arxiv.org/abs/2409.07064</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>潜在空间解释用于风格分析和可解释的作者归属</title>
      <link>https://arxiv.org/abs/2409.07072</link>
      <description><![CDATA[arXiv:2409.07072v1 公告类型：新
摘要：最近最先进的作者归属方法在潜在的、不可解释的空间中学习文本的作者表示，阻碍了它们在实际应用中的可用性。我们的工作提出了一种解释这些学习到的嵌入的新方法，即识别潜在空间中的代表点并利用 LLM 生成每个点的写作风格的信息性自然语言描述。我们评估了我们的可解释空间与潜在空间的一致性，发现它与其他基线相比实现了最佳预测一致性。此外，我们进行了人工评估以评估这些风格描述的质量，验证了它们作为潜在空间解释的效用。最后，我们调查了在系统解释的帮助下，人类在具有挑战性的 AA 任务上的表现是否会提高，发现准确率平均提高了约 +20%。]]></description>
      <guid>https://arxiv.org/abs/2409.07072</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过错误信息理解法学硕士中的知识漂移</title>
      <link>https://arxiv.org/abs/2409.07085</link>
      <description><![CDATA[arXiv:2409.07085v1 公告类型：新
摘要：大型语言模型 (LLM) 彻底改变了众多应用，使其成为我们数字生态系统不可或缺的一部分。然而，它们的可靠性变得至关重要，尤其是当这些模型暴露于错误信息时。我们主要分析最先进的 LLM 在 QnA 场景中遇到虚假信息时对事实不准确的敏感性，这一问题可能导致我们称之为“知识漂移”的现象，从而严重削弱这些模型的可信度。我们根据熵、困惑度和标记概率指标来评估模型响应的真实性和不确定性。我们的实验表明，当由于接触虚假信息而导致问题回答错误时，LLM 的不确定性可能会增加高达 56.6%。同时，反复接触相同的错误信息可以再次降低模型的不确定性（相对于未受污染提示的答案，下降了 52.8%），可能会操纵底层模型的信念并导致其偏离原始知识。这些发现提供了对 LLM 的稳健性和对抗性输入的脆弱性的洞察，为在各个领域开发更可靠的 LLM 应用程序铺平了道路。代码可在 https://github.com/afastowski/knowledge_drift 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.07085</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型的无本体通用领域知识图谱到文本生成数据集合成</title>
      <link>https://arxiv.org/abs/2409.07088</link>
      <description><![CDATA[arXiv:2409.07088v1 公告类型：新
摘要：知识图谱到文本 (G2T) 生成涉及将结构化知识图谱口头化为自然语言文本。预训练语言模型 (PLM) 的最新进展提高了 G2T 性能，但其有效性取决于具有精确图文对齐的数据集。然而，高质量、通用领域 G2T 生成数据集的稀缺限制了通用领域 G2T 生成研究的进展。为了解决这个问题，我们引入了维基百科无本体图文数据集 (WikiOFGraph)，这是一种新的大规模 G2T 数据集，使用一种利用大型语言模型 (LLM) 和 Data-QuestEval 的新方法生成。我们的新数据集包含 5.85M 个通用领域图文对，无需依赖外部本体即可提供高图文一致性。实验结果表明，在 WikiOFGraph 上微调的 PLM 在各种评估指标上均优于在其他数据集上训练的 PLM。事实证明，我们的方法是生成高质量 G2T 数据的可扩展且有效的解决方案，显著推动了 G2T 生成领域的发展。]]></description>
      <guid>https://arxiv.org/abs/2409.07088</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Cross-Refine：通过串联学习改进自然语言解释生成</title>
      <link>https://arxiv.org/abs/2409.07123</link>
      <description><![CDATA[arXiv:2409.07123v1 公告类型：新
摘要：自然语言解释 (NLE) 对于阐明大型语言模型 (LLM) 决策背后的原因至关重要。已经开发了许多使用 LLM 生成 NLE 的技术。然而，与人类一样，LLM 可能并不总是在第一次尝试时产生最佳的 NLE。受人类学习过程的启发，我们引入了 Cross-Refine，它通过分别部署两个 LLM 作为生成器和评论家来采用角色建模。生成器输出第一个 NLE，然后使用评论家提供的反馈和建议来改进这个初始解释。Cross-Refine 不需要任何监督训练数据或额外训练。我们使用三个最先进的开源 LLM 通过自动和人工评估在三个 NLP 任务中验证 Cross-Refine。我们选择 Self-Refine (Madaan et al., 2023) 作为基线，它仅利用自我反馈来改进解释。我们从自动评估和用户研究中得出的结论表明，Cross-Refine 的表现优于 Self-Refine。同时，Cross-Refine 可以在功能较弱的 LLM 上有效发挥作用，而 Self-Refine 仅在 ChatGPT 上产生强劲效果。此外，我们进行了一项消融研究，以评估反馈和建议的重要性。它们在完善解释方面都发挥着重要作用。我们进一步在英语和德语双语数据集上评估了 Cross-Refine。]]></description>
      <guid>https://arxiv.org/abs/2409.07123</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言生成的重新排序法则：传播理论视角</title>
      <link>https://arxiv.org/abs/2409.07131</link>
      <description><![CDATA[arXiv:2409.07131v1 公告类型：新
摘要：为了确保大型语言模型 (LLM) 的安全使用，必须降低它们产生幻觉或生成不可接受答案的倾向。一种简单且经常使用的策略是首先让 LLM 生成多个假设，然后使用重新排序器选择最佳假设。在本文中，我们将这种策略与使用冗余来降低嘈杂通信信道中的错误率进行比较。我们将生成器概念化为通过并行噪声信道传输消息的多个描述的发送者。接收者通过对（可能损坏的）描述进行排序并选择最可靠的描述来解码消息。我们提供了该协议渐近无错误的条件（即几乎肯定会产生可接受的答案），即使在重新排序器不完善（由 Mallows 或 Zipf-Mandelbrot 模型控制）且信道分布具有统计相关性的情况下也是如此。我们使用我们的框架来获得重新排序定律，并使用 LLM 在两个实际任务上进行了实证验证：使用 DeepSeek-Coder 7B 进行文本到代码的生成，以及使用 TowerInstruct 13B 进行医疗数据的机器翻译。]]></description>
      <guid>https://arxiv.org/abs/2409.07131</guid>
      <pubDate>Thu, 12 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>