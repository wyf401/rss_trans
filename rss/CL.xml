<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用LLMS进行K-匿名性估计的概率推理</title>
      <link>https://arxiv.org/abs/2503.09674</link>
      <description><![CDATA[ARXIV：2503.09674V1公告类型：新 
摘要：概率推理是人类和人工智能的关键方面，可以处理决策中的不确定性和歧义。在本文中，我们在不确定性下介绍了一项新颖的数值推理任务，重点是估计包含对隐私信息的用户生成的文档的K匿名性。我们提出了分支，该分支使用LLMS分解了联合概率分布，以估计k值的大小 - 与给定信息匹配的人口的大小，将单个文本信息的单个文本信息作为随机变量匹配。使用独立的llms或检索功能增强的生成系统估算每个因素发生在人群中的可能性，并且这些概率合并为最终的K值。我们的实验表明，该方法在67％的时间内成功估计了正确的K值，与GPT-4O链链的推理相比增加了11％。此外，我们利用LLM的不确定性来开发K-匿名性的预测间隔，其中包括近92％的病例中的正确值。]]></description>
      <guid>https://arxiv.org/abs/2503.09674</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM是否使主动学习过时？调查NLP社区</title>
      <link>https://arxiv.org/abs/2503.09701</link>
      <description><![CDATA[ARXIV：2503.09701V1公告类型：新 
摘要：监督学习取决于注释的数据，这很昂贵。降低注释成本的长期策略是积极学习，这是一个迭代过程，其中人类仅通过模型被视为信息丰富的数据实例。大型语言模型（LLMS）推动了积极学习的有效性，但也改进了诸如少或零学习和文本合成之类的方法，从而引入了潜在的替代方法。这就提出了一个问题：积极学习是否已过时？为了充分回答这一点，我们必须超越文学作用。我们在NLP社区进行了一项在线调查，以收集有关数据注释相关性的先前无形的见解，尤其是专注于积极学习，包括最佳实践，障碍和预期的未来发展。我们的发现表明，注释数据仍然是一个关键因素，并且积极学习仍然是相关的。尽管大多数活跃的学习用户都认为它有效，但与十多年前的社区调查进行了比较揭示了持续的挑战：设置复杂性，降低成本和工具的估计。我们发布了收集的数据集的匿名版本]]></description>
      <guid>https://arxiv.org/abs/2503.09701</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>审查盖德 - 餐厅评论胃肠道疾病检测和提取大型语言模型</title>
      <link>https://arxiv.org/abs/2503.09743</link>
      <description><![CDATA[ARXIV：2503.09743V1公告类型：新 
摘要：食源性胃肠道（GI）疾病是英国健康状况不佳的常见原因。但是，许多情况不与医疗保健系统互动，对传统监视方法提出了重大挑战。大型语言模型（LLMS）中公开可用的在线餐厅评论和进步的增长，通过识别胃肠道疾病的公开报告，提出了扩展疾病监测的潜在机会。在这项研究中，我们介绍了一种新颖的注释模式，该模式是由GI疾病专家开发的，应用于Yelp开放的评论数据集。我们的注释超出了二元疾病检测，包括详细提取有关症状和食物的信息。我们在这三个任务中评估了开放重量LLM的性能：胃肠道疾病检测，症状提取和食物提取。我们将这种性能与基于罗伯塔的分类模型进行了比较，专门针对这些任务进行了微调。我们的结果表明，使用基于及时的方法，LLMS在我们所有三个任务中都能达到90％以上的Micro-F1分数。仅使用提示，我们获得了超过较小微调模型的Micro-F1分数。我们进一步证明了在三个以偏见为中心的实验中，LLM在GI疾病检测中的鲁棒性。我们的结果表明，公开可用的审查文本和LLMS通过实现高效提取关键信息来实现对GI疾病的公共卫生监测的巨大潜力。尽管LLM似乎在处理中表现出最小的偏见，但餐厅评论数据的固有局限性强调了对结果的谨慎解释的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.09743</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效的多任务推理：与Gromov-Wasserstein合并的模型合并</title>
      <link>https://arxiv.org/abs/2503.09774</link>
      <description><![CDATA[ARXIV：2503.09774V1公告类型：新 
摘要：学生反应的自动评分提高了教育的效率，但是为每个任务部署一个单独的神经网络会增加存储需求，维护工作和冗余计算。为了应对这些挑战，本文介绍了Gromov-Wasserstein评分模型合并（GW-SMM）方法，该方法基于通过Gromov-Wasserstein距离测得的特征分布相似性合并模型。我们的方法首先是使用单个模型从学生响应中提取功能，捕获特定于项目的上下文和独特的学习表示。然后，Gromov-Wasserstein距离量化了这些特征分布之间的相似性，从而确定了最兼容的合并模型。通过仅组合分类头之前的共享层，可以合并表现出最小成对距离的模型，通常是成对或三重奏。此策略会导致统一的特征提取器，同时保留单独的分类头进行特定于项目的评分。我们验证了我们违反人类专家知识和基于GPT-O1的合并方法的方法。 GW-SMM始终胜过两者，获得了更高的微型F1分数，宏F1得分，精确匹配的精度和每个标签的精度。与基于GPT-O1的合并相比，微F1和每标签精度的改善在统计学上具有显着意义（P = 0.04，P = 0.01）。此外，GW-SMM将存储需求减少了一半，而不会损害太多准确性，从而证明了其计算效率以及可靠的评分性能。]]></description>
      <guid>https://arxiv.org/abs/2503.09774</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过离散扩散模型的受限语言产生</title>
      <link>https://arxiv.org/abs/2503.09790</link>
      <description><![CDATA[ARXIV：2503.09790V1公告类型：新 
摘要：限制在文本生成中至关重要，因为在确保生成的输出遵守用户定义的指令或一般安全指南时，LLM输出通常是不可靠的。为了解决这一差距，我们提出了受限制的离散扩散（CDD），这是一种通过将离散扩散模型与可区分优化整合到自然语言上的新方法。与常规的文本生成器不同，通常依赖于事后过滤或模型重新进行可控生成，我们将施加约束直接在离散扩散采样过程中。我们说明了如何应用该技术来满足各种自然语言约束，包括（i）通过防止出现的有害含量来减轻毒性，（ii）特征和（iii）具有特定特性依从性的新型分子序列产生（iii）新型分子序列。实验结果表明，我们的约束过程在满足这些要求方面达到了高保真度，同时保持流利性和语义连贯性，表现优于自动回归和现有的离散扩散方法。]]></description>
      <guid>https://arxiv.org/abs/2503.09790</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>注意力揭示的不仅仅是令牌：通过注意引导检索的无训练的长篇文化推理</title>
      <link>https://arxiv.org/abs/2503.09819</link>
      <description><![CDATA[ARXIV：2503.09819V1公告类型：新 
摘要：大型语言模型（LLMS）通常比声称的能力要短得多，尤其是在处理需要从长篇小说的多个部分集成信息并执行多步推理的复杂推理任务时。尽管经过思考链（COT）提示已显示出在降低任务复杂性方面有希望的，但我们的经验分析表明，它无法完全解决此限制。通过受控的实验，我们确定对隐性事实的不良回忆是失败的主要原因，这显着妨碍了推理性能。有趣的是，我们观察到，即使没有明确召回这些事实，也可以有效地理解产生的COT令牌的内部注意力权重有效地理解隐性事实。在这种见解的基础上，我们提出了一种新颖的无培训算法，即归纳，该算法利用注意力的权重从漫长的上下文中检索相关事实，并将其纳入推理过程中。此外，我们发现从COT令牌中选择上下文令牌进一步提高了性能。我们的结果表明，具有各种模型的属性在合成和现实世界中的QA数据集上尤其增强了长篇下说推理能力。]]></description>
      <guid>https://arxiv.org/abs/2503.09819</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>低资源语言尼泊尔语中指定实体识别的生成AI</title>
      <link>https://arxiv.org/abs/2503.09822</link>
      <description><![CDATA[ARXIV：2503.09822V1公告类型：新 
摘要：生成人工智能（Genai），尤其是大语言模型（LLMS），具有明显高级的自然语言处理（NLP）任务，例如命名实体识别（NER），涉及识别诸如文本中的人，位置和组织名称之类的实体。对于低资源语言，LLM尤其有希望，因为它们能够从有限的数据中学习。但是，尚未对尼泊尔语的吉奈模型（一种低资源语言）的性能进行彻底评估。本文调查了最先进的LLM在尼泊尔的应用，并使用各种提示技术进行了实验以评估其有效性。我们的结果提供了对在低资源环境中使用LLM为NER使用LLM的挑战和机遇的见解，并为NLP研究的发展提供了宝贵的贡献。]]></description>
      <guid>https://arxiv.org/abs/2503.09822</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你在屏幕后面谁？使用人工智能的隐式MBTI和性别检测</title>
      <link>https://arxiv.org/abs/2503.09853</link>
      <description><![CDATA[ARXIV：2503.09853V1公告类型：新 
摘要：在个性化技术和心理学研究中，精确检测数字互动的人口特征和人格特征变得越来越重要。这项工作调查了隐式分类，直接从电报对话数据中的语言模式中推断性格和性别变量，而传统的人格预测技术主要取决于明确自我报告的标签。我们完善了基于变压器的语言模型（Roberta），以捕获复杂的语言提示，该模型使用包含1,602个用户注释的1,602个用户的138,866条消息的数据集和195,016条来自2,598个用户的消息，该数据集中有138,866条消息，来自2,598个用户。置信度水平有助于将模型准确性大大提高到86.16 \％，因此证明了罗伯塔能够从对话文本数据中始终如一地识别隐性人格类型的能力。我们的结果突出了变压器拓扑对于隐式性格和性别分类的有用性，因此强调了它们的效率，并强调了在现实的对话环境中的准确性和覆盖范围之间的重要权衡。关于性别分类，该模型的准确性为74.4 \％，因此捕获了特定于性别的语言模式。人格维度分析表明，具有内向和直观偏好的人在基于文本的互动中特别活跃。这项研究强调了平衡准确性和数据覆盖范围的实际问题，因为基于变压器的模型表明了它们在对话文本中的隐性人格和性别预测任务方面的效率。]]></description>
      <guid>https://arxiv.org/abs/2503.09853</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的领域是什么？用知识图和大语言模型绘制科学研究</title>
      <link>https://arxiv.org/abs/2503.09894</link>
      <description><![CDATA[ARXIV：2503.09894V1公告类型：新 
摘要：科学文献的指数增长使跨学科的知识越来越具有挑战性。大型语言模型（LLMS）是理解科学文本的强大工具，但它们无法捕获大型工作的详细关系。非结构化的方法，例如检索增强产生，可以通过此类语料库进行筛选以回顾相关事实。但是，当数百万事实影响答案时，非结构化的方法变得越来越高。结构化表示提供了自然的补充 - 在整个语料库中实现系统分析。最近的工作通过科学概念的非结构化或半结构化表示增强了LLM。为了补充这一点，我们尝试使用LLM提取结构化表示。通过将LLMS的语义理解与科学概念的模式相结合，我们原型制作了一个系统，该系统回答了整个文献的精确问题。我们的模式适用于跨科学领域，我们仅使用20个手动注释的摘要从中提取概念。为了展示系统，我们从30,000篇有关植入天体物理学，流体动力学和进化生物学的论文中提取概念。由此产生的数据库突出了新兴趋势，并通过可视化知识图，提供了探索不断增长的科学知识景观的新方法。演示：HF空间上的Abby101/Surveyor-0。代码：https：//github.com/chiral-carbon/kg-for-science。]]></description>
      <guid>https://arxiv.org/abs/2503.09894</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于规则的临床文本共同参考解决方案</title>
      <link>https://arxiv.org/abs/2503.09896</link>
      <description><![CDATA[ARXIV：2503.09896V1公告类型：新 
摘要：目的：这项研究的目的是建立一个针对生物医学领域量身定制的有效共同参考分辨率系统。材料和方法：本研究中使用的实验材料由2011 I2B2自然语言处理挑战提供。 2011 I2B2挑战涉及医疗文件中的核心解决方案。概念提到在临床文本中已经注释了，每个文档中共同参考的提及应通过核心链链接。通常，有两种方法可以自动发现共同指南的链接。一种是手动构建共同参考分辨率的规则，另一类方法是使用机器学习系统从培训数据集中自动学习，然后在测试数据集中执行分辨率任务。结果：实验表明，现有的共同参考分辨率系统能够找到一些共同指南的链接，而我们的基于规则的系统可以很好地找到大多数共同参考链接。我们的系统在多个医疗数据集上实现了89.6％的总体性能。结论：实验结果表明，基于观察训练数据的手动制作的规则是在关键生物医学领域完成此核心分辨率任务中高性能的有效方法。]]></description>
      <guid>https://arxiv.org/abs/2503.09896</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在电子保健记录系统中使用自然语言处理，开发和评估针对计划外的重症监护术的AI辅助预测模型</title>
      <link>https://arxiv.org/abs/2503.09927</link>
      <description><![CDATA[ARXIV：2503.09927V1公告类型：新 
摘要：简介：专业的神经密集型治疗部门（ITU）及时护理可降低死亡率和住院，而计划的招生比计划外的招生更安全。但是，术后护理决定仍然是主观的。这项研究使用人工智能（AI），特别是自然语言处理（NLP）来分析电子健康记录（EHRS）并预测针对选举手术患者的ITU入院。方法：这项研究分析了使用NLP的伦敦大学学院医院（UCLH）的选举神经外科患者的EHR。将患者分为计划的高依赖单位（HDU）或ITU入院；计划外的HDU或ITU入学；或病房 /过夜恢复（ONR）。医学概念注释工具（MEDCAT）用于识别临床注释中的SNOMED-CT概念。然后，我们探讨了这些确定的概念的实用性，用于一系列经过预测ITU录取的AI算法。结果：Cogstack-Medcat NLP模型最初在医院范围内接受了EHR的培训，进行了两种改进：首先是来自正常压力脑积水（NPH）的患者的数据，然后进行了来自前庭型鞘瘤（VS）患者的数据，从而达到了概念检测F1评分的F1评分为0.93。然后使用该精制模型从2,268名合格神经外科患者的EHR注释中提取概念。我们将提取的概念集成到AI模型中，包括决策树模型和神经时间序列模型。使用更简单的决策树模型，我们为ITU入学而获得了0.87（CI 0.82-0.91）的召回，将人类专家未计划的ITU案例的比例从36％降低到4％。结论：为准确性而完善的NLP模型已证明其在提取相关概念方面的效率，为预测性AI模型提供了可靠的基础，可用于临床有效的应用。]]></description>
      <guid>https://arxiv.org/abs/2503.09927</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>取消培训轮的进步性秘密学习以有效对齐</title>
      <link>https://arxiv.org/abs/2503.09958</link>
      <description><![CDATA[ARXIV：2503.09958V1公告类型：新 
摘要：最近的研究探索了文化学习（ICL）的工作机制。但是，他们主要关注分类和简单生成任务，将其更广泛的应用限制在实践中更复杂的一代任务中。为了解决这一差距，我们调查了示范对实践一致性任务中令牌表示的影响。我们发现，变压器嵌入了从演示的任务函数到分离器令牌表示中，该功能在生成先验响应令牌中起着重要作用。一旦确定了先前的响应令牌，示例就会变得多余。通过这一发现激发了这一发现，我们提出了一个由两个阶段组成的有效渐进性上下文对齐（PICA）方法。在前几个射击阶段，该模型通过标准ICL生成了几个先前的响应令牌，同时提取从分离器令牌表示中存储任务函数的ICL向量。在以下零拍摄阶段，该ICL向量指导该模型在没有进一步演示的情况下产生响应。扩展的实验表明，我们的PICA不仅超过了香草ICL，而且还可以达到与其他对齐调谐方法相当的性能。提出的无培训方法可减少时间成本（例如5.45+），并提高了对齐性能（例如6.57+）。因此，我们的工作强调了ICL在对齐中的应用，并要求对复杂一代的ICL进行更深入的了解。该代码将在https://github.com/hitsz-tmg/pica上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.09958</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用上下文改善单词细分</title>
      <link>https://arxiv.org/abs/2503.10023</link>
      <description><![CDATA[ARXIV：2503.10023V1公告类型：新 
摘要：了解儿童如何获取语言的重要步骤是研究婴儿如何学习单词细分。在先前的研究中已经确定，婴儿可以在语音中使用统计规律来学习单词细分。 Goldwater等人的研究表明，将上下文纳入模型可以提高其学习单词分割的能力。我们实施了他们的两个模型，即umigram和Bigram模型，以研究上下文如何改善统计单词分割。结果与我们的假设一致，即BigRAM模型在预测单词分割方面优于Unigram模型。扩展了Goldwater等人的工作，我们还探索了基本方法来模拟年幼的孩子如何使用以前学习的单词来细分新话语。]]></description>
      <guid>https://arxiv.org/abs/2503.10023</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MLLM基准的信息密度原理</title>
      <link>https://arxiv.org/abs/2503.10079</link>
      <description><![CDATA[ARXIV：2503.10079V1公告类型：新 
摘要：随着多模式大语言模型（MLLM）的出现，已经开发了数百个基准，以确保MLLM在下游任务中的可靠性。但是，评估机制本身可能并不可靠。对于MLLM的开发人员，有关使用哪种基准以及测试结果是否满足其要求的问题。因此，我们提出了信息密度的关键原则，该原则研究了基准可以为MLLM的发展提供多少洞察力。我们从四个关键维度来表征它：（1）谬误，（2）难度，（3）冗余，（4）多样性。通过对10,000多个样本的全面分析，我们测量了19 mllm基准的信息密度。实验表明，与以前的测试相比，在测试中使用最新的基准可以提供更多的见解，但是仍然可以提高其信息密度。我们希望这一原则可以促进未来MLLM基准的发展和应用。项目页面：https：//github.com/lcysyzxdxc/bench4bench]]></description>
      <guid>https://arxiv.org/abs/2503.10079</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么您的COT提示（不）？迅速空间复杂性的理论分析，与LLMS相当推理期间与答案空间的相互作用：经常性观点</title>
      <link>https://arxiv.org/abs/2503.10084</link>
      <description><![CDATA[ARXIV：2503.10084V1公告类型：新 
摘要：尽管大语言模型（LLMS）取得了显着的成功，但其基本变压器架构具有固有的理论局限性，这些局限性限制了其能够以增加计算复杂性来处理推理任务的能力。经过多个理论研究支持的实用解决方案已经出现了一项思维链（COT）的提示。但是，当前基于COT的方法（包括TOT，GOT等）通常使用固定的模板（例如，“思考逐步思考”）跨不同的推理任务采用“单次合适”策略。该方法迫使模型浏览一个极其复杂的提示空间，以识别有效的推理路径。当前的及时设计研究也很大程度上依赖于反复试验，而不是理论上知情的指导。在本文中，我们对两个关键空间之间的复杂性和相互作用进行了严格的理论分析：及时空间（潜在及时结构的空间）和在COT推理中产生的答案空间（LLMS产生的推理解决方案的空间）。我们证明了对单个通用提示的依赖（例如，逐步思考）如何对LLM的理论计算产生负面影响，这说明了提示复杂性直接影响答案空间中导航的结构和有效性。我们的分析强调，有时人类的监督对于有效地导航及时空间至关重要。从理论上讲，我们从经验上表明，特定于任务的提示明显优于无监督的及时生成，强调了在COT提示中进行周到的人类指导的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.10084</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>