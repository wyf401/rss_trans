<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 04 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>MALT：针对低资源语言（乌尔都语）的法学硕士（LLM）中有损翻译的机械消融</title>
      <link>https://arxiv.org/abs/2502.00041</link>
      <description><![CDATA[arXiv:2502.00041v1 公告类型：新
摘要：LLM 主要在英语数据上进行训练，这导致其在低资源语言上的性能显着下降。了解 LLM 如何处理这些语言对于提高其有效性至关重要。本研究以乌尔都语为用例，探索 LLM 在处理低资源语言时面临的挑战。当用另一种语言提示时，LLM 主要用英语推理，最后的几层充当翻译器，将英语响应转换为目标语言。本研究发现，即使对于低资源语言，LLM 在英语中的内部潜在响应也非常连贯；然而，翻译特征是有损的，导致翻译质量差，从而导致性能下降。通过机械地删除这些翻译特征并使用单独的翻译模型来翻译 LLM 的内部潜在响应，LLM 的性能显着提高，同时还保留了低资源语言输入的文化细微差别。]]></description>
      <guid>https://arxiv.org/abs/2502.00041</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于疾病预测的多层大型语言模型框架</title>
      <link>https://arxiv.org/abs/2502.00063</link>
      <description><![CDATA[arXiv:2502.00063v1 公告类型：新
摘要：社交远程医疗通过使患者能够远程分享症状并接受医疗咨询，彻底改变了医疗保健。用户经常在社交媒体和在线健康平台上发布症状，从而生成大量医疗数据，可用于疾病分类和症状严重程度评估。大型语言模型 (LLM)，例如 LLAMA3、GPT-3.5 Turbo 和 BERT，处理复杂的医疗数据以增强疾病分类。本研究探讨了三种阿拉伯语医学文本预处理技术：文本摘要、文本细化和命名实体识别 (NER)。使用 LoRA 评估 CAMeL-BERT、AraBERT 和 Asafaya-BERT，使用 NER 增强文本的 CAMeL-BERT 获得了最佳性能（83% 的类型分类，69% 的严重程度评估）。未微调的模型表现不佳（13%-20% 的类型分类，40%-49% 的严重程度评估）。将 LLM 融入社会远程医疗系统可提高诊断的准确性和治疗效果。]]></description>
      <guid>https://arxiv.org/abs/2502.00063</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BTS：将专业专家培养成通才法学硕士</title>
      <link>https://arxiv.org/abs/2502.00075</link>
      <description><![CDATA[arXiv:2502.00075v1 公告类型：新
摘要：我们提出了分支-训练-缝合 (BTS)，这是一种高效灵活的训练算法，用于将独立训练的大型语言模型 (LLM) 专家组合成一个功能强大的通用模型。按照 Li 等人的方法，我们从单个种子语言模型开始，该模型通过持续的预训练分支为特定领域（例如编码或数学）专家。BTS 使用轻量级缝合层将专家组合成通用模型，这些缝合层插入冻结专家和种子 LLM 之间，并在专家领域的小型数据混合上进行训练。缝合层使种子 LLM 能够在前向传递过程中整合来自任意数量专家的表示，使其能够推广到新领域，尽管保持冻结状态。由于 BTS 不会改变组成 LLM，因此 BTS 提供了一种模块化且灵活的方法：可以轻松删除专家，只需少量训练即可添加新专家。与其他模型合并方法相比，BTS 在各种下游任务中获得了最佳的通才性能，同时保留了每位专家的专业能力。]]></description>
      <guid>https://arxiv.org/abs/2502.00075</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于 Trie 的解码对大型语言模型进行高效的集束搜索</title>
      <link>https://arxiv.org/abs/2502.00085</link>
      <description><![CDATA[arXiv:2502.00085v1 公告类型：新 
摘要：在基于 Transformer 的序列到序列生成中，与贪婪解码相比，束搜索已被证明能有效提高生成序列的质量。传统的束搜索方法通常采用顺序或基于批处理的方法。顺序方法虽然内存效率高，但需要多次解码才能构建完整的搜索树，从而导致推理速度明显变慢。另一方面，基于批处理的方法可以实现跨束的并行计算，但由于需要为每个束维护单独的键值 (KV) 缓存，因此会消耗大量内存。在本研究中，我们介绍了一种基于 trie（前缀树）的新型并行解码方法，解决了基于批处理的束搜索的内存效率低下问题。通过在共享相同前缀的所有束之间共享单个 KV 缓存，所提出的方法不仅可以显着降低内存消耗，还可以实现跨所有分支的并行解码。这种前缀树的创新使用为束搜索提供了一种有效的替代方案，在保持推理速度的同时节省了大量的内存，使其特别适合内存受限的环境或大规模模型部署。]]></description>
      <guid>https://arxiv.org/abs/2502.00085</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>低秩专家适配器集合</title>
      <link>https://arxiv.org/abs/2502.00089</link>
      <description><![CDATA[arXiv:2502.00089v1 公告类型：新
摘要：大型语言模型 (LLM) 的训练和微调通常涉及来自多个来源的不同文本数据，这因梯度方向冲突而带来挑战，阻碍了优化和专业化。这些挑战可能会破坏跨任务的模型泛化，导致下游性能下降。最近的研究表明，在精心挑选的任务特定数据子集上微调 LLM 可以达到甚至超过使用整个数据集的性能。基于这些见解，我们提出了低秩专家适配器集成 (ELREA) 框架来提高模型处理不同任务的能力。ELREA 根据梯度方向对训练指令进行聚类，代表不同的专业领域，从而减少优化过程中的冲突。然后在这些集群上训练专家适配器，利用低秩自适应 (LoRA) 技术来确保训练效率和模型可扩展性。在推理过程中，ELREA 根据输入数据与训练集群的梯度相似性，结合来自最相关专家适配器的预测，确保为每个任务选择最佳适配器。实验表明，我们的方法优于在完整数据集上训练的基准 LoRA 适配器以及在一系列特定领域任务中具有类似训练和推理复杂度的其他集成方法。]]></description>
      <guid>https://arxiv.org/abs/2502.00089</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>消除数字序列歧义以解读古代会计语料库</title>
      <link>https://arxiv.org/abs/2502.00090</link>
      <description><![CDATA[arXiv:2502.00090v1 公告类型：新
摘要：数字系统将抽象的数字量编码为具体的书面字符串。现代文字使用的数字系统往往是精确和明确的，但对于古代和部分破译的原始埃兰 (PE) 文字来说并非如此，其中的书面数字可以有多达四种不同的读法，具体取决于用于读取它们的系统。我们考虑消除这些读法之间的歧义，以确定此语料库中记录的数字量的值。我们通过算法提取每个 PE 数字符号的可能读法列表，并根据原始文档的结构属性和使用引导算法学习的分类器提供两种消歧技术。我们还提供了一个用于评估消歧技术的测试集，以及一种用于谨慎选择引导分类器的规则的新方法。我们的分析证实了关于此文字的现有直觉，并揭示了之前未知的石板内容与数字大小之间的相关性。这项工作对于理解和解读 PE 至关重要，因为该语料库主要侧重于会计，并且包含的​​数字标记比文本标记多得多。]]></description>
      <guid>https://arxiv.org/abs/2502.00090</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏自动编码器对语音嵌入的见解</title>
      <link>https://arxiv.org/abs/2502.00127</link>
      <description><![CDATA[arXiv:2502.00127v1 公告类型：新
摘要：可解释机器学习的最新进展凸显了稀疏自动编码器在发现密集编码嵌入中的单语义特征方面的潜力。虽然大多数研究都集中在大型语言模型 (LLM) 嵌入上，但这种技术在其他领域的适用性仍未得到充分探索。本研究将稀疏自动编码器应用于 Titanet 模型生成的说话人嵌入，证明了该技术在从非文本嵌入数据中提取单语义特征方面的有效性。结果表明，提取的特征表现出与 LLM 嵌入中发现的特征相似的特征，包括特征分割和转向。分析表明，自动编码器可以识别和操纵语言和音乐等特征，而这些特征在原始嵌入中并不明显。研究结果表明，稀疏自动编码器可以成为理解和解释许多领域（包括基于音频的说话人识别）嵌入数据的有力工具。]]></description>
      <guid>https://arxiv.org/abs/2502.00127</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于大型语言模型情境感知伦理协调的三分支制衡框架</title>
      <link>https://arxiv.org/abs/2502.00136</link>
      <description><![CDATA[arXiv:2502.00136v1 公告类型：新
摘要：本文介绍了一种受政府系统启发的大型语言模型 (LLM) 道德协调的三分支制衡框架。它实现了三个独立但相互作用的组成部分：LLM 作为知识生成的行政部门，DIKE 作为建立道德护栏的立法部门，ERIS 作为情境解释的司法部门。对抗性的 DIKE-ERIS 对偶性能够适应不同的文化背景，同时坚持一致的道德原则。该架构通过提供可解释、可适应和具有文化意识的道德推理来解决强化学习与人类反馈 (RLHF) 的局限性。通过自我监督学习和对抗性测试，我们的框架展示了情感建模如何引导语言行为朝着道德结果发展，同时在知识生成、道德监督和情境解释方面保持独立性。]]></description>
      <guid>https://arxiv.org/abs/2502.00136</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解决编辑-取消学习冲突：用于大型语言模型更新的知识码本框架</title>
      <link>https://arxiv.org/abs/2502.00158</link>
      <description><![CDATA[arXiv:2502.00158v1 公告类型：新
摘要：大型语言模型 (LLM) 通过编码大量人类知识在自然语言处理方面表现出色，但它们的实用性依赖于知识发展过程中的及时更新。更新 LLM 同时涉及两个关键任务：取消学习以删除不需要的知识和编辑以合并新信息。现有方法面临两大挑战：知识存储效率低下（太稀疏或太密集）以及编辑和取消学习之间的任务冲突，这已通过我们的理论和实验结果得到验证。为了解决这些问题，我们提出了 LOKA，这是一个基于知识码本的无冲突 LLM 更新框架。在训练期间，更新的知识存储在多个码本存储器中。为了优化知识存储，相似性感知知识映射可确保相关知识片段聚类并分配给同一存储器。此外，LOKA 通过使用由冲突分数指导的任务特定和多任务存储器来解决任务冲突。在推理阶段，LOKA 从代码本中检索最相关的记忆并将其插入原始 LLM 以应用更新的知识。基于学习的路由器控制代码本激活以进一步提高知识利用率。大量实验证明了 LOKA 在 LLM 知识更新任务中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.00158</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型训练中的上下文保留张量重构</title>
      <link>https://arxiv.org/abs/2502.00246</link>
      <description><![CDATA[arXiv:2502.00246v1 公告类型：新
摘要：由于计算限制和低效的上下文保留机制，处理神经架构中的长距离依赖关系一直是一个持续的挑战。张量操作为重构模型表示提供了基础，但传统架构一直在努力在不引入过多复杂性的情况下融入此类技术。一种新方法，即上下文保留张量重构 (CPTR)，通过结构化分解和自适应收缩实现权重张量的动态重组，从而可以在不增加大量计算开销的情况下增强上下文集成。实证评估表明，CPTR 可改善扩展序列之间的连贯性保留，从而显著降低困惑度并提高长上下文任务的回忆准确度。性能比较表明，CPTR 增强模型表现出更高的计算效率和更低的内存消耗，同时保持了具有竞争力的语言生成流畅性和准确性。梯度稳定性指标进一步验证了改进的训练效率，揭示了权重更新中更可控的方差。对基线模型和 CPTR 增强模型的比较研究证实，张量重构有助于实现更稳定、计算效率更高的语言建模。研究结果支持 CPTR 在改进当代神经架构以完成需要长期语境理解和高效记忆利用的任务方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.00246</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学推理中验证者引导搜索的扩展缺陷</title>
      <link>https://arxiv.org/abs/2502.00271</link>
      <description><![CDATA[arXiv:2502.00271v1 公告类型：新
摘要：大型语言模型 (LLM) 难以进行多步推理，其中推理时间扩展已成为一种有前途的性能改进策略。当样本量受到选择和优先考虑有效推理路径的限制时，验证器引导搜索优于重复抽样。然而，我们发现了一个关键的限制：扩展缺陷，在不同模型（Mistral 7B 和 DeepSeekMath 7B）、基准（GSM8K 和 MATH）和验证器（结果价值模型和过程奖励模型）中普遍存在。随着样本量的增加，验证器引导搜索的优势逐渐减弱，最终不如重复抽样。我们的分析将此归因于验证器故障，其中不完美的验证器会错误地对候选者进行排名并错误地修剪所有有效路径。这些问题在具有挑战性和分布不均的问题中进一步加剧，从而限制了搜索的有效性。为了减轻验证者失败的风险，我们探索减少对验证者的依赖，并使用两种简单的方法进行初步调查。我们的研究结果揭示了验证者引导搜索的基本局限性，并提出了未来的发展方向。]]></description>
      <guid>https://arxiv.org/abs/2502.00271</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Logits 估计 LLM 不确定性</title>
      <link>https://arxiv.org/abs/2502.00290</link>
      <description><![CDATA[arXiv:2502.00290v1 公告类型：新
摘要：近年来，大型语言模型 (LLM) 取得了显著的进步，并已广泛应用于各个领域。尽管取得了进展，但 LLM 容易产生幻觉，如果模型缺乏足够的基础知识，则产生的响应可能不可靠。为了缓解这个问题，已经采用了估计不确定性的方法，重点关注关键标记作为可靠性指标。然而，基于概率的方法在评估标记级可靠性方面表现出局限性，因为在训练期间获得的证据强度信息会受到侵蚀。在本文中，我们介绍了 Logits 诱导的标记不确定性 (LogU)，这是一种新颖的框架，旨在实时估计 LLM 中特定于标记的不确定性，而无需多次采样。通过利用证据建模来实现 LogU，我们利用派生的不确定性度量来指导下游任务。我们的实验结果突出了 LogU 的实质性有效性和潜力，标志着在解决模型幻觉挑战方面取得了重大进展。]]></description>
      <guid>https://arxiv.org/abs/2502.00290</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ChunkKV：用于高效长上下文 LLM 推理的语义保留 KV 缓存压缩</title>
      <link>https://arxiv.org/abs/2502.00299</link>
      <description><![CDATA[arXiv:2502.00299v1 公告类型：新
摘要：为了减少使用大型语言模型 (LLM) 进行长上下文推理的内存成本，许多最近的工作都集中在压缩不同标记的键值 (KV) 缓存上。然而，我们发现以前的 KV 缓存压缩方法单独测量标记重要性，忽略了现实世界语言特征中不同标记之间的依赖关系。鉴于此，我们引入了 ChunkKV，将块中的标记分组为基本压缩单元，并保留最具信息量的语义块，同时丢弃不太重要的语义块。此外，观察到 ChunkKV 在不同层之间保留的索引表现出更高的相似性，我们提出了逐层索引重用以进一步减少计算开销。我们在包括 LongBench 和 Needle-In-A-HayStack 在内的尖端长上下文基准以及 GSM8K 和 JailbreakV 上下文学习基准上评估了 ChunkKV。我们对指令调整和多步推理（O1 和 R1）LLM 进行的实验，与现有方法相比，在积极的压缩比下实现了高达 10% 的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2502.00299</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的语境形态发生：一种自组织标记表示的新方法</title>
      <link>https://arxiv.org/abs/2502.00301</link>
      <description><![CDATA[arXiv:2502.00301v1 公告类型：新
摘要：标记表示会影响语言模型的效率和适应性，但传统的标记化策略会施加严格的分割边界，而这些边界不会根据不断变化的上下文关系进行动态调整。上下文形态发生的引入建立了一种自组织机制，该机制根据学习到的上下文依赖关系重新构建标记边界，从而允许嵌入在迭代处理步骤中逐步发展。实证评估表明，动态调整的标记化有助于减少困惑度，同时保持表示稳定性，特别是在语言复杂领域，静态分割无法捕捉细微的依赖关系。与自组织标记结构相关的计算权衡表明，只要优化策略考虑到分割更新效率，额外的处理开销仍然在可行的范围内。对不同语言语料库的比较评估表明，自适应标记化在提高与上下文线索的一致性的同时保留了可解释性，增​​强了形态发生分割机制提高预测准确性的潜力。稳定性分析证实，不断发展的标记结构在不同的文本分布中保持一致的分割行为，从而确保表征适应在语言上保持连贯性。语境形态发生在改善结构稳定性和预测性能方面的有效性凸显了其作为传统标记化方法的替代方案的可行性。对计算效率考虑因素的进一步分析表明，集成静态和动态分割技术的混合策略可以提供一种平衡的方法来优化表征灵活性，同时保持推理效率。]]></description>
      <guid>https://arxiv.org/abs/2502.00301</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DEUCE：冷启动主动学习的双重多样性增强和不确定性意识</title>
      <link>https://arxiv.org/abs/2502.00305</link>
      <description><![CDATA[arXiv:2502.00305v1 公告类型：新
摘要：冷启动主动学习 (CSAL) 从未标记的数据集中选择有价值的实例进行手动注释。它以较低的注释成本为标签稀缺的文本分类提供高质量的数据。然而，现有的 CSAL 方法忽略了弱类和难以代表的例子，导致学习出现偏差。为了解决这些问题，本文提出了一种用于 CSAL 的新型双重多样性增强和不确定性感知 (DEUCE) 框架。具体来说，DEUCE 利用预训练语言模型 (PLM) 来有效地提取文本表示、类别预测和预测不确定性。然后，它构建了一个双邻域图 (DNG) 来结合文本多样性和类别多样性的信息，确保数据分布平衡。它进一步通过基于密度的聚类传播不确定性信息以选择难以代表的实例。DEUCE 在通过双重多样性和信息量选择类别平衡和难以代表的数据方面表现良好。在六个 NLP 数据集上的实验证明了 DEUCE 的优越性和效率。]]></description>
      <guid>https://arxiv.org/abs/2502.00305</guid>
      <pubDate>Tue, 04 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>