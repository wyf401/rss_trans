<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>语音数据在减少毒性检测偏差中的作用</title>
      <link>https://arxiv.org/abs/2411.08135</link>
      <description><![CDATA[arXiv:2411.08135v1 公告类型：新
摘要：文本毒性检测系统表现出明显的偏见，在提及人口统计群体的样本上产生不成比例的假阳性率。但是语音中的毒性检测呢？为了研究基于语音的系统在多大程度上减轻了基于文本的偏见，我们为多语言 MuTox 数据集制作了一组高质量的组注释，然后利用这些注释系统地比较基于语音和文本的毒性分类器。我们的研究结果表明，在推理过程中访问语音数据有助于减少对群体提及的偏见，特别是对于模棱两可和引起分歧的样本。我们的结果还表明，改进分类器而不是转录管道更有助于减少群体偏见。我们公开发布了我们的注释并为未来的毒性数据集构建提供了建议。]]></description>
      <guid>https://arxiv.org/abs/2411.08135</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型可以在长上下文推理中自我改进</title>
      <link>https://arxiv.org/abs/2411.08147</link>
      <description><![CDATA[arXiv:2411.08147v1 公告类型：新
摘要：大型语言模型 (LLM) 在处理长上下文方面取得了实质性进展，但在长上下文推理方面仍然存在困难。现有方法通常涉及使用合成数据对 LLM 进行微调，这取决于人类专家或 GPT-4 等高级模型的注释，从而限制了进一步的发展。为了解决这个问题，我们研究了 LLM 在长上下文推理中自我改进的潜力，并提出了专门为此设计的方法 \ours。这种方法很简单：我们对每个问题抽取多个输出，用最小贝叶斯风险对它们进行评分，然后根据这些输出应用监督微调或偏好优化。在几个领先的 LLM 上进行的大量实验证明了 \ours 的有效性，Llama-3.1-8B-Instruct 的绝对改进为 $4.2$ 分。此外，与依赖人类专家或高级模型生成的数据的先前方法相比，我们的方法取得了卓越的性能。我们预计这项工作将为长上下文场景中的自我改进技术开辟新途径，这对于 LLM 的持续发展至关重要。]]></description>
      <guid>https://arxiv.org/abs/2411.08147</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越安全包：审计有用和无害的数据集</title>
      <link>https://arxiv.org/abs/2411.08243</link>
      <description><![CDATA[arXiv:2411.08243v1 公告类型：新
摘要：为了减轻大型语言模型 (LLM) 的危害，人们已经使用从人类反馈中学习 (LHF) 来引导 LLM 实现既危害更小又更有帮助的输出。尽管 LHF 在实践中被广泛采用，但这种反馈的质量及其作为安全缓解技术的有效性仍不清楚。本研究通过审核 Anthropic 广泛使用的有用和无害 (HH) 数据集来解决这些问题。我们的工作包括：(1) 通过手动和自动评估彻底调查数据集的内容；(2) 实验证明数据集对模型安全性的影响；(3) 分析引用该数据集的 100 篇最具影响力的论文。通过我们的审核，我们展示了 HH 数据集中发现的概念化失败和质量问题如何导致不同人口群体的安全行为不同，从而造成额外的危害。我们的研究结果强调，需要采取更细致入微、更情境敏感的方法来降低 LLM 的安全风险。]]></description>
      <guid>https://arxiv.org/abs/2411.08243</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>欺骗问答模型：一种混合词级对抗方法</title>
      <link>https://arxiv.org/abs/2411.08248</link>
      <description><![CDATA[arXiv:2411.08248v1 公告类型：新
摘要：深度学习支撑了目前大多数先进的自然语言处理 (NLP) 任务，例如文本分类、神经机器翻译 (NMT)、抽象摘要和问答 (QA)。然而，模型（尤其是 QA 模型）对抗对抗攻击的鲁棒性是一个关键问题，但尚未得到充分探索。本文介绍了 QA-Attack（问答攻击），这是一种欺骗 QA 模型的新型词级对抗策略。我们基于注意力的攻击利用定制的注意力机制和删除排名策略来识别和定位上下文段落中的特定单词。它通过仔细选择和替换同义词来创建欺骗性输入，在误导模型产生错误响应的同时保留语法完整性。我们的方法在各种问题类型中都表现出多功能性，特别是在处理大量长文本输入时。在多个基准数据集上进行的大量实验表明，QA-Attack 成功欺骗了基线 QA 模型，并在成功率、语义变化、BLEU 分数、流畅度和语法错误率方面超越了现有的对抗技术。]]></description>
      <guid>https://arxiv.org/abs/2411.08248</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>支持处理网络新闻的大型语言模型的知识库</title>
      <link>https://arxiv.org/abs/2411.08278</link>
      <description><![CDATA[arXiv:2411.08278v2 公告类型：新 
摘要：大型语言模型（LLM）最近在广泛应用中引起了广泛关注。在通过海量数据集进行预训练期间，这种模型会在其隐藏参数中隐式地记忆训练数据集的事实知识。然而，由于缺乏常识推理，参数中隐含的知识通常会使下游应用程序无法有效使用它们。在本文中，我们介绍了一个通用框架，该框架允许借助 LLM 构建知识库，专门用于处理网络新闻。该框架将基于规则的新闻信息提取器（NewsIE）应用于新闻项目以提取其关系元组（称为知识库），然后将其与 LLM 获得的新闻项目的隐性知识事实进行图卷积，以进行分类。它涉及两个轻量级组件：1）NewsIE：用于以关系元组的形式提取每个新闻项目的结构信息； 2）BERTGraph：用于将隐性知识事实与 NewsIE 提取的关系元组进行图卷积。我们在不同的新闻相关数据集下对我们的框架进行了新闻类别分类评估，并获得了令人满意的实验结果。]]></description>
      <guid>https://arxiv.org/abs/2411.08278</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>R3HF：通过奖励重新分配增强从人类反馈中学习的能力</title>
      <link>https://arxiv.org/abs/2411.08302</link>
      <description><![CDATA[arXiv:2411.08302v1 公告类型：新
摘要：从人类反馈中进行强化学习 (RLHF) 为将大型语言模型 (LLM) 与人类偏好相结合提供了一种范例。这涉及基于成对人类反馈的奖励模型的初始训练。随后，奖励模型用于强化学习，以评估每个生成的句子的整体分数，进一步指导 LLM 的优化。然而，当前的方法有一个显著的缺点：\emph{它们将单一、稀疏和延迟的奖励分配给整个输出序列}。这可能会忽略每个 token 对期望结果的一些重要的个人贡献。为了克服这一限制，我们的论文提出了一种称为 R3HF 的新型奖励重新分配方法，该方法有助于实现更细粒度的 token 级奖励分配。具体而言，我们的方法将奖励模型的奖励预测任务视为回归问题。因此，通过评估每个 token 对奖励模型输出的具体贡献来计算重新分配的奖励。这种详细的方法提高了模型对语言细微差别的理解，从而更精确地提高了其性能。我们的方法旨在与大多数当前技术无缝集成，同时将计算成本降至最低。通过对各种数据集和任务的全面实验，我们验证了我们方法的有效性和优越性。]]></description>
      <guid>https://arxiv.org/abs/2411.08302</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士有先见之明吗？以每日新闻为预测的持续评估</title>
      <link>https://arxiv.org/abs/2411.08324</link>
      <description><![CDATA[arXiv:2411.08324v1 公告类型：新
摘要：由于新模型和训练数据的出现，许多现有的大型语言模型 (LLM) 评估基准很快就过时了。这些基准还无法评估 LLM 性能随时间的变化，因为它们由没有时间维度的静态问题组成。为了解决这些限制，我们建议使用未来事件预测作为一种持续评估方法来评估 LLM 的时间泛化和预测能力。我们的基准 Daily Oracle 会自动从每日新闻中生成问答 (QA) 对，挑战 LLM 预测“未来”事件结果。我们的研究结果表明，随着预训练数据的过时，LLM 性能会随着时间的推移而下降。虽然检索增强生成 (RAG) 有可能提高预测准确性，但性能下降模式仍然存在，凸显了持续更新模型的必要性。]]></description>
      <guid>https://arxiv.org/abs/2411.08324</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用基于 Transformer 的标记分类进行孟加拉语语法错误检测</title>
      <link>https://arxiv.org/abs/2411.08344</link>
      <description><![CDATA[arXiv:2411.08344v1 公告类型：新
摘要：孟加拉语是世界上使用人数第七多的语言，但开发这种语言的自动语法检查器是一个研究不足的问题。孟加拉语语法错误检测是一项检测孟加拉语文本中包含语法、标点或拼写错误的子字符串的任务，这对于开发自动孟加拉语打字助手至关重要。我们的方法包括将任务分解为标记分类问题并利用最先进的基于变压器的模型。最后，我们结合这些模型的输出并应用基于规则的后处理来生成更可靠、更全面的结果。我们的系统在一个包含来自不同来源的 25,000 多篇文本的数据集上进行评估。我们最好的模型实现了 1.04 的 Levenshtein 距离得分。最后，我们对系统的不同组件进行了详细分析。]]></description>
      <guid>https://arxiv.org/abs/2411.08344</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用法学硕士 (LLM) 完善翻译：一种约束感知迭代提示方法</title>
      <link>https://arxiv.org/abs/2411.08348</link>
      <description><![CDATA[arXiv:2411.08348v1 公告类型：新
摘要：大型语言模型 (LLM) 已经展示了其在机器翻译 (MT) 方面的卓越能力，即使没有针对相关语言进行过专门的训练。然而，在资源匮乏或特定领域的背景下翻译罕见词汇对 LLM 来说仍然具有挑战性。为了解决这个问题，我们提出了一个多步骤提示链，通过优先考虑对语义准确性至关重要的关键术语来提高翻译的忠实度。我们的方法首先识别这些关键词并从双语词典中检索它们的翻译，然后使用检索增强生成 (RAG) 将它们集成到 LLM 的上下文中。我们通过迭代自检机制进一步减轻了长提示引起的潜在输出幻觉，其中 LLM 根据词汇和语义约束来改进其翻译。在 FLORES-200 和 WMT 数据集上使用 Llama 和 Qwen 作为基础模型进行的实验表明，与基线相比有显著的改进，凸显了我们的方法在增强翻译忠实度和稳健性方面的有效性，特别是在资源匮乏的情况下。]]></description>
      <guid>https://arxiv.org/abs/2411.08348</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释的句法表示支持分层词向量</title>
      <link>https://arxiv.org/abs/2411.08384</link>
      <description><![CDATA[arXiv:2411.08384v1 公告类型：新
摘要：当前使用的分布式表示密集且不可解释，导致解释本身是相对的、过完备的且难以解释。我们提出了一种将这些词向量转换为简化句法表示的方法。得到的表示紧凑且可解释，可以更好地可视化和比较词向量，并且我们相继证明绘制的解释符合人类判断。然后使用句法表示创建分层词向量，使用类似于人类学习的分层方面的增量学习方法。由于这些表示是从预训练向量中提取的，因此生成过程和学习方法在计算上是高效的。最重要的是，我们发现句法表示提供了对向量的合理解释，并且后续的分层向量在基准测试中优于原始向量。]]></description>
      <guid>https://arxiv.org/abs/2411.08384</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CLaSP：通过自然语言监督学习时间序列信号的概念</title>
      <link>https://arxiv.org/abs/2411.08397</link>
      <description><![CDATA[arXiv:2411.08397v1 公告类型：新
摘要：本文提出了一种名为“CLaSP”的基础模型，可以使用自然语言搜索时间序列信号，将信号的特征描述为查询。以前用自然语言表示时间序列信号数据的努力在设计传统的时间序列信号特征类、制定量化方法和创建同义词词典方面遇到了挑战。为了克服这些限制，所提出的方法引入了基于对比学习的神经网络。该网络首先使用数据集 TRUCE 和 SUSHI 进行训练，数据集由时间序列信号及其相应的自然语言描述组成。先前的研究提出了数据分析师用来描述信号特征的词汇表，而 SUSHI 的设计就是为了涵盖这些术语。我们相信，在这些数据集上训练的神经网络将使数据分析师能够使用自然语言词汇进行搜索。此外，我们的方法不需要预定义的同义词词典，它利用嵌入在大规模语言模型 (LLM) 中的常识知识。实验结果表明，CLaSP 可以实现时间序列信号数据的自然语言搜索，并能准确学习信号数据的变化点。]]></description>
      <guid>https://arxiv.org/abs/2411.08397</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一步一步来：语言代理是分步规划者</title>
      <link>https://arxiv.org/abs/2411.08432</link>
      <description><![CDATA[arXiv:2411.08432v1 公告类型：新
摘要：语言代理在动态环境中表现出良好的适应性，可以执行复杂的任务。然而，尽管大型语言模型中嵌入了多种知识，但这些代理在需要规划的任务方面仍然不足。我们引入了 STEP，这是一个新颖的框架，旨在有效地从以前的经验中学习，以增强语言代理在未来步骤中的规划能力。具体来说，STEP 通过四个相互关联的组件发挥作用。首先，规划器承担任务，将其分解为子任务并提供相关见解。然后，执行器生成操作候选，而评估器确保操作与从以前的经验中学习到的规则一致。最后，内存存储经验以指导未来的决策。在 ScienceWorld 基准测试中，我们的结果表明 STEP 始终优于最先进的模型，总分为 67.4，成功完成了 18 项任务中的 12 项。这些发现凸显了 STEP 作为增强语言代理的规划能力的框架的潜力，为在动态环境中更复杂的任务解决铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2411.08432</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用 LLM 增强型分层注意力网络实现客观、公正的决策评估</title>
      <link>https://arxiv.org/abs/2411.08504</link>
      <description><![CDATA[arXiv:2411.08504v2 公告类型：新
摘要：我们在做决定时有多客观和公正？这项工作研究了人类专家在高风险决策过程中对认知偏见的识别，质疑其在现实环境中的有效性，例如大学录取的候选人评估。我们首先进行统计分析，评估当前过程中不同决策点之间的相关性，发现差异意味着认知偏见和决策不一致。这促使我们探索超越人类判断的偏见感知人工智能增强工作流程。我们提出了 BGM-HAN，一种具有字节对编码、门控残差连接和多头注意力的增强型分层注意力网络。使用它作为骨干模型，我们进一步提出了一个候选名单-分析-推荐 (SAR) 代理工作流程，模拟现实世界的决策。在我们的实验中，所提出的模型和代理工作流程都显著改善了人类判断和替代模型，并通过真实世界数据进行了验证。]]></description>
      <guid>https://arxiv.org/abs/2411.08504</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>树表：释放 LLM 的力量，增强对大规模表格的理解</title>
      <link>https://arxiv.org/abs/2411.08516</link>
      <description><![CDATA[arXiv:2411.08516v1 公告类型：新
摘要：表格作为跨各个领域的半结构化数据，其普遍性和价值需要先进的方法来理解其复杂性和大量信息。尽管大型语言模型 (LLM) 在推进自然语言理解前沿方面具有令人印象深刻的能力，但它们在大规模表格数据中的应用仍面临重大挑战，特别是在表格大小和复杂的错综复杂的关系方面。现有的研究已经显示出对小规模表格的前景，但在处理现实世界场景中较大、相互关联的表格所需的复杂推理时，它们往往会陷入困境。为了解决这一差距，我们引入了“Tree-of-Table”，这是一种旨在增强 LLM 对大型复杂表格的推理能力的新方法。我们的方法采用表格压缩和分解来将相关数据提炼和重新组织为可管理的格式，然后构建一个分层的表树以促进树结构推理。通过严谨的 Table-Tree 执行流程，我们系统地解开树形推理链，从而得出解决方案。在 WikiTQ、TableFact、FeTaQA 和 BIRD 等不同数据集上的实验表明，Tree-of-Table 以优异的性能树立了新的标杆，在大规模表格推理中展现了卓越的效率和泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2411.08516</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型进行神经主题建模</title>
      <link>https://arxiv.org/abs/2411.08534</link>
      <description><![CDATA[arXiv:2411.08534v1 公告类型：新
摘要：主题建模是自然语言处理中的一项基本任务，允许发现文本语料库中的潜在主题结构。虽然大型语言模型 (LLM) 在主题发现方面表现出良好的能力，但它们在主题建模中的直接应用存在诸如主题覆盖不完整、主题错位和效率低下等问题。为了解决这些限制，我们提出了 LLM-ITL，这是一种新颖的 LLM-in-the-loop 框架，它将 LLM 与许多现有的神经主题模型 (NTM) 集成在一起。在 LLM-ITL 中，通过 NTM 学习全局主题和文档表示，而 LLM 通过基于置信度加权的最佳传输 (OT) 的对齐目标细化主题。此过程增强了所学习主题的可解释性和连贯性，同时保持了 NTM 的效率。大量实验表明，LLM-ITL 可以帮助 NTM 显著提高其主题可解释性，同时保持文档表示的质量。]]></description>
      <guid>https://arxiv.org/abs/2411.08534</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>