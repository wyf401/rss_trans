<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>认知网络突出了人和LLM模拟的受训者，专家和学术界的STEM心态的差异和相似之处</title>
      <link>https://arxiv.org/abs/2502.19529</link>
      <description><![CDATA[ARXIV：2502.19529V1公告类型：新 
摘要：理解对STEM的态度意味着量化个人以及潜在的大语言模型的认知和情感方式，概念化了此类主题。这项研究使用行为形式网络（BFMN）来研究177名人类参与者和177名由GPT-3.5模拟的人为人类的人类参与者的关联和感知思想的方式，即关联和感知思想的方式。参与者分为3组（学员，专家和学者），以比较专业水平对他们的心态的影响。结果表明，与GPT-3.5相比，人类形式网络表现出明显更高的聚类系数，这表明人的心态在回顾STEM思想的同时表现出形成和紧密的概念关联的趋势。尤其是人类专家表现出强大的聚类系数，反映了STEM概念更好地整合到其认知网络中。相反，GPT-3.5产生了更稀疏的心态。此外，人类和GPT心态都以中立或积极的方式构建数学，与STEM高中生，研究人员和其他在其他作品中采样的大型语言模型不同。这项研究有助于了解心态结构如何提供有关记忆结构和机器限制的认知见解。]]></description>
      <guid>https://arxiv.org/abs/2502.19529</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>赢得小型型号的胜利：知识蒸馏与自我训练以减少质量检查代理的幻觉</title>
      <link>https://arxiv.org/abs/2502.19545</link>
      <description><![CDATA[ARXIV：2502.19545V1公告类型：新 
摘要：通过幻觉生成虚假信息以及专有模型的高成本，大型语言模型（LLM）在客户支持中的部署受到限制。为了应对这些挑战，我们提出了一个检索提出的提问（QA）管道，并探索如何平衡人类的投入和自动化。使用有关三星智能电视用户手册的问题的数据集，我们证明了LLMS生成的合成数据优于众包数据，从而减少了易元模型中的幻觉。我们还比较了自我训练（对自己的输出的微调模型）和知识蒸馏（对更强模型的输出进行微调，例如GPT-4O），并发现自训练可以实现可比的幻觉减少。我们猜想这一令人惊讶的发现可以归因于知识蒸馏案中的暴露偏见问题增加，并通过事后分析来支持这一猜想。我们还通过上下文化的“我不知道”回答来提高无法回答的问题和检索失败的鲁棒性。这些发现表明，可以使用综合数据和开源模型进行自我培训来构建可扩展的，具有成本效益的质量检查系统，从而减少对专有工具的依赖或昂贵的人类注释。]]></description>
      <guid>https://arxiv.org/abs/2502.19545</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当大型语言模型符合语音时：一项关于整合方法的调查</title>
      <link>https://arxiv.org/abs/2502.19548</link>
      <description><![CDATA[ARXIV：2502.19548V1公告类型：新 
摘要：大语言模型（LLM）的最新进展激发了人们对将其应用扩展到基于文本的任务之外的兴趣。大量研究探索了将其他模式与LLM的融合，特别是与文本有关的语音方式。本文调查了语音与LLM的集成，将方法分类为三种主要方法：基于文本的，基于潜在的基于主代表和基于音频的集成。我们还展示了如何在各种语音相关的应用程序中应用这些方法，并强调了该领域的挑战以提供灵感]]></description>
      <guid>https://arxiv.org/abs/2502.19548</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>蒸馏不仅数据，而且还奖励：较小的语言模型能否超越较大的语言模型？</title>
      <link>https://arxiv.org/abs/2502.19557</link>
      <description><![CDATA[ARXIV：2502.19557V1公告类型：新 
摘要：蒸馏大型语言模型（LLM）通常涉及通过监督的微调（SFT）转移教师模型的响应。但是，这种方法忽略了提炼数据（输出内容）和奖励信号（质量评估）的潜力。直接从教师模型中提取可靠的奖励信号是具有挑战性的，因为LLM是针对生成而不是评估的优化，通常会导致偏见或不一致的评估。为了解决这一局限性，我们提出了一种新型的蒸馏管道，可以转移反应和奖励。我们的方法通过自我监督的机制生成伪奖励，该机制利用教师和学生的反应的固有结构，在没有明确的外部评估的情况下实现奖励学习。奖励模型随后指导增强学习（RL），从而在SFT热身阶段迭代了学生模型。 GSM8K和MMLU-PRO的实验表明，我们的方法始终超过传统的基于SFT的方法，从而使学生模型能够超过其教师的表现。这项工作强调了通过结构化的自我监督奖励学习进行可扩展，有效蒸馏的潜力，从而减少了对外部奖励监督的依赖。]]></description>
      <guid>https://arxiv.org/abs/2502.19557</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>保持专注：多代理辩论中的问题漂移</title>
      <link>https://arxiv.org/abs/2502.19559</link>
      <description><![CDATA[ARXIV：2502.19559V1公告类型：新 
摘要：多代理辩论 - 大型语言模型的多个实例讨论基于互动的问题 - 显示了解决知识和推理任务的希望。但是，这些方法显示出局限性，尤其是在将它们扩展到更长的推理链时。在这项研究中，我们揭开了新的多项式辩论的新期：讨论从多个回合中脱离了最初的问题。我们将这一现象定义为问题漂移，并在十个任务中量化其存在（即三个生成，三个知识，三个推理和一项跟随指令的任务）。为了确定此问题的原因，我们对八名专家进行了有关问题漂移的讨论的研究，他们发现最常见的问题是缺乏进展（35％的案件），低质量反馈（占案件的26％）和缺乏明确性（25％的案件）。为了系统地解决问题漂移的问题，我们提出了基于LLM-AS-A-Gudge的方法Drifthenge，以检测测试时间的问题漂移。我们进一步提出了漂流液，这是减轻31％问题漂移案例的方法。我们的研究可以看作是理解多代理辩论的关键局限性的第一步，突出了提高未来有效性的途径。]]></description>
      <guid>https://arxiv.org/abs/2502.19559</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型知道他们知道多少吗？</title>
      <link>https://arxiv.org/abs/2502.19573</link>
      <description><![CDATA[ARXIV：2502.19573V1公告类型：新 
摘要：大型语言模型（LLMS）已成为功能高大的系统，并且越来越多地整合到各种用途中。但是，他们部署的迅速步伐超过了对其内部机制以及对其能力和局限性的描述的全面理解。智能系统的所需属性是其识别其知识范围的能力。为了研究LLM是否体现了这种特征，我们开发了一个基准，旨在挑战这些模型，以枚举它们在特定主题上拥有的所有信息。该基准评估模型是回想起过多，不足还是精确的信息量，从而表明他们对自己的知识的认识。我们的发现表明，所有经过足够规模的测试LLM都证明了他们对特定主题了解的了解。尽管不同的体系结构表现出这种能力出现的不同速率，但结果表明，知识的认识可能是LLM的可概括属性。需要进一步的研究来确认这一潜力并充分阐明潜在的机制。]]></description>
      <guid>https://arxiv.org/abs/2502.19573</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们在哪里？评估非洲语言的LLM表现</title>
      <link>https://arxiv.org/abs/2502.19582</link>
      <description><![CDATA[ARXIV：2502.19582V1公告类型：新 
摘要：非洲丰富的语言遗产在NLP中的代表性不足，这在很大程度上是由于历史政策有利于外语并造成了重要的数据不平等。在本文中，我们将对非洲语言环境的理论见解与使用撒哈拉的经验评估相结合，这是一种综合的基准测试，该基准是根据大规模公开访问的数据集策划的，捕获了大陆的语言多样性。通过系统地评估撒哈拉领先的大语言模型（LLM）的性能，我们演示了政策诱导的数据变化如何直接影响非洲语言的模型有效性。我们的发现表明，尽管几种语言的表现相当出色，但由于数据稀疏，许多土著语言仍然被边缘化。利用这些见解，我们为政策改革和包容性数据实践提供了可行的建议。总体而言，我们的工作强调了对双重方法的迫切需求 - 将理论理解与经验评估相结合，以促进非洲社区AI的语言多样性。]]></description>
      <guid>https://arxiv.org/abs/2502.19582</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>诺伯特：下一代伯特</title>
      <link>https://arxiv.org/abs/2502.19587</link>
      <description><![CDATA[ARXIV：2502.19587V1公告类型：新 
摘要：建筑，预训练和微调的最新创新导致了大型自动回归语言模型（例如Llama and Deepseek）的非凡的内在学习和推理能力。相比之下，尽管许多下游NLP应用是基础，但像Bert和Roberta这样的编码者并未看到相同的进度。为了弥合这一差距，我们介绍了Neobert，Neobert是下一代编码器，通过整合建筑，现代数据和优化的预训练方法中的最新进步，重新定义了双向模型的功能。 Neobert专为无缝采用而设计：它是现有基本型号的插件替代品，依赖于最佳的深度宽度比，并利用了4,096个令牌的扩展上下文长度。尽管具有2500亿个参数足迹，但它在巨大的MTEB基准测试中取得了最先进的结果，在相同的微调条件下，伯特（Bert）大，罗伯塔（Roberta），大，名字师和现代伯特（Modernbert）的表现优于大，罗伯塔（Roberta）。此外，我们严格评估每种修饰对胶水的影响，并为MTEB设计一个均匀的微调和评估框架。我们发布所有代码，数据，检查点和培训脚本，以加速研究和现实世界中的采用。]]></description>
      <guid>https://arxiv.org/abs/2502.19587</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数百万的城市：大规模映射文学社交网络</title>
      <link>https://arxiv.org/abs/2502.19590</link>
      <description><![CDATA[ARXIV：2502.19590V1公告类型：新 
摘要：我们发布了从多语言小说和非小说叙事中提取的70,509个高质量的社交网络。我们还提供了约30,000本文本（73％的非小说类和27％的小说）的元数据，其中1800年至1999年写了58种语言。该数据集以前所未有的规模提供有关历史社会世界的信息，其中包括2,805,482个对亲和力和关系类型注释的2,805,482配对关系中的1,192,855个人的数据。我们通过自动化提取社交网络的先前手动方法来实现这一规模；具体来说，我们将现有注释任务调整为语言模型提示，以确保与结构化输出的使用规模一致。该数据集通过提供有关社会现实的认知模型的数据，为人文和社会科学提供了前所未有的资源。]]></description>
      <guid>https://arxiv.org/abs/2502.19590</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM时代重新访问单词嵌入</title>
      <link>https://arxiv.org/abs/2502.19607</link>
      <description><![CDATA[ARXIV：2502.19607V1公告类型：新 
摘要：大型语言模型（LLMS）最近在各种NLP任务中显示出显着的进步。因此，最近出现了一种流行趋势，NLP研究人员从这些大型解码器模型中提取单词/句子/文档嵌入，并将其用于各种推理任务，并具有令人鼓舞的结果。但是，目前尚不清楚LLM诱导的嵌入的性能是否仅仅是因为规模或产生的潜在嵌入是否与诸如Word2Vec，Glove，Senten-Bert（Sbert）或Universal Senten Endoder（使用）等经典编码模型有显着差异。这是我们在本文中通过系统地比较LLM诱导的嵌入的经典脱皮和上下文化的单词嵌入来研究的中心问题。我们的结果表明，LLMS群集在语义上相关的单词更紧密，并且在脱皮设置中的类比任务上表现更好。但是，在上下文化的设置中，像SIMCSE这样的经典模型在句子级别的相似性评估任务中经常优于llms，强调了它们与细颗粒语义的持续相关性。]]></description>
      <guid>https://arxiv.org/abs/2502.19607</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大语言模型和地理上下文化评估仇恨言语检测</title>
      <link>https://arxiv.org/abs/2502.19612</link>
      <description><![CDATA[ARXIV：2502.19612V1公告类型：新 
摘要：社交媒体上仇恨言论的扩散是对社会产生巨大影响的严重问题之一：暴力，歧视和社会分裂的升级。由于文化，语言和情境复杂性和对抗性操纵，检测仇恨言论的问题本质上是多方面的。在这项研究中，我们系统地研究了LLM在多语言数据集和各种地理环境中检测仇恨言论的性能。我们的工作在三个维度上提出了一个新的评估框架：仇恨言论的二进制分类，地理意识到的上下文检测以及对对抗性生成的文本的鲁棒性。使用来自五个不同地区的1,000条评论的数据集，我们评估了三个最先进的LLM：Llama2（13b），Codellama（7b）（7b）和DeepSeekcoder（6.7b）。 Codellama的二元分类召回率最高，为70.6％，F1得分为52.18％，而DeepSeekcoder在地理敏感性方面的表现最佳，在265个位置正确检测到63个。对抗性鲁棒性的测试也显示出明显的弱点。 Llama2错误分类为62.5％的操纵样品。这些结果揭示了当前版本的LLMS的准确性，上下文理解和鲁棒性之间的权衡。因此，这项工作为通过强调关键优势和局限而开发上下文意识到的多语言仇恨言语检测系统的阶段为舞台奠定了基础，从而为未来的研究和现实世界应用提供了可行的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.19612</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>您的论文是由LLM审查的吗？在同行评审中检测AI文本的新基准数据集和方法</title>
      <link>https://arxiv.org/abs/2502.19614</link>
      <description><![CDATA[ARXIV：2502.19614V1公告类型：新 
摘要：同行评审是确保已发表科学研究完整性的关键过程。对这一过程的信心是基于以下假设：相关领域的专家对提交出版的手稿的优点进行了仔细的考虑。随着大型语言模型（LLMS）的最新快速进步，对同行评审过程的新风险是，疏忽的审稿人将依靠LLM来执行经常耗时的审查论文。但是，缺乏现有资源来基准在同行评审领域中AI文本的可检测性。
  为了解决这一缺陷，我们介绍了一个综合数据集，其中包含788,984个AI写的同行评论，并配对相应的人类评论，涵盖了向两个领先的AI研究会议（ICLR和Neurips）提交的8年论文。我们使用这种新资源来评估18种现有的AI文本检测算法的能力，以区分人类撰写的同行评论和不同的最先进的LLM。在现有方法的缺点的推动下，我们提出了一种新的检测方法，该方法超过了AI书面同行评论的现有方法。我们的工作揭示了在单个同行评审级别识别AI生成的文本的困难，强调了迫切需要对新工具和方法检测这种不道德使用的生成AI的需求。]]></description>
      <guid>https://arxiv.org/abs/2502.19614</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS的意见弱也很重要：意见的混合增强了LLM的数学推理</title>
      <link>https://arxiv.org/abs/2502.19622</link>
      <description><![CDATA[ARXIV：2502.19622V1公告类型：新 
摘要：大型语言模型（LLM）的最新进展引起了人们对其正式推理能力的兴趣，尤其是在数学方面。虽然GPT-4（例如GPT-4）在数学基准（例如GSM8K）上表现良好，但尚不清楚中小型开放式LLM是否可以实现相似的性能，从而质疑其可靠性。为了缩小这一差距，我们提出了一种培训后的方法，利用较弱的辅助LLM的意见（MOO）混合在一起，以增强（相对）强大的LLM的推理。为此，每个训练后样本都通过辅助LLM的思考链（COT）推理步骤和答案进行增强，从而使主要LLM能够从不同的角度学习。我们将MOO与标准监督的微调（SFT），很少的提示以及代理（MOA）方法（MOA）方法进行比较。我们的结果表明，纳入LLMS的意见较弱会使数学推理平均提高5％，从而突出了在推理任务中各种观点的价值。]]></description>
      <guid>https://arxiv.org/abs/2502.19622</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MED-RLVR：通过增强学习从3B基础模型中新兴的医学推理</title>
      <link>https://arxiv.org/abs/2502.19655</link>
      <description><![CDATA[ARXIV：2502.19655V1公告类型：新 
摘要：从可验证的奖励中学习（RLVR）最近因其在没有明确推理监督的情况下从基本语言模型中引起自我发展的推理的能力而引起了人们的关注，如DeepSeek-R1所证明的那样。尽管RLVR的先前工作主要集中在数学和编码域上，但其对其他任务和域的适用性仍未探索。在这项工作中，我们研究了医学推理是否可以从RLVR中出现。我们将MED-RLVR作为对医疗领域的RLVR的初步研究，该研究利用医学多项选择答案（MCQA）数据作为可验证的标签。我们的结果表明，RLVR不仅对数学和编码有效，而且还可以成功地扩展到医疗问题的回答。值得注意的是，MED-RLVR的性能与分布任务的传统监督微调（SFT）相媲美，同时具有8分的精度增长，同时显着改善了分布式概括。对训练动力学的进一步分析表明，在没有明确的推理监督的情况下，从3B参数基本模型中出现了推理。这些发现强调了RLVR在数学和编码之外的领域中的潜力，这为其在医学等知识密集型领域的应用开辟了新的途径。]]></description>
      <guid>https://arxiv.org/abs/2502.19655</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究基于变压器的LLM中的神经元和头部的印刷错误</title>
      <link>https://arxiv.org/abs/2502.19669</link>
      <description><![CDATA[ARXIV：2502.19669V1公告类型：新 
摘要：本文研究了LLMS如何用错别字编码输入。我们假设特定的神经元和注意力头会识别错别字，并使用本地和全球环境在内部修复它们。我们介绍了一种方法来识别输入包含错别字时积极工作的错别字神经元和错字头。我们的实验结果表明：1）LLM可以在激活早期或晚期中的错别字神经元时使用局部环境修复错别字，即使另一个中的类型神经元也没有。 2）中层中的错别字神经元负责与全局上下文的错字固定核心。 3）通过广泛考虑上下文不关注特定令牌来修复错别字。 4）错别字神经元和错字头不仅可以用于打字，还可以理解一般环境。]]></description>
      <guid>https://arxiv.org/abs/2502.19669</guid>
      <pubDate>Fri, 28 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>