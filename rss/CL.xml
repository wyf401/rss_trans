<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 10 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>隐私保护提示工程：调查</title>
      <link>https://arxiv.org/abs/2404.06001</link>
      <description><![CDATA[arXiv:2404.06001v1 公告类型：新
摘要：预训练语言模型（PLM）在解决各种通用自然语言处理（NLP）任务方面表现出显着的熟练程度。研究人员观察到这些模型的性能与其规模之间存在直接相关性。因此，近年来这些模型的规模显着扩大，促使研究人员采用大型语言模型 (LLM) 一词来描述较大规模的 PLM。尺寸的增加伴随着一种被称为情境学习（ICL）的独特能力，它代表了一种特殊的提示形式。通过向法学硕士提供演示示例，同时保持模型参数冻结，可以将法学硕士用于特定的下游任务。尽管有趣，但隐私问题已成为其广泛使用的主要障碍。多项研究检查了与 ICL 和一般提示相关的隐私风险，并设计了减轻这些风险的技术。因此，有必要为了社区的利益而组织这些缓解技术。这项调查系统地概述了 ICL 期间采用的隐私保护方法和一般提示。我们在这个范式下回顾、分析和比较不同的方法。此外，我们还提供了可用于开发这些框架的资源的摘要。最后，我们讨论这些框架的局限性，并对需要进一步探索的有前景的领域进行详细检查。]]></description>
      <guid>https://arxiv.org/abs/2404.06001</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>THOUGHTSCULPT：通过中级修改和搜索进行推理</title>
      <link>https://arxiv.org/abs/2404.05966</link>
      <description><![CDATA[arXiv:2404.05966v1 公告类型：新
摘要：我们提出了 THOUGHTSCULPT，这是一种针对具有可分解为组件的输出的任务的通用推理和搜索方法。 THOUGHTSCULPT 使用蒙特卡罗树搜索 (MCTS) 探索潜在解决方案的搜索树，一次构建解决方案并根据任何特定领域的启发式进行评估，这在实践中通常只是一个 LLM 评估器。至关重要的是，我们的行动空间包括修订行动：THOUGHTSCULPT 可能会选择修订其先前输出的部分内容，而不是继续构建其输出的其余部分。根据经验，THOUGHTSCULPT 在三个具有挑战性的任务中优于最先进的推理方法：故事大纲改进（高达 +30% 的趣味性）、迷你填字游戏（高达 +16% 的单词成功率）和约束生成（高达 +16% 的单词成功率）到 +10% 概念覆盖率）。]]></description>
      <guid>https://arxiv.org/abs/2404.05966</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>通过检索增强个性化大型语言模型的优化方法</title>
      <link>https://arxiv.org/abs/2404.05970</link>
      <description><![CDATA[arXiv:2404.05970v1 公告类型：新
摘要：本文研究了个性化大型语言模型（LLM）的检索增强方法，这可能对各种应用程序和领域产生重大影响。我们提出了首次尝试优化检索模型，将有限数量的个人文档传递给大型语言模型，以实现个性化生成。我们开发了两种优化算法，从下游个性化生成任务中征求反馈以进行检索优化——一种基于强化学习，其奖励函数是使用任意个性化生成指标定义的，另一种基于从下游 LLM 到检索模型的知识蒸馏。本文还介绍了一个生成前和生成后检索器选择模型，该模型决定为每个 LLM 输入选择哪个检索器。对语言模型个性化 (LaMP) 基准的各种任务进行的广泛实验表明，七个数据集中有六个在统计上有显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2404.05970</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>实时搜索中的事件增强检索</title>
      <link>https://arxiv.org/abs/2404.05989</link>
      <description><![CDATA[arXiv:2404.05989v1 公告类型：新
摘要：基于嵌入的检索（EBR）方法广泛应用于主流搜索引擎检索系统中，并且在最近消除 LLM 错觉的检索增强方法中至关重要。然而，现有的EBR模型往往面临“语义漂移”问题以及对关键信息关注不够，导致后续步骤检索结果的采用率较低。这个问题在实时搜索场景中尤为明显，互联网上热门事件的多种表达方式使得实时检索严重依赖于关键事件信息。为了解决这个问题，本文提出了一种称为 EER 的新方法，通过改进传统 EBR 的双编码器模型来增强实时检索性能。我们将对比学习与成对学习结合起来以实现编码器优化。此外，为了加强对事件中关键事件信息的关注，我们在文档编码器之后添加了解码器模块，引入了基于提示调整的生成事件三元组提取方案，并通过比较学习将事件与查询编码器优化相关联。该解码器模块可以在推理过程中删除。大量实验表明，EER 可以显着提高实时搜索检索性能。我们相信这种方法将为信息检索领域提供新的视角。代码和数据集可在 https://github.com/open-event-hub/Event-enhanced_Retrieval 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.05989</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>机器翻译、变音符号和变音符号的相互作用</title>
      <link>https://arxiv.org/abs/2404.05943</link>
      <description><![CDATA[arXiv:2404.05943v1 公告类型：新
摘要：我们研究了两个研究问题：（1）机器翻译（MT）和变音符号如何在多任务学习环境中影响彼此的性能（2）保留（与删除）变音符号对 MT 性能的影响。我们在高资源 (HR) 和低资源 (LR) 环境中通过 55 种不同语言（36 种非洲语言和 19 种欧洲语言）研究这两个问题。对于（1），结果表明，变音符号显着有利于 LR 场景中的 MT，使某些语言的性能提高一倍甚至三倍，但会损害 HR 场景中的 MT。我们发现机器翻译会损害 LR 中的变音符号，但对某些语言的 HR 会带来显着的好处。对于 (2)，无论保留还是删除变音符号，MT 性能都相似。此外，我们提出了两类指标来衡量变音系统的复杂性，发现这些指标与我们的变音模型的性能呈正相关。总体而言，我们的工作为在不同数据大小条件下开发机器翻译和变音系统提供了见解，并且可能具有超出我们研究的 55 种语言的影响。]]></description>
      <guid>https://arxiv.org/abs/2404.05943</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>VisualWebBench：多模式法学硕士在网页理解和基础方面发展了多远？</title>
      <link>https://arxiv.org/abs/2404.05955</link>
      <description><![CDATA[arXiv:2404.05955v1 公告类型：新
摘要：多模态大型语言模型（MLLM）在 Web 相关任务中显示出前景，但由于缺乏全面的基准，评估其在 Web 领域的性能仍然是一个挑战。现有的基准要么是针对一般的多模态任务而设计的，无法捕捉网页的独特特征，要么专注于端到端的Web代理任务，无法衡量OCR、理解和接地等细粒度的能力。在本文中，我们介绍了 \bench{}，这是一个多模式基准测试，旨在评估 MLLM 在各种 Web 任务中的能力。 \bench{} 包含 7 个任务，包含来自 139 个真实网站的 1.5K 个人工管理实例，涵盖 87 个子域。我们在 \bench{} 上评估了 14 个开源 MLLM、Gemini Pro、Claude-3 系列和 GPT-4V(ision)，揭示了重大挑战和性能差距。进一步的分析凸显了当前 MLLM 的局限性，包括在文本丰富的环境中接地不足以及低分辨率图像输入的性能不佳。我们相信 \bench{} 将成为研究社区的宝贵资源，并有助于为网络相关应用创建更强大、更通用的 MLLM。]]></description>
      <guid>https://arxiv.org/abs/2404.05955</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>LLM2Vec：大型语言模型是秘密强大的文本编码器</title>
      <link>https://arxiv.org/abs/2404.05961</link>
      <description><![CDATA[arXiv:2404.05961v1 公告类型：新
摘要：大型仅解码器语言模型 (LLM) 是当今大多数 NLP 任务和基准测试中最先进的模型。然而，社区只是慢慢地将这些模型用于文本嵌入任务，这需要丰富的上下文表示。在这项工作中，我们引入了 LLM2Vec，这是一种简单的无监督方法，可以将任何仅解码器的 LLM 转换为强大的文本编码器。 LLM2Vec 包含三个简单步骤：1）启用双向注意力，2）屏蔽下一个标记预测，3）无监督对比学习。我们通过将 LLM2Vec 应用于从 1.3B 到 7B 参数的 3 个流行的 LLM 来证明 LLM2Vec 的有效性，并评估英语单词和序列级任务的转换模型。我们在字级任务上远远优于仅编码器模型，并在大规模文本嵌入基准（MTEB）上达到了新的无监督最先进性能。此外，当将 LLM2Vec 与监督对比学习相结合时，我们在仅使用公开数据进行训练的模型中在 MTEB 上实现了最先进的性能。我们强有力的实证结果和广泛的分析表明，LLM 可以以参数有效的方式有效地转换为通用文本编码器，而不需要昂贵的适应或合成 GPT-4 生成的数据。]]></description>
      <guid>https://arxiv.org/abs/2404.05961</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>Eagle 和 Finch：具有矩阵值状态和动态递归的 RWKV</title>
      <link>https://arxiv.org/abs/2404.05892</link>
      <description><![CDATA[arXiv:2404.05892v1 公告类型：新
摘要：我们提出了 Eagle (RWKV-5) 和 Finch (RWKV-6)，这是在 RWKV (RWKV-4) 架构上改进的序列模型。我们的架构设计进步包括多头矩阵值状态和动态递归机制，可提高表达能力，同时保持 RNN 的推理效率特征。我们引入了一个包含 1.12 万亿个标记的新多语言语料库和一个基于贪婪匹配的快速标记器，以增强多语言能力。我们训练了四个 Eagle 模型，参数范围为 0.46 到 75 亿个参数，以及两个 Finch 模型，参数范围为 1.6 到 31 亿个参数，发现它们在各种基准测试中都实现了具有竞争力的性能。我们在 Apache 2.0 许可证下在 HuggingFace 上发布了所有模型。模型位于：https://huggingface.co/RWKV 训练代码位于：https://github.com/RWKV/RWKV-LM 推理代码位于：https://github.com/RWKV/ChatRWKV 时间并行训练代码位于：https://github.com/RWKV/RWKV-infctx-trainer]]></description>
      <guid>https://arxiv.org/abs/2404.05892</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>WILBUR：用于稳健且准确的网络代理的自适应上下文学习</title>
      <link>https://arxiv.org/abs/2404.05902</link>
      <description><![CDATA[arXiv:2404.05902v1 公告类型：新
摘要：在网络代理研究领域，同时实现泛化和准确性仍然是一个具有挑战性的问题。由于网站结构差异很大，现有方法常常失败。此外，现有的微调和上下文学习技术无法在多个网站上推广。我们介绍 Wilbur，这是一种使用可微分排序模型和新颖的指令合成技术的方法，可以通过之前运行的任务演示来最佳地填充黑盒大型语言模型的提示。为了最大限度地提高端到端的成功率，我们还提出了一种智能回溯机制，可以学习错误并从错误中恢复。最后，我们展示了我们的排名模型可以根据生成自动课程的数据进行训练，该课程从法学硕士中采样代表性目标，运行代理并自动评估它，无需手动注释。 Wilbur 在 WebVoyager 基准测试中取得了最先进的结果，整体性能比纯文本模型高出 8%，在某些网站上高出 36%。在同一基准上，尽管仅接收文本输入，Wilbur 仍与强大的多模式模型相差 5% 以内，进一步分析表明，大量故障是由于操作网络的工程挑战造成的。]]></description>
      <guid>https://arxiv.org/abs/2404.05902</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>幻觉排行榜——在大型语言模型中测量幻觉的公开努力</title>
      <link>https://arxiv.org/abs/2404.05904</link>
      <description><![CDATA[arXiv:2404.05904v1 公告类型：新
摘要：大型语言模型（LLM）以其卓越的理解和生成类人文本的能力改变了自然语言处理（NLP）的格局。然而，这些模型很容易出现“幻觉”——输出与事实现实或输入上下文不符。本文介绍了幻觉排行榜，这是一项开放倡议，用于定量测量和比较每个模型产生幻觉的趋势。该排行榜使用了一套全面的基准，重点关注幻觉的不同方面，例如事实性和忠实性，以及各种任务，包括回答问题、总结和阅读理解。我们的分析提供了对不同模型性能的见解，指导研究人员和从业者为其应用选择最可靠的模型。]]></description>
      <guid>https://arxiv.org/abs/2404.05904</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>GeniL：泛化语言的多语言数据集</title>
      <link>https://arxiv.org/abs/2404.05866</link>
      <description><![CDATA[arXiv:2404.05866v1 公告类型：新
摘要：法学硕士正在日益改变我们的数字生态系统，但他们经常继承从培训数据中学到的社会偏见，例如将某些属性与特定身份群体相关联的刻板印象。虽然是否以及如何减轻这些偏见可能取决于具体的用例，但能够有效地检测刻板印象延续的实例是至关重要的第一步。当前评估生成语言中刻板印象的存在的方法依赖于简单的模板或基于共现的测量，没有考虑它们所体现的各种句子上下文。我们认为理解句子上下文对于检测泛化实例至关重要。我们区分两种类型的概括：（1）仅提及概括存在的语言（“人们认为法国人非常粗鲁”），以及（2）强化这种概括的语言（“作为法国人，他们一定很粗鲁” ），来自非概括性背景（“我的法国朋友认为我很粗鲁”）。为了进行有意义的刻板印象评估，我们需要可靠地区分此类概括实例。我们引入了检测语言泛化的新任务，并构建了 GeniL，这是一个包含来自 9 种语言（英语、阿拉伯语、孟加拉语、西班牙语、法语、印地语、印度尼西亚语、马来语和葡萄牙语）的超过 50K 个句子的多语言数据集，并为泛化实例进行了注释。我们证明，同现作为泛化实例的可能性通常很低，并且在不同的语言、身份组和属性之间存在差异。我们构建了分类器来检测语言的泛化，总体 PR-AUC 为 58.7，不同语言的性能程度不同。我们的研究提供了数据和工具，使人们能够对刻板印象的延续有细致入微的理解，这是迈向更具包容性和负责任的语言技术的关键一步。]]></description>
      <guid>https://arxiv.org/abs/2404.05866</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>CodecLM：使语言模型与定制的合成数据保持一致</title>
      <link>https://arxiv.org/abs/2404.05875</link>
      <description><![CDATA[arXiv:2404.05875v1 公告类型：新
摘要：指令调优已成为将大型语言模型（LLM）与特定任务指令对齐的关键，从而减少下一个令牌预测目标与用户实际目标之间的差异。为了减少人类收集或注释数据的劳动力和时间成本，研究人员开始探索使用法学硕士来生成指令对齐的合成数据。最近的工作重点是生成多样化的指令并应用 LLM 来增加指令复杂性，通常忽略下游用例。目前尚不清楚如何定制高质量数据，以在不同的目标指令分布和法学硕士中获得更好的指令跟踪能力。为此，我们引入了 CodecLM，这是一个通用框架，用于自适应生成高质量合成数据，用于与不同下游指令分布和 LLM 进行 LLM 对齐。借鉴编码-解码原则，我们使用 LLM 作为编解码器来指导数据生成过程。我们首先将种子指令编码为元数据，元数据是即时生成的简洁关键字，用于捕获目标指令分布，然后解码元数据以创建定制指令。我们还在解码过程中引入了自量规和对比过滤，以定制数据高效的样本。根据基准对四个开放域指令进行的广泛实验验证了 CodecLM 相对于当前最先进技术的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.05875</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>橡皮擦：通过忘却有害知识实现大型语言模型的越狱防御</title>
      <link>https://arxiv.org/abs/2404.05880</link>
      <description><![CDATA[arXiv:2404.05880v1 公告类型：新
摘要：越狱攻击可以使大型语言模型 (LLM) 绕过防护措施并生成有害内容。现有的越狱防御方法未能解决模型中存在有害知识的根本问题，导致法学硕士存在潜在的越狱风险。在本文中，我们提出了一种名为“橡皮擦”的新型防御方法，该方法主要包括三个目标：忘却有害知识、保留一般知识和保持安全一致性。直觉是，如果法学硕士忘记了回答有害问题所需的具体知识，它将不再有能力回答有害问题。 Erase 的训练实际上并不需要模型自身的有害知识，它可以受益于忘却与有害查询相关的一般答案，这意味着它不需要红队的帮助。实验结果表明，Eraser 可以在不损害模型通用能力的情况下，显着降低各种攻击的越狱成功率。]]></description>
      <guid>https://arxiv.org/abs/2404.05880</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>SambaLingo：教授大型语言模型新语言</title>
      <link>https://arxiv.org/abs/2404.05829</link>
      <description><![CDATA[arXiv:2404.05829v1 公告类型：新
摘要：尽管法学硕士广泛存在，但其跨不同语言的能力和可用性仍然存在很大差距。解决这些问题的一种方法是采用现有的预训练法学硕士并继续对其进行新语言的培训。虽然之前的工作已经尝试了语言适应，但尚未涵盖有关最佳实践和方法的许多问题。在本文中，我们对法学硕士对新语言的适应进行了全面的调查。我们的研究涵盖了这个过程中的关键组成部分，包括词汇扩展、直接偏好优化以及低资源语言中人类对齐的数据稀缺问题。我们将这些实验扩展到 9 种语言和 2 个参数尺度（7B 和 70B）。我们将我们的模型与 Llama 2、Aya-101、XGLM、BLOOM 和现有语言专家进行比较，优于所有先前发布的基线。此外，所有评估代码和检查点都是公开的，以方便未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2404.05829</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>\'UFAL LatinPipe at EvaLatin 2024：拉丁语的形态句法分析</title>
      <link>https://arxiv.org/abs/2404.05839</link>
      <description><![CDATA[arXiv:2404.05839v1 公告类型：新
摘要：我们展示了 LatinPipe，这是 EvaLatin 2024 依赖解析共享任务的获奖作品。我们的系统由经过微调的基础 LM 和大型预训练 LM 串联而成，具有用于解析的点积注意力头和用于形态学的 Softmax 分类头，以共同学习依存句法分析和形态分析。它是通过从七个公开可用的拉丁文语料库中采样进行训练的，利用注释的额外协调来实现更统一的注释风格。在微调之前，我们使用冻结权重对系统进行几个初始时期的训练。我们还通过将 BiLSTM 层堆叠在 Transformer 的顶部来添加额外的本地相对上下文。最后，我们集成了七个随机实例化网络的输出概率分布以用于最终提交。该代码可在 https://github.com/ufal/evalatin2024-latinpipe 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.05839</guid>
      <pubDate>Wed, 10 Apr 2024 06:16:56 GMT</pubDate>
    </item>
    </channel>
</rss>