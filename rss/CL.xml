<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 07 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>大型语言模型在概念柏拉图式表征上的跨模型可迁移性</title>
      <link>https://arxiv.org/abs/2501.02009</link>
      <description><![CDATA[arXiv:2501.02009v1 公告类型：新
摘要：了解大型语言模型 (LLM) 的内部工作原理是一个关键的研究前沿。先前的研究表明，单个 LLM 的概念表示可以作为控制向量 (SV) 捕获，从而实现对 LLM 行为的控制（例如，生成有害内容）。我们的工作采用了一种新颖的方法，探索了不同 LLM 之间概念表示之间的复杂关系，与柏拉图的洞穴寓言形成了有趣的对比。特别是，我们引入了一种线性变换方法来连接这些表示，并提出了三个关键发现：1) 可以使用简单的线性变换有效地对齐不同 LLM 之间概念表示，从而通过 SV 实现高效的跨模型传输和行为控制。2) 这种线性变换跨概念进行推广，有助于对齐和控制代表不同 LLM 之间概念的 SV。 3) LLM 概念表示之间存在从弱到强的可转移性，即从较小的 LLM 中提取的 SV 可以有效地控制较大的 LLM 的行为。]]></description>
      <guid>https://arxiv.org/abs/2501.02009</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过可调的安全性能权衡实时保护大型语言模型</title>
      <link>https://arxiv.org/abs/2501.02018</link>
      <description><![CDATA[arXiv:2501.02018v1 公告类型：新
摘要：大型语言模型 (LLM) 已被证明容易受到越狱攻击或用于从模型中引出高风险行为的对抗性攻击。网络犯罪分子和黑帽攻击者利用越狱造成重大伤害，凸显了保护广泛部署的模型的迫切需要。保护方法包括微调模型或让 LLM“自我反思”，可能会延长模型的推理时间、产生计算惩罚、降低输出的语义流畅性并限制“正常”模型行为。重要的是，这些安全性能权衡 (SPT) 仍然是一个研究不足的领域。在这项工作中，我们引入了一种名为 SafeNudge 的新型保护措施，它将受控文本生成与“推动”相结合，或使用文本干预来改变模型的行为。 SafeNudge 在执行越狱攻击时在文本生成过程中触发，通过引导 LLM 做出安全响应，可将越狱成功率降低 30%。它对推理的延迟影响最小，对输出的语义流畅性的影响可以忽略不计。此外，我们允许可调 SPT。SafeNudge 是开源的，可通过 https://pypi.org/ 获取，并且与加载了 Hugging Face“transformers”库的模型兼容。]]></description>
      <guid>https://arxiv.org/abs/2501.02018</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用语义图增强幻觉检测的不确定性建模</title>
      <link>https://arxiv.org/abs/2501.02020</link>
      <description><![CDATA[arXiv:2501.02020v1 公告类型：新
摘要：大型语言模型（LLM）容易产生幻觉，出现非事实或不真实的陈述，从而破坏其在现实场景中的应用。最近的研究集中于基于不确定性的幻觉检测，它利用LLM的输出概率进行不确定性计算，而不依赖于外部知识或来自LLM的频繁采样。然而，大多数方法仅仅考虑每个独立标记的不确定性，而标记和句子之间复杂的语义关系尚未得到很好的研究，这限制了对跨越段落中多个标记和句子的幻觉的检测。在本文中，我们提出了一种使用语义图增强不确定性建模以进行幻觉检测的方法。具体而言，我们首先构建一个可以很好地捕捉实体标记和句子之间关系的语义图。然后，我们结合两个实体之间的关系进行不确定性传播，以增强句子级幻觉检测。鉴于幻觉是由于句子之间的冲突而产生的，我们进一步提出了一种基于图的不确定性校准方法，该方法将句子与语义图中的邻居的矛盾概率结合起来以计算不确定性。在两个数据集上的大量实验证明了我们提出的方法的巨大优势。特别是在段落级幻觉检测中，我们获得了 19.78% 的显着改进。]]></description>
      <guid>https://arxiv.org/abs/2501.02020</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>逻辑思维的递归分解：大型语言模型中高级推理和知识传播的框架</title>
      <link>https://arxiv.org/abs/2501.02026</link>
      <description><![CDATA[arXiv:2501.02026v1 公告类型：新
摘要：增强大型语言模型的推理能力仍然是人工智能面临的一个关键挑战。我们引入了 RDoLT，即逻辑思维提示的递归分解，这是一个显著提高 LLM 推理性能的新框架。RDoLT 基于三个关键创新：（1）将复杂的推理任务递归分解为复杂程度逐渐提高的子任务；（2）采用先进的选择和评分机制来识别最有前途的推理思想；（3）集成知识传播模块，通过跟踪强弱思想来模仿人类学习以进行信息传播。我们的方法在多个基准上进行了评估，包括 GSM8K、SVAMP、MultiArith、LastLetterConcatenation 和 Gaokao2023 Math。结果表明，RDoLT 的表现始终优于现有的最先进技术，使用 ChatGPT-4 在 GSM8K 上实现了 90.98% 的准确率，比最先进技术高出 6.28%。在其他基准测试中也观察到了类似的改进，准确率提高了 5.5% 到 6.75%。这些发现凸显了 RDoLT 推进快速工程的潜力，为复杂的推理任务提供了一种更有效、更通用的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.02026</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CarbonChat：基于大型语言模型的企业碳排放分析与气候知识问答系统</title>
      <link>https://arxiv.org/abs/2501.02031</link>
      <description><![CDATA[arXiv:2501.02031v1 公告类型：新 
摘要：随着全球气候变化影响的加剧，企业碳排放成为全球关注的焦点。针对大型语言模型中气候变化知识更新滞后、传统增强生成架构对复杂问题缺乏专业化和准确性、可持续发展报告分析成本高、时间耗时高等问题，本文提出了CarbonChat：基于大型语言模型的企业碳排放分析与气候知识问答系统，旨在实现精准的碳排放分析与政策理解。首先，提出多样化的索引模块构建方法，处理规则化、长文本的分词和结构化数据的提取，优化关键信息的解析；其次，设计增强型自提示检索增强生成架构，融合意图识别、结构化推理链、混合检索和Text2SQL，提升语义理解和查询转换的效率；其次，基于温室气体核算框架，建立碳排放分析的14个维度，实现报告汇总、相关性评估和定制化响应；最后，通过多层分块机制、时间戳和幻觉检测功能，保证分析结果的准确性和可验证性，降低幻觉发生率，提高响应的准确性。]]></description>
      <guid>https://arxiv.org/abs/2501.02031</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对法学硕士生成的文化遗产文本中价值错位的调查</title>
      <link>https://arxiv.org/abs/2501.02039</link>
      <description><![CDATA[arXiv:2501.02039v1 公告类型：新
摘要：随着大型语言模型 (LLM) 在与文化遗产相关的任务中越来越普遍，例如生成历史古迹的描述、翻译古代文献、保存口头传统和创建教育内容，用户和研究人员越来越依赖它们生成准确且文化一致的文本的能力。然而，生成的文本中可能存在文化价值错位，例如对历史事实的歪曲、文化认同的侵蚀以及对复杂文化叙事的过度简化，这可能会导致严重后果。因此，在文化遗产的 LLM 背景下调查价值错位对于减轻这些风险至关重要，但在这一领域缺乏系统而全面的研究和调查。为了填补这一空白，我们系统地评估了 LLM 在为文化遗产相关任务生成文化一致文本方面的可靠性。我们通过编制 5 个开源 LLM 中的 1066 个查询任务集（涵盖文化遗产知识框架中 5 个被广泛认可的类别和 17 个方面）进行了全面评估，并检查了生成的文本中文化价值观错位的类型和比率。使用自动和手动方法，我们有效地检测和分析了 LLM 生成的文本中的文化价值观错位。我们的发现令人担忧：超过 65% 的生成文本表现出明显的文化错位，某些任务几乎完全显示出与关键文化价值观的错位。除了这些发现之外，本文还介绍了一个基准数据集和一个全面的评估工作流程，它们可以作为未来研究的宝贵资源，旨在提高 LLM 的文化敏感性和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2501.02039</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Med-BERT 上的下次就诊标记预测头推进胰腺癌预测</title>
      <link>https://arxiv.org/abs/2501.02044</link>
      <description><![CDATA[arXiv:2501.02044v1 公告类型：新 
摘要：背景：最近，许多在大量数据上预训练的基础模型已证明在使用电子健康记录 (EHR) 进行疾病预测方面有效。然而，关于如何最好地利用这些模型，尤其是在非常小的微调队列中，仍然存在一些未解答的问题。方法：我们使用 Med-BERT（一种 EHR 特定的基础模型），并将疾病二元预测任务重新表述为标记预测任务和下次访问掩码标记预测任务，以与 Med-BERT 的预训练任务格式保持一致，以提高在少量和完全监督设置中预测胰腺癌 (PaCa) 的准确性。结果：将任务重新表述为标记预测任务（称为 Med-BERT-Sum）在少量场景和较大数据样本中都表现出略微优越的性能。此外，将预测任务重新格式化为下次访问掩码标记预测任务 (Med-BERT-Mask) 在数据大小从 10 到 500 个样本的少样本场景中，其表现明显优于传统的二元分类 (BC) 预测任务 (Med-BERT-BC) 3% 到 7%。这些发现强调，将下游任务与 Med-BERT 的预训练目标相结合可显著增强模型的预测能力，从而提高其预测罕见疾病和常见疾病的有效性。结论：重新格式化疾病预测任务以与基础模型的预训练保持一致可提高预测准确性，从而更早发现并及时干预。这种方法可以提高 PaCa 和其他潜在癌症的治疗效果、存活率和总体患者预后。]]></description>
      <guid>https://arxiv.org/abs/2501.02044</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AGGA：生成式人工智能和大型语言模型学术指南数据集</title>
      <link>https://arxiv.org/abs/2501.02063</link>
      <description><![CDATA[arXiv:2501.02063v1 公告类型：新
摘要：本研究介绍了 AGGA，这是一个包含 80 条学术指南的数据集，用于在学术环境中使用生成式人工智能 (GAI) 和大型语言模型 (LLM)，这些指南是从大学官方网站精心收集的。该数据集包含 188,674 个单词，是需求工程中常用的自然语言处理任务的宝贵资源，例如模型合成、抽象识别和文档结构评估。此外，AGGA 可以进一步注释，作为各种任务的基准，包括歧义检测、需求分类和等效需求的识别。我们的方法严谨，确保了彻底的审查，选择的大学代表了全球各种机构，包括六大洲的顶尖大学。该数据集涵盖了人文学科、技术学科以及公共和私人机构等多个学术领域的观点，为 GAI 和 LLM 在学术界的融合提供了广泛的见解。]]></description>
      <guid>https://arxiv.org/abs/2501.02063</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>领域专业化与模型规模之间的相互作用：法律领域的案例研究</title>
      <link>https://arxiv.org/abs/2501.02068</link>
      <description><![CDATA[arXiv:2501.02068v1 公告类型：新
摘要：到目前为止，语言模型的缩放定律主要集中在从头开始寻找计算最优的模型大小和标记计数。然而，由于从随机初始化的权重训练模型时需要大量的数据，因此实现这种最佳平衡需要大量的计算资源。持续预训练提供了一种经济高效的替代方案，利用预训练模型的计算投资来整合新知识，而无需大量新数据。最近的研究结果表明，数据质量会影响缩放定律中的常数，从而改变最佳参数标记分配率。基于这一见解，我们研究了在计算受限场景下持续预训练期间领域专业化和模型大小之间的相互作用。我们的目标是为这种场景确定一种计算效率高的训练方案，并可能检测出这种相互作用中的模式，这些模式可以推广到不同的模型大小和领域。为了比较一般和专门的训练，我们过滤了一个基于网络的数据集以提取合法领域的数据。我们在未过滤和过滤后的数据集上分别对具有 1.5B、3B、7B 和 14B 个参数的模型进行了预训练，然后评估了它们在法律考试中的表现。结果表明，随着模型规模的增加，专用模型和通用模型之间的计算效率差距越来越大。]]></description>
      <guid>https://arxiv.org/abs/2501.02068</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的指令遵循修剪</title>
      <link>https://arxiv.org/abs/2501.02086</link>
      <description><![CDATA[arXiv:2501.02086v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的快速扩展，结构化剪枝已成为一种广泛使用的技术，用于从大型模型中学习高效、较小的模型，与从头开始训练类似大小的模型相比，可提供卓越的性能。在本文中，我们超越了确定模型固定剪枝掩码的传统静态剪枝方法，并提出了一种动态的结构化剪枝方法。在我们的方法中，剪枝掩码依赖于输入，并根据用户指令中描述的信息动态调整。我们的方法称为“指令跟踪剪枝”，它引入了一个稀疏掩码预测器，它将用户指令作为输入并动态选择与给定任务最相关的模型参数。为了识别和激活有效参数，我们联合优化了稀疏掩码预测器和 LLM，利用指令跟踪数据和预训练语料库。实验结果证明了我们的方法在各种评估基准上的有效性。例如，我们的 3B 激活模型在数学和编码等领域比 3B 密集模型提高了 5-8 个点的绝对优势，并且可与 9B 模型的性能相媲美。]]></description>
      <guid>https://arxiv.org/abs/2501.02086</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>应用文本挖掘分析创造力研究中的人类提问</title>
      <link>https://arxiv.org/abs/2501.02090</link>
      <description><![CDATA[arXiv:2501.02090v1 公告类型：新
摘要：创造力与在感兴趣的领域产生新颖和有效想法的能力有关。这些创造性想法是如何产生的？一种支持创造性想法并越来越受到实证关注的可能机制是提出问题。提问是一种可能的认知机制，可以定义问题，促进创造性解决问题。然而，关于问题在创造力中的确切作用，人们知之甚少。这项工作提出了一种尝试应用文本挖掘方法来衡量问题的认知潜力，其中考虑到（a）问题类型、（b）问题复杂性和（c）答案的内容。本文总结了问题挖掘作为创造力研究的一部分的历史，以及在研究中被认为有用或有帮助的自然语言处理方法。此外，还提出、实施了一种新方法，并将其应用于五个数据集。对获得的实验结果进行了全面分析，表明自然语言处理在创造性研究中发挥着作用。]]></description>
      <guid>https://arxiv.org/abs/2501.02090</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图的大型语言模型个性化检索</title>
      <link>https://arxiv.org/abs/2501.02157</link>
      <description><![CDATA[arXiv:2501.02157v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的发展，它们提供个性化和情境感知响应的能力为改善用户体验提供了变革潜力。然而，现有的个性化方法通常仅依靠用户历史记录来增强提示，限制了它们在生成定制输出方面的有效性，尤其是在数据稀疏的冷启动场景中。为了解决这些限制，我们提出了基于个性化图的检索增强生成 (PGraphRAG)，这是一个利用以用户为中心的知识图来丰富个性化的框架。通过将结构化用户知识直接集成到检索过程中并使用与用户相关的上下文增强提示，PGraphRAG 增强了上下文理解和输出质量。我们还引入了基于个性化图的文本生成基准，旨在评估用户历史记录稀疏或不可用的实际环境中的个性化文本生成任务。实验结果表明，PGraphRAG 在不同任务中的表现明显优于最先进的个性化方法，展现了基于图的个性化检索的独特优势。]]></description>
      <guid>https://arxiv.org/abs/2501.02157</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CPTuning：用于生成关系提取的对比提示调整</title>
      <link>https://arxiv.org/abs/2501.02196</link>
      <description><![CDATA[arXiv:2501.02196v1 公告类型：新
摘要：生成关系提取 (RE) 通常涉及首先将 RE 重新表述为语言建模问题，该问题可以通过预训练语言模型 (PLM) 轻松解决，然后使用监督交叉熵损失对 PLM 进行微调。尽管取得了令人鼓舞的性能，但现有方法仅假设每对实体之间存在一种确定性关系，而没有考虑多个关系可能有效的实际场景，即实体对重叠，导致其应用有限。为了解决这个问题，我们引入了一种新颖的 RE 对比快速调整方法 CPTuning，它学习将两个上下文实体之间的候选关系与高于或低于阈值的概率质量相关联，对应于关系是否存在。除了学习模式之外，CPTuning 还将 RE 组织为语言化的关系生成任务，并使用 Trie 约束解码来确保模型生成有效的关系。它自适应地挑选出在推理中具有高估计似然性的生成候选关系，从而实现多关系提取。我们在四个广泛使用的数据集上进行了广泛的实验来验证我们的方法。结果表明，无论是单关系提取还是多关系提取，使用 CPTuning 微调的 T5-large 都明显优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.02196</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉丰富文档问答调查：方法、挑战和趋势</title>
      <link>https://arxiv.org/abs/2501.02235</link>
      <description><![CDATA[arXiv:2501.02235v1 公告类型：新
摘要：使用大型语言模型 (LLM) 进行视觉丰富文档理解 (VrDU) 显著提高了需要理解和生成的任务（例如问答）的性能，尽管带来了新的挑战。本调查解释了 LLM 增强的 VrDU 模型如何发挥作用，涵盖了将 VrD 功能集成到 LLM 中的方法并强调了关键挑战。]]></description>
      <guid>https://arxiv.org/abs/2501.02235</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>金融命名实体识别：LLM能走多远？</title>
      <link>https://arxiv.org/abs/2501.02237</link>
      <description><![CDATA[arXiv:2501.02237v1 公告类型：新 
摘要：大型语言模型 (LLM) 的激增彻底改变了从越来越多的财务报表、公告和商业新闻中提取和分析关键信息的方式。识别命名实体以构建结构化数据对分析财务文档提出了重大挑战，并且是智能财务分析的基础任务。然而，这些通用 LLM 的有效性以及它们在各种提示下的表现如何仍需要更好的理解。为了填补空白，我们对金融命名实体识别 (NER) 问题中最先进的 LLM 和提示方法进行了系统评估。具体来说，我们的实验结果突出了它们的优势和局限性，确定了五种代表性的失败类型，并深入了解了它们在特定领域任务中的潜力和挑战。]]></description>
      <guid>https://arxiv.org/abs/2501.02237</guid>
      <pubDate>Tue, 07 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>