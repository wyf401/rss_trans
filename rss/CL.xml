<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 11 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Review-LLM：利用大型语言模型生成个性化评论</title>
      <link>https://arxiv.org/abs/2407.07487</link>
      <description><![CDATA[arXiv:2407.07487v1 公告类型：新
摘要：产品评论生成是推荐系统中的一项重要任务，可以为推荐提供解释和说服力。最近，大型语言模型（LLM，例如ChatGPT）表现出卓越的文本建模和生成能力，可以应用于评论生成。然而，直接应用LLM来生成评论可能会受到LLM的“礼貌”现象的困扰，无法生成个性化评论（例如负面评论）。在本文中，我们提出了Review-LLM，它可以定制LLM以生成个性化评论。首先，我们通过聚合用户历史行为来构建提示输入，其中包括相应的项目标题和评论。这使LLM能够捕获用户兴趣特征和评论写作风格。其次，我们将评分作为满意度指标纳入提示中，这可以进一步提高模型对用户偏好的理解和生成评论的情绪倾向控制。最后，我们将提示文本输入到 LLM 中，并使用监督微调 (SFT) 使模型针对给定的用户和目标商品生成个性化评论。在真实数据集上的实验结果表明，我们的微调模型可以实现比现有闭源 LLM 更好的评论生成性能。]]></description>
      <guid>https://arxiv.org/abs/2407.07487</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:51 GMT</pubDate>
    </item>
    <item>
      <title>你只需要 Bucket 预训练</title>
      <link>https://arxiv.org/abs/2407.07495</link>
      <description><![CDATA[arXiv:2407.07495v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种自然语言处理任务中都表现出色。然而，传统的固定长度数据组合预训练策略（包括连接和拆分文档）会引入噪音并限制模型捕获长距离依赖关系的能力。为了解决这个问题，我们首先引入了三个评估数据组合质量的指标：填充率、截断率和连接率。我们进一步提出了一种超越固定长度范式的多桶数据组合方法，为预训练提供了一种更灵活、更高效的方法。大量实验表明，我们提出的方法可以显著提高 LLM 预训练的效率和功效。我们的方法不仅可以降低噪音和保留上下文，还可以加速训练，使其成为 LLM 预训练的一个有前途的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2407.07495</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:51 GMT</pubDate>
    </item>
    <item>
      <title>KpopMT：面向 Kpop 粉丝的术语翻译数据集</title>
      <link>https://arxiv.org/abs/2407.07413</link>
      <description><![CDATA[arXiv:2407.07413v1 公告类型：新
摘要：虽然机器可以从现有语料库中学习，但人类具有建立和接受新语言系统的独特能力。这使得人类在社会群体中形成了独特的语言系统。与此相一致，我们专注于解决社会群体内翻译挑战的空白，其中群体内成员使用独特的术语。我们提出了 KpopMT 数据集，旨在通过实现精确的术语翻译来填补这一空白，选择 Kpop 粉丝圈作为社会群体的一项举措，因为它在全球很受欢迎。专业翻译人员为韩语帖子和评论提供 1k 英文翻译，每个都用社会群体语言系统中的特定术语进行注释。我们评估了 KpopMT 上的现有翻译系统（包括 GPT 模型），以确定它们的失败案例。结果显示总体得分较低，强调了在翻译中反映特定群体的术语和风格的挑战。我们向公众开放 KpopMT。]]></description>
      <guid>https://arxiv.org/abs/2407.07413</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:50 GMT</pubDate>
    </item>
    <item>
      <title>口语理解中的分布外泛化</title>
      <link>https://arxiv.org/abs/2407.07425</link>
      <description><![CDATA[arXiv:2407.07425v1 公告类型：新
摘要：当测试数据与训练数据意外不同时，它被称为分布外 (OOD)，这是机器学习实际用例中的常见挑战。尽管 OOD 泛化近年来引起了人们的兴趣，但很少有研究关注口语理解 (SLU) 任务中的 OOD 泛化。为了促进对该主题的研究，我们引入了流行的 SLU 数据集 SLURP 的修改版本，其中包含用于在 SLU 任务中测试 OOD 泛化的数据分割。我们将修改后的数据集称为 SLURP For OOD 泛化，或 SLURPFOOD。利用我们的 OOD 数据分割，我们发现端到端 SLU 模型的泛化能力有限。此外，通过采用模型可解释性技术，我们阐明了导致模型泛化困难的因素。为了提高泛化能力，我们尝试了两种技术，这可以改善部分但不是所有分割的结果，强调了对新技术的需求。]]></description>
      <guid>https://arxiv.org/abs/2407.07425</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:50 GMT</pubDate>
    </item>
    <item>
      <title>从医学出版物中自动提取疾病风险因素</title>
      <link>https://arxiv.org/abs/2407.07373</link>
      <description><![CDATA[arXiv:2407.07373v1 公告类型：新
摘要：我们提出了一种从医学文献中自动识别疾病风险因素的新方法，利用生物医学领域的预训练模型，同时针对特定任务进行调整。面对医学文章的多样性和非结构化性质的挑战，我们的研究引入了一个多步骤系统，首先识别相关文章，然后根据风险因素讨论的存在对其进行分类，最后通过问答模型提取疾病的特定风险因素信息。
我们的贡献包括开发一个用于自动提取风险因素的综合流程和汇编多个数据集，这些数据集可以作为该领域进一步研究的宝贵资源。这些数据集涵盖了广泛的疾病及其相关的风险因素，通过细粒度的评估方案进行了细致的识别和验证。我们进行了自动和彻底的手动评估，结果令人鼓舞。我们还强调了改进模型和扩大数据集全面性的重要性，以跟上快速发展的医学研究领域。]]></description>
      <guid>https://arxiv.org/abs/2407.07373</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:49 GMT</pubDate>
    </item>
    <item>
      <title>多语言融合：使用语言混合进行 LLM 安全对齐评估</title>
      <link>https://arxiv.org/abs/2407.07342</link>
      <description><![CDATA[arXiv:2407.07342v1 公告类型：新 
摘要：由于安全性在大型语言模型 (LLM) 的整个开发生命周期中仍然是一个关键问题，研究人员和行业从业者越来越关注保护和调整 LLM 行为与人类偏好和道德标准。在广泛的多语言语料库上训练的 LLM 表现出跨不同语言和领域的强大泛化能力。然而，当前的安全对齐实践主要集中在单语言场景上，这使得它们在复杂的多语言环境中的有效性，特别是对于那些复杂的混合语言格式，在很大程度上尚未得到探索。在本研究中，我们介绍了多语言混合，这是一种混合语言查询响应方案，旨在评估各种最先进的 LLM（例如 GPT-4o、GPT-3.5、Llama3）在复杂的多语言条件下的安全性对齐。我们进一步研究了语言可用性、形态和语系等语言模式，这些模式可能会影响多语言混合在破坏 LLM 安全措施方面的有效性。我们的实验结果表明，如果没有精心制作的提示模板，多语言混合会显著放大恶意查询的危害，导致 LLM 安全对齐的绕过率大幅增加（GPT-3.5 上为 67.23%，GPT-4o 上为 40.34%），远远超过单语言基线。此外，多语言混合的性能因内在语言属性而异，不同形态和不同语系的语言更容易逃避安全对齐。这些发现强调了在复杂的多语言环境中评估 LLM 并制定相应的安全对齐策略的必要性，以与其卓越的跨语言泛化能力保持一致。]]></description>
      <guid>https://arxiv.org/abs/2407.07342</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>LokiLM：技术报告</title>
      <link>https://arxiv.org/abs/2407.07370</link>
      <description><![CDATA[arXiv:2407.07370v1 公告类型：新
摘要：在这项工作中，我们引入了 LokiLM，这是一个在 500B 个标记上训练的 1.4B 参数大型语言模型。我们的模型在自然语言推理任务中表现出色，在具有 1.5B 或更少参数的模型中实现了最先进的性能。LokiLM 使用多教师知识提炼和高质量训练数据进行训练，以实现与在更多标记上训练的大型模型相媲美的基准测试结果。我们通过在整个开发过程中引入避免基准测试污染和过度拟合的步骤来支持这些发现。尽管 LokiLM 的性能令人鼓舞，但它表现出令人担忧的幻觉数量，并且在 TruthfulQA 基准测试中得分很低，因此我们不会公开发布该模型。]]></description>
      <guid>https://arxiv.org/abs/2407.07370</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>利用双重推理大型语言模型进行可解释的鉴别诊断</title>
      <link>https://arxiv.org/abs/2407.07330</link>
      <description><![CDATA[arXiv:2407.07330v1 公告类型：新摘要：方法论上的进步可以自动生成鉴别诊断 (DDx)，根据患者的症状描述预测一系列潜在疾病作为鉴别诊断，这对于临床推理和决策支持等应用至关重要。然而，为这些鉴别诊断提供推理或解释更有意义。幸运的是，大型语言模型 (LLM) 具有强大的语言处理能力，并且已被证明在各种相关任务中有效。受此潜力的启发，我们研究了 LLM 在可解释 DDx 中的应用。首先，我们开发了一个新的 DDx 数据集，其中包含 570 份公共临床记录的专家解释。其次，我们提出了一个名为 Dual-Inf 的新框架，使 LLM 能够进行双向推理以进行解释。人工和自动评估都证明了 Dual-Inf 在预测鉴别诊断和诊断解释方面的有效性。具体而言，与基线方法相比，Dual-Inf 的性能提升超过 32% w.r.t。 BERTScore 在 DDx 解释中的应用。此外，实验验证了 Dual-Inf (1) 在解释中犯的错误更少，(2) 具有很强的通用性，(3) 在罕见疾病的诊断和解释方面很有前景。]]></description>
      <guid>https://arxiv.org/abs/2407.07330</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>MixSumm：使用 LLM 进行基于主题的数据增强，实现低资源提取文本摘要</title>
      <link>https://arxiv.org/abs/2407.07341</link>
      <description><![CDATA[arXiv:2407.07341v1 公告类型：新
摘要：低资源提取文本摘要是一个重要但尚未得到充分探索的研究领域。先前的文献要么侧重于抽象文本摘要，要么直接提示大型语言模型 (LLM)（如 GPT-3）来生成摘要。在这项工作中，我们提出了用于低资源提取文本摘要的 MixSumm。具体来说，MixSumm 提示开源 LLM LLaMA-3-70b 生成混合来自多个主题的信息的文档，而不是生成没有混合的文档，然后在生成的数据集上训练摘要模型。我们使用 ROUGE 分数和 L-Eval（一种基于 LLaMA-3 的无参考评估方法）来衡量生成的摘要的质量。我们对由 TweetSumm、WikiHow 和 ArXiv/PubMed 数据集组成的具有挑战性的文本摘要基准进行了广泛的实验，并表明我们基于 LLM 的数据增强框架在低资源提取摘要方面优于近期基于提示的方法。此外，我们的结果还展示了从 LLaMA-3-70b 到基于 BERT 的小型提取摘要器的有效知识提炼。]]></description>
      <guid>https://arxiv.org/abs/2407.07341</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>RAG 与长上下文：检验用于环境评论文档理解的前沿大型语言模型</title>
      <link>https://arxiv.org/abs/2407.07321</link>
      <description><![CDATA[arXiv:2407.07321v1 公告类型：新
摘要：大型语言模型 (LLM) 已应用于各个领域的许多研究问题。LLM 的应用之一是提供迎合不同领域用户的问答系统。基于 LLM 的问答系统的有效性已经在用户提出热门和公共领域（例如琐事和文学）问题时达到了可接受的水平。然而，在传统上需要专业知识的小众领域，它往往没有得到建立。为此，我们构建了 NEPAQuAD1.0 基准来评估三个前沿 LLM——Claude Sonnet、Gemini 和 GPT-4——在回答美国联邦政府机构根据《国家环境法》（NEPA）准备的环境影响声明中的问题时的表现。我们特别衡量了 LLM 在不同情境下理解 NEPA 文件中存在的法律、技术和合规相关信息的细微差别的能力。例如，我们通过提供没有任何背景的问题来测试 LLM 的内部先前 NEPA 知识，并评估 LLM 如何综合长 NEPA 文档中存在的背景信息以促进问答任务。我们比较了长上下文 LLM 和 RAG 驱动模型在处理不同类型问题（例如，问题解决、发散）方面的表现。我们的结果表明，无论选择哪种前沿 LLM，RAG 驱动的模型在答案准确性方面都明显优于长上下文模型。我们的进一步分析表明，许多模型在回答封闭式问题方面的表现比回答发散性和解决问题的问题要好。]]></description>
      <guid>https://arxiv.org/abs/2407.07321</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:46 GMT</pubDate>
    </item>
    <item>
      <title>区分概率揭示大型语言模型中同质性偏差的脆弱性</title>
      <link>https://arxiv.org/abs/2407.07329</link>
      <description><![CDATA[arXiv:2407.07329v1 公告类型：新 
摘要：大型语言模型 (LLM) 中的同质性偏差是指它们倾向于将某些群体的表征与其他群体的表征同质化。记录这种偏差的先前研究主要使用编码器模型，这可能无意中引入了偏差。为了解决这一限制，我们提示 GPT-4 生成与 18 种情境线索相关的单词/表达完成 - 特定的、可衡量的环境元素会影响个人对情境的看法，并使用分化概率比较这些完成的变化。这种方法绕过编码器模型，直接从模型的输出中评估同质性偏差。在五项研究中，我们发现同质性偏差在情境线索和写作提示中高度不稳定，这表明过去工作中观察到的偏差可能反映了编码器模型而不是 LLM 中的偏差。此外，这些结果表明 LLM 中的同质性偏见是脆弱的，因为即使是提示中的微小和任意变化也会显著改变偏见的表达。未来的工作应该进一步探索较长文本生成中的句法特征和主题选择的变化如何影响 LLM 中的同质性偏见。]]></description>
      <guid>https://arxiv.org/abs/2407.07329</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:46 GMT</pubDate>
    </item>
    <item>
      <title>重复使用，不要重新训练：语言模型持续预训练的秘诀</title>
      <link>https://arxiv.org/abs/2407.07263</link>
      <description><![CDATA[arXiv:2407.07263v1 公告类型：新
摘要：随着语言模型的参数数量和预训练数据集大小的扩大，除了资源最丰富的团队外，预训练的计算成本已变得难以解决。这种不断增加的成本使得在完成预训练后能够重用模型变得越来越重要；允许模型的能力进一步提高，而无需从头开始训练。在这项工作中，我们详细介绍了一套指导方针，涵盖如何设计有效的数据分布和学习率计划以持续预训练语言模型。当在训练有素的 15B 参数模型之上的持续预训练运行中应用这些发现时，我们发现与在预训练集上持续训练的基线相比，平均模型准确率提高了 9%。由此产生的配方提供了一个实用的起点，可以通过重用而不是重新训练来开始开发语言模型。]]></description>
      <guid>https://arxiv.org/abs/2407.07263</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>ESM+：大型语言模型时代文本到 SQL 评估的现代见解</title>
      <link>https://arxiv.org/abs/2407.07313</link>
      <description><![CDATA[arXiv:2407.07313v1 公告类型：新
摘要：文本到 SQL 的任务使任何人都可以使用自然语言从 SQL 数据库中检索信息。尽管面临诸多挑战，但最近的模型使用大型语言模型 (LLM) 在这一任务上取得了显着进步。有趣的是，我们发现未经微调的基于 LLM 的模型与经过微调的模型相比表现出不同的性质，导致当前评估指标不足以准确传达其性能。因此，我们分析了两个主要指标，即测试套件执行准确度 (EXE) 和精确集匹配准确度 (ESM)，以检查它们对于此任务的稳健性并解决缺点。我们使用 EXE、原始 ESM 和我们改进的 ESM（称为 ESM+）比较了 9 个基于 LLM 的模型的性能。我们的结果表明，EXE 和 ESM 的误报率和误报率分别为 11.3% 和 13.9%，而 ESM+ 的误报率和误报率分别为 0.1% 和 2.6%，评估结果明显更稳定。我们将 ESM+ 脚本作为开源发布，供社区做出贡献，同时享受对 Text-to-SQL 的更可靠评估。]]></description>
      <guid>https://arxiv.org/abs/2407.07313</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>Nash CoT：具有偏好均衡的多路径推理</title>
      <link>https://arxiv.org/abs/2407.07099</link>
      <description><![CDATA[arXiv:2407.07099v1 公告类型：新
摘要：思路链 (CoT) 提示已成为一种强大的技术，可增强大型语言模型 (LLM) 在复杂问题上的推理能力。在与 CoT 相关的研究中，自我一致性（通过投票进行答案过滤的多路径推理）涉及使用 CoT 框架生成多条推理路径，然后选择最常产生的输出，这是一种简洁而又具有竞争力的方法。虽然自我一致性确实导致了 LLM 推理的改进，但使用多路径推理也会增加部署成本。因此，在降低推理成本的同时保持从多路径推理继承的自我一致性的性能优势具有重要价值。在这项研究中，我们将语言解码概念化为偏好共识游戏，在每个本地路径内构建一个双人游戏系统，并引入纳什思路链 (Nash CoT)。具体来说，对于给定的问题，我们利用 LLM 自主选择上下文相关的模板并生成由该模板引导的输出，旨在达到纳什均衡，同时在每条路径上实现正常生成。这种方法使我们能够在各种推理任务（包括阿拉伯语推理、常识性问答和符号推理）上使用更少的推理路径，同时实现与自洽性相当或更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.07099</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>哥伦比亚 2022 年选举过程中推特上的情绪识别</title>
      <link>https://arxiv.org/abs/2407.07258</link>
      <description><![CDATA[arXiv:2407.07258v1 公告类型：新
摘要：近年来，由于在相对自发的环境中可以获得大量数据，因此对 Twitter 作为一种分析社会现象的手段的研究引起了人们的兴趣。在意见挖掘任务中，情绪检测特别重要，因为它可以比基于极性的传统情绪分析更细致地识别人们对不同社会事件的主观反应。在政治事件的特定情况下，社交网络中的情绪分析可以提供有关候选人、提案和公共辩论其他重要方面的看法的宝贵信息。尽管这一点很重要，但关于西班牙语情绪检测的研究很少，而且据我们所知，哥伦比亚西班牙语的意见挖掘的公开资源很少，这凸显了生成针对这种多样性特定文化特征的资源的必要性。在这项工作中，我们展示了一个与 2022 年哥伦比亚总统选举有关的西班牙语推文小语料库，使用细粒度分类法手动标记情绪。我们使用监督式最新模型（BERT 模型）进行分类实验，并在小样本学习设置中将其与 GPT-3.5 进行比较。我们将数据集和代码公开用于研究目的。]]></description>
      <guid>https://arxiv.org/abs/2407.07258</guid>
      <pubDate>Thu, 11 Jul 2024 06:19:44 GMT</pubDate>
    </item>
    </channel>
</rss>