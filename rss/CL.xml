<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 20 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>假新闻检测：使用生成式 AI 注释数据对 BERT 类模型与大型语言模型进行比较评估</title>
      <link>https://arxiv.org/abs/2412.14276</link>
      <description><![CDATA[arXiv:2412.14276v1 公告类型：新
摘要：假新闻对现代社会的舆论和社会稳定构成了重大威胁。本研究对类似 BERT 的编码器专用模型和自回归解码器专用大型语言模型 (LLM) 进行了假新闻检测的比较评估。我们引入了一个新闻文章数据集，该数据集使用 GPT-4 辅助（一种 AI 标记方法）进行标记，并由人类专家验证以确保可靠性。类似 BERT 的编码器专用模型和 LLM 都在此数据集上进行了微调。此外，我们开发了一种指令调整的 LLM 方法，在推理过程中使用多数投票来生成标签。我们的分析表明，类似 BERT 的模型在分类任务中通常优于 LLM，而 LLM 对文本扰动表现出卓越的鲁棒性。与弱标签（远程监督）数据相比，结果表明具有人工监督的 AI 标签可获得更好的分类结果。这项研究强调了将基于人工智能的注释与人工监督相结合的有效性，并展示了不同类别的机器学习模型在假新闻检测方面的表现]]></description>
      <guid>https://arxiv.org/abs/2412.14276</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Multi-OphthaLingua：​​用于评估和消除中低收入国家法学硕士眼科质量保证偏差的多语言基准</title>
      <link>https://arxiv.org/abs/2412.14304</link>
      <description><![CDATA[arXiv:2412.14304v1 公告类型：新
摘要：当前眼科临床工作流程受到过度转诊、长时间等待以及复杂且异构的医疗记录的困扰。大型语言模型 (LLM) 提供了一种有希望的解决方案，可以自动化各种程序，例如分类、视力评估等初步测试以及报告摘要。然而，LLM 在自然语言问答任务中在不同语言中表现出显著不同的表现，这可能会加剧中低收入国家 (LMIC) 的医疗保健差距。这项研究引入了第一个多语言眼科问答基准，其中手动策划的问题跨语言并行，允许直接进行跨语言比较。我们对 7 种不同语言的 6 种流行 LLM 的评估显示，不同语言之间存在相当大的偏见，凸显了 LLM 在 LMIC 临床部署的风险。现有的去偏方法（例如翻译思路链或检索增强生成 (RAG)）本身无法缩小这种性能差距，通常无法提高所有语言的性能，并且缺乏针对医学领域的特异性。为了解决这个问题，我们提出了 CLARA（跨语言反射代理系统），这是一种利用检索增强生成和自我验证的新型推理时间去偏方法。我们的方法不仅可以提高所有语言的性能，还可以显著缩小多语言偏见差距，促进全球公平的 LLM 应用。]]></description>
      <guid>https://arxiv.org/abs/2412.14304</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>处理定语名词在提高汉英机器翻译质量中的作用</title>
      <link>https://arxiv.org/abs/2412.14323</link>
      <description><![CDATA[arXiv:2412.14323v1 公告类型：新
摘要：在语法惯例截然不同的语言之间进行翻译不仅对人类翻译人员而且对机器翻译系统都提出了挑战。在这项工作中，我们专门针对中文定语名词带来的翻译挑战，这经常导致英语翻译出现歧义。通过手动插入省略的助词 X（“DE”）。在宾夕法尼亚大学中文话语树库的新闻文章标题中，我们开发了一个有针对性的数据集来微调 Hugging Face 中文到英文的翻译模型，特别是改进了对这个关键功能词的处理方式。这种有针对性的方法不仅补充了先前研究提出的更广泛的策略，而且通过专门解决中英文翻译中常见的错误类型提供了实用的增强功能。]]></description>
      <guid>https://arxiv.org/abs/2412.14323</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NomBank 分区的语义角色标注</title>
      <link>https://arxiv.org/abs/2412.14328</link>
      <description><![CDATA[arXiv:2412.14328v1 公告类型：新
摘要：本文是关于 NomBank 注释语料库中英语部分名词的语义角色标注（价格/ARG1 的 5%/REL；价格/ARG1 上涨 5%/REL）。描述了几种使用传统和基于转换器的机器学习以及集成的系统。我们的最高得分系统使用来自 Penn Treebank 的“黄金”解析实现了 91.74% 的 F1，使用 Berkeley Neural 解析器时实现了 91.12% 的 F1。这项研究包括系统开发的课堂和实验设置。]]></description>
      <guid>https://arxiv.org/abs/2412.14328</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>同行评审值得努力吗？</title>
      <link>https://arxiv.org/abs/2412.14351</link>
      <description><![CDATA[arXiv:2412.14351v1 公告类型：新
摘要：同行评审在确定重要论文方面有多有效？我们将这个问题视为一项预测任务。我们能否根据地点和“早期回报”（出版后不久的引用）预测哪些论文将来会被大量引用？我们发现早期回报比地点更具预测性。最后，我们提出了建设性的建议来解决扩展挑战：（a）提交太多和（b）合格的审稿人太少。]]></description>
      <guid>https://arxiv.org/abs/2412.14351</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于法学硕士推理时间自我提升的调查</title>
      <link>https://arxiv.org/abs/2412.14352</link>
      <description><![CDATA[arXiv:2412.14352v1 公告类型：新
摘要：通过增加测试时的计算来增强推理的技术最近引起了人们的关注。在本次调查中，我们从三个不同的角度调查了 LLM 推理时间自我改进的现状：独立自我改进，重点是通过解码或采样方法进行增强；上下文感知自我改进，利用额外的上下文或数据存储；模型辅助自我改进，通过模型协作实现改进。我们对最近的相关研究进行了全面的回顾，提供了深入的分类法，并讨论了挑战和局限性，为未来的研究提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2412.14352</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>状态空间模型是强大的文本重排序工具</title>
      <link>https://arxiv.org/abs/2412.14354</link>
      <description><![CDATA[arXiv:2412.14354v1 公告类型：新
摘要：Transformers 在 NLP 和 IR 中占据主导地位；但它们的推理效率低下以及在推断更长上下文方面的挑战引发了人们对替代模型架构的兴趣。其中，像 Mamba 这样的状态空间模型 (SSM) 提供了有希望的优势，特别是推理中的 $O(1)$ 时间复杂度。尽管 SSM 具有潜力，但它在文本重新排名方面的有效性——一项需要细粒度查询文档交互和长上下文理解的任务——仍未得到充分探索。
本研究将基于 SSM 的架构（特别是 Mamba-1 和 Mamba-2）与基于 Transformer 的模型在不同规模、架构和预训练目标上进行了基准测试，重点关注文本重新排名任务的性能和效率。我们发现 (1) Mamba 架构实现了具有竞争力的文本排名性能，可与类似大小的基于 Transformer 的模型相媲美；(2) 与具有 flash 注意力的 Transformer 相比，它们在训练和推理方面的效率较低； （3）Mamba-2 在性能和效率方面均优于 Mamba-1。这些结果强调了状态空间模型作为变压器替代方案的潜力，并突出了未来 IR 应用中需要改进的领域。]]></description>
      <guid>https://arxiv.org/abs/2412.14354</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>记忆胜过推理？揭露和缓解大型语言模型字符理解评估中的逐字记忆</title>
      <link>https://arxiv.org/abs/2412.14368</link>
      <description><![CDATA[arXiv:2412.14368v1 公告类型：新
摘要：最近，大型语言模型 (LLM) 在人物理解任务中表现出色，例如分析虚构人物的角色、性格和关系。然而，LLM 使用的大量预训练语料库引发了人们的担忧，即它们可能依赖于记忆流行的小说作品，而不是真正理解和推理它们。在这项工作中，我们认为“要点记忆”——捕捉基本含义——应该是人物理解任务的主要机制，而不是“逐字记忆”——字符串的精确匹配。我们介绍了一种简单而有效的方法来减轻人物理解评估中的机械化记忆，同时保留理解和推理所需的基本隐含线索。我们的方法将流行小说作品的记忆驱动性能从 96% 的准确率降低到 72%，并导致各种人物理解任务的准确率下降高达 18%。这些发现强调了现有基准测试中的数据污染问题，这些基准测试通常衡量的是记忆力而不是真正的字符理解力。]]></description>
      <guid>https://arxiv.org/abs/2412.14368</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ECG-Byte：端到端生成心电图语言建模的标记器</title>
      <link>https://arxiv.org/abs/2412.14373</link>
      <description><![CDATA[arXiv:2412.14373v1 公告类型：新
摘要：大型语言模型 (LLM) 已在文本以外的领域表现出显著的适应性，特别是心电图 (ECG)。更具体地说，越来越多的研究正在探索从多通道 ECG 和相应的文本提示生成文本的任务。当前的方法通常涉及使用自监督学习 (SSL) 目标对 ECG 特定编码器进行预训练，并使用预训练编码器输出的特征来微调 LLM 以进行自然语言生成 (NLG)。然而，这些方法受到 1) 两阶段训练效率低下和 2) 编码器生成特征的可解释性挑战的限制。为了解决这些限制，我们引入了 ECG-Byte，这是一种用于 ECG 自回归语言建模的自适应字节对编码 (BPE) 标记器管道。该方法将 ECG 信号压缩并编码为 token，通过直接组合 ECG 和文本 token 实现端到端 LLM 训练，同时由于 ECG token 可以直接映射回原始信号，因此更易于解释。使用 ECG-Byte，我们仅用两阶段方法所需时间的一半和约 48% 的数据就实现了 NLG 任务中的竞争性性能。]]></description>
      <guid>https://arxiv.org/abs/2412.14373</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对特定领域 LLM 的一体化调整和结构修剪</title>
      <link>https://arxiv.org/abs/2412.14426</link>
      <description><![CDATA[arXiv:2412.14426v1 公告类型：新 
摘要：现有的针对特定领域应用的大型语言模型 (LLM) 的修剪技术通常遵循两阶段过程：修剪预训练的通用 LLM，然后在特定领域微调修剪后的 LLM。然而，即使权重已更新，从预训练权重得出的修剪决策在微调期间仍保持不变。因此，这种修剪决策和微调权重的组合可能不是最优的，从而导致不可忽略的性能下降。为了解决这些限制，我们提出了 ATP：一体化调整和结构修剪，这是一种统一的单阶段结构修剪和微调方法，它通过可训练的修剪决策生成器在整个微调阶段动态识别当前最佳子结构。此外，鉴于特定领域应用的可用数据有限，低秩自适应 (LoRA) 成为微调 LLM 的常用技术。在 ATP 中，我们引入了 LoRA 感知的前向和稀疏正则化，以确保在 ATP 过程之后可以直接删除与学习到的修剪决策相对应的子结构。ATP 在法律和医疗保健领域的任务上优于最先进的两阶段修剪方法。更具体地说，当修剪 LLaMA2-7B 和 LLaMA3-8B 模型的 40% 参数时，ATP 分别恢复了密集模型高达 88% 和 91% 的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.14426</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ORBIT：以天文学案例研究为对象，实现大型语言模型领域自适应的经济高效的数据集管理</title>
      <link>https://arxiv.org/abs/2412.14436</link>
      <description><![CDATA[arXiv:2412.14436v1 公告类型：新
摘要：语言建模的最新进展表明，需要高质量的特定领域训练数据，尤其是对于需要专业知识的任务。通用模型虽然用途广泛，但由于领域特定信息有限，往往缺乏专家级任务所需的深度。领域适应训练可以增强这些模型，但它需要大量高质量的数据。为了解决这个问题，我们提出了 ORBIT，这是一种经济高效的方法，用于从嘈杂的网络源中整理大量高质量的特定领域数据集，专为训练专业的大型语言模型而量身定制。以天文学为主要案例研究，我们将 1.3T 标记 FineWeb-Edu 数据集细化为专注于天文学的高质量 10B 标记子集。在 1B 标记天文学子集上对 \textsc{LLaMA-3-8B} 进行微调，可将 MMLU 天文学基准上的性能从 69\% 提高到 76\%，并在天文学专用基准 AstroBench 上取得最佳结果。此外，我们的模型 (Orbit-LLaMA) 的表现优于 \textsc{LLaMA-3-8B-base}，在 1000 个天文学专用问题中，GPT-4o 评估在 73\% 的情况下更倾向于它。此外，我们通过将 ORBIT 应用于法律和医学来验证其通用性，与未过滤的基线相比，数据质量显著提高。我们在 \href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama} 上开源了 ORBIT 方法，包括精选数据集、代码库和生成的模型。]]></description>
      <guid>https://arxiv.org/abs/2412.14436</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从人工注释到法学硕士：管理研究的 SILICON 注释工作流程</title>
      <link>https://arxiv.org/abs/2412.14461</link>
      <description><![CDATA[arXiv:2412.14461v1 公告类型：新
摘要：非结构化文本数据注释和分析是管理研究的基础，通常依赖于众包平台的人工注释。虽然大型语言模型 (LLM) 有望提供一种经济高效且可替代人工注释的方法，但缺乏系统的工作流程来评估 LLM 何时适用或如何以可重复的方式进行基于 LLM 的文本注释。本文通过引入“SILICON”（使用 \textbf{S}Stematic \textbf{I}nference with \textbf{L}LMs for \textbf{I}nformation \textbf{C}Classificati\textbf{o}n and \textbf{N}otation）工作流程来解决这一方法论上的差距。该工作流程将已建立的人工注释原则与系统提示优化和模型选择相结合，解决了诸如制定强大的注释指南、建立高质量的人工基线、优化提示以及确保跨 LLM 的可重复性等挑战。我们通过七个案例研究验证了 SILICON 工作流程，这些案例研究涵盖了常见的管理研究任务，包括商业提案评估、对话意图和细分分析、评论属性检测。我们的研究结果强调了验证注释指南一致性的重要性、专家开发的人工基线相对于众包基线的优越性、提示优化的迭代性质以及测试多个 LLM 的必要性。值得注意的是，我们提出了一种基于回归的方法来实证比较 LLM跨提示和模型的输出。我们的工作流程通过建立可重复的 LLM 注释流程来推进管理研究，同时保持科学严谨性。我们为研究人员提供实用指导，以有效驾驭生成式 AI 工具不断发展的格局，同时保持透明度和可重复性。]]></description>
      <guid>https://arxiv.org/abs/2412.14461</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Agent-SafetyBench：评估 LLM 代理的安全性</title>
      <link>https://arxiv.org/abs/2412.14470</link>
      <description><![CDATA[arXiv:2412.14470v1 公告类型：新 
摘要：随着大型语言模型 (LLM) 越来越多地被部署为代理，它们与交互式环境和工具使用的集成带来了新的安全挑战，而不仅仅是与模型本身相关的挑战。然而，缺乏用于评估代理安全性的综合基准对有效评估和进一步改进构成了重大障碍。在本文中，我们介绍了 Agent-SafetyBench，这是一个旨在评估 LLM 代理安全性的综合基准。Agent-SafetyBench 包含 349 个交互环境和 2,000 个测试用例，评估了 8 类安全风险，涵盖了不安全交互中经常遇到的 10 种常见故障模式。我们对 16 个流行的 LLM 代理的评估揭示了一个令人担忧的结果：没有一个代理的安全分数超过 60%。这凸显了 LLM 代理面临的重大安全挑战，并强调了改进的巨大需求。通过定量分析，我们确定了关键故障模式，并总结了当前 LLM 代理中的两个基本安全检测：缺乏稳健性和缺乏风险意识。此外，我们的研究结果表明，仅依靠防御提示不足以解决这些安全问题，这强调了对更先进、更强大的策略的需求。我们在 \url{https://github.com/thu-coai/Agent-SafetyBench} 发布了 Agent-SafetyBench，以促进代理安全评估和改进方面的进一步研究和创新。]]></description>
      <guid>https://arxiv.org/abs/2412.14470</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>为何构建本地大型语言模型：来自 35 名日语和多语言法学硕士的观察分析</title>
      <link>https://arxiv.org/abs/2412.14471</link>
      <description><![CDATA[arXiv:2412.14471v1 公告类型：新
摘要：为什么要建立本地大型语言模型（LLM）？本地 LLM 应该从目标语言中学习什么？哪些能力可以从其他语言中迁移？是否存在特定于语言的缩放定律？为了探索这些研究问题，我们将日语作为本地语言，在 19 个日语和英语评估基准上评估了 35 个日语、英语和多语言 LLM。采用观察方法，我们分析了基准分数的相关性，并对分数进行了主成分分析（PCA），以得出本地 LLM 的 \textit{能力因子}。我们发现，对英语文本进行训练可以提高日语学术科目（JMMLU）的分数。此外，无需专门对日语文本进行训练即可提高解决日语代码生成、算术推理、常识和阅读理解任务的能力。相比之下，对日语文本进行训练可以提高有关日语知识和英日翻译的问答任务，这表明解决这两个任务的能力可以被视为 LLM 的 \textit{日语能力}。此外，我们证实日语能力与日语文本的计算预算成正比。]]></description>
      <guid>https://arxiv.org/abs/2412.14471</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是否捍卫推理语义学？：论法学硕士的逻辑表达主义和反表征主义</title>
      <link>https://arxiv.org/abs/2412.14501</link>
      <description><![CDATA[arXiv:2412.14501v1 公告类型：新
摘要：语言哲学历来都是通过人类中心主义的视角发展起来的，但现在由于大型语言模型 (LLM) 的出现而被迫走向后人类中心主义，例如 ChatGPT (OpenAI)、Claude (Anthropic)，这些模型被认为具有与人类相当的语言能力。传统上，LLM 是通过分布语义作为其基础语义来解释的。然而，最近的研究正在探索分布语义之外的替代基础语义。本文提出了罗伯特·布兰登 (Robert Brandom) 的推理语义作为 LLM 的合适基础语义，特别关注这种后人类中心主义趋势中的语言表征主义问题。在这里，我们表明推理语义的反表征主义和逻辑表达主义以及准组合性有助于解释 LLM 的特征和行为。此外，我们提出了针对法学硕士的\emph{真理共识理论}。本文认为，法学硕士的特点挑战了语言哲学的主流假设，例如语义外在主义和组合性。我们相信，本文中的论点将导致对反\连字符表征主义语言观点的重新评估，并可能导致语言哲学的新发展。]]></description>
      <guid>https://arxiv.org/abs/2412.14501</guid>
      <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>