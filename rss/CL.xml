<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Thu, 13 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>在LLM投资建议中暴露产品偏差</title>
      <link>https://arxiv.org/abs/2503.08750</link>
      <description><![CDATA[ARXIV：2503.08750V1公告类型：新 
摘要：作为新一代推荐引擎的大型语言模型（LLMS）具有强大的摘要和数据分析功能，超过了范围和性能的传统推荐系统。一个有希望的申请是投资建议。在本文中，我们在LLM投资建议中揭示了一种新的产品偏见，其中LLMS对特定产品表现出系统性的偏好。这样的偏好可以巧妙地影响用户投资决策，这可能导致产品和财务泡沫的估值膨胀，从而对个人投资者和市场稳定构成风险。为了全面研究产品偏见，我们开发了一条自动管道，以在五个资产类别（股票，共同基金，加密货币，储蓄和投资组合）的数据集中创建567,000个样本的数据集。使用此数据集，我们介绍了BF在LLM投资建议中有关产品偏见的首次研究。我们的发现表明，LLMS表现出清晰的产品偏好，例如某些股票（例如，来自Apple的AAPL&#39;和Microsoft的“ MSFT”）。值得注意的是，即使在应用了辩护技术之后，这种偏见仍然存在。我们敦促AI研究人员注意LLM投资建议及其含义中的产品偏见，以确保数字空间和市场的公平性和安全性。]]></description>
      <guid>https://arxiv.org/abs/2503.08750</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ESNLIR：具有因果关系的西班牙多流派数据集</title>
      <link>https://arxiv.org/abs/2503.08803</link>
      <description><![CDATA[ARXIV：2503.08803V1公告类型：新 
摘要：自然语言推断（NLI），也称为识别文本元素（RTE），是自然语言处理（NLP）领域内的关键领域。该领域从根本上使机器能够辨别各种文本部分之间的语义关系。尽管已经为英语执行了大量工作，但已经观察到西班牙语的努力相对稀少。考虑到这一点，本文着重于为NLI（ESNLIR）生成一个多流派的西班牙数据集，尤其是为因果关系而言。初步的基线已被概念化并经过评估，利用了伯特家族的模型。研究结果表明，流派的富集基本上有助于富集模型的概括能力。
  该实验的代码，笔记本和整个数据集可在以下网址提供：https：//zenodo.org/records/15002575。如果您仅对数据集有兴趣，则可以在此处找到：https：//zenodo.org/records/15002371。]]></description>
      <guid>https://arxiv.org/abs/2503.08803</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>盘问：评估大语言模型生成的解释的一致性</title>
      <link>https://arxiv.org/abs/2503.08815</link>
      <description><![CDATA[ARXIV：2503.08815V1公告类型：新 
摘要：通常要求大型语言模型（LLMS）解释其输出以提高准确性和透明度。但是，有证据表明，这些解释可能会歪曲模型的真实推理过程。在这些解释中确定不准确或遗漏的一种有效方法是通过一致性检查，这通常涉及提出后续问题。本文介绍了一种盘问，这是一种基于模型对初始问题的解释来生成后续问题的新方法。我们的方法将符号信息提取与语言模型驱动的问题的产生相结合，从而比仅LLMS产生的问题提出了更好的后续问题。此外，这种方法比其他方法更灵活，并且可以产生更广泛的后续问题。]]></description>
      <guid>https://arxiv.org/abs/2503.08815</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EVOBPE：进化蛋白序列令牌化</title>
      <link>https://arxiv.org/abs/2503.08838</link>
      <description><![CDATA[ARXIV：2503.08838V1公告类型：新 
摘要：计算生物学的最新进步在蛋白质序列和语言结构之间取得了引人注目的相似之处，突出了对捕获蛋白质序列复杂进化动力学的复杂令牌化方法的需求。当前主要用于自然语言处理的当前子单词令牌技术通常无法充分代表蛋白质序列的复杂结构和功能性能。这项研究介绍了Evobpe，这是一种新型的令牌化方法，将进化突变模式整合到序列分割中，解决了现有方法中的临界局限性。通过利用已建立的替代矩阵，Evobpe超越了传统的基于频率的令牌化策略。该方法通过生物学知情的突变生成候选令牌对，并根据成对比对分数和频率阈值对它们进行评估。关于人类蛋白质序列的广泛实验表明，EVOBPE在多个维度上的性能更好。域保护分析表明，EvoBPE始终胜过标准字节对编码，尤其是随着词汇量的增加。此外，使用ESM-2嵌入相似性分析表明，基于突变的令牌替换比任意取代更有效地保护生物序列特性。该研究通过引入一种突变感知的令牌化方法来促进蛋白质序列表示，从而更好地捕获进化性的细微差别。通过桥接计算语言学和分子生物学，Evobpe为蛋白质功能预测，结构建模和进化分析中的机器学习应用开辟了新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2503.08838</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与LLM的多党对话的对比性演讲者意识学习</title>
      <link>https://arxiv.org/abs/2503.08842</link>
      <description><![CDATA[ARXIV：2503.08842V1公告类型：新 
摘要：由于多个扬声器和交织的对话线程的复杂相互作用，多方对话生成提出了重大挑战。传统方法通常在捕获这些复杂性方面差不多，尤其是在依靠手动注释的对话关系时。本文介绍了演讲者指导LLM（SA-LLM），这是一种新颖的生成模型，利用预先训练的大型语言模型（LLM）和一种说话者意识到的对比学习策略来应对这些挑战。 SA-LLM结合了讲话者的输入编码和一个对比度学习目标，以隐式学习上下文连贯性和说话者角色而没有明确的关系注释。关于Ubuntu IRC和电影对话数据集的广泛实验表明，SA-LLM在自动和人类评估中的最先进基线表现明显优于最先进的基线，从而在流利性，连贯性，信息性和响应多样性方面取得了出色的表现。消融研究和详细的错误分析进一步验证了拟议的说话者训练方法的有效性，从而强调了其在不同的说话者角色和上下文长度之间的鲁棒性。结果强调了SA-LLM作为高质量多方对话生成的强大且无注释的解决方案的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.08842</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过自然语言摘要使用LLMS来解释和稳健的对话状态跟踪</title>
      <link>https://arxiv.org/abs/2503.08857</link>
      <description><![CDATA[ARXIV：2503.08857V1公告类型：新 
摘要：本文介绍了一种新颖的对话状态跟踪方法（DST），该方法利用大型语言模型（LLMS）生成对话状态的自然语言描述，而不是传统的插槽值表示。传统的DST方法与开放域对话和嘈杂的输入相加。我们的自然语言DST（NL-DST）框架是由LLM的生成能力的动机训练LLM，以直接综合人类可读的状态描述。我们通过对多沃兹2.1和TaskMaster-1数据集进行的广泛实验来证明NL-DST显着胜过基于规则的基于规则的基于BERT的DST基准，以及生成的插槽填充GPT-2 DST模型，在关节目标准确性和SLOT精度中均具有。消融研究和人类评估进一步验证了自然语言状态产生的有效性，突出了其对噪声的稳健性和增强的解释性。我们的发现表明，NL-DST为对话状态跟踪提供了一种更灵活，准确和人为理解的方法，为更健壮和适应性的任务对话系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2503.08857</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS知道要丢弃什么：自我发挥的指导KV缓存驱逐有效的长期推断</title>
      <link>https://arxiv.org/abs/2503.08879</link>
      <description><![CDATA[ARXIV：2503.08879V1公告类型：新 
摘要：高效的长篇小说推断至关重要，因为大语言模型（LLMS）采用上下文窗口，范围从128K到1M令牌。但是，不断增长的键值（KV）缓存以及注意力的高计算复杂性在记忆使用和延迟中创造了重要的瓶颈。在本文中，我们发现各种长篇文章任务中的注意力表现出稀疏性，而LLMS隐含地“知道”了哪些令牌可以在预填充阶段后在头部降低或驱逐。基于这种见解，我们提出了自我发挥的指导驱逐〜（Sage-kv），这是一种简单有效的KV驱逐缓存方法，用于长篇小说推断。预填充后，我们的方法在令牌和头部级别上都进行了一次top-K选择，以压缩KV缓存，从而有效地推断了减少的缓存。对Longbench和三个长篇小说LLM（Llama3.1-8B-Instruct-128K，Llama3-8B-Prolong-512K-Instruct和Qwen2.5-7B-Instruct-1128K）的评估表明，Sage-kv保持了与全部注意力相当的准确性，同时提高了效率。具体而言，SAGE-KV比静态KV缓存选择方法的精度提高了4倍的内存效率，并且比动态KV缓存选择方法任务更高，并具有更高精度的2倍内存效率。]]></description>
      <guid>https://arxiv.org/abs/2503.08879</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>plainqafact：生物医学普通语言摘要生成的自动事实评估度量</title>
      <link>https://arxiv.org/abs/2503.08890</link>
      <description><![CDATA[ARXIV：2503.08890V1公告类型：新 
摘要：语言模型的幻觉输出在医疗领域构成风险，尤其是对于做出健康相关决定的外行观众而言。现有的事实评估方法，例如基于问题和提问的基于问题的（QA），由于精明的解释现象而与普通语言摘要（PLS）生成斗争，该现象引入了外部内容（例如，定义，背景，示例，示例），从源文档中缺乏来提高理解。为了解决这个问题，我们介绍了PlainQafact，该框架是在细粒度的，人类通知的plainfact上训练的框架，以评估源简化和精心解释的句子的事实。 PlainQafact首先将事实类型分类，然后使用基于QA的检索QA评分方法评估事实。我们的方法轻巧且计算上有效。经验结果表明，现有的事实指标无法有效地评估PLS中的事实，尤其是对于精明的解释，而PlainQafact实现了最新的表现。我们进一步分析了其跨外部知识来源，回答提取策略，重叠措施和记录粒度水平的有效性，从而完善了其整体事实评估。]]></description>
      <guid>https://arxiv.org/abs/2503.08890</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估者：通过层次能力树对语言模型进行分析</title>
      <link>https://arxiv.org/abs/2503.08893</link>
      <description><![CDATA[ARXIV：2503.08893V1公告类型：新 
摘要：理想的模型评估应实现两个目标：确定模型失败的位置并提供可行的改进指导。朝着这些语言模型（LM）评估的目标，我们制定了产生弱点概况的问题，鉴于LM在基准中的每个实例上的表现，都以自然语言表达的一组弱点。我们引入了一套定量评估，以比较不同的弱点分析方法。我们还提出了一个弱点分析方法评估者。它构建了一个能力树，其中每个节点代表自然语言中描述的能力，并与专门评估该能力的基准实例的子集相连。然后，它提取LM表现不佳以产生弱点的节点。在数学和Wildchat的基准上，我们表明，通过更精确，更全面地识别弱点，评估者的表现优于基线弱点方法。弱点概况进一步使弱点引导的数据收集和以评估者识别的弱点为指导的培训数据收集比其他数据收集策略更能提高LM的性能。我们还展示了评估者如何在Chatbot Arena的基于人类投票的评估实践中暴露缺陷。为了促进未来的工作，我们发布了代码和一个界面，该界面允许从业者交互式探索评估者构建的功能树。]]></description>
      <guid>https://arxiv.org/abs/2503.08893</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>回溯安全</title>
      <link>https://arxiv.org/abs/2503.08919</link>
      <description><![CDATA[ARXIV：2503.08919V1公告类型：新 
摘要：大型语言模型（LLM）在各种任务中都表现出了显着的功能，但是确保其与人类价值观的安全性和一致性仍然至关重要。当前的安全一致性方法，例如受监督的微调和基于加强学习的方法，可以表现出对对抗性攻击的脆弱性，并且通常会导致较浅的安全对准，主要集中于在生成的产出的初始标记中防止有害内容。尽管重置等方法可以通过丢弃前代币并重新启动生成过程来帮助从不安全的世代中恢复过来，但它们不适合解决细微的安全违规行为，例如在否则良性和漫长的一代中可能出现的毒性。在本文中，我们提出了一种旨在解决这些局限性的新型回溯方法。我们的方法使该模型可以恢复到更安全的一代状态，而不一定是在发电期间发生安全违规的情况下。这种方法可以针对有问题的细分市场进行有针对性的纠正，而无需丢弃整个生成的文本，从而确保了效率。我们证明，我们的方法大大降低了通过生成过程出现的毒性，对效率的影响很小。]]></description>
      <guid>https://arxiv.org/abs/2503.08919</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度引导的注意图编辑：迈向有效的上下文幻觉缓解</title>
      <link>https://arxiv.org/abs/2503.08963</link>
      <description><![CDATA[ARXIV：2503.08963V1公告类型：新 
摘要：在诸如摘要和开放式问题答案（QA）之类的任务中，大语言模型（LLMS）经常遇到“上下文幻觉”，尽管可以访问准确的源信息，但它们会产生无关紧要或不正确的答案。这通常是因为这些模型倾向于在输入上下文中优先考虑自我生成的内容，从而导致它们无视相关的细节。为了应对这一挑战，我们介绍了一种名为“指导注意力图编辑”（游戏）的新方法，该方法会动态调整注意力图以提高上下文相关性。在推论期间，Game采用训练有素的分类器来确定容易引起幻觉和执行目标干预措施的注意力图。这些干预措施以梯度信息的“编辑指示”为指导，从战略上重新分配了各种头部的注意力，以有效地减少幻觉。对挑战性汇总和QA任务的全面评估表明，游戏一致地表明，游戏会始终如一地降低各种开放式模型的幻觉。专门为X型降低了10％的XPONTIONS。与最先进的基线相比，效率。]]></description>
      <guid>https://arxiv.org/abs/2503.08963</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于科学发现的代理AI：对进度，挑战和未来方向的调查</title>
      <link>https://arxiv.org/abs/2503.08979</link>
      <description><![CDATA[ARXIV：2503.08979V1公告类型：新 
摘要：将代理AI集成到科学发现中标志着研究自动化的新领域。这些能够推理，计划和自主决策的AI系统正在改变科学家如何执行文献综述，产生假设，进行实验和分析结果。这项调查提供了针对科学发现，对现有系统和工具进行分类的代理AI的全面概述，并强调了化学，生物学和材料科学等领域的最新进展。我们讨论关键评估指标，实施框架以及常用的数据集，以详细了解该领域的现状。最后，我们应对关键挑战，例如文献审查自动化，系统可靠性和道德问题，同时概述了强调人类协作和增强系统校准的未来研究方向。]]></description>
      <guid>https://arxiv.org/abs/2503.08979</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Semeval-2025任务7的Word2Winners 7：多语言和跨语言事实检查的主张检索</title>
      <link>https://arxiv.org/abs/2503.09011</link>
      <description><![CDATA[ARXIV：2503.09011V1公告类型：新 
摘要：本文介绍了我们的Semeval 2025任务7：以前的事实检查的主张检索。该任务需要从广泛的多语言多声称数据集中检索给定输入索赔的相关事实检查，该数据集包括几种语言的社交媒体帖子和事实检查。为了应对这一挑战，我们首先使用最先进的英语和多语言检索模型评估了零拍的性能，然后微调了最有前途的系统，利用机器翻译以增强跨语言检索。我们的最佳模型在跨语言数据上获得了85％的精度，单语数据的准确度为92％。]]></description>
      <guid>https://arxiv.org/abs/2503.09011</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与什么保持一致？限制基于RLHF的对齐</title>
      <link>https://arxiv.org/abs/2503.09025</link>
      <description><![CDATA[ARXIV：2503.09025V1公告类型：新 
摘要：从人类反馈（RLHF）中学习的强化学习越来越多地将大型语言模型（LLMS）与人类的偏好保持一致。但是，RLHF在解决潜在偏见方面的有效性尚不清楚。这项研究调查了LLM中RLHF与秘密偏见之间的关系，尤其是针对非裔美国人的偏见。我们将各种RLHF技术（DPO，ORPO和RLOO）应用于Llama 3 8B，并使用匹配的旋转探测和显式偏置测试评估了所得模型的秘密和明显偏见。我们在不同的基本模型和数据集上使用DPO进行了其他测试。在几种含义中，我们发现RLHF之前的SFT会钙化模型偏差。此外，我们扩展了测量多模式模型偏见的工具。通过我们的实验，我们收集了证据表明，目前的一致性技术不足以用于减轻秘密偏见等模糊任务，突出了对功能强大的数据集，数据策划技术或对齐工具的需求。]]></description>
      <guid>https://arxiv.org/abs/2503.09025</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAST：在大语言模型上难以意识到自我训练</title>
      <link>https://arxiv.org/abs/2503.09029</link>
      <description><![CDATA[ARXIV：2503.09029V1公告类型：新 
摘要：目前的大型语言模型（LLM）自训练方法总是在具有挑战性的查询上样本不足，从而导致对限制LLMS能力的困难问题的学习不足。因此，这项工作提出了一个困难的自我训练（DAST）框架，重点是改善自我训练期间有挑战性查询的自我生成反应的数量和质量。 DAST在三个组件中指定：1）基于抽样的难度水平估计，2）难以感知的数据增强和3）分别使用SFT和DPO的自我训练算法。关于数学任务的实验证明了DAST的有效性和概括，强调了困扰策略在推进LLM自我训练中的关键作用。]]></description>
      <guid>https://arxiv.org/abs/2503.09029</guid>
      <pubDate>Thu, 13 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>