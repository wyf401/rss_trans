<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>高棉语作者同音词拼写纠正模型重要性调查</title>
      <link>https://arxiv.org/abs/2411.10477</link>
      <description><![CDATA[arXiv:2411.10477v1 公告类型：新
摘要：同音词对任何语言的作者来说都是一个重大挑战，因为它们的发音相似，但含义和拼写不同。这个问题在高棉语中尤为明显，因为高棉语结构复杂，字符集广泛，同音词丰富。这项研究旨在解决高棉作者在写作中使用同音词时面临的困难，并根据广泛的文献综述和调查分析提出了潜在的解决方案。对 108 名高棉语母语人士（包括学生、员工和专业人士）的调查显示，许多人在写作中经常遇到同音词的挑战，经常难以根据上下文选择正确的单词。调查还强调了缺乏有效的工具来解决高棉语中的同音词错误，这使写作过程变得复杂。此外，对英语、阿塞拜疆语和孟加拉语等其他语言拼写纠正的现有研究的回顾发现，缺乏专门针对同音词的研究，尤其是高棉语。总之，这项研究强调了需要一种专门的工具来解决高棉语同音词错误。通过弥补当前研究和可用资源方面的差距，这种工具将提高高棉语作者写作的信心和准确性，从而有助于丰富和保存该语言。在这一领域继续努力对于确保高棉语能够有效利用技术和语言学的进步至关重要。]]></description>
      <guid>https://arxiv.org/abs/2411.10477</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>“论语言理论的目标”：人工智能时代的乔姆斯基理论再探</title>
      <link>https://arxiv.org/abs/2411.10533</link>
      <description><![CDATA[arXiv:2411.10533v1 公告类型：新
摘要：理论语言学试图解释人类语言是什么以及为什么存在。语言学家和认知科学家提出了不同的语言理论模型，以及塑造语言的认知因素，使人类能够“产生”、“理解”和“获得”自然语言。然而，人类可能不再是唯一学习“生成”、“解析”和“学习”自然语言的人：人工智能 (AI) 模型（例如大型语言模型）已被证明具有令人印象深刻的语言能力。因此，许多人质疑这些模型在帮助理论语言学实现其最终研究目标方面应该发挥什么作用（如果有的话）。在本文中，我们建议通过重申生成语言学（该领域的领先思想流派）的原则，并考虑作为语言理论的人工智能模型如何与这些重要概念中的每一个相关来回答这个问题。具体来说，我们考虑了三个基本原则，这些原则的根源可以追溯到诺姆·乔姆斯基的早期著作：（1）理论充分性水平；（2）语言理论发展程序；（3）语言可学习性和通用语法。在讨论每个原则时，我们特别关注两种类型的人工智能模型：神经语言模型和神经语法归纳模型。我们将论证这些模型，特别是神经语法归纳模型，确实可以发挥作用，但这种作用在很大程度上受到人们对这三个指导原则所持立场的影响。]]></description>
      <guid>https://arxiv.org/abs/2411.10533</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提示格式对 LLM 表现有影响吗？</title>
      <link>https://arxiv.org/abs/2411.10541</link>
      <description><![CDATA[arXiv:2411.10541v1 公告类型：新
摘要：在大型语言模型 (LLM) 领域，提示优化对于模型性能至关重要。尽管先前的研究已经探索了诸如重新措辞提示上下文、使用各种提示技术（如上下文学习和思路链）和排序少量示例等方面，但我们对 LLM 对提示模板的敏感性的理解仍然有限。因此，本文研究了不同提示模板对 LLM 性能的影响。我们将相同的上下文格式化为各种人类可读的模板，包括纯文本、Markdown、JSON 和 YAML，并使用 OpenAI 的 GPT 模型评估它们在自然语言推理、代码生成和翻译等任务中的影响。实验表明，GPT-3.5-turbo 的性能在代码翻译任务中根据提示模板的不同而变化高达 40\%，而像 GPT-4 这样的大型模型对这些变化更具鲁棒性。我们的分析强调需要重新考虑使用固定提示模板，因为不同的格式会显著影响模型性能。]]></description>
      <guid>https://arxiv.org/abs/2411.10541</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MLAN：基于语言的指令调整可改善多模态大型语言模型的零样本泛化</title>
      <link>https://arxiv.org/abs/2411.10557</link>
      <description><![CDATA[arXiv:2411.10557v2 公告类型：新
摘要：我们提出了一种新颖的指令调整方法，以改进多模态大型语言模型的零样本任务泛化。与严重依赖视觉指令的现有指令调整机制相比，我们的方法专注于基于语言的指令调整，为多模态指令调整提供了一条独特且更高效的训练路径。我们在语言和视觉模态的 9 个未见数据集上评估了所提出方法的性能。我们的结果表明，我们的纯语言指令调整能够显著提高基于 Llama 2 和 Vicuna 的两个预训练多模态模型在这些未见数据集上的性能。有趣的是，语言指令跟踪能力还有助于解锁模型以遵循视觉指令而无需明确训练。与主要基于视觉指令的最先进的多模态指令调整方法相比，我们基于语言的方法不仅实现了卓越的性能，而且显著提高了训练效率。例如，由于对视觉数据的需求显著减少，仅对语言指令的调整在评估的数据集上产生了具有竞争力的平均性能（在语言数据集上的性能甚至更好），并且训练效率显著提高（平均提高了 4 倍）。借助少量的视觉指令，这种新兴的语言指令跟随能力可以很好地转移到看不见的视觉数据集，以更高的训练效率超越了最先进的技术。]]></description>
      <guid>https://arxiv.org/abs/2411.10557</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多语言神经机器翻译中的快捷学习</title>
      <link>https://arxiv.org/abs/2411.10581</link>
      <description><![CDATA[arXiv:2411.10581v1 公告类型：新
摘要：在本研究中，我们重新审视了多语言神经机器翻译 (MNMT) 中常见的脱靶问题。通过精心设计不同 MNMT 场景和模型的实验，我们将脱靶问题归因于 (非中心、中心) 语言映射的快捷方式的过度拟合。具体而言，学习到的快捷方式会导致 MNMT 错误地将非中心语言翻译成中心语言，而不是零样本翻译的预期非中心语言。学习动态分析表明，快捷方式学习通常发生在模型训练的后期，多语言预训练会加速和加剧快捷方式学习。基于这些观察，我们提出了一种简单有效的训练策略，利用模型训练的遗忘特性来消除 MNMT 模型中的快捷方式。与标准训练的唯一区别在于，我们在模型训练的后期删除了可能诱发快捷方式学习的训练实例。无需引入任何额外的数据和计算成本，我们的方法可以通过减轻不同 MNMT 模型和基准的快捷学习来持续显著地提高零样本翻译性能。]]></description>
      <guid>https://arxiv.org/abs/2411.10581</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于 Newcomb 类问题的决策理论推理问题数据集</title>
      <link>https://arxiv.org/abs/2411.10588</link>
      <description><![CDATA[arXiv:2411.10588v1 公告类型：新
摘要：我们在所谓的 Newcomb 类问题决策理论中引入了一组自然语言问题。Newcomb 类问题包括，例如，一个代理与另一个类似的代理交互的决策问题，因此必须推理另一个代理可能会以类似的方式推理。评估关于 Newcomb 类问题的 LLM 推理非常重要，因为基于基础模型的代理之间的交互通常是 Newcomb 类的。一些关于 Newcomb 类问题的推理方式可能允许模型之间进行更大的合作。
我们的数据集包含能力问题（即具有独特、无可争议的正确答案的问题）和态度问题（即决策理论家不同意的问题）。我们使用数据集来调查决策理论能力和表达态度及其在现有模型（OpenAI、Anthropic、Meta、GDM、Reka 等的不同模型）以及基于简单提示干预的模型中的相互作用。除其他外，我们发现现有模型之间的态度差异很大；能力越高，对所谓证据决策理论的态度就越有利；并且态度在不同类型的问题中是一致的。]]></description>
      <guid>https://arxiv.org/abs/2411.10588</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型进行高效的表征学习以实现实体解析</title>
      <link>https://arxiv.org/abs/2411.10629</link>
      <description><![CDATA[arXiv:2411.10629v1 公告类型：新
摘要：在本文中，作者提出了 TriBERTa，这是一种监督实体解析系统，它利用预先训练的大型语言模型和三重态损失函数来学习实体匹配的表示。该系统包括两个步骤：首先，将名称实体记录输入到 Transformers 的句子双向编码器表示 (SBERT) 模型中以生成向量表示，然后使用基于三重态损失函数的对比学习对其进行微调。微调后的表示用作实体匹配任务的输入，结果表明，所提出的方法比最先进的表示（包括没有微调的 SBERT 和传统的词频-逆文档频率 (TF-IDF)）高出 3 - 19%。此外，TriBERTa 生成的表示表现出更高的稳健性，在一系列数据集上始终保持更高的性能。作者还讨论了实体解析在当今数据驱动环境中的重要性，以及识别和协调不同来源的重复数据时出现的挑战。他们还描述了 ER 流程，该流程涉及几个关键步骤，包括阻止、实体匹配和聚类。]]></description>
      <guid>https://arxiv.org/abs/2411.10629</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减轻孟加拉语分类任务的性别偏见</title>
      <link>https://arxiv.org/abs/2411.10636</link>
      <description><![CDATA[arXiv:2411.10636v1 公告类型：新
摘要：在本研究中，我们调查了孟加拉语预训练语言模型中的性别偏见，这是资源匮乏的语言中一个尚未得到充分探索的领域。为了评估这种偏见，我们将性别名称交换技术应用于现有数据集，创建了四个手动注释的任务特定数据集，用于情绪分析、毒性检测、仇恨言论检测和讽刺检测。通过更改名称和性别特定术语，我们确保这些数据集适合检测和减轻性别偏见。然后，我们提出了一种联合损失优化技术，以减轻任务特定预训练模型中的性别偏见。我们的方法与现有的偏见缓解方法进行了评估，结果表明，与其他基线方法相比，我们的技术不仅有效地减少了偏见，而且还保持了竞争性准确性。为了促进进一步的研究，我们已公开提供我们的实现和数据集 https://github.com/sajib-kumar/Gender-Bias-Mitigation-From-Bangla-PLM]]></description>
      <guid>https://arxiv.org/abs/2411.10636</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAM 解码：通过后缀自动机进行推测解码</title>
      <link>https://arxiv.org/abs/2411.10666</link>
      <description><![CDATA[arXiv:2411.10666v1 公告类型：新
摘要：大型语言模型 (LLM) 通过将任务统一到文本生成中，彻底改变了自然语言处理，但它们的参数大小和自回归性质限制了推理速度。SAM-Decoding 通过引入一种新颖的基于检索的推测解码方法解决了这个问题，该方法使用后缀自动机进行高效准确的草稿生成。与现有方法使用的 n-gram 匹配不同，SAM-Decoding 在生成文本和文本语料库中找到最长的后缀匹配，实现每个生成步骤的平均时间复杂度为 $O(1)$。SAM-Decoding 分别为文本语料库和输入提示构建静态和动态后缀自动机，实现快速而精确的草稿生成。同时，它被设计为一种可以与现有方法相结合的方法，允许 SAM-Decoding 根据匹配长度自适应地选择草稿生成策略，从而提高 LLM 的推理速度。评估表明，与 Token Recycling 结合使用时，SAM-Decoding 的表现优于现有的无模型方法，在 Spec-Bench 上比自回归解码的速度提高了 $2.27\times$。与 EAGLE2 结合使用时，速度提高了 $2.49\times$，超越了所有当前方法。我们的代码可在 https://github.com/hyx1999/SAM-Decoding 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.10666</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IntentGPT：使用大型语言模型进行小样本意图发现</title>
      <link>https://arxiv.org/abs/2411.10670</link>
      <description><![CDATA[arXiv:2411.10670v1 公告类型：新
摘要：在当今数字化驱动的世界中，对话系统在增强用户交互方面发挥着关键作用，从客户服务到虚拟助手。在这些对话中，自动识别用户的目标以及时解决他们的需求非常重要。这需要集成执行意图检测的模型。但是，用户的意图是多种多样且动态的，因此很难维护一组固定的预定义意图。因此，更实用的方法是开发一种能够在新意图出现时识别它们的模型。我们解决了意图发现的挑战，这一领域在最近的研究中引起了广泛关注。现有方法需要对大量数据进行训练才能正确识别新意图，这需要大量的人力。为了克服这个问题，我们引入了 IntentGPT，这是一种新颖的免训练方法，可以有效地促使大型语言模型 (LLM)（例如 GPT-4）以最少的标记数据发现新意图。 IntentGPT 包含一个 \textit{上下文提示生成器}，用于为上下文学习生成信息提示，一个 \textit{意图预测器}，用于对话语中的用户意图进行分类和发现，以及一个 \textit{语义少量样本采样器}，用于选择相关的少量样本示例和一组已知意图注入提示中。我们的实验表明，在包括 CLINC 和 BANKING 等在内的流行基准测试中，IntentGPT 的表现优于以前需要大量特定领域数据和微调的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.10670</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>心理健康结构化对话系统：利用 PM+ 指南的 LLM 聊天机器人</title>
      <link>https://arxiv.org/abs/2411.10681</link>
      <description><![CDATA[arXiv:2411.10681v1 公告类型：新
摘要：结构化对话系统（简称 SuDoSys）是一种创新的基于大型语言模型 (LLM) 的聊天机器人，旨在提供心理咨询。SuDoSys 利用世界卫生组织 (WHO) 的问题管理加 (PM+) 指南提供阶段感知的多轮对话。在多轮心理咨询中使用 LLM 的现有方法通常涉及使用生成的对话进行直接微调，通常忽略了咨询会话的动态阶段转变。与以前的方法不同，SuDoSys 考虑了咨询的不同阶段并在整个咨询过程中存储基本信息，确保对话连贯且有针对性。该系统采用 LLM、阶段感知指令生成器、响应解包器、主题数据库和阶段控制器来维持对话流。此外，我们提出了一种新技术，模拟咨询客户与评估系统交互并自动评估其性能。当使用客观和主观评估进行评估时，SuDoSys 证明了其在生成逻辑连贯的响应方面的有效性。该系统的代码和用于评估的程序脚本是开源的。]]></description>
      <guid>https://arxiv.org/abs/2411.10681</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HJ-Ky-0.1：吉尔吉斯语词嵌入评估数据集</title>
      <link>https://arxiv.org/abs/2411.10724</link>
      <description><![CDATA[arXiv:2411.10724v1 公告类型：新
摘要：现代应用计算语言学的关键任务之一是构建词向量表示（词嵌入），词向量表示被广泛用于解决自然语言处理任务，如情绪分析、信息提取等。为了选择合适的方法来生成这些词嵌入，质量评估技术通常是必要的。一种标准方法包括计算具有专家评估“相似性”的单词向量之间的距离。这项工作为吉尔吉斯语中的此类任务引入了第一个“银标准”数据集，同时训练相应的模型并通过质量评估指标验证数据集的适用性。]]></description>
      <guid>https://arxiv.org/abs/2411.10724</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>阿拉伯语和英语讽刺新闻检测的多语言和双语模型比较</title>
      <link>https://arxiv.org/abs/2411.10730</link>
      <description><![CDATA[arXiv:2411.10730v1 公告类型：新
摘要：讽刺新闻是真实新闻与幽默评论或夸张内容的结合，它经常模仿真实新闻的格式和风格。然而，讽刺新闻经常被误解为错误信息，尤其是来自不同文化和社会背景的人。这项研究利用英语和阿拉伯语的多语言讽刺检测方法解决了区分讽刺和真实新闻的挑战。我们使用两种语言模型 Jais-chat(13B) 和 LLaMA-2-chat(7B) 探索零样本和思维链 (CoT) 提示。我们的结果表明，CoT 提示为 Jais-chat 模型提供了比 LLaMA-2-chat 模型显著的优势。具体来说，Jais-chat 在使用 CoT 提示时取得了最佳表现，英语的 F1 分数为 80\%。这些结果强调了 CoT 中结构化推理的重要性，它增强了上下文理解，对于讽刺检测等复杂任务至关重要。]]></description>
      <guid>https://arxiv.org/abs/2411.10730</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>普通法学硕士能否帮助分析临床观察中自闭症儿童与成人的互动？</title>
      <link>https://arxiv.org/abs/2411.10761</link>
      <description><![CDATA[arXiv:2411.10761v1 公告类型：新
摘要：大型语言模型 (LLM) 在理解人类交流和互动方面表现出巨大潜力。然而，它们在儿童包容性互动领域（包括临床环境）的表现仍未得到充分探索。在这项工作中，我们评估了通用 LLM 在涉及自闭症儿童的临床相关环境中分析儿童与成人二元互动的能力。具体来说，我们探索 LLM 执行四项任务的能力：对儿童与成人的话语进行分类、预测参与活动、识别语言技能和理解临床相关的特征。我们的评估表明，通用 LLM 能够很好地分析临床观察会议中的长而复杂的对话，通常超过非专家人类评估者的表现。结果表明，它们有潜力细分感兴趣的互动、协助语言技能评估、识别参与活动以及为评估提供临床相关的背景。]]></description>
      <guid>https://arxiv.org/abs/2411.10761</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的信息焦虑</title>
      <link>https://arxiv.org/abs/2411.10813</link>
      <description><![CDATA[arXiv:2411.10813v1 公告类型：新
摘要：大型语言模型 (LLM) 作为知识库表现出色，使模型能够理解用户查询并生成准确且具有上下文感知的响应。广泛的评估设置证实了 LLM 的检索能力与其预训练语料库中实体的频率之间的正相关性。我们通过对 LLM 的内部推理和检索机制进行全面分析，进一步进行了调查。我们的工作重点是三个关键维度——实体流行度的影响、模型对查询表述中的词汇变化的敏感性以及 LLM 层之间隐藏状态表示的进展。我们的初步研究结果表明，热门问题有助于内部状态早期收敛到正确答案。然而，随着查询的流行度增加，跨词汇变化检索到的属性变得越来越不相似，准确性也越来越低。有趣的是，我们发现，在处理非常流行的主题时，LLM 很难将基于不同关系的事实与参数记忆区分开来。通过案例研究，我们探索了 LLM 在处理非常流行的查询时的潜在压力，我们将这种现象称为信息焦虑。LLM 中信息焦虑的出现强调了以语言变化形式出现的对抗性注入，并要求对经常出现的实体进行更全面的评估。]]></description>
      <guid>https://arxiv.org/abs/2411.10813</guid>
      <pubDate>Tue, 19 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>