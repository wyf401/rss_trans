<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 28 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>SlideSpawn：用于研究出版物的自动幻灯片生成系统</title>
      <link>https://arxiv.org/abs/2411.17719</link>
      <description><![CDATA[arXiv:2411.17719v1 公告类型：新
摘要：研究论文是结构良好的文档。它们有文本、图表、方程式、表格等，以传达他们的想法和发现。它们分为介绍、模型、实验等部分，涉及研究的不同方面。这些特点使研究论文有别于普通文件，并使我们能够显著改善其总结。在本文中，我们提出了一个新颖的系统 SlideSpwan，该系统以研究文档的 PDF 作为输入，并生成高质量的演示文稿，以直观和简洁的方式提供其摘要。该系统首先将论文的 PDF 转换为具有有关各种元素的结构信息的 XML 文档。然后，使用在 PS5K 数据集和 Aminer 9.5K Insights 数据集（我们介绍）上训练的机器学习模型来预测论文中每个句子的显着性。使用 ILP 选择幻灯片的句子，并根据它们的相似性进行聚类，每个聚类都被赋予合适的标题。最后，通过将所选句子中引用的任何图形元素放在它们旁边来生成幻灯片。在 650 对论文和幻灯片的测试集上进行的实验表明，我们的系统可以生成质量更高的演示文稿。]]></description>
      <guid>https://arxiv.org/abs/2411.17719</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态大型语言模型中的高效自我改进：一种模型级无评判方法</title>
      <link>https://arxiv.org/abs/2411.17760</link>
      <description><![CDATA[arXiv:2411.17760v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 中的自我改进对于提高其可靠性和稳健性至关重要。然而，当前的方法通常严重依赖 MLLM 本身作为判断者，导致计算成本高昂，并可能存在奖励黑客和模型崩溃等陷阱。本文介绍了一种新颖的模型级无判断者自我改进框架。我们的方法采用受控反馈机制，同时消除了验证循环中对 MLLM 的需求。我们使用可控幻觉机制生成偏好学习对，并通过利用轻量级对比语言图像编码器在必要时评估和反转对来优化数据质量。跨公共基准和我们新推出的旨在挑战幻觉控制的 IC 数据集的评估表明，我们的模型优于传统技术。我们实现了卓越的精度和召回率，同时显著降低了计算需求。该方法为 MLLM 中的可扩展自我改进提供了一条有效途径，在性能提升与资源需求减少之间取得平衡。]]></description>
      <guid>https://arxiv.org/abs/2411.17760</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$H^3$Fusion：有益、无害、诚实的法学硕士融合</title>
      <link>https://arxiv.org/abs/2411.17792</link>
      <description><![CDATA[arXiv:2411.17792v1 公告类型：新
摘要：使用基于指令的数据集对预训练的 LLM 进行对齐对于创建反映人类偏好的微调模型至关重要。最近出现了越来越多的基于对齐的微调算法和基准，推动了对预训练的 LLM 进行有效对齐的努力，以确保来自开源和闭源 LLM 的答案有用、无害且诚实。本文通过开发一种对齐融合方法（称为 $H^3$Fusion）来解决这个问题，该方法具有三个独特的特征。首先，$H^3$Fusion 集成多个单独对齐的 LLM 以创建最终的微调对齐模型，该模型的功能比单个模型更强，通过促进有用、无害、诚实的融合来实现稳健的对齐。其次，$H^3$Fusion 分两步利用混合专家 (MoE) 方法。我们首先冻结每个单独模型的多头注意力权重，同时在对齐融合期间调整 FFN 层。然后，我们根据输入指令的类型将对齐的模型权重与专家路由器合并，并动态选择最适合产生输出响应的专家子集。最后，我们通过引入门控损失和正则化项来提高生成的 $H^3$3Fusion 模型的性能。前者惩罚专家路由器的选择错误，后者调解专家权重在微调过程中的漂移，并通过引导专家的激活来动态调整生成的模型的融合行为。对三个基准数据集的广泛评估表明，$H^3$3Fusion 从两个方面更有帮助、危害更小、更诚实：它比每个单独对齐的模型高出 $11.37\%$，并且与最先进的 LLM 集成方法相比，它提供了更强的鲁棒性 $13.77\%$。代码可在 github.com/sftekin/h3fusion 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.17792</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Arabic-Nougat：针对阿拉伯语 OCR 和 Markdown 提取的微调视觉变换器</title>
      <link>https://arxiv.org/abs/2411.17835</link>
      <description><![CDATA[arXiv:2411.17835v1 公告类型：新
摘要：我们介绍了 Arab-Nougat，这是一套用于将阿拉伯语书页转换为结构化 Markdown 文本的 OCR 模型。基于 Meta 的 Nougat 架构，Arabic-Nougat 包括三个专门的模型：arabic-small-nougat、arabic-base-nougat 和 arabic-large-nougat。这些模型在合成数据集 arabic-img2md 上进行了微调，该数据集包含 13.7k 对阿拉伯语书页及其 Markdown 表示。主要贡献包括 Aranizer-PBE-86k 标记器（专为高效标记化而设计），以及使用 torch.bfloat16 精度和 Flash Attention 2 来优化训练和推理。我们的模型实现了最先进的性能，其中 arabic-large-nougat 提供最高的 Markdown 结构准确度和最低的字符错误率。此外，我们还发布了一个大型数据集，其中包含使用我们性能最佳的模型从 8,500 多本书中提取的 11 亿个阿拉伯语标记，为阿拉伯语 OCR 研究提供了宝贵的资源。所有模型、数据集和代码都是开源的，可在 https://github.com/MohamedAliRashad/arabic-nougat 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.17835</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LongKey：长文档的关键短语提取</title>
      <link>https://arxiv.org/abs/2411.17863</link>
      <description><![CDATA[arXiv:2411.17863v1 公告类型：新
摘要：在信息过载的时代，手动注释大量且不断增长的文档和学术论文语料库越来越不切实际。自动关键短语提取通过识别文本中的代表性术语来解决这一挑战。然而，大多数现有方法都侧重于短文档（最多 512 个标记），在处理长上下文文档方面存在差距。在本文中，我们介绍了 LongKey，这是一种从长文档中提取关键短语的新框架，它使用基于编码器的语言模型来捕获扩展的文本复杂性。LongKey 使用最大池化嵌入器来增强关键短语候选表示。在综合 LDKP 数据集和六个不同的、看不见的数据集上进行验证，LongKey 始终优于现有的无监督和基于语言模型的关键短语提取方法。我们的研究结果证明了 LongKey 的多功能性和卓越性能，标志着不同文本长度和领域的关键短语提取的进步。]]></description>
      <guid>https://arxiv.org/abs/2411.17863</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型和主题建模进行毒性分类</title>
      <link>https://arxiv.org/abs/2411.17876</link>
      <description><![CDATA[arXiv:2411.17876v1 公告类型：新
摘要：内容审核和毒性分类是具有重大社会影响的关键任务。然而，研究表明，主要的分类模型表现出放大或减少偏见的趋势，并可能在分类过程中忽视或不利于某些边缘群体。研究人员认为，注释者的位置性会影响模型从中学习的黄金标准标签，从而传播注释者的偏见。为了进一步研究注释者位置性的影响，我们深入研究了在使用主题建模策略进行内容审核的同时对数据集上的 BERTweet 和 HateBERT 进行微调。结果表明，与其他著名分类模型（如 GPT-4、PerspectiveAPI 和 RewireAPI）生成的预测相比，对特定主题的模型进行微调可显着提高模型的 F1 得分。这些发现进一步表明，与早期方法相比，最先进的大型语言模型在准确检测和解释文本毒性方面表现出很大的局限性。代码可在 https://github.com/aheldis/Toxicity-Classification.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.17876</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HOPPR 医疗级医学影像 AI 平台</title>
      <link>https://arxiv.org/abs/2411.17891</link>
      <description><![CDATA[arXiv:2411.17891v1 公告类型：新
摘要：人工智能 (AI) 的技术进步推动了大型视觉语言模型 (LVLM) 的开发，这些模型在数百万对图像和文本样本上进行训练。后续的研究表明，LVLM 在医学成像用例（例如，放射学报告生成）中具有实现高性能的巨大潜力，但仍然存在阻碍广泛部署这些解决方案的障碍。这些障碍包括开发大规模模型的大量计算要求的成本、开发复杂 AI 模型的专业知识，以及难以访问能够充分代表要部署 LVLM 解决方案的人群的大量高质量数据集。HOPPR 医疗级平台通过提供强大的计算基础设施、一套基础模型（开发人员可以在其基础上针对其特定用例进行微调）以及一个强大的质量管理系统（为评估在临床环境中部署的微调模型设定标准）来解决这些障碍。 HOPPR 平台可以访问来自数百个不同人群的成像中心的数百万个成像研究和文本报告，以预先训练基础模型并启用特定于用例的群组进行微调。所有数据都经过去识别处理并安全存储，以符合 HIPAA 要求。此外，开发人员可以安全地在 HOPPR 平台上托管模型并通过 API 访问它们，以便在既定的临床工作流程中使用这些模型进行推理。借助医疗级平台，HOPPR 的使命是加快部署用于医学成像的 LVLM 解决方案，并最终优化放射科医生的工作流程并满足该领域日益增长的需求。]]></description>
      <guid>https://arxiv.org/abs/2411.17891</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估生成式 AI 增强内容：使用定性、定量和混合方法的概念框架</title>
      <link>https://arxiv.org/abs/2411.17943</link>
      <description><![CDATA[arXiv:2411.17943v1 公告类型：新
摘要：生成式人工智能 (GenAI) 彻底改变了内容生成，提供了改善语言连贯性、可读性和整体质量的变革性能力。本论文探讨了定性、定量和混合方法研究方法在评估 GenAI 模型在增强科学写作方面的表现方面的应用。使用涉及协作医学成像手稿的假设用例，我们展示了每种方法如何提供对 GenAI 影响的独特见解。定性方法收集专家审阅者的深入反馈，使用主题分析工具分析他们的回复，以捕捉细微的改进并确定局限性。定量方法采用自动化指标（例如 BLEU、ROUGE 和可读性分数）以及用户调查，客观地衡量连贯性、流畅性和结构的改进。混合方法研究整合了这些优势，将统计评估与详细的定性见解相结合，以提供全面的评估。这些研究方法能够量化 GenAI 生成内容的改进水平，解决语言质量和技术准确性的关键方面。它们还提供了一个强大的框架，用于将 GenAI 工具与传统编辑流程进行基准测试，确保这些技术的可靠性和有效性。通过利用这些方法，研究人员可以评估 GenAI 带来的性能提升，改进其应用，并指导其在医疗保健和科学研究等高风险领域的负责任采用。这项工作强调了严格的评估框架对于促进 GenAI 的信任和创新的重要性。]]></description>
      <guid>https://arxiv.org/abs/2411.17943</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>QuaLLM-Health：基于 LLM 的在线健康讨论定量数据提取框架的改编</title>
      <link>https://arxiv.org/abs/2411.17967</link>
      <description><![CDATA[arXiv:2411.17967v1 公告类型：新
摘要：Reddit 等社交媒体上的健康相关讨论提供了有价值的见解，但从非结构化文本中提取定量数据具有挑战性。在这项工作中，我们提出了一个从 QuaLLM 到 QuaLLM-Health 的改编框架，用于使用大型语言模型 (LLM) 从 Reddit 关于胰高血糖素样肽 1 (GLP-1) 受体激动剂的讨论中提取临床相关的定量数据。我们在 2024 年 7 月使用 Reddit API 从五个 GLP-1 相关社区收集了 410k 个帖子和评论。在过滤与癌症相关的讨论后，剩下 2,059 个唯一条目。我们制定了注释指南，以手动提取变量，例如癌症存活率、家族癌症史、提到的癌症类型、风险认知以及与医生的讨论。两位领域专家独立注释了 100 个条目的随机样本，以创建黄金标准数据集。然后，我们在黄金标准数据集上采用 OpenAI 的“GPT-4o-mini”进行迭代提示工程，构建了一个优化的管道，使我们能够从大型数据集中提取变量。优化后的 LLM 对所有变量的准确率均超过 0.85，精度、召回率和 F1 分数宏平均 &gt; 0.90，表明性能均衡。稳定性测试显示，运行中的匹配率为 95%，证实了一致性。将该框架应用于完整数据集可以有效提取下游分析所需的变量，成本不到 3 美元，大约一小时即可完成。QuaLLM-Health 表明，LLM 可以有效、高效地从非结构化社交媒体内容中提取临床相关的定量数据。结合人类专业知识和迭代提示细化可确保准确性和可靠性。该方法可以适用于跨各个健康领域的患者生成数据的大规模分析，为医疗保健研究提供有价值的见解。]]></description>
      <guid>https://arxiv.org/abs/2411.17967</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>以忠诚为中心的自然语言处理新可解释性范式</title>
      <link>https://arxiv.org/abs/2411.17992</link>
      <description><![CDATA[arXiv:2411.17992v1 公告类型：新
摘要：随着机器学习变得越来越普遍并用于越来越关键的应用中，为这些模型提供解释以防止意外行为非常重要。不幸的是，许多当前的可解释性方法在忠实性方面存在困难。因此，这篇博士论文研究了“如何为复杂的通用神经 NLP 模型提供并确保忠实的解释？”的问题。主要论点是我们应该开发可解释性的新范式。这是通过首先开发可靠的忠实度指标，然后应用从这次调查中获得的经验教训来开发新范式来实现的。探索的两个新范式是忠实度可测量模型 (FMM) 和自我解释。自我解释的想法是让大型语言模型自我解释，我们发现当前的模型无法始终如一地做到这一点。但是，我们提出了如何实现这一点。FMM 的想法是创建设计为可以廉价而精确地测量忠实度的模型。这使得优化解释以实现最大程度的忠实性成为可能，这使得 FMM 被设计为可解释的。我们发现 FMM 给出的解释在忠实性方面接近理论最优。总体而言，从所有对忠实性的调查来看，结果表明事后解释和内在解释默认依赖于模型和任务。然而，在使用 FMM 时情况并非如此，即使使用相同的事后解释方法也是如此。这表明，即使是对模型进行简单的修改，例如随机屏蔽训练数据集（如在 FMM 中所做的那样），也可以极大地改变情况并产生始终如一的忠实解释。这回答了如何提供和确保忠实解释的问题。]]></description>
      <guid>https://arxiv.org/abs/2411.17992</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DRS：具有结构化输出的深度问题重构</title>
      <link>https://arxiv.org/abs/2411.17993</link>
      <description><![CDATA[arXiv:2411.17993v1 公告类型：新 
摘要：问答是大型语言模型（LLM）的基本能力。然而，当人们遇到全新的知识文本时，由于对知识的理解不足，他们常常会提出文本无法回答的问题。最近的研究表明，大型语言模型可以识别问题的不可回答性，但缺乏帮助人们重新表述问题的能力。即使是像 GPT-3.5 这样强大的模型在这方面的表现也很差。为了增强 LLM 协助人类重新表述问题以从新文档中提取相关知识的能力，我们提出了一种称为 DRS：具有结构化输出的深度问题重新表述的零样本方法。我们提出的方法利用大型语言模型和基于 DFS 的算法迭代搜索可能的实体组合并用某些实体约束输出，有效地提高了大型语言模型在这方面的能力。大量实验结果表明，我们的零样本 DRS 方法显著提高了 GPT-3.5 的重构准确率，从 23.03% 提高到 70.42%，并有效提高了 Gemma2-9B 等开源大型语言模型的得分，从 26.35% 提高到 56.75%。]]></description>
      <guid>https://arxiv.org/abs/2411.17993</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>双向编码器能否成为基础模型下游应用的最终赢家？</title>
      <link>https://arxiv.org/abs/2411.18021</link>
      <description><![CDATA[arXiv:2411.18021v1 Announce Type: new 
摘要：在过去的几十年里，人工智能（AI）从最初的机器学习阶段发展到深度学习阶段，现在又进入了基础模型阶段。基础模型具有预训练、迁移学习和自监督学习的特点，预训练模型可以进行微调并应用于各种下游任务。在基础模型的框架下，BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pre-trained Transformer）等模型极大地推动了自然语言处理（NLP）的发展，尤其是基于BERT的众多模型的出现。BERT通过使用掩码语言模型突破了预训练中仅使用单向方法进行语言建模的限制。它可以捕获双向上下文信息来预测序列中被掩码的单词，从而提高模型的特征提取能力。这使得该模型对于下游任务非常有用，尤其是对于专门的应用程序。使用双向编码器的模型可以更好地理解领域知识，从而更好地应用于这些下游任务。所以我们希望帮助理解这项技术在基础模型的背景下是如何演变并提升模型在各类自然语言处理任务中的表现的，并揭示其在捕捉上下文信息和提升模型在下游任务上的表现方面的重要性。本文分析了基于GPT和BERT的单向和双向模型，并根据模型的用途比较了它们之间的差异。并简要分析了BERT以及一些基于BERT的模型的改进。对比了模型在斯坦福问答数据集(SQuAD)和通用语言理解评估(GLUE)上的表现。]]></description>
      <guid>https://arxiv.org/abs/2411.18021</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 2 位层判别 KV 缓存突破 LLM 推理的极限</title>
      <link>https://arxiv.org/abs/2411.18077</link>
      <description><![CDATA[arXiv:2411.18077v1 公告类型：新
摘要：由于 LLM 对内存和计算的要求过高，如何在实践中有效地为其提供服务变得极具挑战性。在本研究中，我们研究了 KV 缓存的优化，它的内存占用是 LLM 推理的一个关键瓶颈，尤其是在处理长上下文任务时。为了应对这一挑战，我们引入了 MiniKV，这是一种 KV 缓存优化方法，它通过一种新颖的 2 位层区分 KV 缓存，在保持长上下文任务准确性的同时显著减少了 KV 缓存大小。更重要的是，我们开发了专门的 CUDA 内核，使 MiniKV 与 FlashAttention 兼容。在广泛的长上下文任务上的实验表明，MiniKV 有效地实现了 86% 的 KV 缓存压缩率，同时恢复了 98.5% 以上的准确率，超越了最先进的方法，同时实现了出色的测量系统性能改进。]]></description>
      <guid>https://arxiv.org/abs/2411.18077</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微调小嵌入以提高性能</title>
      <link>https://arxiv.org/abs/2411.18099</link>
      <description><![CDATA[arXiv:2411.18099v1 公告类型：新
摘要：上下文嵌入在各种自然语言处理任务中产生了最先进的结果。然而，这些嵌入受到需要大量数据和巨大计算能力的模型的限制。对于尼泊尔语等资源匮乏的语言来说，这是一个问题，因为互联网上可用的数据量并不总是足以满足模型的需求。这项工作采用了一个不完整的 BERT 模型，该模型有六个注意力头，在尼泊尔语上进行了预训练，并在以前未见过的数据上对其进行了微调。将内在和外在评估的结果与从原始模型基线和在尼泊尔语上预训练的完整 BERT 模型中得出的结果进行了比较，作为 oracle。结果表明，尽管 oracle 平均水平更好，但与原始基线相比，对小嵌入进行微调可以显着改善结果。]]></description>
      <guid>https://arxiv.org/abs/2411.18099</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于模板的数据生成来训练和评估语言模型</title>
      <link>https://arxiv.org/abs/2411.18104</link>
      <description><![CDATA[arXiv:2411.18104v1 公告类型：新
摘要：GPT-3、PaLM 和 Llama 等大型语言模型 (LLM) 的快速发展极大地改变了自然语言处理，展示了理解和生成语言的卓越能力。然而，这些模型在需要复杂推理的任务中往往举步维艰，特别是在数学问题解决中，部分原因是缺乏训练复杂推理能力所需的大规模、高质量、领域特定数据集。为了解决这一限制，我们引入了基于模板的数据生成 (TDG)，这是一种利用 LLM (GPT-4) 自动生成参数化元模板的新方法，然后使用这些元模板来合成大量高质量的问题和解决方案。利用 TDG，我们创建了 TemplateMath 第 I 部分：TemplateGSM，这是一个包含超过 700 万道合成生成的小学数学问题的数据集——每道题都附有基于代码和自然语言的解决方案——并且有可能生成无限数量的其他问题。该数据集缓解了大规模数学数据集的稀缺性，是预训练、微调和评估数学推理 LLM 的宝贵资源。我们的方法不仅可以生成几乎无限的数据，还可以通过使用 GPT-4 进行元模板生成将数据增强提升到一个新的水平，确保问题结构的多样性和高质量。TemplateMath 第一部分：TemplateGSM 数据集可在 https://huggingface.co/datasets/math-ai/TemplateGSM 上公开获取。代码可在 https://github.com/iiis-ai/TemplateMath 上获取。]]></description>
      <guid>https://arxiv.org/abs/2411.18104</guid>
      <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>