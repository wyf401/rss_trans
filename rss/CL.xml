<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 12 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向透明度：通过可视化主题建模和语义框架探索 LLM 培训数据集</title>
      <link>https://arxiv.org/abs/2406.06574</link>
      <description><![CDATA[arXiv:2406.06574v1 公告类型：新
摘要：LLM 现在负责代表人类做出许多决定：从回答问题到对事物进行分类，它们已成为日常生活的重要组成部分。虽然近年来计算和模型架构迅速扩展，但对训练数据集的策划工作仍处于起步阶段。对训练数据集的低估导致 LLM 创建了有偏见和低质量的内容。为了解决这个问题，我们介绍了 Bunka，这是一款利用人工智能和认知科学来改进文本数据集细化的软件。我们展示了主题建模与二维制图相结合如何提高数据集的透明度。然后，我们展示了如何将相同的主题建模技术应用于偏好数据集，以加速微调过程并提高模型在不同基准上的容量。最后，我们展示了如何使用框架分析来洞察训练语料库中现有的偏见。总的来说，我们认为我们需要更好的工具来探索和提高 LLM 培训数据集的质量和透明度。]]></description>
      <guid>https://arxiv.org/abs/2406.06574</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:27 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络增强检索用于法学硕士问答</title>
      <link>https://arxiv.org/abs/2406.06572</link>
      <description><![CDATA[arXiv:2406.06572v1 公告类型：新
摘要：检索增强生成通过提供事实支持彻底改变了大型语言模型 (LLM) 的输出。然而，它很难捕捉到复杂推理问题所需的所有知识。现有的检索方法通常将参考文档分成段落，单独处理它们。然而，这些段落往往是相互关联的，例如连续的段落或共享相同关键词的段落。因此，识别相关性对于增强检索过程至关重要。在本文中，我们提出了一种名为 GNN-Ret 的新型检索方法，该方法利用图神经网络 (GNN) 通过考虑段落之间的相关性来增强检索。具体来说，我们首先通过连接与结构相关和与关键词相关的段落来构建段落图。然后利用图神经网络 (GNN) 来利用段落之间的关系并改进支持段落的检索。此外，我们扩展了我们的方法，使用名为 RGNN-Ret 的循环图神经网络 (RGNN) 来处理多跳推理问题。在每个步骤中，RGNN-Ret 都会集成来自先前步骤的段落图，从而增强对支持段落的检索。在基准数据集上进行的大量实验表明，GNN-Ret 使用单个 LLM 查询实现的问答准确率高于需要多个查询的强基线，并且 RGNN-Ret 进一步提高了准确率并实现了最先进的性能，在 2WikiMQA 数据集上的准确率提高了 10.4%。]]></description>
      <guid>https://arxiv.org/abs/2406.06572</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>MedFuzz：探索大型语言模型在医学问答中的稳健性</title>
      <link>https://arxiv.org/abs/2406.06573</link>
      <description><![CDATA[arXiv:2406.06573v1 公告类型：新
摘要：大型语言模型 (LLM) 在医学问答基准上取得了令人印象深刻的表现。然而，高基准准确度并不意味着性能可以推广到现实世界的临床环境。医学问答基准依赖于与量化 LLM 性能一致的假设，但在临床的开放世界中可能不成立。然而，LLM 学习广泛的知识，可以帮助 LLM 推广到实际情况，而不管著名基准中的不切实际的假设如何。我们试图量化当基准假设被违反时 LLM 医学问答基准性能的推广程度。具体来说，我们提出了一种对抗性方法，我们称之为 MedFuzz（用于医学模糊测试）。MedFuzz 尝试以旨在混淆 LLM 的方式修改基准问题。我们通过针对 MedQA 基准中提出的关于患者特征的强假设来展示该方法。成功的“攻击”会修改基准项目，这种方式不太可能欺骗医学专家，但会“欺骗”法学硕士将正确答案改为错误答案。此外，我们提出了一种排列测试技术，可以确保成功的攻击具有统计意义。我们展示了如何在“MedFuzzed”基准上使用性能，以及单个成功的攻击。这些方法有望深入了解法学硕士在更现实的环境中稳健运行的能力。]]></description>
      <guid>https://arxiv.org/abs/2406.06573</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>计算文献学综述</title>
      <link>https://arxiv.org/abs/2406.06570</link>
      <description><![CDATA[arXiv:2406.06570v1 公告类型：新
摘要：计算铭文是指利用计算方法从石刻铭文中提取文本、音译、解释和归因的过程。传统的铭文方法耗时长，提取文本时容易损坏石刻铭文。此外，解释和归因是主观的，不同的铭文学家可能有所不同。然而，使用现代计算方法不仅可以用来提取文本，还可以以稳健的方式解释和归因文本。我们调查并记录了现有的有助于完成上述铭文任务的计算方法。]]></description>
      <guid>https://arxiv.org/abs/2406.06570</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>SUBLLM：一种用于 LLM 的具有标记序列子采样的新型高效架构</title>
      <link>https://arxiv.org/abs/2406.06571</link>
      <description><![CDATA[arXiv:2406.06571v1 公告类型：新 
摘要：虽然大型语言模型 (LLM) 在各个领域取得了显著的成功，但训练和推理的效率仍然是一个重大挑战。为了解决这个问题，我们提出了 SUBLLM，即子采样-上采样-旁路大型语言模型的缩写，这是一种创新的架构，通过结合子采样、上采样和旁路模块扩展了核心解码器框架。子采样模块负责缩短序列，而上采样模块恢复序列长度，旁路模块增强收敛。与 LLaMA 相比，提出的 SUBLLM 在训练和推理速度以及内存使用方面均表现出显着的增强，同时保持了具有竞争力的少样本性能。在训练期间，SUBLLM 将速度提高了 26%，并将每个 GPU 的内存减少了 10GB。在推理中，它将速度提高了 37%，并将每个 GPU 的内存减少了 1GB。当上下文窗口扩展到8192时，训练和推理速度可分别提高34％和52％。我们将在发布的版本中发布所提架构的源代码。]]></description>
      <guid>https://arxiv.org/abs/2406.06571</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>利用合成数据增强临床文档：利用生成模型提高准确性</title>
      <link>https://arxiv.org/abs/2406.06569</link>
      <description><![CDATA[arXiv:2406.06569v1 公告类型：新
摘要：准确全面的临床文档对于提供高质量的医疗保健、促进提供者之间的有效沟通以及确保遵守监管要求至关重要。但是，手动转录和数据输入过程可能耗时、容易出错且容易出现不一致，从而导致医疗记录不完整或不准确。本文提出了一种通过利用合成数据生成技术来生成真实多样的临床记录来增强临床文档的新方法。我们提出了一种方法，将最先进的生成模型（例如生成对抗网络 (GAN) 和变分自动编码器 (VAE)）与现实世界的临床记录和其他形式的临床数据相结合以生成合成记录。然后可以使用这些合成记录来补充现有的文档工作流程，为自然语言处理模型提供额外的训练数据，并实现更准确、更高效的转录过程。通过对大量匿名临床转录本数据集进行大量实验，我们证明了我们的方法在生成与真实数据非常相似的高质量合成转录本方面的有效性。定量评估指标（包括困惑度分数和 BLEU 分数）以及领域专家的定性评估验证了生成的合成转录本的保真度和实用性。我们的研究结果强调了合成数据生成在解决临床文档挑战、改善患者护理、减轻行政负担和提高医疗保健系统效率方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.06569</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>MixEval：从 LLM 基准混合中获取群体智慧</title>
      <link>https://arxiv.org/abs/2406.06565</link>
      <description><![CDATA[arXiv:2406.06565v1 公告类型：新
摘要：评估大型语言模型 (LLM) 具有挑战性。传统的基于真实值的基准测试无法捕捉现实世界查询的全面性和细微差别，而 LLM 作为评判基准测试则受到评分偏差和查询数量有限的影响。随着时间的推移，它们都可能受到污染。面向用户的评估，例如 Chatbot Arena，提供了可靠的信号，但成本高昂且速度慢。在这项工作中，我们提出了 MixEval，这是一种通过战略性地混合现成的基准测试来建立高效、黄金标准的 LLM 评估的新范式。它通过将从网络挖掘出的查询与现有基准测试中的类似查询进行匹配，连接 (1) 全面且分布良好的现实世界用户查询和 (2) 高效且公平分级的基于真实值的基准测试。基于 MixEval，我们进一步构建了 MixEval-Hard，为模型改进提供了更大的空间。我们的基准测试的优势在于：(1) 由于高度公正的查询分布和评分机制，与 Chatbot Arena 的模型排名相关性达到 0.96；(2) 执行速度快、成本低且可重复（MMLU 的时间和成本为 6%）；(3) 通过快速稳定的数据更新管道实现动态评估。我们为我们和现有的 LLM 基准测试提供了广泛的元评估和分析，以加深社区对 LLM 评估的理解并指导未来的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2406.06565</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>RAG 推动了有关家庭电力监控的对话</title>
      <link>https://arxiv.org/abs/2406.06566</link>
      <description><![CDATA[arXiv:2406.06566v1 公告类型：新
摘要：在本文中，我们研究了检索增强生成 (RAG) 与大型语言模型 (LLM)（如 ChatGPT、Gemini 和 Llama）的集成，以提高对有关电力数据集的复杂问题的回答的准确性和特异性。认识到 LLM 在生成精确且上下文相关的答案方面的局限性，因为它们依赖于训练数据中的模式而不是事实理解，我们提出了一种利用专门的电力知识图的解决方案。这种方法有助于检索准确的实时数据，然后将其与 LLM 的生成能力进行合成。我们的研究结果表明，RAG 方法不仅可以减少通常由 LLM 生成的不正确信息的发生率，而且还可以通过将响应建立在可验证数据中来显着提高输出的质量。本文详细介绍了我们的方法，对有和没有 RAG 的响应进行了比较分析，并讨论了我们的研究结果对未来 AI 在能源数据分析等专业领域的应用的影响。]]></description>
      <guid>https://arxiv.org/abs/2406.06566</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>在小型语言模型中实现稀疏激活</title>
      <link>https://arxiv.org/abs/2406.06562</link>
      <description><![CDATA[arXiv:2406.06562v1 公告类型：新
摘要：稀疏激活在推理中选择性地仅激活一组输入相关的神经元，是一种有用的技术，可以降低大型语言模型 (LLM) 的计算成本，而无需重新训练或调整工作。然而，它是否可以应用于最近出现的小型语言模型 (SLM) 仍是值得怀疑的，因为 SLM 通常比 LLM 参数化程度更低。在本文中，我们的目标是在 SLM 中实现稀疏激活。我们首先表明，LLM 中现有的基于神经元输出幅度的稀疏激活方案不能应用于 SLM，而基于神经元的归因分数激活神经元是一种更好的选择。此外，我们展示并量化了现有归因指标在用于稀疏激活时的巨大误差，这是由于不同层之间神经元的归因分数相互依赖。基于这些观察，我们提出了一种新的归因指标，可以证明可以纠正此类错误并实现精确的稀疏激活。在多个流行的 SLM 和数据集上进行的实验表明，我们的方法可以实现 80% 的稀疏化率，模型准确率损失小于 5%，与 LLM 中实现的稀疏激活相当。源代码可在以下位置获得：https://github.com/pittisl/Sparse-Activation。]]></description>
      <guid>https://arxiv.org/abs/2406.06562</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>Skywork-MoE：深入探讨混合专家语言模型的训练技术</title>
      <link>https://arxiv.org/abs/2406.06563</link>
      <description><![CDATA[arXiv:2406.06563v1 公告类型：新
摘要：在本技术报告中，我们介绍了在 Skywork-MoE 开发过程中实施的训练方法，Skywork-MoE 是一种高性能混合专家 (MoE) 大型语言模型 (LLM)，具有 1460 亿个参数和 16 位专家。它从我们 Skywork-13B 模型的预先存在的密集检查点初始化。我们探索了升级与从头开始训练初始化的比较效果。我们的研究结果表明，在这两种方法之间进行选择应该同时考虑现有密集检查点的性能和 MoE 训练预算。我们重点介绍了两种创新技术：门控逻辑归一化，可提高专家多样化，以及自适应辅助损失系数，允许对辅助损失系数进行层特定调整。我们的实验结果验证了这些方法的有效性。利用这些技术和见解，我们在 SkyPile 语料库的压缩子集上训练了升级的 Skywork-MoE。评估结果表明，我们的模型在广泛的基准测试中表现出色。]]></description>
      <guid>https://arxiv.org/abs/2406.06563</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>逆宪法人工智能：将偏好压缩为原则</title>
      <link>https://arxiv.org/abs/2406.06560</link>
      <description><![CDATA[arXiv:2406.06560v1 公告类型：新
摘要：反馈数据在微调和评估最先进的 AI 模型中起着重要作用。通常使用成对文本偏好：给定两个文本，人类（或 AI）注释者选择“更好”的文本。此类反馈数据被广泛用于将模型与人类偏好对齐（例如，从人类反馈中进行强化学习），或根据人类偏好对模型进行排名（例如，Chatbot Arena）。尽管它被广泛使用，但先前的研究表明，人类注释的成对文本偏好数据经常表现出意想不到的偏见。例如，在某些情况下，人类注释者被证明更喜欢自信的文本而不是真实的文本。基于这些数据训练或评估的模型可能会以难以识别的方式隐式编码这些偏见。在本文中，我们将现有成对文本偏好数据的解释表述为压缩任务：逆宪法 AI (ICAI) 问题。在宪法 AI 中，一组原则（或宪法）用于提供反馈和微调 AI 模型。ICAI 问题颠覆了这个过程：给定一个反馈数据集，我们的目标是提取一个宪法，使大型语言模型 (LLM) 能够最好地重建原始注释。我们提出了相应的初始 ICAI 算法，并根据重建的注释定量验证了其生成的宪法。生成的宪法有许多潜在的用例——它们可能有助于识别不良偏见、将反馈扩展到看不见的数据或帮助根据个人用户偏好调整 LLM。我们在各种数据集上展示了我们的方法：(a) 具有已知基本原则的合成反馈数据集；(b) 交叉注释的人类反馈的 AlpacaEval 数据集；(c) 众包 Chatbot Arena 数据集。我们在 https://github.com/rdnfn/icai 发布了我们的算法和实验的代码。]]></description>
      <guid>https://arxiv.org/abs/2406.06560</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>头脑风暴为知识推理的大型语言模型提供动力</title>
      <link>https://arxiv.org/abs/2406.06561</link>
      <description><![CDATA[arXiv:2406.06561v1 Announce Type: new 
摘要：大型语言模型（LLM）在语言生成、文本理解和知识推理等方面展现出了惊人的能力。虽然单个强大的模型已经可以处理多个任务，但依赖单一的视角可能会导致结果出现偏差且不稳定。近期研究通过引入多模型协作，进一步提升了模型在广泛任务上的推理能力。然而，不同能力的模型对同一问题可能会产生相互冲突的答案，如何从多个候选模型中合理地获取正确答案成为一个具有挑战性的问题。本文提出了基于提示的多模型头脑风暴，将不同的模型纳入一个小组进行头脑风暴，经过多轮推理细化和重新推理，在小组内达成共识答案。我们在三种不同类型的数据集上进行了实验，结果表明头脑风暴可以显著提高逻辑推理和事实提取的有效性。此外，我们发现通过头脑风暴，两个小参数模型可以达到接近大参数模型的精度，这为LLM的分布式部署提供了新的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.06561</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>增强文本真实性：一种用于人工智能生成文本检测的新型混合方法</title>
      <link>https://arxiv.org/abs/2406.06558</link>
      <description><![CDATA[arXiv:2406.06558v1 公告类型：新
摘要：大型语言模型 (LLM) 的快速发展开启了一个 AI 生成的文本与人类生成的内容越来越难以区分的时代。检测 AI 生成的文本已成为打击错误信息、确保内容真实性和防止恶意使用 AI 的必要条件。在本文中，我们提出了一种新颖的混合方法，该方法将传统的 TF-IDF 技术与先进的机器学习模型相结合，包括贝叶斯分类器、随机梯度下降 (SGD)、分类梯度提升 (CatBoost) 和 12 个 Deberta-v3-large 模型实例。我们的方法旨在通过利用传统特征提取方法和最先进的深度学习模型的优势来解决与检测 AI 生成的文本相关的挑战。通过对综合数据集进行大量实验，我们证明了我们提出的方法在准确区分人类和 AI 生成的文本方面的有效性。与现有方法相比，我们的方法取得了卓越的性能。这项研究有助于推动人工智能生成的文本检测技术的进步，并为开发强大的解决方案以缓解人工智能生成内容带来的挑战奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2406.06558</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:20 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型获取商业和媒体洞察</title>
      <link>https://arxiv.org/abs/2406.06559</link>
      <description><![CDATA[arXiv:2406.06559v1 公告类型：新
摘要：本文介绍了财富分析语言模型 (FALM)。FALM 使用户能够直接访问全面的业务分析，包括市场趋势、公司绩效指标和专家见解。与通用的 LLM 不同，FALM 利用从专业新闻报道中构建的精选知识库，使其能够为复杂的业务问题提供精确而深入的答案。用户可以进一步利用自然语言查询直接可视化财务数据，生成富有洞察力的图表和图形，以清楚地了解不同业务部门的趋势。FALM 通过三种新颖的方法培养用户信任并确保输出准确性：1) 时间感知推理保证准确的事件注册并优先考虑最近的更新。2) 主题趋势分析明确检查主题随时间的变化，提供对新兴商业格局的洞察。3) 内容引用和任务分解提高了答案的保真度和数据可视化的准确性。我们进行了自动和人工评估，证明了 FALM 相对于基线方法的显着性能改进，同时优先考虑负责任的 AI 实践。这些基准确立了 FALM 作为商业和媒体领域的前沿法学硕士的地位，具有卓越的准确性和可信度。]]></description>
      <guid>https://arxiv.org/abs/2406.06559</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:20 GMT</pubDate>
    </item>
    <item>
      <title>通过多阶段端到端方法增强 LLM 的演示幻灯片生成能力</title>
      <link>https://arxiv.org/abs/2406.06556</link>
      <description><![CDATA[arXiv:2406.06556v1 公告类型：新
摘要：从包含文本和图像等多模态元素的长文档生成演示文稿幻灯片是一项重要任务。如果手动完成，这很耗时，并且需要领域专业知识。现有的从文档生成丰富演示文稿的方法通常是半自动的，或者只将平面摘要放入幻灯片中，而忽略了良好叙述的重要性。在本文中，我们通过提出一种使用 LLM 和 VLM 组合的多阶段端到端模型来解决这一研究空白。我们通过实验表明，与直接将 LLM 与最先进的提示相结合相比，我们提出的多阶段解决方案在自动化指标和人工评估方面更胜一筹。]]></description>
      <guid>https://arxiv.org/abs/2406.06556</guid>
      <pubDate>Wed, 12 Jun 2024 06:19:19 GMT</pubDate>
    </item>
    </channel>
</rss>