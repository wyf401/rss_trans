<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>MemeGuard：基于 LLM 和 VLM 的通过 Meme 干预推进内容审核的框架</title>
      <link>https://arxiv.org/abs/2406.05344</link>
      <description><![CDATA[arXiv:2406.05344v1 公告类型：新
摘要：在数字世界中，模因具有传播有害内容的潜力，因此对内容审核提出了独特的挑战。尽管检测方法已经得到改进，但干预等主动解决方案仍然有限，当前的研究主要集中在基于文本的内容上，而忽略了模因等多模态内容的广泛影响。为了解决这一差距，我们提出了 \textit{MemeGuard}，这是一个利用大型语言模型 (LLM) 和视觉语言模型 (VLM) 进行模因干预的综合框架。 \textit{MemeGuard} 利用经过特别微调的 VLM，\textit{VLMeme} 进行模因解释，并利用多模态知识选择和排名机制 (\textit{MKS}) 来提炼相关知识。然后，通用 LLM 使用这些知识来生成适合上下文的干预措施。本研究的另一个重要贡献是 \textit{\textbf{I}ntervening} \textit{\textbf{C}yberbullying in \textbf{M}ultimodal \textbf{M}emes (ICMM)} 数据集，这是一个高质量的标记数据集，包含有毒模因及其相应的人工注释干预措施。我们利用 \textit{ICMM} 来测试 \textit{MemeGuard}，展示其在生成对有毒模因的相关有效响应方面的熟练程度。]]></description>
      <guid>https://arxiv.org/abs/2406.05344</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:42 GMT</pubDate>
    </item>
    <item>
      <title>实现可靠的临时科学信息提取：以两个材料数据集为例</title>
      <link>https://arxiv.org/abs/2406.05348</link>
      <description><![CDATA[arXiv:2406.05348v1 公告类型：新
摘要：我们探索 GPT-4 从科学文献中执行基于临时模式的信息提取的能力。我们特别评估它是否可以通过基本的提示方法复制两个现有的材料科学数据集，前提是它们最初是从手稿中手动提取的。我们聘请材料科学家进行详细的手动错误分析，以评估模型在忠实提取所需信息方面存在哪些困难，并借鉴他们的见解提出研究方向来解决这一广泛重要的任务。]]></description>
      <guid>https://arxiv.org/abs/2406.05348</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:42 GMT</pubDate>
    </item>
    <item>
      <title>助教在环：在低预算场景中改进不完善教师模型的知识提炼</title>
      <link>https://arxiv.org/abs/2406.05322</link>
      <description><![CDATA[arXiv:2406.05322v1 公告类型：新
摘要：人们越来越有兴趣将特定于任务的知识从大型语言模型 (LLM) 提炼到较小的学生模型。尽管如此，LLM 提炼仍面临双重挑战：1) 查询教师 LLM（例如 GPT-4）以收集大量演示的成本很高；2) 教师 LLM 可能会提供不完美的输出，从而对学生的学习过程产生负面影响。为了在资源受限、不完美的教师场景中提高样本效率，我们提出了一个利用三种信号类型的三组件框架。第一个信号是学生的自我一致性（学生多个输出的一致性），它是学生信心的代理。具体来说，我们引入了一个“助教”（TA）模型，通过置信度评分来评估学生和老师输出的不确定性，这作为学生训练的另外两个信号。此外，我们提出了一个两阶段训练方案，首先用一小部分数据让学生热身，以更好地利用学生的信号。实验表明，我们提出的框架对于四个复杂的推理任务具有优越性。平均而言，与没有任何跨数据集信号的微调相比，我们提出的两阶段框架带来了高达 20.79% 的相对改进。]]></description>
      <guid>https://arxiv.org/abs/2406.05322</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:41 GMT</pubDate>
    </item>
    <item>
      <title>推进语义文本相似性建模：具有翻译 ReLU 和平滑 K2 损失的回归框架</title>
      <link>https://arxiv.org/abs/2406.05326</link>
      <description><![CDATA[arXiv:2406.05326v1 公告类型：新
摘要：自 BERT 和 RoBERTa 推出以来，语义文本相似性 (STS) 的研究取得了突破性进展。特别是，对比学习的采用大大提升了各种 STS 基准测试中的最新性能。然而，对比学习将文本对分为语义相似或不相似，无法利用细粒度的注释信息，并且需要较大的批量大小以防止模型崩溃。这些限制对从事需要细微相似度水平的 STS 任务或计算资源有限的研究人员构成了挑战，迫使他们探索像 Sentence-BERT 这样的替代方案。尽管如此，Sentence-BERT 从分类的角度处理 STS 任务，忽视了语义关系的渐进性，导致性能不佳。为了弥补这一差距，本文提出了一个创新的回归框架，并提出了两个简单但有效的损失函数：Translated ReLU 和 Smooth K2 Loss。实验分析表明，我们的方法在七个已建立的 STS 基准中取得了令人信服的性能，尤其是在补充特定于任务的训练数据时。]]></description>
      <guid>https://arxiv.org/abs/2406.05326</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:41 GMT</pubDate>
    </item>
    <item>
      <title>隐藏的问题表征揭示了大型语言模型内部和跨大型语言模型的非事实性</title>
      <link>https://arxiv.org/abs/2406.05328</link>
      <description><![CDATA[arXiv:2406.05328v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 取得了显著进步，但非事实性反应的普遍性仍然是一个常见问题。这项工作研究了非事实性预测 (NFP)，它预测 LLM 是否会在生成过程之前对问题生成非事实性反应。以前对 NFP 的研究通常依赖于大量计算。在这项工作中，我们进行了广泛的分析，以探索使用轻量级探测器从问题的隐藏表示中引出“LLM 是否知道”的能力。此外，我们发现非事实性探测器在多个 LLM 中对 NFP 采用类似的模式。受这一有趣发现的启发，我们对跨 LLM NFP 进行了有效的迁移学习，并提出了一种问题对齐策略，以确保基于小批量的训练的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.05328</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:41 GMT</pubDate>
    </item>
    <item>
      <title>语言模型中的概念形成和对齐：将潜在空间中的统计模式与概念分类联系起来</title>
      <link>https://arxiv.org/abs/2406.05315</link>
      <description><![CDATA[arXiv:2406.05315v1 公告类型：新
摘要：本文探讨了语言模型 (LM) 领域中的概念形成和对齐。我们提出了一种机制，用于识别各种 LM 学习到的语义表示中的概念及其层次组织，涵盖从早期模型（如 Glove）到基于转换器的语言模型（如 ALBERT 和 T5）的范围。我们的方法利用这些模型生成的语义嵌入中存在的固有结构来提取概念的分类及其层次关系。这项研究揭示了 LM 如何发展概念理解，并为进一步研究打开了大门，以提高其推理和利用现实世界知识的能力。我们进一步进行了实验，并观察了将这些提取的概念表示与基于转换器的 LM 的推理模块分离的可能性。观察到的概念形成以及概念表示与推理模块的隔离可以使有针对性的标记工程为知识转移、可解释的人工智能以及开发更模块化和概念化的语言模型的潜在应用打开大门。]]></description>
      <guid>https://arxiv.org/abs/2406.05315</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:40 GMT</pubDate>
    </item>
    <item>
      <title>行为结构化器：通过结构化标记学习玩家表征</title>
      <link>https://arxiv.org/abs/2406.05274</link>
      <description><![CDATA[arXiv:2406.05274v1 公告类型：新
摘要：在本文中，我们介绍了 Behavior Structformer，这是一种在基于 Transformer 的架构中使用结构化标记对用户行为进行建模的方法。通过将跟踪事件转换为密集标记，此方法可提高模型训练效率和有效性。我们通过消融研究和与传统表格和半结构化基线的基准测试证明了其卓越的性能。结果表明，具有顺序处理的结构化标记显著改善了行为建模。]]></description>
      <guid>https://arxiv.org/abs/2406.05274</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:39 GMT</pubDate>
    </item>
    <item>
      <title>SuperPos-Prompt：通过叠加多个标记嵌入来增强语言模型的软提示调整</title>
      <link>https://arxiv.org/abs/2406.05279</link>
      <description><![CDATA[arXiv:2406.05279v1 公告类型：新
摘要：软提示调优技术最近获得了关注，成为一种有效的策略，用于对预训练语言模型进行参数高效的调优，特别是最大限度地减少了所需的模型参数调整。尽管它们的使用越来越广泛，但使用软提示实现最佳调优，尤其是对于较小的数据集，仍然是一个巨大的挑战。这项研究在这个领域做出了两点贡献：（i）我们引入了 SuperPos-Prompt，这是一种新的重新参数化技术，采用多个预训练词汇嵌入的叠加来改进软提示的学习。我们在多个 GLUE 和 SuperGLUE 基准测试中的实验始终突出了 SuperPos-Prompt 优于 Residual Prompt 调优的优势，在 T5-Small 中平均得分增加了 $+6.4$，在 T5-Base 中平均得分增加了 $+5.0$，并且收敛速度更快。值得注意的是，SuperPos-Prompt 有时甚至比完整的微调方法表现更好。 (ii) 此外，我们通过省略冻结网络中的丢失来展示增强的性能和快速收敛，从而在各种场景和调整方法中实现一致的改进。]]></description>
      <guid>https://arxiv.org/abs/2406.05279</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:39 GMT</pubDate>
    </item>
    <item>
      <title>DeviceBERT：应用迁移学习与有针对性的注释和词汇丰富来识别 FDA 召回摘要中的医疗器械和组件术语</title>
      <link>https://arxiv.org/abs/2406.05307</link>
      <description><![CDATA[arXiv:2406.05307v1 公告类型：新 
摘要：FDA 医疗器械召回是关键且时间敏感的事件，需要迅速识别受影响的设备以告知公众召回事件并确保患者安全。OpenFDA 设备召回数据集包含有关正在进行的设备召回行动的宝贵信息，但从召回行动摘要中手动提取相关设备信息是一项耗时的任务。命名实体识别 (NER) 是自然语言处理 (NLP) 中的一项任务，涉及识别和分类非结构化文本中的命名实体。现有的 NER 模型（包括 BioBERT 等领域特定模型）难以正确识别这些摘要中的医疗器械商品名、零件编号和组件术语。为了解决这个问题，我们提出了 DeviceBERT，这是一种医疗器械注释、预处理和丰富管道，它以 BioBERT 为基础，以更高的准确性识别和标记设备召回摘要中的医疗器械术语。此外，我们证明了我们的方法可以有效地应用于在训练数据有限或稀疏的情况下执行实体识别任务。]]></description>
      <guid>https://arxiv.org/abs/2406.05307</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:39 GMT</pubDate>
    </item>
    <item>
      <title>生成式探索-利用：使用 LLM 优化器对生成式推荐系统进行无训练优化</title>
      <link>https://arxiv.org/abs/2406.05255</link>
      <description><![CDATA[arXiv:2406.05255v1 公告类型：新
摘要：推荐系统被广泛用于推荐引人入胜的内容，大型语言模型 (LLM) 催生了生成式推荐器。此类系统可以直接生成项目，包括问题建议等开放集任务。虽然 LLM 的世界知识可以实现良好的推荐，但通过用户反馈改进生成的内容具有挑战性，因为持续微调 LLM 的成本过高。我们提出了一种无需训练的方法来优化生成式推荐器，方法是将用户反馈循环连接到基于 LLM 的优化器。我们提出了一种生成式探索-利用方法，它不仅可以利用已知高参与度的生成项目，还可以主动探索和发现隐藏的人口偏好以提高推荐质量。我们在两个领域（电子商务和一般知识）评估了我们的问题生成方法，并使用点击率 (CTR) 对用户反馈进行建模。实验表明，我们基于 LLM 的探索-利用方法可以迭代改进推荐，并持续提高点击率。消融分析表明，生成性探索是了解用户偏好的关键，可避免贪婪的纯开发方法的陷阱。人工评估有力地支持了我们的定量研究结果。]]></description>
      <guid>https://arxiv.org/abs/2406.05255</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:38 GMT</pubDate>
    </item>
    <item>
      <title>TLEX：一种从 TimeML 时间图中提取精确时间线的有效方法</title>
      <link>https://arxiv.org/abs/2406.05265</link>
      <description><![CDATA[arXiv:2406.05265v1 公告类型：新
摘要：时间线提供了事件和时间的完全排序，可用于许多自然语言理解任务。然而，可以直接从文本中得出的定性时间图（例如 TimeML 注释）通常仅明确显示事件和时间的部分排序。在这项工作中，我们将解决点代数问题的先前工作应用于从 TimeML 注释文本中提取时间线的任务，并开发了一种精确的端到端解决方案，我们称之为 TLEX（时间线提取）。TLEX 将 TimeML 注释转换为按主干和分支结构排列的时间线集合。与之前的工作一样，TLEX 检查时间图的一致性并解决它；但是，它增加了两个新功能。首先，它可以识别出涉及不一致的特定关系（然后可以手动纠正）；其次，TLEX 对时间线中顺序不确定的部分进行了新颖的识别，这些信息对于下游任务（例如对齐来自不同时间线的事件）至关重要。我们对 TLEX 中的算法组件进行了详细描述和分析，并通过将 TLEX 应用于来自四个语料库的 385 个 TimeML 注释文本进行了实验评估。我们发现 123 个文本不一致，181 个文本有多个“现实世界”或主时间线，四个语料库中有 2,541 个不确定部分。抽样评估表明，TLEX 在五个维度上的准确率为 98--100%，置信度为 95%：时间点的排序、主时间线的数量、时间点在主时间线和从属时间线上的位置、分支时间线的连接点以及不确定部分的位置。我们提供了 TLEX 的参考实现、所有文本的提取时间线以及不一致文本的手动更正。]]></description>
      <guid>https://arxiv.org/abs/2406.05265</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:38 GMT</pubDate>
    </item>
    <item>
      <title>自然语言生成中的主观不确定性量化与校准</title>
      <link>https://arxiv.org/abs/2406.05213</link>
      <description><![CDATA[arXiv:2406.05213v1 公告类型：新
摘要：大型语言模型的应用通常涉及自由形式响应的生成，在这种情况下，不确定性量化变得具有挑战性。这是因为需要识别特定于任务的不确定性（例如，关于语义），这在一般情况下似乎很难定义。这项工作从贝叶斯决策理论的角度解决了这些挑战，从我们的效用由相似性度量表征的假设开始，该相似性度量将生成的响应与假设的真实响应进行比较。我们讨论了这一假设如何实现对模型的主观不确定性及其校准的原则量化。我们进一步推导出一种认知不确定性的度量，基于缺失数据视角及其作为超额风险的特征。所提出的度量可以应用于黑盒语言模型。我们在问答和机器翻译任务中展示了所提出的方法，它们从 GPT 和 Gemini 模型中提取了具有广泛意义的不确定性估计并量化了它们的校准。]]></description>
      <guid>https://arxiv.org/abs/2406.05213</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:37 GMT</pubDate>
    </item>
    <item>
      <title>改进基于 Logits 的检测器（无需使用黑盒 LLM 的 Logits）</title>
      <link>https://arxiv.org/abs/2406.05232</link>
      <description><![CDATA[arXiv:2406.05232v2 公告类型：新
摘要：大型语言模型 (LLM) 的出现彻底改变了文本生成，产生了与人类写作非常相似的输出。机器文本和人类书写文本之间的界限变得模糊，这给区分两者带来了新的挑战，而领先的专有 LLM 的频繁更新和封闭性使这一任务变得更加复杂。当黑盒 LLM 无法提供精确的 logit 时，传统的基于 logit 的检测方法利用代理模型来识别 LLM 生成的内容。然而，这些方法难以解决代理模型和通常未公开的目标模型之间的分布不一致的问题，从而导致性能下降，尤其是在引入新的闭源模型时。此外，虽然当前的方法在识别源模型时通常是有效的，但在模型版本未知或测试集包含来自各种源模型的输出的情况下，它们就会失效。为了解决这些限制，我们提出了分布对齐的 LLM 检测 (DALD)，这是一个创新框架，即使没有来自源 LLM 的逻辑，它也能重新定义黑盒文本检测的最新性能。DALD 旨在将代理模型的分布与未知目标 LLM 的分布对齐，从而确保以最少的训练投资增强检测能力和对快速模型迭代的适应能力。通过利用 ChatGPT、GPT-4 和 Claude-3 等高级模型的公开输出中的语料库样本，DALD 可以微调代理模型以有效地与未知源模型分布同步。]]></description>
      <guid>https://arxiv.org/abs/2406.05232</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:37 GMT</pubDate>
    </item>
    <item>
      <title>相关性并不意味着补偿：词汇的复杂性和无规则性</title>
      <link>https://arxiv.org/abs/2406.05186</link>
      <description><![CDATA[arXiv:2406.05186v1 公告类型：新
摘要：有人声称，在一种语言中，形态不规则的单词更有可能在音位上简单，而形态规则的单词更有可能在音位上复杂。这种反比关系已在英语中得到证实，但尚未在更大的语言样本中得到证实。此外，频率和词长已知会影响音位复杂性和形态不规则性，它们可能是这种关系中的混杂因素。因此，我们研究了这四个变量的所有对之间的关​​系，以使用改进的方法评估先前发现的稳健性，并作为理解潜在因果关系的一步。我们使用 UniMorph 中 25 种语言的音位复杂性和形态不规则性的信息论测量方法（Pimentel 等人，2020 年；Wu 等人，2019 年）发现，有证据表明，在语言中，形态不规则性和音位复杂性之间存在正相关关系，尽管不同语言之间的方向有所不同。我们还发现了一些此前未发现的词长和形态不规则性之间存在负相关关系的微弱证据，而且关于这四个变量之间关系的一些现有发现并不像之前认为的那样稳健。]]></description>
      <guid>https://arxiv.org/abs/2406.05186</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:36 GMT</pubDate>
    </item>
    <item>
      <title>LLM 并非聪明的思考者：引入数学主题树基准对 LLM 进行综合评估</title>
      <link>https://arxiv.org/abs/2406.05194</link>
      <description><![CDATA[arXiv:2406.05194v1 公告类型：新
摘要：大型语言模型 (LLM) 在数学推理方面表现出令人印象深刻的能力。然而，尽管取得了这些成就，但目前的评估大多局限于特定的数学主题，而且 LLM 是否真正参与推理仍不清楚。为了解决这些差距，我们提出了数学主题树 (MaTT) 基准，这是一个具有挑战性的结构化基准，它提供了 1,958 个涉及广泛数学学科的问题，每个问题都与一个详细的层次主题链配对。在使用 MaTT 基准评估不同的 LLM 后，我们发现最先进的模型 GPT-4 在多项选择题场景中的准确率仅为 54%。有趣的是，即使采用思维链提示，我们也几乎没有观察到显着的改进。此外，当问题没有提供选择时，LLM 的准确率急剧下降了 24.2 个百分点。对 LLM 在一系列主题上的表现进行进一步详细分析后发现，即使是在同一数学领域内密切相关的子主题，也存在显著差异。为了找出 LLM 表现背后的原因，我们在有选择的情况下对 GPT-4 生成的解释的完整性和正确性进行了手动评估。令人惊讶的是，我们发现，在模型提供正确答案的情况下，只有 53.3% 的案例中，附带的解释被认为是完整和准确的，即模型进行了真正的推理。]]></description>
      <guid>https://arxiv.org/abs/2406.05194</guid>
      <pubDate>Wed, 12 Jun 2024 03:15:36 GMT</pubDate>
    </item>
    </channel>
</rss>