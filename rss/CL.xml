<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Thu, 20 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>私人文字生成大型语言模型提示</title>
      <link>https://arxiv.org/abs/2502.13193</link>
      <description><![CDATA[ARXIV：2502.13193V1公告类型：新 
摘要：我们探讨了如何通过适当提示大型语言模型（LLM）来生成私人合成文本。这是针对像医院这样的组织挑战，该组织拥有诸如患者病历的敏感文本数据，并希望分享它以培训机器学习模型的医疗任务，同时保留患者隐私。依靠培训或填充模型的方法可能是由于第三方LLM的API限制而无法实现的，或者是由于与LLM本身共享私人数据的道德和法律禁令。
  我们建议通过仅通过私有化的提示访问LLM，从而从敏感输入语料库中生成私有合成文本语料库，从而从敏感输入语料库中生成私有合成的文本语料库。它是基于从短语嵌入的分布中的私人样本中播种提示，从而捕获输入语料库，同时实现必要的输出多样性并保持差异隐私。我们在下游ML文本分类任务上评估了DP-KPS，并证明其生成的Corpora可以保留原始内容的许多预测能力。我们的发现提供了希望，即机构可以通过用简单的提示和几乎没有计算来私下共享数据来收获ML洞察力。]]></description>
      <guid>https://arxiv.org/abs/2502.13193</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言概括不是规则：对LMS评估的影响</title>
      <link>https://arxiv.org/abs/2502.13195</link>
      <description><![CDATA[ARXIV：2502.13195V1公告类型：新 
摘要：LMS概括地产生或理解新文本的语言评估通常隐含地认为自然语言是由符号规则产生的。语法被认为是通过是否遵守此类规则来确定的。人们认为，解释是由在有意义的单词上运行的句法规则在作曲上产生的。语义解析旨在将句子映射到形式逻辑中。 LMS遵守严格规则的失败是为了揭示LMS不会像人类那样产生或理解语言。在这里，我们建议LMS未能遵守符号规则可能是一个功能，而不是错误，因为自然语言不是基于规则的。通过灵活的相互关联和上下文依赖性的模式或构造的结合，产生和理解了新的话语。我们鼓励研究人员重新构想适当的基准和分析，以承认构成自​​然语言的丰富灵活的概括。]]></description>
      <guid>https://arxiv.org/abs/2502.13195</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在（灰色）框外思考：基于上下文的分数，用于评估神经文本生成中的价值和原创性</title>
      <link>https://arxiv.org/abs/2502.13207</link>
      <description><![CDATA[ARXIV：2502.13207V1公告类型：新 
摘要：尽管大型语言模型用于创造性任务，但它们的输出通常缺乏多样性。通用解决方案（例如在较高温度下采样）可能会损害结果的质量。利用信息理论，我们提出了一个基于上下文的分数，以定量评估价值和原创性。该分数激发了准确性和对请求的遵守，同时促进了与学习分布的分歧。我们建议在加强学习框架中以分数为奖励，以微调大语言模型，以提高表现。我们通过诗歌产生和数学问题解决的实验来验证我们的策略，表明它增强了生成的解决方案的价值和独创性。]]></description>
      <guid>https://arxiv.org/abs/2502.13207</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SearchRag：搜索引擎对基于LLM的医疗问题答案有帮助吗？</title>
      <link>https://arxiv.org/abs/2502.13233</link>
      <description><![CDATA[ARXIV：2502.13233V1公告类型：新 
摘要：大型语言模型（LLM）在一般领域显示出了显着的功能，但通常在需要专业知识的任务上挣扎。传统的检索型发电（RAG）技术通常从静态知识库中检索外部信息，这些信息可能过时或不完整，缺少精确的医疗问题答案必不可少的细粒度临床细节。在这项工作中，我们提出了SearchRag，这是一个新颖的框架，通过利用实时搜索引擎来克服这些局限性。我们的方法采用综合查询生成将复杂的医疗问题转换为搜索引擎友好的查询，并利用基于不确定的知识选择过滤并将最相关和最有用的医学知识纳入LLM的输入中。实验结果表明，我们的方法显着提高了医学问答任务的响应准确性，尤其是对于需要详细和最新知识的复杂问题。]]></description>
      <guid>https://arxiv.org/abs/2502.13233</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当人们成为洪水时：用大语言模型分析移民话语中的非人性隐喻</title>
      <link>https://arxiv.org/abs/2502.13246</link>
      <description><![CDATA[ARXIV：2502.13246V1公告类型：新 
摘要：隐喻，讨论一个概念，在政治上很丰富，可以塑造人们如何理解重要问题。我们开发了一种计算方法来衡量隐喻语言，重点是社交媒体上的移民论述。基于定性社会科学研究，我们确定了在移民话语中引起的七个概念（例如“水”或“害虫”）。我们提出并评估一种利用单词级别和文档级信号来衡量这些概念的隐喻的新颖技术。然后，我们研究了40万美国有关移民的推文中隐喻，政治意识形态和用户参与之间的关系。尽管保守派倾向于使用非人性化的隐喻，而不是自由主义者，但这种效果在概念上差异很大。此外，与生物相关的隐喻与更多的转发有关，尤其是对于自由作者而言。我们的工作突出了计算方法的潜力，可以补充定性方法，以理解政治话语中的微妙和隐性语言。]]></description>
      <guid>https://arxiv.org/abs/2502.13246</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过知识图接地LLM推理</title>
      <link>https://arxiv.org/abs/2502.13247</link>
      <description><![CDATA[ARXIV：2502.13247V1公告类型：新 
摘要：知识图（kgs）是代表结构化格式实体之间关系的宝贵工具。传统上，这些知识库被询问以提取特定信息。然而，由于自然语言的内在复杂性与结构化格式和这些图的大小相比，因此，此类公斤的提问（QA）构成了挑战。尽管面临这些挑战，KG的结构性性质仍可以为基础大型语言模型（LLM）的产出提供坚实的基础，从而为组织提供了提高的可靠性和控制力。
  LLM的最新进步在推理时间引入了推理方法，以提高其性能并最大程度地提高其能力。在这项工作中，我们建议将这些推理策略与KG集成，以锚定KG数据中推理链的每个步骤或“思考”。具体而言，我们在几种推理策略中都评估了代理和自动化搜索方法，包括经过思考链（COT），经营之树（TOT）和经过思考图（GOT），使用GRBENCH，一个基准数据集用于使用特定域图的图形推理。我们的实验表明，这种方法始终优于基线模型，突出了结构化KG数据中接地LLM推理过程的好处。]]></description>
      <guid>https://arxiv.org/abs/2502.13247</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经关注搜索</title>
      <link>https://arxiv.org/abs/2502.13251</link>
      <description><![CDATA[ARXIV：2502.13251V2公告类型：新 
摘要：我们提出了神经注意力搜索（NAT），该框架自动评估每个令牌在序列中的重要性，并在几个步骤后确定是否可以删除相应的令牌。这种方法可以有效地减少推理过程中基于变压器的模型所需的KV缓存大小，从而降低推理成本。在本文中，我们设计了一个包含三种令牌类型的搜索空间：（i）所有以下令牌将保留和查询全局令牌。 （ii）本地令牌生存，直到出现下一个全球令牌。 （iii）滑动窗口令牌对下一个以下令牌的固定大小的推断有影响。与单发神经体系结构搜索方法类似，可以通过可学习的注意力掩码共同学习这种令牌型信息。在两种训练新变压器上都从头开始和微调现有大语模型的实验表明，NAT可以在保持模型的性能的同时有效地降低模型所需的KV高速缓存尺寸。]]></description>
      <guid>https://arxiv.org/abs/2502.13251</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用机器翻译数据预处理多语言语言模型</title>
      <link>https://arxiv.org/abs/2502.13252</link>
      <description><![CDATA[ARXIV：2502.13252V1公告类型：新 
摘要：诸如英语之类的高资源语言，可以预测高质量的大语言模型（LLMS）。对于大多数其他语言来说，也不能说这是同样的，因为LLM在非英语语言中仍然表现不佳，这可能是由于可用多语言读图的质量和多样性差异所致。在这项工作中，我们发现来自单个高质量源语言的机器翻译文本可以对多语言LLM的预处理质量产生重大贡献。我们将高质量的英语Web数据集的FineWeb-Edu转换为九种语言，从而产生了1.7亿美元的数据集，我们将其称为TransWebedu，并将其定为1.3B参数模型，TransWebllm，Transwebllm，从该数据集上的Sckatch。在九项非英语推理任务中，我们表明transwebllm匹配或胜过最先进的多语言模型，但使用封闭数据（例如llama3.2，qwen2.5和Gemma）培训，尽管使用了较小的数据级，但。我们证明，将不到5％的Transwebedu添加为特定领域的预处理数据设置了一个新的阿拉伯语，意大利语，印度尼西亚人，斯瓦希里语和威尔士理解和常识性推理任务的新最新。为了促进可重复性，我们将我们的语料库，模型和培训管道发布在开源倡议批准的许可下。]]></description>
      <guid>https://arxiv.org/abs/2502.13252</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Humt Dumt：LLMS中的测量和控制类似人类的语言</title>
      <link>https://arxiv.org/abs/2502.13259</link>
      <description><![CDATA[ARXIV：2502.13259V1公告类型：新 
摘要：LLM是否会产生使它们看起来人性化的语言？类似人类的语言可能会改善用户体验，但也可能导致过度依赖和定型观念。评估这些潜在影响需要一种系统的方法来测量LLM输出中类似人类的音调。我们基于LLM的相对概率介绍了Humt和Sociot，类似人类语调的指标以及文本数据中社会感知的其他方面。通过在偏好和使用数据集中测量HUMT，我们发现用户更喜欢LLMS的类似人类的输出。 Humt还提供了对拟人化的影响的见解：类似人类的LLM输出与温暖，社会亲密，女性气质和低地位高度相关，这与上述危害密切相关。我们介绍了Dumt，这是一种使用HUMT系统控制和降低类似人类语调的方法的方法，同时保持模型性能。杜姆（Dumt）提供了一种实用方法，用于减轻与拟人化语言相关的风险。]]></description>
      <guid>https://arxiv.org/abs/2502.13259</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大型语言模型中，逐步的困惑引导的完善，以实现有效的思考推理</title>
      <link>https://arxiv.org/abs/2502.13260</link>
      <description><![CDATA[ARXIV：2502.13260V1公告类型：新 
摘要：将复杂的任务分解为中间推理步骤的基础链（COT）推理，已大大提高了大语言模型（LLMS）在具有挑战性的任务上的性能。但是，COT中的详细推理过程通常会产生长时间的时间和高计算成本，部分原因是包括不必要的步骤。为了解决这一问题，我们提出了一种使用困惑的方法来识别关键推理步骤的方法：如果其去除会导致困惑显着增加，则认为一步至关重要。我们的方法使模型能够仅专注于生成这些关键步骤。这可以通过两种方法来实现：精炼演示示例中的几个小婴儿床或使用仅包括关键步骤的选定示例对模型进行微调。全面的实验验证了我们方法的有效性，这在COT的推理准确性和效率之间取得了更好的平衡。]]></description>
      <guid>https://arxiv.org/abs/2502.13260</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Realtalk：长期对话的21天真实数据集</title>
      <link>https://arxiv.org/abs/2502.13270</link>
      <description><![CDATA[ARXIV：2502.13270V1公告类型：新 
摘要：长期，开放域对话能力对于旨在回忆过去互动并展示情绪智力（EI）的聊天机器人至关重要。然而，大多数现有的研究依赖于合成，LLM生成的数据，留下有关现实世界对话模式的开放问题。为了解决这一差距，我们介绍了Realtalk，这是一个21天真实的消息应用程序对话，为真正的人类互动提供了直接的基准。
  我们首先进行数据集分析，重点介绍EI属性和角色一致性，以了解实际对话所带来的独特挑战。通过与LLM生成的对话进行比较，我们强调了关键差异，包括各种情感表达和人格稳定性的变化，合成对话通常无法捕获。
  在这些见解的基础上，我们介绍了两个基准任务：（1）角色模拟，其中模型代表特定用户继续进行对话的对话中的对话； （2）模型回答有针对性问题的内存探测需要长期记忆过去相互作用的问题。
  我们的发现表明，模型很难仅从对话历史记录中模拟用户，而对特定用户聊天进行微调可以改善性格仿真。此外，现有模型在召回和利用现实世界中的长期背景方面面临重大挑战。]]></description>
      <guid>https://arxiv.org/abs/2502.13270</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用端到端，转移学习，分布式和可解释的AI模型对文本和表情符号数据的情感分析的绩效评估</title>
      <link>https://arxiv.org/abs/2502.13278</link>
      <description><![CDATA[ARXIV：2502.13278V1公告类型：新 
摘要：在当今的数字世界中经常使用表情符号，比以往任何时候都更像简单到复杂的思想表达。因此，它们也用于情感分析和针对性的营销活动。在这项工作中，我们从Kaggle进行了推文以及表情符号数据集的情感分析。由于推文是句子，因此我们使用了通用句子编码器（使用）和句子双向编码器表示，来自变形金刚（Sbert）端到端句子嵌入模型的句子来生成用于训练标准完全连接的神经网络（NN）的嵌入式，和LSTM NN模型。我们观察到两种模型的文本分类精度几乎相同，约为98％。相反，当使用训练集中不存在的表情符号构建验证集时，这两个模型的准确性都大大降低至70％。此外，还使用分布式训练方法，而不是传统的单读模型来培训这些模型，以提高可扩展性。使用分布式训练方法，我们能够将运行时间减少约15％，而不会损害准确性。最后，作为可解释的AI的一部分，使用SHAP算法来解释模型行为并检查给定特征集的模型偏差。]]></description>
      <guid>https://arxiv.org/abs/2502.13278</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解和解决个体自然语言中的标签错误理解</title>
      <link>https://arxiv.org/abs/2502.13297</link>
      <description><![CDATA[ARXIV：2502.13297V1公告类型：新 
摘要：自然语言理解（NLU）是一项使机器能够理解人类语言的任务。某些任务，例如立场检测和情感分析，与个人主观观点密切相关，因此称为个人级别的NLU。以前，这些任务通常被简化为文本级NLU任务，忽略了个体因素。这不仅使推理变得困难且无法解释，而且在创建数据集时通常会导致大量标签错误。为了解决上述限制，我们提出了基于个人级别因素的新的NLU注释指南。具体来说，我们通过同一个人结合了其他帖子，然后在考虑所有个人帖子后注释个人主观观点。我们使用此指南来扩展和重新注释立场检测和基于主题的情感分析数据集。我们发现样品中的错误率高达31.7 \％和23.3 \％。我们进一步使用大型语言模型来对重新注册数据集进行实验，并发现在添加个别因素后，大型语言模型在两个数据集上都表现良好。 GPT-4O和LLAMA3-70B均可在重新注释数据集中获得大于87 \％的精度。我们还通过消融研究来验证个体因素的有效性。我们呼吁未来的研究人员在创建此类数据集时添加个人因素。我们的重新通信数据集可以在https://github.com/24yearsoldstudent/individual-nlu上找到]]></description>
      <guid>https://arxiv.org/abs/2502.13297</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过迅速链接和细粒度的反馈来改善以任务为导向的对话系统中的多转弯任务完成</title>
      <link>https://arxiv.org/abs/2502.13298</link>
      <description><![CDATA[ARXIV：2502.13298V1公告类型：新 
摘要：面向任务的对话框（TOD）系统促进用户通过自然语言完成复杂的多转弯任务。尽管传统方法依赖于每个域的广泛微调和注释数据，但指导调节的大语言模型（LLMS）提供了更灵活的替代方案。但是，LLM努力可靠地处理多转任任务的完成，尤其是在没有明确演示的情况下准确地生成API呼叫并适应新领域。为了应对这些挑战，我们提出了Realtod，这是一个新颖的框架，可以通过迅速链接和细粒度的反馈机制来增强TOD系统。及时链接可以通过两阶段提示策略适应零射击域，从而消除了对人类策划的示范的需求。同时，通过验证针对域模式的API调用并在检测到错误时提供精确的纠正反馈来改善任务完成。我们使用四个LLM对SGD和BITOD基准进行了广泛的实验。 Realtod提高了API准确性，SGD上的AUTOTOD超过37.74％，而SimpleTOD则在BITOD上超过了11.26％。与现有方法相比，与房地产用运集成的LLM与RealTOD集成的LLM相比实现了卓越的任务完成，流利和信息性。]]></description>
      <guid>https://arxiv.org/abs/2502.13298</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估和增强以任务完成的任务完成的跨域概括，无需转交级对话框注释</title>
      <link>https://arxiv.org/abs/2502.13310</link>
      <description><![CDATA[ARXIV：2502.13310V1公告类型：新 
摘要：传统面向任务的对话框（TOD）系统在很大程度上依赖于劳动密集型的转向级注释，例如对话状态和政策标签，用于培训。这项工作探讨了大型语言模型（LLMS）是否仅在自然语言对话框中进行微调以执行TOD任务，而无需进行此类注释。我们评估了他们概括地看不见域并将其性能与经过完全注释数据训练的模型进行比较的能力。通过对三个不同大小的开源LLM和两个不同TOD数据集进行的广泛实验，我们发现模型在没有转交级注释的情况下进行了微调，从而产生连贯且上下文适当的响应。但是，通过准确执行API调用，他们的任务完成性能仍然次优，最佳模型在看不见的域中仅取得了53％的成功。为了改善任务完成，我们提出了Zerotod，该框架结合了架构增强机制，以提高API调用准确性和整体任务完成率，尤其是在室外设置中。我们还将Zerotod与无微调的替代方案进行了比较，例如提示现成的LLMS，并发现我们的框架可实现较小的，微调的模型，以优于大规模专有的LLMS。此外，一项评估信息性，流利性和任务完成的人类研究证实了我们的经验发现。这些发现表明，为真实应用应用开发具有成本效益，可扩展性和零击的TOD系统的可行性。]]></description>
      <guid>https://arxiv.org/abs/2502.13310</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>