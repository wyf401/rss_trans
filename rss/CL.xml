<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 11 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>LLM为法律问题提供不稳定的答案</title>
      <link>https://arxiv.org/abs/2502.05196</link>
      <description><![CDATA[ARXIV：2502.05196V1公告类型：新 
摘要：LLM多次被问及相同的问题时，如果得出相同的结论，则LLM是稳定的。我们发现诸如GPT-4O，Claude-3.5和Gemini-1.5之类的领先LLM在为硬法律问题提供答案时，即使将温度设置为0，我们也会不稳定。从真实案件中提取的问题，涉及两个当事方，事实，竞争法律论点，以及哪个方应占上风的问题。当提出完全相同的问题时，我们观察到LLM有时会说一方应该获胜，而其他时候则说另一方应该获胜。这种不稳定对依赖这些LLM的法律AI产品，法律程序和律师的数量越来越多。]]></description>
      <guid>https://arxiv.org/abs/2502.05196</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加速LLM推断异构词汇的无损投机解码算法</title>
      <link>https://arxiv.org/abs/2502.05202</link>
      <description><![CDATA[ARXIV：2502.05202V1公告类型：新 
摘要：加速大型语言模型（LLM）的推断是生成AI的关键挑战。投机解码（SD）方法通过使用单个目标向前传递生成多个令牌来提供可观的效率提高。但是，现有的SD方法要求起草者和目标模型共享相同的词汇，从而限制了可能的起草者的池，通常需要从头开始训练起草人。我们提出了三种新的SD方法，用于删除此共享 - 唱机约束。所有三种方法都保留目标分布（即它们是无损的），并与现成的模型一起使用，而无需进行额外的培训或修改。从经验上讲，在汇总，编程和长篇文章任务上，我们的算法比标准自回旋解码实现了重要的加速。通过使任何现成的模型能够充当起草者并不需要再培训，这项工作在实践中实质上扩大了SD框架的适用性。]]></description>
      <guid>https://arxiv.org/abs/2502.05202</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效的知识馈送到语言模型：一种新颖的集成编码器架构</title>
      <link>https://arxiv.org/abs/2502.05233</link>
      <description><![CDATA[ARXIV：2502.05233V1公告类型：新 
摘要：本文通过将检索和生成过程集成到统一的框架中，从而在预测过程中提出了一种新颖的方法，可以在预测过程中有效地喂养知识（LLMS）。虽然检索功能的生成（RAG）模型解决了LLMS培训数据和知识限制中的差距，但它受到令牌限制限制和对检索系统准确性的依赖的阻碍。我们提出的体系结构结合了文化载体（ICV），以克服这些挑战。 ICV通过使用LLMS的潜在嵌入来创建一个捕获基本任务信息的向量来重铸在上下文中学习。然后，该向量用于移动LLM的潜在状态，从而增强了生成过程，而无需在提示中添加演示示例。 ICV将信息直接集成到模型中，使其能够更有效地处理此信息。我们广泛的实验评估表明，ICV在提问，信息检索和其他任务中的表现优于标准的文本学习和微调。这种方法减轻了当前的抹布模型的局限性，并为处理广泛和多样化的数据集提供了更强大的解决方案。尽管利用了一小部分参数，但我们的ICV增强模型仍针对Llama-3，Gemma和Phi-3等模型达到了竞争性能，从而大大降低了计算成本和内存要求。 ICV降低了及时的长度，易于控制，超过令牌限制，并且与微调相比在计算上有效。]]></description>
      <guid>https://arxiv.org/abs/2502.05233</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强知识图构造：评估幻觉，遗漏和图形相似性指标</title>
      <link>https://arxiv.org/abs/2502.05239</link>
      <description><![CDATA[ARXIV：2502.05239V1公告类型：新 
摘要：大语言模型中的最新进步表明，在非结构化文本的自动构造知识图中具有巨大的潜力。本文建立在我们以前的工作[16]的基础上，该工作使用精度，召回，F1分数，三重匹配和图形匹配评估了各种模型，并介绍了一种精致的方法来解决幻觉和遗漏的关键问题。我们提出了一个增强的评估框架，其中包含用于图形相似性的Bertscore，为图形匹配设置了95％的实际阈值。我们的实验集中在Mistral模型上，比较了其原始和微调版本中的零拍摄和少量设置。我们使用KELM-SUB训练数据集的示例进一步扩展了实验，表明微型模型可显着提高知识图构造精度，同时降低了确切的幻觉和遗漏。但是，我们的发现还表明，微调模型在KELM-SUB数据集上的概括任务中的性能差。这项研究强调了全面评估指标在从文本数据中推进知识图构造中最新的最新时间的重要性。]]></description>
      <guid>https://arxiv.org/abs/2502.05239</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SEER：大型语言模型表示的自我解释性增强</title>
      <link>https://arxiv.org/abs/2502.05242</link>
      <description><![CDATA[ARXIV：2502.05242V1公告类型：新 
摘要：解释大语言模型（LLMS）的隐藏表示形式是了解LLMS的基本推论逻辑并提高其在应用程序方案中的可靠性的观点。但是，以前的方法介绍了外部的“黑盒”“模块”来解释“黑框” LLM，从而增加了潜在的不确定性并无法提供忠实的解释。在本文中，我们提出了一种自我解释的方法，通过汇总相同的概念并解散表示空间中不同概念的方法来增强LLMS的解释性。这样，Seer提供了由LLMS的输出同步表示的忠实解释。此外，我们展示了SEER在与可信度相关的任务（例如，安全风险分类和排毒任务）上的应用，在这些任务中，自我解释的LLMS可以在解释性和性能方面保持一致的改善。更重要的是，我们从理论上分析了通过最佳运输理论对LLMS的概括能力的改进。]]></description>
      <guid>https://arxiv.org/abs/2502.05242</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估大语言模型中的人格特征：心理问卷的见解</title>
      <link>https://arxiv.org/abs/2502.05248</link>
      <description><![CDATA[ARXIV：2502.05248V1公告类型：新 
摘要：心理评估工具长期以来帮助人类了解行为模式。尽管大型语言模型（LLM）可以生成与人类相当的内容，但我们探索它们是否表现出人格特征。为此，这项工作将心理工具应用于LLM，以产生人格概况。使用既定的基于特质的问卷，例如五巨头库存，并通过解决培训数据污染的可能性，我们研究了LLM在五个核心人格方面的维度变异性和优势：开放性，尽责，良心，外向性，同意和神经质性。我们的发现表明，LLM在同一模型家族中也表现出独特的主导性状，不同的特征和独特的人格概况。]]></description>
      <guid>https://arxiv.org/abs/2502.05248</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GSM无限：您的LLM在无限增加上下文长度和推理复杂性方面的行为如何？</title>
      <link>https://arxiv.org/abs/2502.05252</link>
      <description><![CDATA[ARXIV：2502.05252V1公告类型：新 
摘要：长篇文章大语模型（LLMS）最近在信息检索和长篇文档质量检查中表现出强烈的性能。但是，要解决最具挑战性的智力问题，LLM必须在漫长而复杂的环境中有效地理解（例如边境数学研究）。研究LLM如何处理提高的推理复杂性和上下文长度是必不可少的，但是现有的基准缺乏定量评估的坚实基础。受到GSM-8K问题作为计算图的抽象的启发，以及通过添加不必要的节点和边缘引入噪音的能力，我们开发了一个小学的数学问题生成器，能够在细粒度的控制下产生无限难度和上下文的算术问题。使用我们新合成的GSM侵入基准测试，我们全面评估了现有的LLM。我们发现，随着复杂性的提高，推理性能的一致性下降，以及系统的推理缩放趋势：指数增加的推理计算只能获得线性性能提高。这些发现强调了当前长篇小说LLM的基本局限性以及扩展推理能力的关键挑战。我们的GSM Infinite基准提供了一个可扩展且可控制的测试台，用于在漫长而复杂的环境中系统地研究和推进LLM推理。]]></description>
      <guid>https://arxiv.org/abs/2502.05252</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以教会自己更好地预测未来</title>
      <link>https://arxiv.org/abs/2502.05253</link>
      <description><![CDATA[ARXIV：2502.05253V1公告类型：新 
摘要：我们提出了一个以结果为导向的微调框架，可增强大语言模型（LLM）的预测能力，而无需依靠人类策划的推理样本。我们的方法利用模型自我播放来生成一对多样的推理轨迹和概率预测，用于在模型的知识截止日期之后解决的一系列不同问题。然后，我们将这些推理跟踪的对成对通过它们到通过直接偏好优化（DPO）微调模型之前的实际结果的距离对。在单独的测试集中，我们的方法在基本模型上增加了PHI-4 14B和DeepSeek-R1 14B的预测准确性在7--10 \％之间，并带有随机标签的DPO微型控制模型诸如GPT-4O之类的更大边界模型的预测功能。]]></description>
      <guid>https://arxiv.org/abs/2502.05253</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以对较小的LLM的有害性进行排名吗？我们还没在那里</title>
      <link>https://arxiv.org/abs/2502.05291</link>
      <description><![CDATA[ARXIV：2502.05291V1公告类型：新 
摘要：大型语言模型（LLM）已无处不在，因此了解其风险和局限性很重要。较小的LLM可以在计算资源受到限制（例如边缘设备）的情况下部署，但具有不同的倾向以产生有害输出。缓解LLM危害通常取决于注释LLM输出的有害性，LLM输出的危害是昂贵的。这项工作研究了两个问题：较小的LLM如何在产生有害内容的情况下排名？更大的LLM可以注释有害性吗？我们促使三个小型LLM引起各种类型的有害内容，例如歧视性语言，进攻性内容，隐私入侵或负面影响，并收集其产量的人类排名。然后，我们评估了三个最先进的大型LLM，以注释这些反应的有害性。我们发现较小的模型在有害性方面有所不同。我们还发现，大型LLM与人类表现出低至中等的一致性。这些发现强调了在LLM中进行进一步减轻损害的进一步工作的必要性。]]></description>
      <guid>https://arxiv.org/abs/2502.05291</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>为了开发平衡的合成数据，以纠正阿拉伯语语法错误：基于错误标记模型和合成数据生成模型的方法</title>
      <link>https://arxiv.org/abs/2502.05312</link>
      <description><![CDATA[ARXIV：2502.05312V1公告类型：新 
摘要：合成数据生成被广泛认为是增强神经语法误差校正（GEC）系统质量的一种方式。但是，当前的方法通常缺乏多样性或过于简单，无法产生人类造成的各种语法错误，尤其是对于阿拉伯语等低资源语言。在本文中，我们将开发错误标记模型和合成数据生成模型，以在阿拉伯语中创建一个大型的合成数据集，以进行语法误差校正。在错误标记模型中，使用DEBERTAV3模型将正确的句子分为多种错误类型。阿拉伯错误类型注释工具（ARETA）用于指导错误标记模型中的多标签分类任务，其中每个句子被分类为26个错误标签。合成数据生成模型是一个基于反向翻译的模型，它通过在使用ARAT5模型从错误标记模型生成的正确句子之前附加错误标签来生成错误的句子。在QALB-14和QALB-15测试集中，错误标记模型达到了94.42％F1，这是在识别干净句子中识别错误标签的最新时间。由于我们在语法误差校正方面的句法数据培训的结果，我们在QALB-14测试集中达到了F1得分的新最新结果：79.36％。我们通过使用合成数据生成模型生成30,219,310合成句子对。]]></description>
      <guid>https://arxiv.org/abs/2502.05312</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微调的LLM是通过书籍跟踪社会偏见的“时间胶囊”</title>
      <link>https://arxiv.org/abs/2502.05331</link>
      <description><![CDATA[ARXIV：2502.05331V1公告类型：新 
摘要：书籍虽然经常富含文化见解，但也可以反映其时代的社会偏见 - 大型语言模型（LLMS）可能会在培训期间学习和永久化。我们介绍了一种新的方法，可以使用微调的LLM追踪和量化这些偏见。我们开发了书本，这是一个在七十年（1950-2019）中包含593本虚构书籍的语料库，以跟踪偏见的演变。通过对每十年的书籍进行微调LLM，并使用有针对性的提示，我们研究了与性别，性取向，种族和宗教有关的偏见的转变。我们的发现表明，经过十年特定书籍训练的LLM表现出反映了他们时代的偏见，既有逐渐的趋势又有明显的转变。例如，模型的回答表明，从1950年代到2010年代，女性在领导角色中的描绘逐渐增加，在1990年代（4％增加到12％），可能与之一致，可能与第三波女权主义。从1980年代到2000年代（从0％到10％），同性关系的参考文献明显增加，反映了LGBTQ+可见性的增长。关于伊斯兰教的负面描述在2000年代（26％至38％），可能反映了9/11之后的情绪。重要的是，我们证明这些偏见主要源于书籍的内容，而不是模型的体系结构或初始培训。我们的研究通过桥接AI，文学研究和社会科学研究来对社会偏见趋势有了新的看法。]]></description>
      <guid>https://arxiv.org/abs/2502.05331</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大语模型中上下文推断的概率子空间歧管</title>
      <link>https://arxiv.org/abs/2502.05346</link>
      <description><![CDATA[ARXIV：2502.05346V1公告类型：新 
摘要：将令牌嵌入表示为学到的流形上的概率分布，可以进行更灵活的上下文推断，从而降低表示刚性的同时增强语义粒度。比较评估表明，概率嵌入可以提高邻域的一致性和降低的冗余，从而确保令牌关系在微调迭代中在结构上保持更连贯。概率子空间在注意机制内的整合促进了更适应性的上下文加权，从而使模型能够捕获常规嵌入中否则会遮盖的潜在依赖性。实验结果强调了对对抗性修饰的鲁棒性提高，即使在基于扰动的评估方案下，概率嵌入也可以保留上下文完整性。绩效评估表明，概率表示在特定领域的应用中实现了更大的适应性，从而减轻了跨语言领域转移时进行广泛重新训练的需求。计算权衡仍保持在操作可行的限制范围内，而推断潜伏期的边缘延长与增强的表示稳定性和上下文表现力的好处的边缘增加。编码结构不确定性的能力在生成建模任务中提供了优势，尤其是在扩展序列之间保持连贯性的情况下，需要一个能够处理模棱两可或与上下文相关的语言结构的表示框架。]]></description>
      <guid>https://arxiv.org/abs/2502.05346</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>韵律在口头问题回答中的作用</title>
      <link>https://arxiv.org/abs/2502.05389</link>
      <description><![CDATA[ARXIV：2502.05389V1公告类型：新 
摘要：迄今为止的口语理解研究通常具有繁重的文本视角。大多数数据集源自文本，然后将其合成到语音中，大多数模型通常依赖于语音的自动转录。这损害了韵律 - 语音信号所携带的添加信息超出了单词本身的语音，并且很难单独从文本中恢复。在这项工作中，我们调查了韵律在口头问题回答中的作用。通过隔离由自然语音组成的SLUE-SQA-5数据集上的韵律和词汇信息，我们证明，仅对韵律信息培训的模型可以通过使用韵律提示来很好地表现。但是，我们发现，当有词汇信息可用时，模型倾向于主要依靠它。我们的发现表明，虽然韵律提示提供了有价值的补充信息，但需要更有效的整合方法来确保韵律与词汇特征并肩作用更大。]]></description>
      <guid>https://arxiv.org/abs/2502.05389</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从文化学习学习任务表示</title>
      <link>https://arxiv.org/abs/2502.05390</link>
      <description><![CDATA[ARXIV：2502.05390V1公告类型：新 
摘要：大型语言模型（LLMS）已经证明了对内部文化学习（ICL）的熟练程度，其中模型通过基于示例的提示来适应新任务，而无需参数更新。但是，了解内部编码和广义的任务是一个挑战。为了解决文献中一些经验和技术差距，我们引入了一种自动配方，以在ICL提示中编码任务信息，以作为变压器体系结构中注意力头的函数。这种方法将单个任务向量计算为加权的注意力头，而权重通过梯度下降优化了。我们的发现表明，现有方法无法有效地推广到文本以外的模式。作为回应，我们还设计了一个基准，以评估任务向量是否可以保留功能回归任务中的任务保真度。所提出的方法成功地从文本和回归任务中脱颖而出，成功提取了特定于任务的信息，并在文本和回归任务中表现出色，从而证明了其跨模式的普遍性。此外，消融研究表明，我们的方法的有效性源于将最后一个隐藏状态与最佳执行的内部文化学习模型的分布对齐。]]></description>
      <guid>https://arxiv.org/abs/2502.05390</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型中的分层词汇流投影：多尺度语义表示的新型机制</title>
      <link>https://arxiv.org/abs/2502.05395</link>
      <description><![CDATA[ARXIV：2502.05395V1公告类型：新 
摘要：将结构化层次嵌入到基于变压器的体系结构中的结构化层次嵌入引入了一种精致的词汇表示方法，以确保在不损害计算效率的情况下保留多尺度语义关系。将代币映射到结构化歧管上的投影机制提供了改进的词汇对准，从而增强了各种语言任务跨越单词表示的适应性。结构化编码框架可确保层次嵌入在不同的抽象水平之间保持连贯性，从而可以在局部句法特征和全局语义结构之间进行稳定的过渡。实验评估表明，层次嵌入始终优于常规令牌表示，提高了语言基准的准确性，同时保持较低的计算开销。跨多个领域的比较分析突出了层次嵌入保持上下文一致性的能力，尤其是在结构化词汇比对的专业语言应用中。统计评估进一步表明，层次嵌入在扰动条件下表现出增强的鲁棒性，从而确保语言结构在对抗性文本修改中保持稳定。将层次投影与变压器注意机制的集成能够改善上下文适应，从而确保基于不同语言分布动态调整令牌表示。精致的嵌入层次结构组织在词汇建模中提供了更大的可解释性，从而促进了各种文本处理任务的增强概括能力。]]></description>
      <guid>https://arxiv.org/abs/2502.05395</guid>
      <pubDate>Tue, 11 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>