<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 18 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>幻觉与真理：抹布，洛拉和多拉的全面精确评估</title>
      <link>https://arxiv.org/abs/2502.10497</link>
      <description><![CDATA[ARXIV：2502.10497V1公告类型：新 
摘要：生成AI的最新进展显着提高了自然语言处理（NLP）系统的效率和适应性，尤其是通过检索功能增强的生成（RAG），低排名适应性（LORA）和重量分解的低级适应（多拉）。 RAG集成了外部知识，以增强生成产出的事实一致性，而Lora则可以对大型语言模型（LLMS）进行参数有效的微调。 Dora通过通过自适应参数排名和域吸引力的权重调整来优化微调，从而进一步完善了这一过程，从而提高了学习效率，同时保持了推理性能。
  本文对RAG，LORA和DORA进行了大规模的经验评估，对20,000个基于FAQ的查询进行了微调和发电性能，而知识库跨越了400,000个条目。该研究分析了关键绩效指标，例如准确性，相关性和推理潜伏期。实验结果表明，朵拉（Dora）达到了最高的精度（90.1％），相关性评分（0.88）和最低的延迟（每个查询110毫秒），在现实世界中的域，域特异性AI应用中的表现都优于Lora和rag。
  此外，这项研究研究了不同模型的微调效率，计算成本和实时适应性之间的权衡。发现突出了抹布在知识接地，洛拉（Lora）的成本效益领域的适应性以及朵拉（Dora）在模型精度上平衡微调效率的能力。这些见解为在医疗保健，金融和法律服务等准确性领域中部署AI驱动的生成系统提供了实用的指导，从而确保在动态环境中确保可伸缩性，可靠性和最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2502.10497</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人制作的语言模型？评估LLMS的男性泛滥偏见的永久性</title>
      <link>https://arxiv.org/abs/2502.10577</link>
      <description><![CDATA[ARXIV：2502.10577V1公告类型：新 
摘要：大型语言模型（LLM）已显示出在特定或受约束的上下文中以英语和其他语言传播甚至扩大性别偏见。但是，到目前为止，尚无研究的重点是LLMS对通用指令的反应传达的性别偏见，尤其是在男性仿制药（MG）方面。 MG是许多性别标记的语言中发现的语言特征，表示使用男性性别作为“默认性”或所谓的中性性别，指的是男女混合群体，或者是无关紧要或不明性别的人。许多心理语言学研究表明，MG不是中立的，并且会引起性别偏见。这项工作旨在分析专有和本地LLM在对通用指令的响应中对MG的使用并评估其MG偏置率。我们专注于法语，并从现有的词汇资源中创建人类名词数据库。我们过滤现有的法语指令数据集以检索通用说明并分析6种不同LLM的响应。总体而言，我们发现$ \ $ \ $ 39.5 \％llms对通用说明的响应是mg偏见的（$ \ $ \ $ \ $ \ $ 73.1 \％\％\％\％在人类名词的响应中）。我们的发现还表明，LLM不愿自发地使用性别对语言。]]></description>
      <guid>https://arxiv.org/abs/2502.10577</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>塞尔维亚法律文件的指定实体识别：设计，方法论和数据集开发</title>
      <link>https://arxiv.org/abs/2502.10582</link>
      <description><![CDATA[ARXIV：2502.10582V1公告类型：新 
摘要：自然语言处理领域（NLP），尤其是大型语言模型（LLMS）及其众多应用的最新进展，在文档归档，搜索和检索的过程中，研究的注意力引起了对不同文档处理工具和增强功能的研究的关注。由于每天生成的大量数据以及有兴趣的从业者（律师，律师事务所，行政人员，行政人员，州机构和公民）的重要社区，因此官方法律文件的领域尤其有趣。因此，提供有效的方法来自动化涉及法律文件的日常工作，预计将对不同的领域产生重大影响。在这项工作中，我们为用塞尔维亚语言编写的法律文件提供了一个基于LLM的解决方案（NER）。它利用了来自变形金刚（BERT）的预训练的双向编码器表示，这些表示器已仔细地适应了从文本内容中识别和分类特定数据点的特定任务。除了针对塞尔维亚语言的新型数据集开发（涉及公共法院裁决），提出的系统设计和应用方法论外，本文还讨论了实现的绩效指标及其对拟议解决方案的客观评估的影响。在创建的手动标记数据集上进行了交叉验证测试，平均$ f_1 $得分为0.96，并在有意修改的文本输入的示例上进行了其他结果，确认了已开发的NER解决方案的拟议系统设计和鲁棒性的适用性。]]></description>
      <guid>https://arxiv.org/abs/2502.10582</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>培训后的LLM抹布？训练自我生成的示威</title>
      <link>https://arxiv.org/abs/2502.10596</link>
      <description><![CDATA[ARXIV：2502.10596V1公告类型：新 
摘要：大型语言模型（LLM）经常在知识密集的NLP任务中挣扎，例如回答“谁赢得了最新的世界杯？”因为他们在培训期间学到的知识可能不足或过时。在检索文档（一种称为检索增强发电（RAG）的技术）上生成的调节生成，通过允许该模型利用文字信息中的信息来减轻这些缺点。从业人员可以通过对检索提示的说明进行微调来改善LLM抹布性能，但必须提防这可能会导致诸如幻觉之类的不良模型行为。我们将这种退化归因于以下事实：训练数据可能是该模型的分布情况，并且可能遭受质量问题的困扰，例如检索和目标响应之间的差异（由于经常在事后添加回收）。我们建议使用自我生成的演示进行培训培训抹布的LLM的配方，从而避免在分发文本上进行培训并将检索整合到LLM响应中。我们评估我们的方法是关于知识密集的问题回答（QA）任务的方法，并表明我们的方法教授LLM可以正确处理中文检索，并避免问题可能会误会。与常规的RA-IT方法相比，我们的方法可以防止非剥离设置中的模型降解，同时表现出卓越的质量检查性能。]]></description>
      <guid>https://arxiv.org/abs/2502.10596</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检索授权的编码器，用于极端多标签文本分类</title>
      <link>https://arxiv.org/abs/2502.10615</link>
      <description><![CDATA[ARXIV：2502.10615V1公告类型：新 
摘要：极端多标签分类（XMC）试图从给定文本输入的非常大的标签集合中找到相关标签。为了解决如此庞大的标签空间，当前的最新方法分为两类。单个ALL（OVA）方法使用每个标签的可学习标签嵌入式，在记忆方面表现出色（即，捕获详细的培训信号以进行准确的头部标签预测）。相比之下，双编码器（DE）模型映射输入和标记文本成一个共享的嵌入空间，以更好地泛化（即，预测具有有限培训数据的尾标标签的能力），但在记忆时可能会缺乏。为了实现概括和记忆，现有的XMC方法通常将DE和OVA模型结合在一起，涉及复杂的训练管道。受到检索功能模型的成功的启发，我们提出了XMC（RAEXMC）的检索式编码器，这是一个新型框架，该框架为DE模型提供了带有检索功能的有效记忆的能力，以实现有效的记忆，而无需其他可训练的参数。在训练过程中，RAEXMC通过由输入实例和标签组成的知识记忆的对比损失进行了优化。在推断期间，给定测试输入，RAEXMC从知识内存中检索顶部$ K $键，并将相应值汇总为预测分数。我们展示了RAEXMC对四个公共LF-XMC基准的有效性和效率。 RAEXMC不仅可以推进最先进的方法DEXML（SOTA）DEXML，而且在同一8 A100 GPUS培训环境下，最大的LF-Amazontitles-1.3M数据集上的最大LF-Amazontitles-1.3M数据集实现了超过10倍的速度。]]></description>
      <guid>https://arxiv.org/abs/2502.10615</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代码混合泰卢固语 - 英语仇恨言论检测</title>
      <link>https://arxiv.org/abs/2502.10632</link>
      <description><![CDATA[ARXIV：2502.10632V1公告类型：新 
摘要：泰卢固语（Telugu）等低资源语言中的仇恨言论检测是NLP的日益增长的挑战。这项研究调查了基于变形金刚的模型，包括泰卢律哈特伯特，Hatebert，Deberta，Muril，Indiabert，Roberta和Hindi Abusive-Muril，用于在泰卢固语中对仇恨言论进行分类。我们使用低级适应（LORA）微调这些模型，以优化效率和性能。此外，我们通过使用Google翻译将泰卢固语文本翻译成英文来评估其对分类准确性的影响，从而探索了一种多语言方法。
  我们的实验表明，与直接在泰卢固语文本上的培训相比，Deberta和Hindi-Abusive-Muril在翻译后的性能提高了，Deberta和Hindi-Abusive-Muril获得了更高的准确性和F1分数。值得注意的是，印地语滥用 - 穆里尔在原始泰卢固语数据集和翻译的数据集中均优于所有其他模型，这在不同的语言设置上展示了其稳健性。这表明翻译使模型能够利用英语可用的更丰富的语言功能，从而改善了分类性能。结果表明，多语言处理可以是低资源语言中仇恨语音检测的有效方法。这些发现表明，当适当的微调时，变压器模型可以显着改善泰卢固语中的仇恨言论检测，为更强大的多语言NLP应用铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2502.10632</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在段落中丢失：段落级别的内在学习学习不一定需要“段落”</title>
      <link>https://arxiv.org/abs/2502.10634</link>
      <description><![CDATA[ARXIV：2502.10634V1公告类型：新 
摘要：通过简单地将示范纳入上下文中，在上下文学习（ICL）使大型语言模型（LLMS）可以在许多任务上产生出色的性能。在本文中，我们专注于用于生成任务的段落级别的长篇小说ICL，并发现LLM无法学习演示段与生成输出之间的内在关系。我们对两种典型的一代任务进行了不同的LLM进行实验，包括单案QA和干扰物生成，表明即使是完全毫无意义的演示段，其长度为1/4，也比原始的完整段落更好地实现了性能。通过注意分数的分析表明，与迅速的其他组件相比，LLM很少关注通道，而从段落到其他部分的其他部分则很少流向演示的其他部分，这进一步证实了我们的发现。此外，在上下文压缩上进行实验表明，在其他长篇小说任务中被证明有效的压缩方法不适合通过ICL，因为仅使用较短的毫无意义的演示段落就可以实现竞争性能。]]></description>
      <guid>https://arxiv.org/abs/2502.10634</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>寻求公平访问：利用众包评论来调查公众对健康资源可及性的看法</title>
      <link>https://arxiv.org/abs/2502.10641</link>
      <description><![CDATA[ARXIV：2502.10641V1公告类型：新 
摘要：获得卫生资源是公共福祉和社会韧性的关键决定因素，尤其是在对医疗服务和预防保健潮流的需求时，在公共卫生危机时期。但是，可访问性的差异持续存在于人口统计和地理群体之间，从而引起了人们对公平的担忧。由于覆盖范围，成本和及时性的限制，传统的调查方法通常缺乏。这项研究利用Google Maps评论中的众包数据，应用了先进的自然语言处理技术，尤其是ModernBert，以提取有关Covid-19期间美国对美国健康资源可及性的公众看法的见解。此外，我们采用部分最小二乘回归来研究可及性观念与关键的社会经济和人口统计学因素之间的关系，包括政治隶属关系，种族组成和教育成就。我们的发现表明，在美国，公众对卫生资源可及性的看法差异很大，大流行期间危机和危机后略有缓解的差异达到了峰值。政治隶属关系，种族人口统计和教育水平是塑造这些看法的关键因素。这些发现强调了针对有针对性的干预措施和政策措施解决不平等现象的必要性，从而促进了更具包容性的医疗保健基础设施，从而可以更好地承受未来的公共卫生挑战。]]></description>
      <guid>https://arxiv.org/abs/2502.10641</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Babylm Turn 3：致电2025 Babylm研讨会的论文</title>
      <link>https://arxiv.org/abs/2502.10645</link>
      <description><![CDATA[ARXIV：2502.10645V1公告类型：新 
摘要：BabyLM旨在解散认知建模和语言建模之间的界限。我们呼吁进行研讨会论文，并要求研究人员参加第三届Babylm比赛。与往年一样，我们呼吁参与者在一般轨道中进行数据效率高的预处理挑战。今年，我们还提供了新的曲目：互动。这条新曲目鼓励互动行为，向老师学习，并向学生调整教材。我们还呼吁在任何相关领域的竞争之外提出论文。这些包括培训效率，认知合理的研究，弱模型评估等等。]]></description>
      <guid>https://arxiv.org/abs/2502.10645</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有大语言模型的用户配置文件：构建，更新和基准测试</title>
      <link>https://arxiv.org/abs/2502.10660</link>
      <description><![CDATA[ARXIV：2502.10660V1公告类型：新 
摘要：用户配置文件建模在个性化系统中起关键作用，因为它需要构建准确的配置文件并使用新信息更新它们。在本文中，我们介绍了两个高质量的开源用户配置文件数据集：一个用于个人资料构建，另一个用于个人资料更新。这些数据集为在动态设置中评估用户配置文件建模技术提供了强大的基础。我们还展示了一种使用大型语言模型（LLM）来解决个人资料构建和更新的方法。我们的方法使用概率框架来预测输入文本的用户配置文件，从而可以生成精确和上下文感知的配置文件。我们的实验表明，像Mistral-7b和Llama2-7b这样的模型在这两个任务中都表现出色。 LLM提高了生成的概况的精度和回忆，高评估得分证实了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.10660</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大语言模型中探索突触共鸣：一种新的方法，用于上下文记忆集成</title>
      <link>https://arxiv.org/abs/2502.10699</link>
      <description><![CDATA[ARXIV：2502.10699V1公告类型：新 
摘要：上下文记忆集成在语言模型的开发中仍然是一个巨大的挑战，尤其是在需要维持扩展序列连贯性的任务中。传统方法（例如自我发挥机制和记忆启发架构）通常优先考虑短期依赖性，从而导致远距离上下文理解中的分裂和不一致。受到生物神经系统中观察到的突触可塑性原理的启发，一种新型机制，突触共振，被引入在训练和推理过程中动态增强相关的记忆途径。与静态存储器表示不同，该机制根据上下文相关性不断调整突触权重矩阵，从而可以改善信息保留而没有过多的计算开销。在开源语言模型上进行的评估表明，在上下文连贯性中的增强和对输入噪声的鲁棒性增加的降低，突出了增强驱动的内存调制的有效性。针对基线模型的比较分析进一步表明，所提出的方法在保持计算可行性的同时，可实现更高的存储率效率。体系结构修改将无缝集成到现有的基于变压器的框架中，从而在不牺牲可扩展性的情况下确保稳定的收敛性和有效的推断。从改善的长期背景一致性（例如对话系统和文档摘要）中受益的应用程序将从这种方法中获利。经验发现表明，动态增强的内存途径为传统记忆机制提供了一种有希望的替代方案，可以解决扩展序列建模中的长期局限性。]]></description>
      <guid>https://arxiv.org/abs/2502.10699</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将特定领域的知识注入大语言模型：一项全面调查</title>
      <link>https://arxiv.org/abs/2502.10708</link>
      <description><![CDATA[ARXIV：2502.10708V1公告类型：新 
摘要：大型语言模型（LLMS）在各种任务中表现出了巨大的成功，例如自然语言理解，文本摘要和机器翻译。但是，它们的通用性质通常会限制其在需要专门知识的特定领域应用中的有效性，例如医疗保健，化学或法律分析。为了解决这个问题，研究人员探索了通过集成特定领域知识来增强LLM的各种方法。在这项调查中，我们提供了这些方法的全面概述，我们将其归类为四种关键方法：动态知识注入，静态知识嵌入，模块化适配器和及时的优化。每种方法都提供了独特的机制，可以为LLM提供域专业知识，平衡灵活性，可扩展性和效率之间的权衡。我们讨论了这些方法如何使LLM能够解决专业任务，比较其优势和缺点，评估针对一般LLM的领域特定LLM，并突出此新兴领域的挑战和机遇。对于那些有兴趣深入研究这一领域的人，我们还总结了常用的数据集和基准。为了使研究人员在最新研究中进行最新信息，我们在以下网址保持开放源代码：https：//github.com/abilliyb/knowledge_indoction_survey_papers，致力于记录专业LLM领域的研究。]]></description>
      <guid>https://arxiv.org/abs/2502.10708</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型评估中不确定性的经验分析</title>
      <link>https://arxiv.org/abs/2502.10709</link>
      <description><![CDATA[ARXIV：2502.10709V1公告类型：新 
摘要：随着LLM-AS-A-Gudge作为评估大语言模型（LLM）的新范式的出现，人们对LLM评估者的一致性，偏见和稳定性提出了担忧。尽管大量工作集中在对齐和偏见上，但很少研究集中在LLM评估者的稳定性上。在本文中，我们进行了广泛的实验，其中涉及2种不同评估设置中的9个广泛使用的LLM评估者，以研究基于模型的LLM评估的不确定性。我们指出，LLM评估者根据模型家族和大小表现出不同的不确定性。通过仔细的比较分析，我们发现采用特殊的提示策略（无论是在推论还是在培训后）可以在某种程度上减轻评估不确定性。通过利用不确定性来增强LLM在分布（OOD）数据中的可靠性和检测能力，我们进一步调整了不确定性感知的LLM评估者Condilm，并使用人类注销的微调集并评估Confilm的OOD评估能力手动设计的测试集来自2024年奥运会。实验结果表明，将不确定性纳入微调阶段的其他信息可以在很大程度上改善模型在OOD方案中的评估性能。代码和数据以：https：//github.com/hasakixie123/llm--evaluator-uncneyty发布。]]></description>
      <guid>https://arxiv.org/abs/2502.10709</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Propnet：一个白色框和类人的网络，用于句子表示</title>
      <link>https://arxiv.org/abs/2502.10725</link>
      <description><![CDATA[ARXIV：2502.10725V1公告类型：新 
摘要：近年来，基于变压器的嵌入方法统治了句子表示领域。尽管他们在NLP任务上取得了出色的表现，例如语义文本相似性（STS）任务，其黑手盒的性质和大型DATA驱动的培训风格引起了人们的关注，包括与偏见，信任和安全有关的问题。已经做出了许多努力来提高嵌入模型的解释性，但是这些问题尚未从根本上解决。为了实现固有的可解释性，我们提出了一个纯粹的白色框和类似人类的句子表示网络Propnet。受认知科学的发现的启发，Propnet基于句子中包含的命题构建了一个层次网络。虽然实验表明，与STS任务中的最先进的嵌入模型相比，PropNet具有显着的差距，但案例研究显示了很大的改进空间。此外，Propnet使我们能够分析和理解STS基准的人类认知过程。]]></description>
      <guid>https://arxiv.org/abs/2502.10725</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OPTISHEAR：通过进化优化对大语言模型进行高效和适应性修剪</title>
      <link>https://arxiv.org/abs/2502.10735</link>
      <description><![CDATA[ARXIV：2502.10735V1公告类型：新 
摘要：随着大型语言模型（LLM）的不断增长，培训后修剪已成为一种关键的优化技术。但是，不同LLM的重量分布的显着变化使得固定的修剪策略不足以适用于多种模型。在本文中，我们介绍了\ textbf {\ textsc {optishear}}，这是一个有效的自适应llm修剪的进化优化框架。我们的框架具有两个关键的创新：一个有效的搜索空间，建立在我们的元修剪度量标准上，以处理各种重量分布，以及在搜索试验期间快速评估的模型重建错误。我们采用非主导的排序遗传算法III（NSGA-III）来优化修剪指标和层次稀疏比。通过对多个基准的Llama-1/2/3和Mistral模型（7b-70b）的广泛评估，我们证明我们的自适应修剪指标始终超过现有方法。此外，我们发现的图层稀疏比增强了其他修剪指标的有效性。该框架表现出强大的交叉任务和交叉模型的概括性，为模型压缩提供了一种成本效益的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2502.10735</guid>
      <pubDate>Tue, 18 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>