<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 13 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>智能法律助理：法律问答互动澄清系统</title>
      <link>https://arxiv.org/abs/2502.07904</link>
      <description><![CDATA[arXiv:2502.07904v1 公告类型：新
摘要：大型语言模型的兴起为用户寻求法律建议开辟了新的途径。然而，用户往往缺乏专业的法律知识，这会导致问题遗漏关键信息。这一缺陷使得传统的法律问答系统难以准确识别用户的实际需求，往往导致不精确或泛泛的建议。在这项工作中，我们开发了一个名为智能法律助理的法律问答系统，它可以与用户交互以准确捕捉他们的需求。当用户提出问题时，系统要求用户选择他们的地理位置以确定适用的法律。然后，它会根据用户初始问题中缺少的关键信息生成澄清问题和选项。这允许用户选择并提供必要的细节。一旦提供了所有必要的信息，系统就会产生深入的法律分析，涵盖三个方面：总体结论、法理分析和解决建议。]]></description>
      <guid>https://arxiv.org/abs/2502.07904</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提升法律法学硕士的回应：利用可训练的逻辑结构和语义知识进行法律推理</title>
      <link>https://arxiv.org/abs/2502.07912</link>
      <description><![CDATA[arXiv:2502.07912v1 公告类型：新
摘要：大型语言模型 (LLM) 在许多领域取得了令人瞩目的成果，但它们在法律问答任务中存在明显缺陷。LLM 通常会生成一般性反应，缺乏专家法律建议所需的逻辑特异性，并且容易产生幻觉，提供看似正确但不可靠的答案。检索增强生成 (RAG) 技术提供了解决这一挑战的部分解决方案，但现有方法通常仅关注语义相似性，而忽略了法律推理所必需的逻辑结构。在本文中，我们提出了逻辑语义集成模型 (LSIM)，这是一种连接语义和逻辑一致性的新型监督框架。 LSIM 由三个部分组成：强化学习预测每个问题的结构化事实规则链，可训练的深度结构化语义模型 (DSSM) 通过整合语义和逻辑特征检索最相关的候选问题，上下文学习使用检索到的内容生成最终答案。我们在现实世界的法律问答数据集上进行的实验（通过自动指标和人工评估进行验证）表明，与现有方法相比，LSIM 显著提高了准确性和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2502.07912</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将多语言嵌入模型适应历史卢森堡语</title>
      <link>https://arxiv.org/abs/2502.07938</link>
      <description><![CDATA[arXiv:2502.07938v1 公告类型：新
摘要：数字化历史文本的数量不断增长，需要使用文本嵌入进行有效的语义搜索。然而，预先训练的多语言模型通常在当代文本上进行评估，由于 OCR 噪声和过时的拼写，在历史数字化内容方面面临挑战。我们探索使用多语言嵌入对历史卢森堡语（一种资源匮乏的语言）进行跨语言语义搜索。我们收集了跨越不同时期的历史卢森堡语新闻文章，并使用 GPT-4o 将它们分割并翻译成密切相关的语言，为每个语言对创建 20,000 个并行训练句子。我们进一步创建了一个历史双语文本挖掘评估集，发现这些模型很难对历史卢森堡语进行跨语言搜索。为了解决这个问题，我们提出了一种使用领域内训练数据的简单自适应方法，在跨语言评估中实现了高达 98% 的准确率。我们发布了经过改编的模型和历史卢森堡-德语/法语双语文本，以支持进一步的研究。]]></description>
      <guid>https://arxiv.org/abs/2502.07938</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>陷入文字之网：法学硕士是否会陷入医学文献的欺骗之中？</title>
      <link>https://arxiv.org/abs/2502.07963</link>
      <description><![CDATA[arXiv:2502.07963v1 公告类型：新
摘要：医学研究在将新疗法转化为临床实践方面面临着众所周知的挑战。出版激励措施鼓励研究人员提出“积极”的发现，即使经验结果模棱两可。因此，有充分的证据表明，作者经常歪曲研究结果，尤其是在文章摘要中。这种歪曲会影响临床医生对证据的解释，并可能影响患者的护理决策。在这项研究中，我们询问大型语言模型 (LLM) 提供的试验结果解释是否同样受到歪曲的影响。这很重要，因为 LLM 越来越多地被用于搜索和综合已发表的医学证据。我们评估了 22 个 LLM，发现它们比人类更容易受到歪曲的影响。它们还可能将歪曲传播到它们的输出中：我们发现证据表明，例如，LLM 隐式地将歪曲纳入它们生成的通俗语言摘要中。然而，我们还发现，LLM 通常能够识别自旋，并且可以以某种方式提示以减轻自旋对 LLM 输出的影响。]]></description>
      <guid>https://arxiv.org/abs/2502.07963</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>训练稀疏混合专家文本嵌入模型</title>
      <link>https://arxiv.org/abs/2502.07972</link>
      <description><![CDATA[arXiv:2502.07972v1 公告类型：新 
摘要：基于 Transformer 的文本嵌入模型通过增加参数数量提高了其在 MIRACL 和 BEIR 等基准测试中的表现。然而，这种扩展方法带来了重大的部署挑战，包括增加推理延迟和内存使用量。这些挑战在检索增强生成 (RAG) 应用程序中尤其严重，其中大型模型增加的内存需求限制了数据集的提取能力，并且它们的更高延迟直接影响查询时间性能。虽然因果语言模型使用混合专家 (MoE) 架构解决了类似的效率挑战，但这种方法尚未成功适应一般的文本嵌入设置。在本文中，我们介绍了 Nomic Embed v2，这是第一个通用的 MoE 文本嵌入模型。我们的模型在单语和多语言基准测试中都优于同一参数类别的模型，同时还保持了与其两倍大小的模型的竞争性能。我们开源所有代码、模型和评估数据，以确保我们的训练流程的完全可重复性。]]></description>
      <guid>https://arxiv.org/abs/2502.07972</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MetaSC：语言模型的测试时间安全规范优化</title>
      <link>https://arxiv.org/abs/2502.07985</link>
      <description><![CDATA[arXiv:2502.07985v1 公告类型：新 
摘要：我们提出了一种新颖的动态安全框架，该框架可在推理时优化语言模型 (LM) 安全推理，而无需修改模型权重。基于自我批评方法的最新进展，我们的方法利用元批评机制，该机制迭代更新安全提示（称为规范），以自适应地推动批评和修订过程。这种测试时间优化不仅可以提高对抗性越狱请求的性能，还可以提高各种一般安全相关任务的性能，例如避免道德伤害或追求诚实的回应。我们对几种语言模型的实证评估表明，与固定系统提示和静态自我批评防御相比，动态优化的安全提示可产生更高的安全分数。代码将在 https://github.com/vicgalle/meta-self-critique.git 发布。]]></description>
      <guid>https://arxiv.org/abs/2502.07985</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提示的几何：揭示语言模型中任务适应的不同机制</title>
      <link>https://arxiv.org/abs/2502.08009</link>
      <description><![CDATA[arXiv:2502.08009v1 公告类型：新
摘要：仅解码器语言模型能够根据输入提示在各种计算任务之间动态切换。尽管提示有许多成功的应用，但人们对这种灵活性背后的内部机制的理解非常有限。在这项工作中，我们研究了不同的提示方法如何影响这些模型中表示的几何形状。采用基于统计物理学的框架，我们发现各种提示技术虽然实现了相似的性能，但通过不同的表示机制进行任务适应。我们的分析强调了输入分布样本和标签语义在少数场景上下文学习中的关键作用。我们还证明了在表示层面上不同任务之间存在协同和干扰相互作用的证据。我们的工作有助于从理论角度理解大型语言模型，并为开发更有效的、表示感知的提示策略奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2502.08009</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推测，然后合作：在解码过程中融合语言模型知识</title>
      <link>https://arxiv.org/abs/2502.08020</link>
      <description><![CDATA[arXiv:2502.08020v1 公告类型：新
摘要：大型语言模型 (LLM) 通常在特定领域表现出色，但由于训练的局限性，在其他领域表现不佳。因此，通过整合互补知识使 LLM 能够协作解决问题有望提高其跨领域的性能。为了实现这一潜力，我们引入了一种新颖的协作推测解码 (CoSD) 算法，该算法可以在测试时实现高效的 LLM 知识融合，而无需额外的模型训练。CoSD 使用草稿模型来生成初始序列，并使用易于学习的规则或决策树来决定何时调用辅助模型来改进这些草稿。CoSD 不仅增强了知识融合，还提高了推理效率，可跨领域和模型转移，并提供更高的可解释性。实验结果表明，与现有方法相比，CoSD 在基准测试中将准确率提高了 10\%，为基于 LLM 的应用程序提供了可扩展且有效的解决方案]]></description>
      <guid>https://arxiv.org/abs/2502.08020</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于大型语言模型表征结构细化的上下文子空间流形投影</title>
      <link>https://arxiv.org/abs/2502.08026</link>
      <description><![CDATA[arXiv:2502.08026v1 公告类型：新
摘要：深度神经架构内的内部表示对语言结构的高维抽象进行编码，但它们通常在特征分布方面表现出低效率，从而限制了表达力和适应性。上下文子空间流形投影引入了一种结构化细化技术，该技术通过受控的子空间约束有选择地重新配置标记嵌入，从而确保更稳定且几何定义良好的特征分布。实证评估表明，结构化干预降低了各向异性，从而提高了表示紧凑性，同时保持了跨变压器层的语义保真度。聚类分析表明，标记嵌入表现出更大的特征可分离性，这强化了结构化投影技术在不牺牲语言连贯性的情况下增强内部表示组织的假设。梯度幅度分布表明该方法引入了更平滑的优化轨迹，可能有助于在整个训练过程中实现更稳定的参数更新。与投影操作相关的计算开销保持最小，确保改进不会对模型效率或推理速度造成重大影响。与标准嵌入改进技术的比较表明，结构化流形约束提供了一种直接的机制来提高表示质量，而无需额外的基于梯度的优化。困惑度评估证实，调整不会对序列一致性产生负面影响，进一步验证了所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.08026</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Franken-Adapter：通过嵌入手术实现法学硕士的跨语言适应</title>
      <link>https://arxiv.org/abs/2502.08037</link>
      <description><![CDATA[arXiv:2502.08037v1 公告类型：新
摘要：低资源语言中的大型语言模型 (LLM) 的能力远远落后于英语，这使得它们的通用可访问性成为一项重大挑战。为了缓解这一问题，我们提出了 $\textit{Franken-Adapter}$，这是一种针对仅解码器的 LLM 的模块化语言自适应方法，具有嵌入手术。我们的方法首先为目标语言创建定制词汇表，并通过对多语言数据的嵌入调整来执行语言自适应。这些预训练的嵌入随后与已在英语对齐数据上进行指令调整的 LLM 集成，以实现零样本跨语言传输。我们对具有多达 27B 个参数的 $\texttt{Gemma2}$ 模型的实验表明，在 96 种语言中，涵盖判别任务和生成任务的改进高达 20%，英语的回归最小（&lt;$1%）。进一步的深入分析揭示了自定义标记器在增强语言适应性以及提高推理效率方面的关键作用。此外，我们展示了我们方法的多功能性，在 20 种语言中实现了比数学优化的 LLM 14% 的改进，提供了一种模块化解决方案，可以在事后跨语言迁移推理能力。]]></description>
      <guid>https://arxiv.org/abs/2502.08037</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>打破复选框：挑战法学硕士中文化契合度的封闭式评估</title>
      <link>https://arxiv.org/abs/2502.08045</link>
      <description><![CDATA[arXiv:2502.08045v1 公告类型：新
摘要：大量研究依靠封闭式多项选择调查来评估大型语言模型 (LLM) 中的文化一致性。在这项工作中，我们挑战了这种受约束的评估范式，并探索了更现实、不受约束的方法。以世界价值观调查 (WVS) 和霍夫斯泰德文化维度为案例研究，我们证明 LLM 在较少约束的环境中表现出更强的文化一致性，其中响应不是强制的。此外，我们表明，即使是微小的变化，例如重新排序调查选项，也会导致输出不一致，从而暴露了封​​闭式评估的局限性。我们的研究结果提倡更强大、更灵活的评估框架，重点关注特定的文化代理，鼓励对 LLM 中的文化一致性进行更细致入微和准确的评估。]]></description>
      <guid>https://arxiv.org/abs/2502.08045</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论提取式问答的机械电路</title>
      <link>https://arxiv.org/abs/2502.08059</link>
      <description><![CDATA[arXiv:2502.08059v1 公告类型：新
摘要：大型语言模型越来越多地用于处理文档并促进对文档的问答。在我们的论文中，我们为这个现实世界的语言建模任务提取了机械电路：用于提取问答 (QA) 任务的上下文增强语言建模，并了解电路对下游应用（例如数据归因于上下文信息）的潜在好处。我们使用因果中介分析技术提取电路作为内部模型组件（例如，注意力头、MLP）的函数。利用提取的电路，我们首先了解模型对参数记忆的使用与检索到的上下文之间的相互作用，从而更好地机械地理解上下文增强语言模型。然后，我们在电路中识别出一小组注意力头，它们默认执行可靠的数据归因，从而仅在模型的前向传递中免费获得归因。利用这一见解，我们随后引入了 ATTNATTRIB，这是一种快速数据归因算法，可在各种提取式 QA 基准中获得最先进的归因结果。最后，我们展示了通过在前向传递过程中使用来自 ATTNATTRIB 的归因作为附加信号，引导语言模型从上下文而不是参数记忆中进行回答的可能性。除了机械理解之外，我们的论文还以可靠的数据归因和模型转向的形式提供了电路的切实应用。]]></description>
      <guid>https://arxiv.org/abs/2502.08059</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微观 NLI：原子假设分解揭示了什么</title>
      <link>https://arxiv.org/abs/2502.08080</link>
      <description><![CDATA[arXiv:2502.08080v1 公告类型：新
摘要：将文本分解为原子命题是一个灵活的框架，允许更仔细地检查输入和输出文本。我们在两个自然语言推理任务（传统 NLI 和可废止 NLI）中使用假设的原子分解来形成原子子问题，或模型在解决整体问题时必须权衡的细粒度推理。这些原子子问题可作为进一步理解 NLI 和可废止推理的结构、探究模型的一致性和对不同推理的理解以及衡量基准数据集中示例多样性的工具。我们的结果表明，LLM 仍然在原子 NLI 和可废止 NLI 子问题的逻辑一致性方面遇到困难。最后，我们确定可废止 NLI 示例的关键原子子问题，或对整体标签贡献最大的子问题，并提出一种测量模型推理一致性的方法，该度量旨在捕捉模型在不同背景下对同一事实做出一致正确或错误预测的程度。]]></description>
      <guid>https://arxiv.org/abs/2502.08080</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GCoT：图的思路链提示学习</title>
      <link>https://arxiv.org/abs/2502.08092</link>
      <description><![CDATA[arXiv:2502.08092v1 公告类型：新
摘要：思路链 (CoT) 提示在自然语言处理 (NLP) 中取得了显著成功。然而，它在图形方面的巨大潜力仍未得到充分开发。这就提出了一个有趣的问题：我们如何设计图形的 CoT 提示来指导图形模型逐步学习？一方面，与自然语言不同，图是非线性的，具有复杂的拓扑结构。另一方面，许多图缺乏文本数据，因此很难制定基于语言的 CoT 提示。在这项工作中，我们提出了第一个用于无文本图的 CoT 提示学习框架 GCoT。具体来说，我们将每个下游任务的适应过程分解为一系列推理步骤，每个步骤包括基于提示的推理、“思想”生成和思想条件提示学习。虽然这些步骤模仿了 NLP 中的 CoT 提示，但确切的机制却大不相同。具体来说，在每个步骤中，首先将输入图连同提示一起输入到预先训练的图编码器中进行基于提示的推理。然后，我们聚合编码器的隐藏层以构建一个“想法”，该想法捕获当前步骤中每个节点的工作状态。在此想法的条件下，我们根据当前状态学习特定于每个节点的提示。这些提示被输入到下一个推理步骤中，重复该循环。为了评估和分析 GCoT 的有效性，我们对八个公共数据集进行了全面的实验，证明了我们方法的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.08092</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HuDEx：整合幻觉检测和可解释性，以提高 LLM 响应的可靠性</title>
      <link>https://arxiv.org/abs/2502.08109</link>
      <description><![CDATA[arXiv:2502.08109v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展显示出令人鼓舞的改进，通常在自然语言处理的各种下游任务中超越现有方法。然而，这些模型仍然面临挑战，这可能会阻碍它们的实际应用。例如，众所周知，幻觉现象会损害 LLM 的可靠性，特别是在对事实精度要求高的领域。当前的基准主要侧重于幻觉检测和事实性评估，但并未超出识别范围。本文提出了一种解释增强的幻觉检测模型，称为 HuDEx，旨在通过检测幻觉和提供详细解释来提高 LLM 生成的响应的可靠性。所提出的模型提供了一种将检测与解释相结合的新方法，并使用户和 LLM 本身都能理解和减少错误。我们的测量结果表明，所提出的模型在幻觉检测准确率方面超越了更大的 LLM，例如 Llama3 70B 和 GPT-4，同时保持了可靠的解释。此外，所提出的模型在零样本和其他测试环境中都表现良好，展示了其在各种基准数据集中的适应性。所提出的方法通过引入一种将可解释性与幻觉检测相结合的新方法，进一步增强了幻觉检测研究，从而进一步提高了评估语言模型中幻觉的性能和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2502.08109</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>