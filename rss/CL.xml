<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>多跳密度检索的动量后正规化</title>
      <link>https://arxiv.org/abs/2502.20399</link>
      <description><![CDATA[ARXIV：2502.20399V1公告类型：新 
摘要：多跳问题回答（QA）通常需要顺序检索（多跳检索），其中每个跳跃都会根据以前的啤酒花的信息来检索缺失的知识。为了促进更有效的检索，我们旨在将知识从后验检索中提取，该检索可以访问后验信息，例如答案，并在无法获得此类信息的情况下，在推理期间使用的先前检索。不幸的是，由于两个问题，当前的一次性检索中知识蒸馏的方法对于多跳量质量质量质量检查是无效的：1）后验信息通常定义为响应（即答案），而没有中等检索的情况下可能无法清楚地连接到查询； 2）先验和后检索之间的较大知识差距使现有的蒸馏方法不稳定，甚至导致绩效损失。因此，我们提出了MOPO（动量后正规化），并具有两个关键的创新：1）一个HOP的后验信息定义为从上一个和当前啤酒花的黄金知识中的查询对焦摘要； 2）我们制定了一种有效的训练策略，在该策略中，通过动量移动平均方法对后验检索以及先前的检索进行更新，从而使其更加顺畅，有效蒸馏。 HOTPOTQA和StrategyQA的实验表明，MOPO在检索和下游QA任务中的表现都优于现有基准。]]></description>
      <guid>https://arxiv.org/abs/2502.20399</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>长篇小说理解的暂停调整：LLM注意重新校准的轻量级方法</title>
      <link>https://arxiv.org/abs/2502.20405</link>
      <description><![CDATA[ARXIV：2502.20405V1公告类型：新 
摘要：LLM在理解任务方面表现出了非常熟练的水平，但继续在长期以来的理解中挣扎，尤其是在广泛投入的内容中。这种限制被称为中间（LITM）问题，它阻碍了模型在漫长的环境中完全处理和利用信息。为了解决这个问题，我们引入了暂停调整，该技术将注意力重新分布以增强对长篇文本输入的理解。我们的方法涉及具有人工插入的暂停令牌的数据集上的微调语言模型，这些模型将输入分为较小，更易于管理的部分。我们使用针中的基准测试对替代方法评估暂停调节，其中模型必须检索嵌入在多达128K代币的上下文中的信息。实验结果表明，Llama 3.2 3B指导模型和Llama 3.1 8B指导模型平均提高了10.61％和3.57％，这表明暂停调查成功增强了注意力再分配并改善了长期文本保留。代码和数据可在https://anonymon.4open.science/r/litm-pausetokens-7357上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20405</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Chitranuvad：调整多语言LLM用于多模式翻译</title>
      <link>https://arxiv.org/abs/2502.20420</link>
      <description><![CDATA[ARXIV：2502.20420V1公告类型：新 
摘要：在这项工作中，我们在亚洲翻译研讨会（WAT2024）的研讨会上提供了作为英语的一部分提交的系统描述。我们介绍了Chitranuvad，这是一种多模型模型，可有效整合多语言LLM和一个视觉模块以进行多模式翻译。我们的方法使用VIT图像编码器将视觉表示形式提取为视觉令牌嵌入，通过适配器层将其投影到LLM空间并以自动回归方式生成翻译。我们参与了指示语言（即英文翻译到印地语，孟加拉语和马利亚拉姆）的所有三个曲目（图像字幕，仅文本和多模式翻译任务），并在挑战集中在挑战集中获得了印地语的SOTA结果，同时在共享任务中对其他语言保持竞争力。]]></description>
      <guid>https://arxiv.org/abs/2502.20420</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SEKI：通过大语言模型的基于自我进化和知识灵感的神经架构搜索</title>
      <link>https://arxiv.org/abs/2502.20422</link>
      <description><![CDATA[ARXIV：2502.20422V1公告类型：新 
摘要：我们介绍了SEKI，这是一种新型的大型语言模型（LLM）的神经结构搜索（NAS）方法。受到现代LLMS中的经过思考链（COT）范式的启发，Seki在两个关键阶段运作：自我进化和知识蒸馏。在自我进化阶段，LLMS最初缺乏足够的参考示例，因此我们实施了一种迭代精致机制，该机制可以根据性能反馈来增强体系结构。随着时间的流逝，此过程积累了高性能体系结构的存储库。在知识蒸馏阶段，LLMS分析了这些体系结构之间的共同模式，以生成新的优化设计。结合了这两个阶段，SEKI极大地利用了LLM在NAS上的能力，而无需任何特定领域的数据。实验结果表明，SEKI在各个数据集和搜索空间上实现了最先进的（SOTA）性能，同时仅需要0.05 GPU-days，在效率和准确性方面都优于现有方法。此外，SEKI表现出强大的概括能力，在多个任务中实现了SOTA竞争的结果。]]></description>
      <guid>https://arxiv.org/abs/2502.20422</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>其中：一个基于游戏的框架，用于评估LLMS的说服力</title>
      <link>https://arxiv.org/abs/2502.20426</link>
      <description><![CDATA[ARXIV：2502.20426V1公告类型：新 
摘要：大语言模型（LLM）和自主AI代理的扩散引起了人们对它们具有自动说服力和社会影响力的潜力的担忧。尽管现有研究探讨了基于LLM的操纵的孤立实例，但对不同模型的说服能力的系统评估仍然有限。在本文中，我们介绍了以美国为灵感的游戏框架，用于评估受控环境中的LLM欺骗技能。提出的框架使得可以按游戏统计数据比较LLM模型，并根据社会心理学和修辞学的25种说服策略来量化游戏中的操作。 8种流行类型和大小的流行语言模型之间的实验表明，所有经过测试的模型都表现出说服力，成功地采用了25种预期技术中的22种。我们还发现，较大的模型不能比较小的模型提供任何说服力的优势，并且更长的模型输出与赢得的游戏数量负相关。我们的研究提供了对LLMS欺骗能力的见解，以及用于促进该主题的未来研究的工具和数据。]]></description>
      <guid>https://arxiv.org/abs/2502.20426</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>零阴影：区分不可思议的性能</title>
      <link>https://arxiv.org/abs/2502.20469</link>
      <description><![CDATA[ARXIV：2502.20469V1公告类型：新 
摘要：有些事情是不可能的，但是有些事情可能比不可能更不可能。在我们的世界中，使用一个人的思想悬浮羽毛是不可能的，但是适合我们可能的世界的直觉理论，而在任何可能的世界中都无法想象使用第五的羽毛（“不可思议”）。尽管先前的工作已经检查了不可能的事件和不可能的事件之间的区别，但几乎没有关于不可思议性的实证研究。在这里，我们调查人们是否保持不可能和不可思议性之间的区别，以及如何做出这种区别。我们发现，使用类似于研究不可能和不可能之间的差异的分类研究，人们可以很容易地将不可能与不可想象的研究区分开来（实验1）。但是，这种区别并不能通过人们对事件可能性的主观评分来解释，事件可能性接近零且无法想象的事件描述（实验2）。最后，我们询问统计语言模型（LMS）分配给事件描述的概率是否可用于分开模态类别，以及这些概率是否与人们的评分一致（实验3）。我们发现人和LMS之间的高级相似性：在不可能的事件描述和不可想象的事件描述之间区分，LM衍生的字符串概率可以预测人们对模态类别中事件可能性的评分。我们的发现表明，可以通过对语言形式的统计学习来学习有关极少数事件（即不可能和不可思议）的细粒度知识（即，不可能的和不可思议的），但请留下一个问题，即人们是否代表了不可能的和不可想象的差异，而不是程度的差异，而是类型的差异。]]></description>
      <guid>https://arxiv.org/abs/2502.20469</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>促进，压制，迭代：语言模型如何回答一对多的事实查询</title>
      <link>https://arxiv.org/abs/2502.20475</link>
      <description><![CDATA[ARXIV：2502.20475V1公告类型：新 
摘要：要回答一对一的事实查询（例如，一个国家的列表城市），语言模型（LM）必须同时回忆知识并避免重复以前的答案。这两个子任务如何在内部实施和集成？在多个数据集和模型中，我们确定了一种促进式的抑制机制：该模型首先回忆所有答案，然后抑制先前生成的答案。具体而言，LMS使用主题和先前的答案令牌来执行知识回忆，并通过关注主题信息和MLP来促进答案。然后，注意并抑制先前的答案令牌，而MLP会放大抑制信号。 Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both \emph{Token Lens}, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens.总体而言，我们提供了有关LMS内部组件如何与不同输入令牌相互作用以支持复杂事实召回的新见解。 Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.]]></description>
      <guid>https://arxiv.org/abs/2502.20475</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释的临床结果预测AI：对临床医生的看法和偏好的调查</title>
      <link>https://arxiv.org/abs/2502.20478</link>
      <description><![CDATA[ARXIV：2502.20478V1公告类型：新 
摘要：可解释的AI（XAI）技术是必要的，以帮助临床医生理解AI预测并将预测整合到其决策工作流程中。在这项工作中，我们进行了一项调查研究，以了解不同XAI技术的临床医生偏好，以解释基于文本的EHR数据的模型预测。我们在结果预测模型上实施了四种XAI技术（石灰，基于注意力的跨度亮点，示例性患者检索以及LLMS产生的自由文本理由），该模型使用ICU录取注释来预测患者体验内医生死亡率的可能性。使用这些XAI实施，我们设计和进行了一项针对32位实践临床医生的调查研究，收集了他们对四种技术的反馈和偏好。我们将我们的发现综合为一组建议，描述了何时每种XAI技术更合适，它们的潜在局限性以及改进建议。]]></description>
      <guid>https://arxiv.org/abs/2502.20478</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>保护多模式大语模型免受误导性可视化</title>
      <link>https://arxiv.org/abs/2502.20503</link>
      <description><![CDATA[ARXIV：2502.20503V1公告类型：新 
摘要：我们评估了多模式大语言模型对误导可视化的脆弱性 - 图表使用诸如截断或倒轴等技术扭曲了基础数据的图表，导致读者得出不准确的结论，这些结论可能支持误导性或阴谋理论。我们的分析表明，这些扭曲严重损害了多模式的大语言模型，将其提问的准确性降低到随机基线的水平。为了减轻这种脆弱性，我们介绍了六种推理时间方法，以提高MLLM在误导性可视化方面的性能，同时保留其在非误导性方面的准确性。最有效的方法涉及（1）提取基础数据表，以及（2）使用仅文本大型语言模型来回答基于表的问题。这种方法将误导性可视化的性能提高了15.4至19.6个百分点。]]></description>
      <guid>https://arxiv.org/abs/2502.20503</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一千个单词或图像：研究角色方式在多模式LLM中的影响</title>
      <link>https://arxiv.org/abs/2502.20504</link>
      <description><![CDATA[ARXIV：2502.20504V1公告类型：新 
摘要：大型语言模型（LLMS）最近在体现各种角色，增强其作为对话代理和虚拟助手的有效性方面取得了显着进步。因此，LLM在处理和整合多模式信息方面已取得了重大进步。但是，即使可以在文本和图像中表达人类角色，角色的形式影响LLM的实施方案的程度仍然在很大程度上没有探索。在本文中，我们研究了不同模态在多模式LLM中的表现力如何影响。为此，我们创建了一个新颖的模式平行数据集，该数据集的年龄，性别，职业和位置各不相同。这包括四种方式，可以等效地代表角色：仅图像，仅文本，图像和小文本的组合以及印刷图像，在该图像上进行了视觉风格的文本以传达与角色相关的属性。然后，我们创建一个系统的评估框架，其中有60个问题和相应的指标，以评估LLM在其属性和场景中体现每个角色的表现如何。 $ 5 $多模式LLMS的综合实验表明，由详细文本代表的角色显示出更多的语言习惯，而印刷图像通常显示出与角色的一致性。我们的结果表明，LLM经常忽略通过图像传达的特定于人格特定的细节，突出了基本的局限性，并为将来的研究铺平了这一差距。我们在https://github.com/claws-lab/persona-modality上发布数据和代码。]]></description>
      <guid>https://arxiv.org/abs/2502.20504</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tripcraft：时空上粒细菌的旅行计划的基准</title>
      <link>https://arxiv.org/abs/2502.20508</link>
      <description><![CDATA[ARXIV：2502.20508V1公告类型：新 
摘要：探索大语言模型（LLM）的最新进步已经探索了其潜在的潜力作为个性化的旅行计划代理，但是现有的基准在现实世界中的适用性仍然有限。现有的数据集，例如Travel Planner和TravelPlanner+，患有半合成数据的依赖，空间不一致以及缺乏关键的旅行限制，从而无法获得实用的行程生成。为了解决这些差距，我们介绍了TripCraft，这是一个时空连贯的旅行计划数据集，该数据集集成了现实世界的限制，包括公共交通计划，事件可用性，各种吸引力类别和用户角色，以增强个性化。为了评估LLM生成的计划以外的现有二进制验证方法，我们提出了五个连续的评估指标，即时间进餐得分，时间吸引分数，空间得分，订购得分和角色分数，以评估跨多个维度的行程质量。我们的参数通知设置可显着增强进餐时间表，在7天情景中将临时进餐评分从61％提高到80％。 TripCraft为LLM驱动的个性化旅行计划建立了新的基准，为行程生成提供了更现实，更约束的意识框架。接受后，数据集和代码库将在接受后公开可用。]]></description>
      <guid>https://arxiv.org/abs/2502.20508</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>监督的微调LLM在编程教育中表现为教学代理</title>
      <link>https://arxiv.org/abs/2502.20527</link>
      <description><![CDATA[ARXIV：2502.20527V1公告类型：新 
摘要：大型语言模型（LLM）越来越多地在高等教育中探索，但他们作为教学代理人的有效性仍然没有被忽视。在本文中，我们介绍了Guidelm的开发，Guidelm是一款专为编程教育而设计的精通LLM。 Guidelm已集成到调试C编译器（DCC）中，这是一种教育C编译器，利用LLMS生成教学上有声音错误的解释。此前，DCC依靠现成的OpenAI模型，尽管有相反的提示，但该模型通常通过直接提供解决方案而过度辅助学生。
  为了解决这个问题，我们在528个学生疑问/教师回答对的数据集上使用了监督的微调（SFT），创建了两种模型：Guidelm和Guidelm-Mini，分别在Chatgpt-4O和4O-Mini上进行了微调。我们对每个模型进行了400个响应的专家分析，将其教学效率与基本OpenAI模型进行了比较。我们的评估基于建构主义和认知负载理论，评估了概念脚手架，清晰度和苏格拉底指导等因素。
  结果表明，与GPT-4O相比，Guidelm和Guidelm-Mini提高了教学绩效，苏格拉底指南增长了8％，单词经济性提高了58％。但是，这种改进是以略有降低的一般准确性来实现的。尽管需要进一步的工作，但我们的发现表明，使用有针对性数据集的微调LLM是开发更适合教育环境的模型的有前途的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.20527</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Nanogpt：纳米技术研究的查询驱动的大型语言模型检索生成系统</title>
      <link>https://arxiv.org/abs/2502.20541</link>
      <description><![CDATA[ARXIV：2502.20541V1公告类型：新 
摘要：本文介绍了针对纳米技术研究量身定制的大型语言模型检索型生成（LLM-rag）系统的开发和应用。该系统利用复杂的语言模型的能力来充当智能研究助理，增强了纳米技术领域文献评论的效率和全面性。该LLM-rag系统的核心是其高级查询后端检索机制，该机制集成了来自多个信誉量的数据。该系统通过利用Google Scholar的高级搜索并刮除来自Elsevier，Springer Nature和ACS出版物的开放式论文来检索相关文献。这种多方面的方法可确保广泛而多样的最新学术文章和论文集合。拟议的系统通过提供简化，准确和详尽的文献检索过程，在帮助研究人员方面具有巨大潜力，从而加速了纳米技术的研究进步。 LLM-rag系统的有效性通过严格的测试进行了验证，这说明了其能力大大减少全面文献综述所需的时间和精力，同时保持高准确性，查询相关性和胜过标准的公开，公开可用的LLMS。]]></description>
      <guid>https://arxiv.org/abs/2502.20541</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Huamr：匈牙利AMR解析器和数据集</title>
      <link>https://arxiv.org/abs/2502.20552</link>
      <description><![CDATA[ARXIV：2502.20552V1公告类型：新 
摘要：我们介绍了Huamr，这是第一个摘要含义表示（AMR）数据集和匈牙利语基于大型语言模型的大型AMR解析器的套件，目的是针对非英语语言的语义资源的稀缺性。为了创建HUAMR，我们采用Llama-3.1-70B自动生成银色标准AMR注释，然后手动进行完善以确保质量。在此数据集的基础上，我们研究了不同的模型体系结构-MT5大型和Llama-3.2-1b-以及微调策略如何影响AMR解析性能。
  在将来自Llama-3.1-70B的银色标准AMR纳入较小模型的训练数据中并不能始终提高整体分数，但我们的结果表明，这些技术有效提高了匈牙利新闻数据（Huamr的领域）的解析准确性。我们使用Smatch分数评估了我们的解析器，并确认了Huamr和我们的解析器的潜力来推进语义解析研究。]]></description>
      <guid>https://arxiv.org/abs/2502.20552</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从来源到引文的嘈杂路径：衡量学者如何参与过去的研究</title>
      <link>https://arxiv.org/abs/2502.20581</link>
      <description><![CDATA[ARXIV：2502.20581V1公告类型：新 
摘要：学术引用广泛用于评估研究和追踪知识流。这种用途通常依赖于原始引文计数和忽略引文类型的可变性。特别是，由于引用研究的原始知识可能会被解释，汇总或重新解释，因此引用的忠诚可能会有所不同，这可能会导致从引用论文到引用论文的多少信息变化。在这项研究中，我们引入了一条计算管道，以量化引文保真度。使用论文的全文，管道可以在引用论文和引用论文中的相应主张中确定引用，并应用监督的模型来衡量句子级别的忠诚度。分析约有1300万个引用句子对的大规模多学科数据集，我们发现当作者引用论文1）1）更近且智力上更接近的论文时，引用保真度更高，2）更易于访问，3）第一位作者的h-index较低，作者团队中等大小。使用准体验，我们建立了“电话效应”  - 当引用论文对原始索赔的保真度较低时，引用引用纸的未来论文和原始论文对原始论文的保真度较低。我们的工作揭示了引文保真度的系统差异，强调了仅依赖引文数量的分析的局限性以及证据扭曲的可能性。]]></description>
      <guid>https://arxiv.org/abs/2502.20581</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>