<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 05 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>MMLU-Pro+：评估法学硕士中的高阶推理和捷径学习</title>
      <link>https://arxiv.org/abs/2409.02257</link>
      <description><![CDATA[arXiv:2409.02257v1 公告类型：新
摘要：现有的大型语言模型 (LLM) 基准越来越难以区分表现最佳的模型，这凸显了对更具挑战性的评估框架的需求。我们推出了 MMLU-Pro+，这是一个基于 MMLU-Pro 的增强基准，用于评估 LLM 中的捷径学习和高阶推理。通过结合不同领域的多个正确答案的问题，MMLU-Pro+ 测试了 LLM 进行复杂推理和抵制简单问题解决策略的能力。我们的结果表明，MMLU-Pro+ 保持了 MMLU-Pro 的难度，同时提供了更严格的模型判别测试，特别是在多正确答案场景中。我们引入了快捷选择率和正确对识别率等新指标，为模型行为和锚定偏差提供了更深入的见解。对五种最先进的 LLM 的评估揭示了显著的性能差距，突出了推理能力和偏见敏感性的差异。我们在 \url{https://github.com/asgsaeid/mmlu-pro-plus} 发布数据集和评估代码。]]></description>
      <guid>https://arxiv.org/abs/2409.02257</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Arctic-SnowCoder：揭开代码预训练中高质量数据的神秘面纱</title>
      <link>https://arxiv.org/abs/2409.02326</link>
      <description><![CDATA[arXiv:2409.02326v1 公告类型：新
摘要：最近的研究越来越多地表明，高质量的数据对于语言模型的有效预训练至关重要。然而，“高质量”的确切定义仍未得到充分探索。专注于代码领域，我们推出了 Arctic-SnowCoder-1.3B，这是一个数据高效的基础代码模型，通过三个阶段逐步细化的数据对 555B 个标记进行预训练：（1）使用 500B 标准质量代码标记进行一般预训练，通过基本过滤、重复数据删除和去污进行预处理，（2）继续使用 50B 高质量标记进行预训练，这些标记由第一阶段中选择的 BERT 风格质量注释器进行训练，以区分好代码和随机数据，使用从高质量代码文件中提取的正例，以及来自 Magicoder 和 StarCoder2-Instruct 的指令数据，以及（3）使用第二阶段数据作为种子，使用 Llama-3.1-70B 创建的 5B 合成数据进行增强预训练，采用 Magicoder 方法进行预训练。尽管是在有限的数据集上进行训练的，Arctic-SnowCoder 在 BigCodeBench（一种专注于实际和具有挑战性的编程任务的编码基准）上实现了最先进的性能，与在不超过 1T 令牌上训练的类似大小的模型相比，其性能比 Phi-1.5-1.3B 高出 36%。在所有评估的基准中，Arctic-SnowCoder-1.3B 都胜过在 1T 令牌上预训练的 StarCoderBase-3B。此外，它的性能与在数万亿个令牌上训练的领先小型基础代码模型相当。例如，Arctic-SnowCoder-1.3B 在 HumanEval+（一种评估函数级代码生成的基准）上超越了在超过 3.3T 令牌上预训练的 StarCoder2-3B，并且在 BigCodeBench 上仍然具有竞争力。我们的评估提供了全面的分析，证明了 Arctic-SnowCoder 的各种设计选择是合理的。最重要的是，我们发现高质量数据的关键在于它与下游应用程序的分布保持一致。]]></description>
      <guid>https://arxiv.org/abs/2409.02326</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多样化-验证-适应：高效且稳健的检索增强模糊问答</title>
      <link>https://arxiv.org/abs/2409.02361</link>
      <description><![CDATA[arXiv:2409.02361v1 公告类型：新
摘要：检索增强生成 (RAG) 框架通过检索涵盖所有合理解释的段落并根据段落生成全面的响应来解决 QA 系统中用户查询的歧义问题。然而，我们的初步研究表明，单一检索过程通常会产生低质量的结果，因为检索到的段落经常无法捕获所有合理的解释。虽然已经提出了迭代 RAG 方法来解决这个问题，但它是以效率显著降低为代价的。为了解决这些问题，我们提出了多样化-验证-适应 (DIVA) 框架。DIVA 首先使检索到的段落多样化以包含不同的解释。随后，DIVA 验证段落的质量并根据其质量采用最合适的方法。这种方法通过处理模糊问题中的低质量检索问题来提高 QA 系统的准确性和稳健性，同时提高效率。]]></description>
      <guid>https://arxiv.org/abs/2409.02361</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是否具有情感敏感性？</title>
      <link>https://arxiv.org/abs/2409.02370</link>
      <description><![CDATA[arXiv:2409.02370v1 公告类型：新
摘要：大型语言模型 (LLM) 最近展示了其在语言理解方面的非凡能力。然而，如何全面评估 LLM 的情感能力仍然是一个挑战。本文研究了 LLM 在文本模式中检测和响应情绪的能力。随着 LLM 越来越多地融入各种应用程序，了解它们对情绪基调的敏感性变得至关重要，因为它会影响用户体验和情绪驱动任务的有效性。我们进行了一系列实验来评估几个著名的 LLM 在识别和适当响应积极、消极和中性情绪等情绪方面的表现。在各种情绪基准上分析模型的输出，并将其响应与人类评估进行比较。我们的发现表明，尽管 LLM 表现出对情绪的基本敏感性，但它们的准确性和一致性存在很大差异，强调需要进一步增强其训练过程以更好地捕捉微妙的情绪线索。以我们的研究结果为例，在某些情况下，模型可能会错误地将强烈的积极情绪归类为中性情绪，或者无法识别文本中的讽刺或反讽。这种错误分类凸显了情绪分析的复杂性以及模型需要改进的领域。另一个方面是，不同的 LLM 在同一组数据上的表现可能不同，这取决于它们的架构和训练数据集。这种差异需要更深入地研究导致性能差异的因素以及如何优化它们。]]></description>
      <guid>https://arxiv.org/abs/2409.02370</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型对隐私保护有多在行？合规与隐私技术审查案例研究</title>
      <link>https://arxiv.org/abs/2409.02375</link>
      <description><![CDATA[arXiv:2409.02375v1 公告类型：新 
摘要：大型语言模型 (LLM) 的最新进展大大扩展了它们在语言生成、摘要和复杂问答等各个领域的应用。然而，它们在隐私合规性和技术隐私审查方面的应用仍未得到充分探索，这引发了人们对它们遵守全球隐私标准和保护敏感用户数据的能力的严重担忧。本文旨在通过提供一个全面的案例研究来解决这一差距，该研究评估了 LLM 在隐私相关任务中的表现，例如隐私信息提取 (PIE)、法律和监管关键点检测 (KPD) 以及问答 (QA) 在隐私政策和数据保护法规方面的表现。我们引入了一个隐私技术审查 (PTR) 框架，强调了它在减轻软件开发生命周期中的隐私风险方面的作用。通过实证评估，我们调查了几个著名的 LLM，包括 BERT、GPT-3.5、GPT-4 和自定义模型，在执行隐私合规性检查和技术隐私审查方面的能力。我们的实验从多个维度对模型进行了基准测试，重点关注模型在提取隐私敏感信息和检测关键监管合规点方面的准确率、召回率和 F1 分数。虽然 LLM 在自动化隐私审查和识别监管差异方面表现出色，但在完全遵守不断发展的法律标准方面仍然存在重大差距。我们为增强 LLM 在隐私合规方面的能力提供了可行的建议，强调需要进行强大的模型改进并更好地与法律和监管要求相结合。这项研究强调了开发隐私感知 LLM 的重要性日益增加，这种 LLM 既可以支持企业的合规工作，又可以保护用户的隐私权。]]></description>
      <guid>https://arxiv.org/abs/2409.02375</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STAB：语音标记器评估基准</title>
      <link>https://arxiv.org/abs/2409.02384</link>
      <description><![CDATA[arXiv:2409.02384v1 公告类型：新
摘要：将语音表示为离散标记提供了一个将语音转换为与文本非常相似的格式的框架，从而使语音能够作为广泛成功的大型语言模型 (LLM) 的输入。目前，虽然已经提出了几种语音标记器，但对于标记器对特定下游任务及其整体通用性所期望的属性存在歧义。评估标记器在不同下游任务中的性能是一项计算密集型工作，对可扩展性提出了挑战。为了规避这一要求，我们提出了 STAB（语音标记器评估基准），这是一个系统的评估框架，旨在全面评估语音标记器并阐明其固有特征。该框架提供了对语音标记化底层机制的更深入了解，从而为加快未来标记器模型的进步和使用标准化基准进行比较分析提供了宝贵的资源。我们评估 STAB 指标并将其与一系列语音任务和标记器选择的下游任务性能相关联。]]></description>
      <guid>https://arxiv.org/abs/2409.02384</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习确定语系</title>
      <link>https://arxiv.org/abs/2409.02393</link>
      <description><![CDATA[arXiv:2409.02393v1 公告类型：新
摘要：我们使用 c-GAN（卷积生成对抗）神经网络来分析现存、已无法理解和一种已无法解密（塞浦路斯-米诺斯）语言的音译文本片段，以建立语言亲和性。本文对翻译和/或解密持中立态度。然而，人们希望所提出的方法能够用于使用更复杂的神经网络技术进行解密。]]></description>
      <guid>https://arxiv.org/abs/2409.02393</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>抽象文本摘要：现状、挑战和改进</title>
      <link>https://arxiv.org/abs/2409.02413</link>
      <description><![CDATA[arXiv:2409.02413v1 公告类型：新
摘要：本调查特别关注抽象文本摘要的前景，而不是提取技术，提供了全面的概述，深入研究了最先进的技术、普遍存在的挑战和未来的研究方向。我们将这些技术分为传统的序列到序列模型、预训练的大型语言模型、强化学习、分层方法和多模态摘要。与之前没有详细研究复杂性、可扩展性和技术比较的研究不同，本综述采用了一种全面的方法，涵盖了最先进的方法、挑战、解决方案、比较、局限性，并列出了未来的改进——为研究人员提供了广泛的概述，以推进抽象摘要研究。我们提供了跨分类技术的重要比较表——提供了对模型复杂性、可扩展性和适当应用的见解。本文重点介绍了一些挑战，例如意义表达不足、事实一致性、可控文本摘要、跨语言摘要和评估指标等。提出了利用知识整合和其他创新策略的解决方案来应对这些挑战。本文最后强调了新兴的研究领域，例如事实不一致、领域特定、跨语言、多语言和长文档摘要，以及处理噪声数据。我们的目标是为研究人员和从业者提供该领域的结构化概述，使他们能够更好地了解当前的情况并确定进一步研究和改进的潜在领域。]]></description>
      <guid>https://arxiv.org/abs/2409.02413</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>规范化会丢失什么？探索多语言 ASR 模型评估中的陷阱</title>
      <link>https://arxiv.org/abs/2409.02449</link>
      <description><![CDATA[arXiv:2409.02449v1 公告类型：新
摘要：本文探讨了评估多语言自动语音识别 (ASR) 模型的缺陷，特别关注印度语脚本。我们研究了领先的 ASR 模型（包括 OpenAI Whisper、Meta 的 MMS、Seamless 和 Assembly AI 的 Conformer）所采用的文本规范化例程，以及它们对性能指标的意外影响。我们的研究表明，当前的文本规范化实践虽然旨在通过消除拼写、标点符号和特殊字符的变化等不一致之处来标准化 ASR 输出以进行公平比较，但在应用于印度语脚本时存在根本缺陷。通过使用文本相似性分数进行实证分析和深入的语言学检查，我们证明这些缺陷导致印度语的性能指标人为膨胀。最后，我们建议转向开发利用本地语言专业知识的规范化例程，以确保对多语言 ASR 模型进行更稳健和准确的评估。]]></description>
      <guid>https://arxiv.org/abs/2409.02449</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DetectiveQA：评估侦探小说的长上下文推理</title>
      <link>https://arxiv.org/abs/2409.02465</link>
      <description><![CDATA[arXiv:2409.02465v1 Announce Type: new 
摘要：随着大型语言模型（LLM）的快速发展，长上下文信息的理解与处理成为学术界和工业界研究的热点。然而，衡量LLM处理长上下文信息能力的基准似乎并没有跟上LLM发展的步伐。尽管出现了各种长上下文评估基准，但评估的能力类型仍然有限，没有新的能力维度。本文介绍了一个叙事推理基准DetectiveQA，其平均上下文长度超过10万个token。DetectiveQA专注于评估LLM的长上下文推理能力，这不仅需要充分理解上下文，还需要从上下文中提取重要证据，并根据提取的证据进行推理以回答给定的问题。这是一个新的能力评估维度，更符合当前LLM的智能水平。我们使用侦探小说作为数据源，其中自然包含各种推理元素。最后，我们手动标注了 600 个中文问题，然后提供了英文版的上下文信息和问题。我们在 DetectiveQA 上评估了许多长上下文 LLM，包括商业和开源模型，结果表明，现有的长上下文 LLM 仍需要重大改进才能有效处理真正的长上下文依赖问题。]]></description>
      <guid>https://arxiv.org/abs/2409.02465</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图卷积网络中用于自动问题分类的单词和短语特征</title>
      <link>https://arxiv.org/abs/2409.02481</link>
      <description><![CDATA[arXiv:2409.02481v1 公告类型：新
摘要：有效的问题分类对于人工智能驱动的教育工具至关重要，它使自适应学习系统能够按技能领域、难度级别和能力对问题进行分类。这种分类不仅支持教育诊断和分析，而且还通过将问题与相关类别关联起来，增强了信息检索和问答等复杂任务。传统方法通常基于词嵌入和传统分类器，难以捕捉自然语言中的细微关系，导致性能不佳。为了解决这个问题，我们提出了一种利用图卷积网络 (GCN) 的新方法，称为短语问题图卷积网络 (PQ-GCN)，以更好地模拟问题的固有结构。通过将问题表示为图形 - 其中节点表示单词或短语，边表示句法或语义关系 - 我们的方法允许 GCN 更有效地从语言的相互关联性中学习。此外，我们还探索了如何结合基于短语的特征来提高分类准确率，尤其是在资源匮乏的环境中。我们的研究结果表明，GCN 加上这些特征后，可以为更准确、更具有情境感知的问题分类提供有希望的解决方案，从而弥合图神经网络研究与实际教育应用之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2409.02481</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言被过度分析时会变得可怕：用论证理论驱动的提示来解读隐含的厌女推理</title>
      <link>https://arxiv.org/abs/2409.02519</link>
      <description><![CDATA[arXiv:2409.02519v1 公告类型：新
摘要：我们提出将厌女症检测作为一项论证推理任务，并研究大型语言模型 (LLM) 理解用于传达意大利语和英语厌女症的隐性推理的能力。主要目的是生成信息与编码厌女症的隐含含义之间缺失的推理联系。我们的研究以论证理论为基础，在零样本和少样本设置中形成提示集合。这些提示整合了不同的技术，包括思路链推理和增强知识。我们的研究结果表明，LLM 在厌女症评论的推理能力上存在不足，而且它们主要依靠从内化的关于女性的常见刻板印象中获得的隐性知识来产生隐含的假设，而不是归纳推理。]]></description>
      <guid>https://arxiv.org/abs/2409.02519</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更多就是更多：大型语言模型中的附加偏差</title>
      <link>https://arxiv.org/abs/2409.02569</link>
      <description><![CDATA[arXiv:2409.02569v1 公告类型：新
摘要：在本文中，我们研究了大型语言模型 (LLM) 中是否存在加法偏差，这与人类中观察到的认知偏差相似，在人类中，个体倾向于喜欢加法而不是减法变化。通过一系列受控实验，我们测试了各种 LLM，包括 GPT-3.5 Turbo、Claude 3.5 Sonnet、Mistral、Math$\Sigma$tral 和 Llama 3.1，这些任务旨在衡量它们对加法修改与减法修改的倾向。我们的研究结果表明，在所有测试模型中，加法变化都具有显著的偏好。例如，在回文创建任务中，Llama 3.1 97.85% 的时间倾向于添加字母而不是删除字母。同样，在乐高塔平衡任务中，GPT-3.5 Turbo 76.38% 的时间选择添加砖块而不是移除砖块。在文本摘要任务中，当被要求改进自己或他人的写作时，Mistral 7B 在 59.40% 至 75.10% 的情况下会生成更长的摘要。这些结果表明，与人类类似，LLM 表现出明显的附加偏差，这可能会在大规模使用 LLM 时产生影响。附加偏差可能会增加资源使用和环境影响，从而导致过度消费和浪费造成的更高经济成本。在开发和应用 LLM 时应考虑这种偏差，以确保平衡有效的问题解决方法。]]></description>
      <guid>https://arxiv.org/abs/2409.02569</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PUB：绘制理解基准和数据集，用于评估合成视觉数据解释的大型语言模型</title>
      <link>https://arxiv.org/abs/2409.02617</link>
      <description><![CDATA[arXiv:2409.02617v1 公告类型：新
摘要：大型语言模型 (LLM) 解释数据视觉表示的能力对于推进其在数据分析和决策过程中的应用至关重要。本文提出了一种新颖的合成数据集，旨在评估 LLM 解释各种形式的数据可视化的能力，包括时间序列、直方图、小提琴图、箱线图和聚类图等。我们的数据集是使用受控参数生成的，以确保全面覆盖潜在的现实场景。我们使用多模式文本提示和与图像中的视觉数据相关的问题来对 ChatGPT 或 Gemini 等几种最先进的模型进行基准测试，评估它们的理解和解释准确性。
为了确保数据完整性，我们的基准数据集是自动生成的，使其完全是新的，并且不受正在测试的模型的先前接触。这种策略使我们能够评估模型真正解释和理解数据的能力，消除预先学习响应的可能性，并允许对模型的功能进行公正的评估。我们还引入了定量指标来评估模型的性能，从而提供了一个强大而全面的评估工具。
使用此数据集对几个最先进的 LLM 进行基准测试，可以发现不同程度的成功，突出解释各种类型视觉数据的具体优势和劣势。结果为 LLM 的当前能力提供了宝贵的见解，并确定了需要改进的关键领域。这项工作为旨在增强语言模型的视觉解释能力的未来研究和开发建立了基础基准。未来，具有强大视觉解释技能的改进型 LLM 可以极大地帮助自动数据分析、科学研究、教育工具和商业智能应用。]]></description>
      <guid>https://arxiv.org/abs/2409.02617</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenFact 参加 CheckThat! 2024：结合多种攻击方法实现有效的对抗性文本生成</title>
      <link>https://arxiv.org/abs/2409.02649</link>
      <description><![CDATA[arXiv:2409.02649v1 公告类型：新
摘要：本文介绍了 CLEF 2024 任务 6 的 CheckThat！实验室的实验和结果：使用对抗性示例 (InCrediblAE) 进行可信度评估的稳健性。此任务的主要目标是在五个问题领域中生成对抗性示例，以评估广泛使用的文本分类方法（微调 BERT、BiLSTM 和 RoBERTa）在应用于可信度评估问题时的稳健性。
本研究探讨了集成学习在增强对自然语言处理 (NLP) 模型的对抗性攻击中的应用。我们在各种错误信息任务的五个数据集上系统地测试和改进了几种对抗性攻击方法，包括 BERT-Attack、遗传算法、TextFooler 和 CLARE。通过开发 BERT-Attack 的修改版本和混合方法，我们显著提高了攻击效果。我们的结果表明，修改和结合多种方法来创建更复杂、更有效的对抗性攻击策略具有潜力，有助于开发更为稳健、更为安全的系统。]]></description>
      <guid>https://arxiv.org/abs/2409.02649</guid>
      <pubDate>Thu, 05 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>