<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 08 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向包容性教育 AI：通过多元化视角审核前沿法学硕士</title>
      <link>https://arxiv.org/abs/2501.03259</link>
      <description><![CDATA[arXiv:2501.03259v1 公告类型：新
摘要：随着 GPT-4 和 Llama 3 等大型语言模型 (LLM) 成为教育环境不可或缺的一部分，人们越来越担心这些技术中存在的文化偏见、权力不平衡和道德限制。尽管生成式人工智能工具旨在增强学习体验，但它们往往反映出植根于西方、受过教育、工业化、富裕和民主 (WEIRD) 文化范式的价值观，可能会忽视多样化的全球视角。本文提出了一个框架，通过应用多元性的视角来评估和减轻 LLM 中的文化偏见。多元性受到 Senturk 等人的启发，植根于伊斯兰和其他智慧传统，强调不同文化观点的共存，支持融合经验科学和规范价值观的多层次认识论。我们的分析表明，LLM 经常表现出文化两极分化，偏见既出现在明显的反应中，也出现在微妙的背景线索中。为了解决固有偏见并将多元性纳入 LLM，我们提出了两种策略：\textit{上下文实现的多元 LLM}，它将多元原则直接嵌入系统提示中，在基础层面影响 LLM 输出，独立于单个提示；\textit{多智能体系统 (MAS) 实现的多元 LLM}，其中多个 LLM 智能体（每个代表不同的文化观点）协作生成平衡的综合响应。我们的研究结果表明，随着缓解策略从上下文提示发展到 MAS 实现，文化包容性显著提高，这表现为观点分布分数 (PDS) 显著上升，PDS 熵从基线的 3.25\% 增加到 MAS 实现的多元 LLM 的 98\%。情绪分析进一步表明，跨文化情绪转向积极情绪，...]]></description>
      <guid>https://arxiv.org/abs/2501.03259</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE++：一种简单有效的大型语言模型对齐方法</title>
      <link>https://arxiv.org/abs/2501.03262</link>
      <description><![CDATA[arXiv:2501.03262v1 公告类型：新
摘要：强化学习从人类反馈 (RLHF) 已成为将大型语言模型与人类偏好相结合的关键方法，通过近端策略优化 (PPO)、直接偏好优化 (DPO)、REINFORCE 留一法 (RLOO)、ReMax 和组相对策略优化 (GRPO) 等方法见证了算法的快速发展。我们提出了 REINFORCE++，这是经典 REINFORCE 算法的增强版本，它结合了 PPO 的关键优化技术，同时消除了对批评者网络的需求。REINFORCE++ 实现了三个主要目标：(1) 简单 (2) 增强训练稳定性，(3) 减少计算开销。通过广泛的实证评估，我们证明 REINFORCE++ 与 GRPO 相比表现出更好的稳定性，并且在保持可比性能的同时实现了比 PPO 更高的计算效率。实现可在 \url{https://github.com/OpenRLHF/OpenRLHF} 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.03262</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 内容审核和用户满意度：来自 Chatbot Arena 中回复拒绝的证据</title>
      <link>https://arxiv.org/abs/2501.03266</link>
      <description><![CDATA[arXiv:2501.03266v1 公告类型：新
摘要：LLM 安全性和道德一致性被广泛讨论，但内容审核对用户满意度的影响仍未得到充分探索。为了解决这个问题，我们使用一种新颖的微调 RoBERTa 模型分析了近 50,000 个 Chatbot Arena 响应对，我们用手工标记的数据训练该模型，以区分由于道德问题而拒绝与由于技术障碍或缺乏信息而拒绝。我们的研究结果显示，内容审核的拒绝惩罚很严重，与标准响应相比，用户选择基于道德的拒绝的频率大约是他们首选的 LLM 响应的四分之一。然而，上下文和措辞起着至关重要的作用：对高度敏感的提示（例如非法内容）的拒绝比不太敏感的道德问题获得更高的胜率，与提示紧密相关的较长的响应表现更好。这些结果强调需要细致入微的审核策略来平衡道德保障和用户满意度。此外，我们发现，使用 LLM-as-a-Judge 方法进行的评估中拒绝惩罚明显较低，突出了用户和自动评估之间的差异。]]></description>
      <guid>https://arxiv.org/abs/2501.03266</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ComMer：用于个性化压缩和合并用户数据的框架</title>
      <link>https://arxiv.org/abs/2501.03276</link>
      <description><![CDATA[arXiv:2501.03276v1 公告类型：新
摘要：大型语言模型 (LLM) 擅长处理各种任务，但由于资源和计算限制，使其适应新数据（尤其是个性化应用）带来了重大挑战。现有方法要么依赖于通过提示将新数据暴露给模型，这受到上下文大小的限制，并且在推理时计算成本高昂，要么依赖于微调，这会产生大量的训练和更新成本。在本文中，我们介绍了 ComMer - 压缩和合并 - 一种新颖的框架，它通过将用户的文档压缩为紧凑的表示形式来有效地个性化 LLM，然后将其合并并输入到冻结的 LLM 中。我们在两种类型的个性化任务上评估 ComMer - 个性化技能学习，使用来自 LaMP 基准的推文释义数据集和个性化新闻标题生成数据集，以及知识密集型，使用 PerLTQA 数据集。我们的实验表明，在推理预算受限的情况下，ComMer 在技能学习任务中取得了卓越的质量，同时由于详细信息的丢失，在知识密集型环境中也表现出局限性。这些结果为个性化多文档压缩的权衡和潜在优化提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2501.03276</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HonkaiChat：来自动漫的、充满活力的伙伴！</title>
      <link>https://arxiv.org/abs/2501.03277</link>
      <description><![CDATA[arXiv:2501.03277v1 公告类型：新
摘要：现代对话代理，包括动漫主题的聊天机器人，经常是被动的和个性驱动的，但无法捕捉人类互动的动态性质。我们提出了一个事件驱动的对话框架来解决这些限制，通过在对话提示中嵌入动态事件并在特定于角色的数据上微调模型。对 GPT-4 的评估和与行业领先基线的比较表明，事件驱动的提示显着提高了对话参与度和自然度，同时减少了幻觉。本文探讨了这种方法在《崩坏：星轨》背景下创建逼真的聊天机器人交互中的应用，展示了基于动态事件的系统改变角色扮演和交互式对话的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.03277</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ADePT：自适应分解快速调整，实现参数高效微调</title>
      <link>https://arxiv.org/abs/2501.03291</link>
      <description><![CDATA[arXiv:2501.03291v1 公告类型：新 
摘要：提示调优 (PT) 通过优化少量软虚拟标记（这些标记被添加到输入标记嵌入中）使预训练大型语言模型 (PLM) 能够适应下游任务。最近，分解提示调优 (DePT) 通过将软提示分解为较短的软提示和一对低秩矩阵，展示了卓越的适应能力。这对低秩矩阵的乘积被添加到输入标记嵌入中以抵消它们。此外，由于软提示较短，DePT 与 PT 相比实现了更快的推理。然而，在本文中，我们发现 DePT 基于位置的标记嵌入偏移限制了其跨不同模型输入进行泛化的能力，并且跨许多标记嵌入共享的嵌入偏移导致了次优化。为了解决这些问题，我们引入了 \textbf{A}daptive \textbf{De}composed \textbf{P}rompt \textbf{T}uning (ADePT)，它由一个短软提示和一个浅层 token-shared 前馈神经网络组成。ADePT 利用 token-shared 前馈神经网络来学习每个 token 的嵌入偏移量，从而实现根据模型输入而变化的自适应嵌入偏移量，并更好地优化 token 嵌入偏移量。与 vanilla PT 及其变体相比，这使 ADePT 能够实现卓越的自适应性能，而无需更多的推理时间或额外的可训练参数。在 23 个自然语言处理 (NLP) 任务和 4 个不同规模的典型 PLM 的全面实验中，我们表明 ADePT 始终超越领先的参数高效微调 (PEFT) 方法，甚至在某些情况下优于完整的微调基线。代码可在 \url{https://github.com/HungerPWAY/ADePT} 获得。]]></description>
      <guid>https://arxiv.org/abs/2501.03291</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Facebook 的整体偏见数据集分析瑞士联邦最高法院判决中的偏见：对语言模型训练的影响</title>
      <link>https://arxiv.org/abs/2501.03324</link>
      <description><![CDATA[arXiv:2501.03324v1 公告类型：新
摘要：自然语言处理 (NLP) 对于计算机处理和准确响应人类语言至关重要。然而，训练数据中的偏见可能会导致不公平，尤其是在预测法律判断时。本研究重点分析瑞士判断预测数据集 (SJP-Dataset) 中的偏见。我们的目标是确保在法律背景下 NLP 模型做出公平决策所必需的无偏见事实描述。我们使用来自整体偏见数据集的社会偏见描述符分析数据集，并采用先进的 NLP 技术（包括注意力可视化）来探索不喜欢的描述符对模型预测的影响。该研究确定了偏见并检查了它们对模型行为的影响。挑战包括数据集不平衡和影响模型性能的标记限制。]]></description>
      <guid>https://arxiv.org/abs/2501.03324</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于社交媒体上社交支持检测的高级机器学习技术</title>
      <link>https://arxiv.org/abs/2501.03370</link>
      <description><![CDATA[arXiv:2501.03370v1 公告类型：新
摘要：社交媒体的广泛使用凸显了了解其影响的必要性，特别是在线社交支持的作用。本研究使用了一个专注于在线社交支持的数据集，其中包括社交媒体上社交支持内容的二分类和多分类。社交支持的分类分为三个任务。第一个任务侧重于区分支持性和不支持性。第二项任务旨在确定支持是针对个人还是群体。第三项任务对特定类型的社会支持进行分类，将其分为国家、LGBTQ、黑人、女性、宗教和其他（如果不符合前面提到的类别）等类别。为了解决这些任务中的数据不平衡问题，我们采用 K-means 聚类来平衡数据集，并将结果与​​原始不平衡数据进行比较。使用先进的机器学习技术，包括 GPT3、GPT4 和 GPT4-o 的 transformer 和零样本学习方法，我们可以预测各种情况下的社会支持水平。使用不同学习方法的基线模型评估数据集的有效性，其中基于 Transformer 的方法表现出优异的性能。此外，与之前使用心理语言学和基于一元语法的 TF-IDF 值的传统机器学习的研究相比，我们在第二项任务中实现了 0.4\% 的宏观 F1 得分增加，在第三项任务中实现了 0.7\% 的宏观 F1 得分增加。]]></description>
      <guid>https://arxiv.org/abs/2501.03370</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BoundingDocs：具有空间注释的文档问答统一数据集</title>
      <link>https://arxiv.org/abs/2501.03403</link>
      <description><![CDATA[arXiv:2501.03403v1 公告类型：新
摘要：我们提出了一个统一的文档问答 (QA) 数据集，该数据集结合了与文档 AI 和视觉丰富的文档理解 (VRDU) 相关的几个公共数据集。我们的主要贡献有两个方面：一方面，我们将现有的文档 AI 任务（例如信息提取 (IE)）重新表述为问答任务，使其成为训练和评估大型语言模型的合适资源；另一方面，我们发布所有文档的 OCR，并将文档图像中要找到的答案的确切位置作为边界框包含在内。使用此数据集，我们探索不同的提示技术（可能包括边界框信息）对开放权重模型性能的影响，从而确定最有效的文档理解方法。]]></description>
      <guid>https://arxiv.org/abs/2501.03403</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DAMAGE：检测对抗性修改的 AI 生成文本</title>
      <link>https://arxiv.org/abs/2501.03437</link>
      <description><![CDATA[arXiv:2501.03437v1 公告类型：新
摘要：AI 人性化器是一类新的在线软件工具，旨在以一种允许它们逃避 AI 检测软件的方式解释和重写 AI 生成的文本。我们研究了 19 种 AI 人性化器和解释工具，并定性评估了它们在保留原始文本含义方面的效果和忠实度。我们表明，许多现有的 AI 检测器无法检测到人性化文本。最后，我们展示了一个强大的模型，该模型可以使用以数据为中心的增强方法检测人性化的 AI 文本，同时保持较低的误报率。我们攻击我们自己的检测器，训练我们自己的微调模型，针对我们的检测器的预测进行优化，并表明我们的检测器的跨人性化器泛化足以保持对这种攻击的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2501.03437</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>寻找声音：评估非洲裔美国人方言的生成以适应聊天机器人技术</title>
      <link>https://arxiv.org/abs/2501.03441</link>
      <description><![CDATA[arXiv:2501.03441v1 公告类型：新
摘要：随着聊天机器人越来越多地融入日常任务，设计适应不同用户群体的系统对于培养信任、参与度和包容性至关重要。本研究调查了当代大型语言模型 (LLM) 生成非裔美国人白话英语 (AAVE) 的能力，并评估了 AAVE 的使用对聊天机器人应用程序中用户体验的影响。我们分析了三个 LLM 系列 (Llama、GPT 和 Claude) 在不同方言强度下产生类似 AAVE 的话语的表现，并评估了医疗保健和教育等多个领域的用户偏好。尽管 LLM 在生成类似 AAVE 的语言方面很熟练，但研究结果表明，讲 AAVE 的用户更喜欢标准美式英语 (SAE) 聊天机器人，AAVE 水平越高，各种特征的评分就越低，包括聊天机器人的可信度和角色适当性。这些结果凸显了创建包容性人工智能系统的复杂性，并强调需要进一步探索多样性以增强人机交互。]]></description>
      <guid>https://arxiv.org/abs/2501.03441</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>文本到带隙：预训练语言模型作为半导体带隙预测的编码器</title>
      <link>https://arxiv.org/abs/2501.03456</link>
      <description><![CDATA[arXiv:2501.03456v1 公告类型：新
摘要：在本研究中，我们探索使用基于变换器的语言模型作为编码器，直接从文本描述中预测半导体材料的带隙。量子化学模拟，包括密度泛函理论 (DFT)，计算密集且耗时，这限制了它们在高通量材料筛选中的实用性，特别是对于复杂系统。浅层机器学习 (ML) 模型虽然有效，但通常需要大量的数据预处理才能将非数值材料属性转换为数值输入。相比之下，我们的方法直接利用文本数据，无需复杂的特征工程。我们以两种格式生成材料描述：结合特征的格式化字符串和使用 ChatGPT API 生成的自然语言文本。我们证明，在自然语言处理任务上预先训练的 RoBERTa 模型可以有效地作为预测任务的编码器。只需进行少量微调，它就能实现约 0.33 eV 的平均绝对误差 (MAE)，其表现优于支持向量回归、随机森林和 XGBoost 等浅层机器学习模型。即使只训练线性回归头，同时保持 RoBERTa 编码器层冻结，其准确率也几乎与完全训练的模型相同。这表明，预先训练的 RoBERTa 编码器具有高度适应性，可以处理与材料特性（例如带隙）相关的领域特定文本，从而大大减少了大量重新训练的需要。这项研究强调了基于 Transformer 的语言模型作为半导体材料特性预测任务的高效通用编码器的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.03456</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ISSR：词汇测试干扰项生成的自我审查迭代选择</title>
      <link>https://arxiv.org/abs/2501.03462</link>
      <description><![CDATA[arXiv:2501.03462v1 公告类型：新
摘要：词汇习得对于第二语言学习至关重要，因为它是所有核心语言技能的基础。准确的词汇评估在标准化考试中尤为重要，因为考试题目评估学习者对单词的理解和语境使用。先前的研究已经探索了生成干扰项以帮助设计英语词汇测试的方法。然而，当前的方法通常依赖于词汇数据库或预定义规则，并且经常产生干扰项，这些干扰项通过引入多个正确选项而有可能使问题无效。在本研究中，我们关注台湾大学入学考试的英语词汇问题。我们分析学生的回答分布以深入了解这些测试项目的特点并为未来的研究提供参考。此外，我们还确定了大型语言模型 (LLM) 如何支持教师为词汇测试设计生成干扰项的关键限制。为了应对这些挑战，我们提出了迭代选择与自我审查 (ISSR) 框架，该框架利用一种基于 LLM 的新型自我审查机制来确保干扰项在提供多样化选项的同时仍然有效。实验结果表明，ISSR 在生成合理的干扰项方面取得了良好的效果，而自我审查机制有效地过滤掉了可能使问题无效的干扰项。]]></description>
      <guid>https://arxiv.org/abs/2501.03462</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MTRAG：用于评估检索增强生成系统的多轮对话基准</title>
      <link>https://arxiv.org/abs/2501.03468</link>
      <description><![CDATA[arXiv:2501.03468v1 公告类型：新
摘要：检索增强生成 (RAG) 最近已成为大型语言模型 (LLM) 的一项非常流行的任务。在多轮 RAG 对话中对它们进行评估是一项重要且经常被忽视的任务，其中要求系统在先前对话的上下文中生成对问题的回答，这还有几个额外的挑战。我们提出了 MTRAG：一个端到端的人工生成的多轮 RAG 基准，它反映了用于评估整个 RAG 管道的不同维度上的几个真实世界属性。MTRAG 包含 110 个对话，平均每个对话 7.7 轮，涉及四个领域，总共 842 个任务。我们还通过合成数据和 LLM-as-a-Judge 评估探索自动化路径。我们的人工和自动评估表明，即使是最先进的 LLM RAG 系统在 MTRAG 上也会遇到困难。我们证明了需要强大的检索和生成系统来处理后续轮次、无法回答的问题、非独立问题和多个领域。MTRAG 可在 https://github.com/ibm/mt-rag-benchmark 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.03468</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有意图地阅读——中和意图</title>
      <link>https://arxiv.org/abs/2501.03475</link>
      <description><![CDATA[arXiv:2501.03475v1 公告类型：新
摘要：对大型语言模型 (LLM) 的查询可以分为两部分：指令/问题和伴随的上下文。大多数基准测试中检索增强生成 (RAG) 系统的上下文来自维基百科或类似维基百科的文本，这些文本以中性和事实语气编写。然而，当 RAG 系统检索基于互联网的内容时，它们会遇到具有不同语气和语言风格的文本，这给下游任务带来了挑战。有意图阅读任务通过评估上下文段落中不同语气如何影响模型性能来解决此问题。在先前专注于讽刺的工作的基础上，我们通过构建一个数据集扩展了这一范式，其中使用更好的合成数据生成方法将上下文段落转换为 $11$ 种不同的情绪。使用这个数据集，我们训练了一个情感翻译模型，以系统地将段落适应指定的情感语气。人工评估表明，经过微调成为情绪翻译器的 LLM 受益于合成生成的数据。最后，情绪翻译器用于有意识阅读任务，将段落转换为中性语气。通过中和段落，它减轻了讽刺段落带来的挑战，并将这项任务的整体结果提高了约 $3\%$。]]></description>
      <guid>https://arxiv.org/abs/2501.03475</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>