<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 25 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>将领域知识集成到大型语言模型中，以增强时尚建议</title>
      <link>https://arxiv.org/abs/2502.15696</link>
      <description><![CDATA[ARXIV：2502.15696V1公告类型：新 
摘要：时尚，深深地植根于社会文化动态，随着个体效仿由影响者和标志性人物普及的样式而发展的。为了使用人工智能复制这种精致的口味，传统的时尚合奏方法主要使用了监督的学习来模仿样式图标的决策，当面对分配变化时，这些方法会使风格复制差异，从而导致风格复制差异，这是由于轻微的输入变化而触发的。同时，大型语言模型（LLMS）在各个领域都变得突出，以用户友好的界面，强大的对话技能和高级推理能力认可。为了应对这些挑战，我们介绍了时尚大语言模型（FLLM），该模型采用自动推出的生成培训策略来增强其在保留基本领域知识的同时提供个性化时尚建议的能力。此外，通过在推断期间集成检索扩展技术，该模型可以更好地适应个人偏好。我们的结果表明，这种方法在准确性，可解释性和很少的学习能力方面超过了现有模型。]]></description>
      <guid>https://arxiv.org/abs/2502.15696</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>市政厅辩论提示：通过多人互动增强LLM中的逻辑推理</title>
      <link>https://arxiv.org/abs/2502.15725</link>
      <description><![CDATA[ARXIV：2502.15725V1公告类型：新 
摘要：辩论是人类交流的一种常用形式，因为其效率是为了解决问题的一种。从根本上讲，辩论允许在解决问题的问题中提出多个观点，并且对于复杂的问题，每个观点都为解决问题的新途径打开了新的途径。在这项工作中，我们通过提出市政厅式的辩论提示（THDP）将此概念应用于LLM决策，这是一种提示方法，将语言模型拼接到多个角色中，可以互相辩论以达到结论。我们的实验管道各种角色的角色数量和人格类型都不同，以找到由Zebralogic Bench衡量的最佳市政厅的大小和个性，以实现基准表现，这是一种以多项选择和填充为特征的推理密集型基准测试。空白的问题。我们的实验结果表明，一个具有LLM确定个性类型的5个角色的市政厅大小在Zebralogic方面表现最佳，在GPT-4O中的每单元素准确性方面的每电池准确性比一次性的COT基本线取得了13 \％的改善，9％的难题的精度提高了9％在Claude 3.5十四行诗中，硬性难题准确性的提高了10-15％。]]></description>
      <guid>https://arxiv.org/abs/2502.15725</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于大语模型在自动化科学文本中的有效性</title>
      <link>https://arxiv.org/abs/2502.15745</link>
      <description><![CDATA[ARXIV：2502.15745V1公告类型：新 
摘要：大型语言模型（LLM）的快速发展导致了许多应用程序机会。信息检索系统的一项传统任务是对文本的汇总和分类，这对于支持人类在大型文学机构中的浏览至关重要，例如。存在科学出版物。由于科学知识的迅速发展，最近的研究一直旨在建立研究信息系统，不仅提供传统的关键字搜索功能，而且还提供新的功能，例如自动检测学术界知识组织中存在的研究领域和行业。为了促进这一想法，我们介绍了通过评估各种LLM的能力将科学出版物分类为层次分类系统的能力而获得的结果。我们发现，使用FORC数据集作为地面真相数据，我们发现最近的LLM（例如Meta Llama 3.1）能够达到高达0.82的准确性，比传统的BERT模型高达0.08。]]></description>
      <guid>https://arxiv.org/abs/2502.15745</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的零射合常识验证和推理：Semeval-2020任务4数据集评估</title>
      <link>https://arxiv.org/abs/2502.15810</link>
      <description><![CDATA[ARXIV：2502.15810V1公告类型：新 
摘要：本研究评估了大语模型（LLM）在Semeval-2020任务4数据集上的性能，重点是常识验证和解释。我们的方法涉及使用零拍摄提示技术评估多个LLM，包括Llama3-70B，Gemma2-9b和Mixtral-8x7b。对两个任务进行了测试：任务A（常识验证），其中模型确定陈述是否与常识性知识和任务B（常识说明）相一致，其中模型在其中确定了令人难以置信的语句背后的推理。根据精度评估性能，并将结果与​​基于微调的变压器模型进行比较。结果表明，较大的模型的表现优于先前的模型，并且与人体评估A的表现紧密相关，而Llama3-70B在任务A中达到了98.40％的最高准确度，而落后于任务B中的93.40％的模型。但是，模型。他们有效地识别出令人难以置信的陈述，在选择最相关的解释时面临挑战，突出了因果和推论推理的局限性。]]></description>
      <guid>https://arxiv.org/abs/2502.15810</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有双维层次元数据的表格的表格嵌入</title>
      <link>https://arxiv.org/abs/2502.15819</link>
      <description><![CDATA[ARXIV：2502.15819V1公告类型：新 
摘要：嵌入是现实世界实体的凝结向量表示，在自然语言处理（NLP）中找到应用程序，计算机视觉和数据管理跨不同的下游任务。在这里，我们介绍了针对表格中的复杂2-D上下文的复杂性进行优化的新型专业嵌入式，并量身定制，以水平，垂直层次元素元素和嵌套。为此，我们通过引入一个新的可见性矩阵，编码单元并通过嵌入，专门针对模仿这种复杂结构化数据的复杂性来定义了双维表坐标，单独的水平水平，垂直元数据和数据上下文。通过评估5个大尺度结构化数据集和3个流行的下游任务，我们观察到，我们的解决方案的表现优于最先进的模型，其显着地图Delta高达0.28。 GPT-4 LLM+RAG的表现略高于我们的MRR Delta高达0.1，而我们的MAP DELTA的表现高达0.42。]]></description>
      <guid>https://arxiv.org/abs/2502.15819</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>进行稳健的ESG分析针对绿色风险：跨类别概括的方面行动分析</title>
      <link>https://arxiv.org/abs/2502.15821</link>
      <description><![CDATA[ARXIV：2502.15821V1公告类型：新 
摘要：可持续性报告是评估公司的环境，社会和治理，ESG绩效的关键，但是它们的内容越来越被绿色的绿色 - 可持续性主张所掩盖，这些声明具有误导，夸张和捏造。然而，现有的NLP方法进行ESG分析的方法缺乏稳健性，并且经常提取见证人的见解，这些见解反映出误导性或夸张的可持续性主张，而不是客观的ES​​G绩效。为了弥合这一差距，我们引入了A3CG  - 具有跨类别概括的方面分析，作为一种新型数据集，以在绿色洗涤率的流行率中提高ESG分析的鲁棒性。通过将可持续性方面明确联系起来，A3CG促进了对可持续性主张的更细粒度和透明的评估，以确保洞察力以可验证的行动为基础，而不是模糊或误导性的言论。此外，A3CG强调跨类别概括。即使公司更改报告以选择性地支持某些可持续性领域，这也可以确保在方面分析中的稳健模型性能。通过对A3CG的实验，我们分析了最先进的监督模型和LLM，发现了它们的局限性，并概述了未来研究的关键方向。]]></description>
      <guid>https://arxiv.org/abs/2502.15821</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>来：一种基于未学习的无冲突模型编辑的方法</title>
      <link>https://arxiv.org/abs/2502.15826</link>
      <description><![CDATA[ARXIV：2502.15826V1公告类型：新 
摘要：大语言模型（LLMS）通常会保留前培训中过时或不正确的信息，这破坏了其可靠性。尽管已经开发出模型编辑方法来解决此类错误而没有进行全面重新训练，但它们经常遭受知识冲突的困扰，而过时的信息会干扰新知识。在这项工作中，我们提出了无冲突模型编辑（COME），这是一个新颖的框架，通过选择性地删除过时的知识来增强LLM中知识更新的准确性。来利用学习来减轻知识干扰，可以整合新信息而不会损害相关的语言特征。通过使用RunterFact和ZSRE数据集在GPT-J和Llama-3上进行实验，我们证明，当应用于现有编辑方法时，它可以提高编辑精度和模型可靠性。我们的结果强调，有针对性的去除过时的知识对于增强模型编辑效果并保持模型的生成性能至关重要。]]></description>
      <guid>https://arxiv.org/abs/2502.15826</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>务实推理改善LLM代码生成</title>
      <link>https://arxiv.org/abs/2502.15835</link>
      <description><![CDATA[ARXIV：2502.15835V1公告类型：新 
摘要：大型语言模型（LLMS）在将自然语言（NL）说明转化为程序代码方面表现出了令人印象深刻的潜力。但是，用户说明通常包含固有的歧义，这使得LLMS生成准确反映用户真正意图的代码具有挑战性。为了应对这一挑战，研究人员提出了生产程序代码的多个候选者，然后将其重新确定以确定最佳解决方案。在本文中，我们提出了基于《理性语音法》（RSA）框架的新型代码候选机制Codersa，旨在指导LLMS对用户意图进行更全面的务实推理。我们使用流行的代码生成数据集上的最新LLM中评估Codersa。我们的实验结果表明，Codersa始终胜过公共基线，在大多数情况下超过了最新方法，并证明了稳健的总体表现。这些发现强调了将务实推理整合到代码候选者重新骑行中的有效性，从而为提高LLM中代码生成质量提供了有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2502.15835</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大语言模型中，软令牌攻击无法可靠地审核</title>
      <link>https://arxiv.org/abs/2502.15836</link>
      <description><![CDATA[ARXIV：2502.15836V1公告类型：新 
摘要：大型语言模型（LLM）变得越来越流行。他们的紧急功能可以归因于他们的大规模培训数据集。但是，这些数据集通常包含不良或不适当的内容，例如有害文本，个人信息和受版权保护的材料。这促进了旨在从训练有素的模型中删除信息的机器研究的研究。特别是，近似学者试图通过战略性编辑模型而不是完整的模型再培训来实现信息删除。
  最近的工作表明，软令牌攻击（STA）可以从LLMS中成功提取据称是未读取的信息，从而暴露于当前未学习方法中的局限性。在这项工作中，我们揭示了Stas是审核未学习的工具不足。通过对普通学习基准测试的系统评估（谁是哈利·波特？和豆腐），我们证明了这种攻击可以从LLM中获取任何信息，无论（1）已部署的未读取算法以及（2）查询内容是否最初是最初的内容出现在培训语料库中。此外，我们表明只有少数柔软令牌（1-10）的STA可以引起400个字符的随机字符串。因此表明Stas太强大了，并且歪曲了学习方法的有效性。
  我们的工作强调了需要更好的评估基线的需求，以及更合适的审核工具来评估LLMS中学习的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.15836</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有变质关系的大语言模型中的幻觉检测</title>
      <link>https://arxiv.org/abs/2502.15844</link>
      <description><![CDATA[ARXIV：2502.15844V1公告类型：新 
摘要：大语言模型（LLM）容易幻觉，例如，在其回答中，实际上是错误的信息。这些幻觉对基于LLM的应用提出了挑战，这些应用需要高度的事实准确性。现有的幻觉检测方法主要取决于外部资源，这可能会遭受诸如低可用性，不完全覆盖范围，隐私问题，高潜伏期，低可靠性和可扩展性差的问题。还有一些方法，具体取决于输出概率，对于GPT型号（例如GPT模型）通常无法访问。本文介绍了Metaqa，这是一种独立的幻觉检测方法，利用变质关系和迅速的突变。与现有方法不同，Metaqa在没有任何外部资源的情况下运行，并且与开源和封闭源LLM兼容。 Metaqa基于以下假设：如果LLM的响应是幻觉，则将违反设计的变质关系。我们将Metaqa与最新的零资源幻觉检测方法，跨多个数据集以及两个开源和两个封闭源LLMS进行了比较。我们的结果表明，从精确度，召回和F1得分方面，Metaqa的表现优于自我检查。对于我们研究的四个LLM，MetaQA的优于自我检查的优于优势范围为0.041-0.113（精确），0.143-0.430（用于召回）和0.154-0.368（对于F1分数）。例如，在Mistral-7b的情况下，Metaqa的平均F1得分为0.435，而SelfCheckCgpt的F1分数为0.205，改善率为112.2％。 Metaqa还表现出所有不同类别的问题的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.15844</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>验证何时不确定：超越黑匣子幻觉检测</title>
      <link>https://arxiv.org/abs/2502.15845</link>
      <description><![CDATA[ARXIV：2502.15845V1公告类型：新 
摘要：大语言模型（LLMS）遭受幻觉问题的困扰，这阻碍了它们在敏感应用中的可靠性。在黑盒环境中，已经提出了几种基于自抗性的技术来进行幻觉检测。我们从经验上研究了这些技术，并表明它们的性能接近受监督的（仍然是黑盒）甲骨文的性能，这表明在此范式中几乎没有改进的空间。为了解决此限制，我们探索了目标模型和其他验证者LLM之间的跨模型一致性检查。借助这些额外的信息，我们观察到与纯粹基于自稳定的方法相比，甲骨文的性能提高了。然后，我们提出了一种预算友好的，两阶段的检测算法，该算法仅针对案例的一部分调用验证器模型。它基于自符抗性分类器的不确定性间隔，在自遇到和跨矛盾之间动态切换。我们通过内核平均嵌入镜头提供了基于一致性的幻觉检测方法的几何解释，从而提供了更深入的理论见解。广泛的实验表明，这种方法保持高检测性能，同时大大降低了计算成本。]]></description>
      <guid>https://arxiv.org/abs/2502.15845</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测边界语言模型代理能力</title>
      <link>https://arxiv.org/abs/2502.15850</link>
      <description><![CDATA[ARXIV：2502.15850V1公告类型：新 
摘要：随着语言模型（LMS）越来越多地作为自主代理人运作，准确地预测其能力对于社会准备就至关重要。我们评估了六种预测LM代理下游能力的预测方法。我们使用“一步”方法，可以直接从计算或模型发布日期（Twiptep”方法等输入指标来预测基准分数，或者首先预测中间度量的“两步”方法，例如跨基准性能（PC-1）的主要成分（PC-1）和人类评估的竞争性ELO评分。我们通过在OpenLLM 2排行榜的38 LMS数据集上对预测方法进行了对预测方法的评估。然后，我们使用经过验证的两步方法（发布日期$ \ to $ elo $ \ to $基准）来预测三个基准的Frontier模型的LM代理性能：SWE-Bench验证（软件开发），Cybench（网络安全评估），（网络安全评估），，和Rebench（ML研究工程）。我们的预测预测，到2026年初，在经过验证的SWE-Bench上，具有低功能启发的非专业LM代理将达到54％的成功率，而最先进的LM代理将达到87％的成功率。 。我们的方法不能解释推理计算扩展方面的最新进展，因此可能太保守了。]]></description>
      <guid>https://arxiv.org/abs/2502.15850</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>控制幻觉：大语言模型中指导层次结构的失败</title>
      <link>https://arxiv.org/abs/2502.15851</link>
      <description><![CDATA[ARXIV：2502.15851V1公告类型：新 
摘要：大型语言模型（LLMS）越来越多地通过分层指令方案部署，其中某些指令（例如，系统级指令）有望优先于其他指令（例如用户消息）。但是，我们对这些层次控制机制的有效程度缺乏系统的了解。我们介绍了一个基于约束优先级的系统评估框架，以评估LLMS的执行指导层次结构的很好。我们在六个最先进的LLMS进行的实验表明，即使是简单的格式冲突，模型即使是一致的指导优先级。我们发现，广泛的系统/用户提示分离无法建立可靠的指令层次结构，并且模型对某些约束类型表现出强烈的固有偏见，而不管其优先级指定如何。虽然受控的及时工程和模型微调显示出适度的改进，但我们的结果表明，指令层次结构执行并未实现强大的实现，呼吁除了表面级修改以外的更深层次的体系结构创新。]]></description>
      <guid>https://arxiv.org/abs/2502.15851</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PPC-GPT：通过修剪和经过经过经过经过经过经过经验的蒸馏的联合任务特定于大语模型的压缩</title>
      <link>https://arxiv.org/abs/2502.15857</link>
      <description><![CDATA[ARXIV：2502.15857V1公告类型：新 
摘要：将大型语言模型（LLM）压入特定于任务的小语言模型（SLM）遇到两个重大挑战：保护特定领域的知识隐私并管理有限的资源。为了应对这些挑战，我们提出了PPC-GPT，这是一种创新的保护隐私的联合框架，该框架专门设计用于通过修剪和思想链（COT）蒸馏将LLMS压缩到特定于任务的SLM中。 PPC-GPT在服务器 - 客户联合体系结构上工作，在该体系结构中，客户端将特定于任务特定的任务数据发送到服务器的LLM。然后，LLM生成综合数据及其相应的理由。随后将此合成数据用于LLM修剪和再培训过程。此外，我们利用COT知识蒸馏，利用合成数据进一步改善结构延伸的SLM的重新培训。我们的实验结果证明了PPC-GPT在各种文本生成任务中的有效性。通过将LLMS压缩到特定于任务的SLM中，PPC-GPT不仅可以实现竞争性能，而且还优先考虑数据隐私保护。]]></description>
      <guid>https://arxiv.org/abs/2502.15857</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>合成与黄金：LLM生成的标签和数据在网络欺凌检测中的作用</title>
      <link>https://arxiv.org/abs/2502.15860</link>
      <description><![CDATA[Arxiv：2502.15860V1公告类型：新 
摘要：这项研究研究了LLM生成的合成数据在网络欺凌检测中的作用。我们进行了一系列实验，其中我们用合成数据替换一些或所有真实数据，或者使用合成数据来增强真实数据。我们发现，合成网络欺凌数据可以是训练分类器检测的基础，该损害检测能够达到接近经过精神数据的分类器的绩效。将真实的数据与合成数据相结合，显示了仅在所有三个LLMS试验的测试数据上培训基线的改进。这些结果突出了合成数据作为在网络欺凌检测中的可扩展性，在道德上可行的替代方案的可行性，同时强调了LLM选择对性能结果的关键影响。]]></description>
      <guid>https://arxiv.org/abs/2502.15860</guid>
      <pubDate>Tue, 25 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>