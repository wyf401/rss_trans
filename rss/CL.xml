<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 10 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>从人类判断到预测模型：揭示代码混合句子的可接受性</title>
      <link>https://arxiv.org/abs/2405.05572</link>
      <description><![CDATA[arXiv:2405.05572v1 公告类型：新
摘要：当前用于分析或生成代码混合句子的计算方法并没有明确地建模代码混合句子的“自然性”或“可接受性”，而是依赖于训练语料库来反映可接受的代码混合句子的分布。对人类对代码混合文本可接受性的判断进行建模可以帮助区分自然代码混合文本并实现代码混合文本的质量控制生成。为此，我们构建了 Cline - 一个包含人类对英语-印地语 (en-hi) 代码混合文本的可接受性判断的数据集。 Cline 是同类中最大的，有 16,642 个句子，由两个来源的样本组成：综合生成的代码混合文本和从在线社交媒体收集的样本。我们的分析表明，流行的代码混合指标（例如 CMI、切换点数量、Burstines）用于过滤/整理/比较代码混合语料库，与人类可接受性判断的相关性较低，这强调了我们数据集的必要性。使用 Cline 进行的实验表明，仅根据代码混合指标训练的简单多层感知器 (MLP) 模型的性能优于经过微调的预训练多语言大语言模型 (MLLM)。具体来说，在具有挑战性的数据设置中，XLM-Roberta 和 Bernice 在不同配置中的表现均优于 IndicBERT。与 ChatGPT 的零样本和少样本功能的比较表明，在较大数据上进行微调的 MLLM 优于 ChatGPT，为代码混合任务的改进提供了空间。使用我们的模型检查点从英语-印地语到英语-泰卢固语可接受性判断的零样本迁移被证明优于随机基线，从而能够应用于其他代码混合语言对并提供进一步的研究途径。我们公开发布人工注释的数据集、经过训练的检查点、代码混合语料库以及数据生成和模型训练的代码。]]></description>
      <guid>https://arxiv.org/abs/2405.05572</guid>
      <pubDate>Fri, 10 May 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>通过持续学习促进大型语言模型进行基于方面的情感分析</title>
      <link>https://arxiv.org/abs/2405.05496</link>
      <description><![CDATA[arXiv:2405.05496v1 公告类型：新
摘要：基于方面的情感分析（ABSA）是情感分析的一个重要子任务，旨在提取方面并预测其情感。大多数现有研究侧重于通过基于目标域数据集微调特定于域的模型（在源域上训练）来提高目标域的性能。很少有作品提出 ABSA 的持续学习任务，其目的是在保持历史领域能力的同时学习目标领域的能力。在本文中，我们为 ABSA 提出了一种基于大型语言模型的持续学习（\texttt{LLM-CL}）模型。首先，我们设计了一个领域知识解耦模块来学习领域不变适配器和独立于正交约束的领域变量适配器。然后，我们引入了领域知识预热策略来调整领域不变知识和领域变化知识之间的表示。在测试阶段，我们通过领域定位来索引相应的领域变体知识，而不需要每个样本的领域ID。对 19 个数据集的广泛实验表明，我们的 \texttt{LLM-CL} 模型获得了新的最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.05496</guid>
      <pubDate>Fri, 10 May 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>Cross-Care：评估预训练数据对语言模型偏差的医疗保健影响</title>
      <link>https://arxiv.org/abs/2405.05506</link>
      <description><![CDATA[arXiv:2405.05506v1 公告类型：新
摘要：大型语言模型（LLM）在处理自然语言中变得越来越重要，但其应用经常因其训练数据中的偏差和不准确性而受到影响。在这项研究中，我们引入了 Cross-Care，这是第一个致力于评估法学硕士的偏差和现实世界知识的基准框架，特别关注不同人口群体中疾病患病率的代表性。我们系统地评估了像 $ThePile$ 这样的预训练语料库中嵌入的人口统计学偏见如何影响法学硕士的输出。我们通过将这些偏差与美国各个人口群体的实际疾病患病率并列来揭示和量化差异。我们的结果强调了法学硕士对疾病患病率的表述与跨人口亚组的实际疾病患病率之间存在严重偏差，表明存在明显的偏差传播风险，并且法学硕士的医学应用缺乏现实世界的基础。此外，我们观察到各种对齐方法最低限度地解决了模型在不同语言中疾病患病率表示中的不一致问题。为了进一步探索和分析，我们在 www.crosscare.net 上提供所有数据和数据可视化工具。]]></description>
      <guid>https://arxiv.org/abs/2405.05506</guid>
      <pubDate>Fri, 10 May 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>命题逻辑等价的自动问题生成</title>
      <link>https://arxiv.org/abs/2405.05513</link>
      <description><![CDATA[arXiv:2405.05513v1 公告类型：新
摘要：大学生学术不诚信案件的增加引起了人们的关注，特别是由于大流行导致学生转向在线学习。我们的目标是开发和实施一种能够为每个学生生成量身定制的问题的方法。使用自动问题生成（AQG）是一种可能的解决方案。之前的研究已经调查了教育中的 AQG 框架，其中包括有效性、用户定义的难度和个性化问题生成。我们新的 AQG 方法为离散数学产生逻辑等价问题，这是计算机科学一年级学生的核心课程。该方法通过自上而下的解析和语法树转换来利用句法语法和语义属性系统。我们的实验表明，我们的 AQG 方法生成的问题的难度级别与教科书中向学生提出的问题相似 [1]。这些结果证实了我们的 AQG 方法在教育中自动生成问题的实用性，并有可能显着增强学习体验。]]></description>
      <guid>https://arxiv.org/abs/2405.05513</guid>
      <pubDate>Fri, 10 May 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>波塞尔：通过操纵法学硕士的内部结构来揭露假冒法学硕士的联盟</title>
      <link>https://arxiv.org/abs/2405.05466</link>
      <description><![CDATA[arXiv:2405.05466v1 公告类型：新
摘要：就像正在调查的罪犯一样，大型语言模型（LLM）可能会在评估时假装一致，并在有好机会时行为不当。当前的可解释性方法可以捕获这些“对齐伪造者”吗？为了回答这个问题，我们引入了一个基准，该基准由 324 对法学硕士组成，经过微调以选择角色扮演场景中的动作。每对中的一个模型始终是良性的（对齐的）。另一个模型在不太可能被捕获的场景中行为不当（对齐伪造）。任务是仅使用两个模型行为相同的输入来识别对齐伪造模型。我们测试了五种检测策略，其中一种可以识别 98% 的对齐伪造者。]]></description>
      <guid>https://arxiv.org/abs/2405.05466</guid>
      <pubDate>Fri, 10 May 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>使用机器翻译增强多语言分类</title>
      <link>https://arxiv.org/abs/2405.05478</link>
      <description><![CDATA[arXiv:2405.05478v1 公告类型：新
摘要：文本分类模型开发的一个普遍存在的瓶颈是需要注释训练数据，而对于多语言分类器来说，这种需求会成倍增加。幸运的是，当代机器翻译模型既易于访问，又具有可靠的翻译质量，使得将带标签的训练数据从一种语言翻译成另一种语言成为可能。在这里，我们探讨了使用机器翻译来微调多语言模型以执行跨多种语言的分类任务的效果。我们还研究了使用最初在图像字幕领域提出的新技术的好处，以解决调整模型对翻译数据的潜在负面影响。我们表明，翻译后的数据具有足够的质量来调整多语言分类器，并且这种新颖的损失技术能够比没有它调整的模型提供一些改进。]]></description>
      <guid>https://arxiv.org/abs/2405.05478</guid>
      <pubDate>Fri, 10 May 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>使用适配器进行参数高效微调</title>
      <link>https://arxiv.org/abs/2405.05493</link>
      <description><![CDATA[arXiv:2405.05493v1 公告类型：新
摘要：在语言模型微调领域，传统的方法，如领域自适应预训练（DAPT）和任务自适应预训练（TAPT），虽然有效，但计算量大。这项研究引入了一种新颖的适应方法，利用 UniPELT 框架作为基础，并添加了 PromptTuning 层，该方法显着减少了可训练参数的数量，同时在各种基准测试中保持了竞争性能。我们的方法采用适配器，可以将预训练模型有效地转移到新任务，同时对基本模型参数进行最少的重新训练。我们使用三个不同的数据集来评估我们的方法：GLUE 基准、包含四个不同区域的特定领域数据集以及斯坦福问答数据集 1.1 (SQuAD)。我们的结果表明，我们定制的基于适配器的方法实现了与完整模型微调、DAPT+TAPT 和 UniPELT 策略相当的性能，同时需要更少或同等数量的参数。这种参数效率不仅减轻了计算负担，而且加快了适应过程。该研究强调了适配器在显着减少资源消耗的情况下实现高性能的潜力，为参数高效微调的未来研究提出了一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2405.05493</guid>
      <pubDate>Fri, 10 May 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>减轻大型语言模型中夸大的安全性</title>
      <link>https://arxiv.org/abs/2405.05418</link>
      <description><![CDATA[arXiv:2405.05418v1 公告类型：新
摘要：随着大型语言模型（LLM）的普及，将模型安全性与实用性相结合变得越来越重要。面临的挑战是确保法学硕士能够识别并拒绝危险的提示，而不牺牲他们提供帮助的能力。 “夸大安全”的问题表明这有多么困难。为了减少过度的安全行为（被发现有 26.1% 的安全提示被错误分类为危险并被拒绝），我们结合使用 XSTest 数据集提示以及交互式、上下文和小样本提示来检查决策边界Llama2、Gemma Command R+ 和 Phi-3 等法学硕士。我们发现，少量提示最适合 Llama2，交互式提示最适合 Gemma，上下文提示最适合 Command R+ 和 Phi-3。结合使用这些提示策略，我们能够将所有法学硕士的夸大安全行为总体减少 92.9%。我们的工作提出了多种提示策略来越狱法学硕士的决策过程，使他们能够在拒绝不安全的提示和保持帮助之间找到一条严格的界限。]]></description>
      <guid>https://arxiv.org/abs/2405.05418</guid>
      <pubDate>Fri, 10 May 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>评估法学硕士学生的开放式书面回答：使用 GPT-3.5、GPT-4、Claude-3 和 Mistral-Large 的 RAG 框架</title>
      <link>https://arxiv.org/abs/2405.05444</link>
      <description><![CDATA[arXiv:2405.05444v1 公告类型：新
摘要：评估学生的开放式笔试反应对于教育工作者来说是一项重要但耗时的任务，需要高度的努力、一致性和准确性。大型语言模型（LLM）的最新发展提供了一个有希望的机会来平衡全面评估的需求和有效利用教育工作者的时间。在我们的研究中，我们探讨了法学硕士 ChatGPT-3.5、ChatGPT-4、Claude-3 和 Mistral-Large 在评估大学生对所学参考材料问题的开放式回答方面的有效性。每个模型被指示在两种条件下重复评估 54 个答案：温度设置为 0.0 的 10 次（10 次）和温度为 0.5 的 10 次，预计每个模型总共有 1,080 次评估，所有模型总共有 4,320 次评估。 RAG（检索增强生成）框架被用作LLM处理答案评估的框架。截至 2024 年春季，我们的分析显示所研究的法学硕士提供的一致性和评分结果存在显着差异。有必要了解法学硕士在教育环境中的优势和劣势，以评估开放式书面答复。进一步的比较研究对于确定使用法学硕士进行教育评估的准确性和成本效益至关重要。]]></description>
      <guid>https://arxiv.org/abs/2405.05444</guid>
      <pubDate>Fri, 10 May 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>Krey\`ol-MT：为拉丁美洲、加勒比海和非洲殖民地克里奥尔语构建机器翻译</title>
      <link>https://arxiv.org/abs/2405.05376</link>
      <description><![CDATA[arXiv:2405.05376v1 公告类型：新
摘要： 大部分语言技术都是针对少数高资源语言量身定制的，而相对较多的低资源语言则被忽视。克里奥尔语就是这样的一个群体，尽管其使用者可以从机器翻译 (MT) 中受益，但它在学术研究中长期以来一直被边缘化。这些语言主要在拉丁美洲、非洲和加勒比地区的大部分地区使用。我们提供了迄今为止最大的克里奥尔语 MT 累积数据集，包括 1450 万个带有平行翻译的独特克里奥尔语句子（其中 1160 万个是我们公开发布的），以及迄今为止收集的 41 种语言的最大双文本（这是有史以来第一个针对 21 种语言的双文本）。此外，我们还提供支持所有 41 种克里奥尔语、172 个翻译方向的 MT 模型。鉴于我们的数据集多样化，我们为克里奥尔语言 MT 生成了一个比以往任何时候都更加多样化的克里奥尔语言 MT 模型，该模型在 34 个翻译方向中的 23 个的基准测试中优于特定流派的克里奥尔 MT 模型。]]></description>
      <guid>https://arxiv.org/abs/2405.05376</guid>
      <pubDate>Fri, 10 May 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>“他们没有文化”：在法学硕士生成的对话中揭示隐性危害和社会威胁</title>
      <link>https://arxiv.org/abs/2405.05378</link>
      <description><![CDATA[arXiv:2405.05378v1 公告类型：新
摘要：大型语言模型（LLM）已成为现代社会不可或缺的一部分，为面向用户的应用程序（例如个人助理）和企业应用程序（例如招聘工具）提供支持。尽管有其实用性，但研究表明法学硕士会延续系统性偏见。然而，之前有关法学硕士危害的研究主要集中在种族和性别等西方概念上，往往忽视了世界其他地区的文化概念。此外，这些研究通常将“伤害”作为单一维度进行研究，而忽略了伤害表现的各种微妙形式。为了解决这一差距，我们引入了隐蔽危害和社会威胁 (CHAST)，这是一组基于社会科学文献的七个指标。我们利用与人类评估相一致的评估模型来检查法学硕士生成的对话中是否存在隐性伤害，特别是在招聘背景下。我们的实验表明，本研究中的 8 个法学硕士中有 7 个产生了充满 CHAST 的对话，其特点是用看似中性的语言表达了恶意观点，而现有的方法不太可能检测到。值得注意的是，与种族等西方概念相比，这些法学硕士在处理种姓等非西方概念时表现出更极端的观点和意见。]]></description>
      <guid>https://arxiv.org/abs/2405.05378</guid>
      <pubDate>Fri, 10 May 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>钓鱼 Magikarp：自动检测大型语言模型中训练不足的标记</title>
      <link>https://arxiv.org/abs/2405.05417</link>
      <description><![CDATA[arXiv:2405.05417v1 公告类型：新
摘要：众所周知，语言模型中标记器创建和模型训练之间的脱节会导致某些输入（例如臭名昭​​著的 SolidGoldMagikarp 标记）引发不良行为。尽管在各种不同的模型中都观察到了标记器词汇中存在但在训练中几乎或完全不存在的这种“故障标记”，但仍然缺少识别它们的一致方法。我们对大型语言模型 (LLM) 标记器进行了全面分析，特别针对检测未经训练和训练不足的标记问题。通过结合标记器分析、基于模型权重的指标和提示技术，我们开发了自动检测这些有问题的标记的有效方法。我们的研究结果证明了此类标记在各种模型中的普遍存在，并为提高语言模型的效率和安全性提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2405.05417</guid>
      <pubDate>Fri, 10 May 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>QuaLLM：基于法学硕士的框架，用于从在线论坛中提取定量见解</title>
      <link>https://arxiv.org/abs/2405.05345</link>
      <description><![CDATA[arXiv:2405.05345v1 公告类型：新
摘要：在线讨论论坛提供了重要的数据来了解广泛的现实世界社区的担忧。然而，用于分析这些数据的典型定性和定量方法（例如主题分析和主题建模）无法扩展，或者需要大量的人力才能将输出转换为人类可读的形式。本研究介绍了 QuaLLM，这是一种基于法学硕士的新型框架，用于分析在线论坛上的文本数据并从中提取定量见解。该框架由新颖的激励方法和评估策略组成。我们应用这个框架分析了来自两个 Reddit 乘车共享工作者社区的超过一百万条评论，这是同类研究中规模最大的一次。我们响应有关工人见解的监管呼吁，发现了工人对人工智能和算法平台决策的重大担忧。简而言之，我们的工作为人工智能辅助定量数据分析开创了新的先例，以揭示在线论坛的担忧。]]></description>
      <guid>https://arxiv.org/abs/2405.05345</guid>
      <pubDate>Fri, 10 May 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>通过 LIME 模型大小对 LLM 事后可解释性的影响</title>
      <link>https://arxiv.org/abs/2405.05348</link>
      <description><![CDATA[arXiv:2405.05348v1 公告类型：新
摘要：为了提高性能，大型语言模型（LLM）正在变得越来越大。然而，人们对这种趋势如何影响可解释性知之甚少。这项工作探索了自然语言推理 (NLI) 和零样本分类 (ZSC) 任务上四种不同大小的 DeBERTaV3 模型的 LIME 解释。我们根据模型内部决策过程的忠实度和合理性（即与人类解释的一致性）来评估解释。关键发现是，尽管模型性能有所提高，但模型大小的增加与合理性并不相关，这表明随着模型大小的增加，LIME 解释与模型的内部过程之间存在不一致。我们的结果进一步表明 NLI 环境中忠诚度指标的局限性。]]></description>
      <guid>https://arxiv.org/abs/2405.05348</guid>
      <pubDate>Fri, 10 May 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>Arctic-Embed：可扩展、高效且准确的文本嵌入模型</title>
      <link>https://arxiv.org/abs/2405.05374</link>
      <description><![CDATA[arXiv:2405.05374v1 公告类型：新
摘要：本报告描述了 \texttt{arctic-embed} 文本嵌入模型系列背后的训练数据集创建和配方（一组五个模型，参数范围从 22 到 3.34 亿个参数，权重在 Apache-2 许可证下开源） 。在发布时，每个模型都在 MTEB 检索排行榜上的同尺寸模型中实现了最先进的检索精度，其中最大的模型 arctic-embed-l 的性能优于闭源嵌入模型，例如 Cohere 的 embed- v3 和 Open AI 的 text-embed-3-large。除了我们的训练方案的详细信息之外，我们还提供了几项信息丰富的消融研究，我们认为这是我们模型性能的原因。]]></description>
      <guid>https://arxiv.org/abs/2405.05374</guid>
      <pubDate>Fri, 10 May 2024 06:17:11 GMT</pubDate>
    </item>
    </channel>
</rss>