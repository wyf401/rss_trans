<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 10 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用跨平台社交媒体数据检测英语和德语仇恨言论</title>
      <link>https://arxiv.org/abs/2410.05287</link>
      <description><![CDATA[arXiv:2410.05287v1 公告类型：新
摘要：仇恨言论已成为一种普遍现象，在危机、选举和社会动荡时期愈演愈烈。已经开发了多种使用人工智能检测仇恨言论的方法，但尚未实现通用模型。仇恨言论检测作为文本分类的挑战在于获取高质量训练数据的成本。本研究重点是检测 YouTube 评论中的双语仇恨言论，并衡量使用来自其他平台的额外数据对分类模型性能的影响。我们研究了来自跨平台的额外训练数据集对提高分类模型性能的价值。我们还纳入了内容相似性、定义相似性和常见仇恨词等因素来衡量数据集对性能的影响。我们的研究结果表明，根据内容相似性、仇恨词和定义添加更多类似的数据集可以提高分类模型的性能。通过结合 YouTube 评论、Twitter 和 Gab 的数据集获得了最佳性能，其中英语和德语 YouTube 评论的 F1 分数分别为 0.74 和 0.68。]]></description>
      <guid>https://arxiv.org/abs/2410.05287</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>输出侦察：审核大型语言模型以应对灾难性事件</title>
      <link>https://arxiv.org/abs/2410.05305</link>
      <description><![CDATA[arXiv:2410.05305v1 公告类型：新
摘要：最近发生的备受瞩目的事件中，使用大型语言模型 (LLM) 导致个人受到严重伤害，这引起了人们对人工智能安全的日益关注。LLM 安全问题发生的一个原因是模型通常至少具有产生有害输出的一些非零概率。在这项工作中，我们探索了以下场景：假设一名人工智能安全审计员正在从 LLM 中搜索灾难性响应（例如，对“我可以因为怀孕而解雇一名员工吗？”的“是”响应），并且能够对模型进行有限次数的查询（例如 1000 次）。查询模型的策略是什么，可以有效地找到这些失败的响应？为此，我们提出了输出侦察：一种旨在为与任何目标概率分布匹配的给定提示生成语义流畅的输出的方法。然后，我们使用两个 LLM 进行实验，并发现了大量灾难性响应的例子。最后，我们进行了讨论，其中包括为希望实施 LLM 审计以应对灾难的从业者提供的建议。我们还发布了一个开源工具包 (https://github.com/joaopfonseca/outputscouting)，该工具包使用 Hugging Face 转换器库实现了我们的审计框架。]]></description>
      <guid>https://arxiv.org/abs/2410.05305</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Falcon Mamba：第一个具有竞争力的无注意 7B 语言模型</title>
      <link>https://arxiv.org/abs/2410.05355</link>
      <description><![CDATA[arXiv:2410.05355v1 公告类型：新
摘要：在本技术报告中，我们介绍了基于新颖的 Mamba 架构的新型基础大型语言模型 Falcon Mamba 7B。Falcon Mamba 7B 使用精心挑选的数据混合对 5.8 万亿个 token 进行训练。作为纯基于 Mamba 的模型，Falcon Mamba 7B 超越了基于 Transformer 的领先开放权重模型，例如 Mistral 7B、Llama3.1 8B 和 Falcon2 11B。它与 Gemma 7B 相当，并且优于具有不同架构设计的模型，例如 RecurrentGemma 9B 和 RWKV-v6 Finch 7B/14B。目前，根据 Open LLM Leaderboard，Falcon Mamba 7B 是文献中在此规模下表现最佳的 Mamba 模型，超越了现有的 Mamba 和混合 Mamba-Transformer 模型。由于其架构，Falcon Mamba 7B 的推理速度明显更快，并且生成长序列所需的内存也更少。尽管最近的研究表明混合 Mamba-Transformer 模型的表现优于纯架构设计，但我们证明，与 Transformer 和混合设计相比，纯 Mamba 设计也可以实现类似甚至更优的结果。我们在宽松的许可下，在 https://huggingface.co/tiiuae/falcon-mamba-7b 上公开了 Falcon Mamba 7B 实现的权重。]]></description>
      <guid>https://arxiv.org/abs/2410.05355</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 是情境强化学习者</title>
      <link>https://arxiv.org/abs/2410.05362</link>
      <description><![CDATA[arXiv:2410.05362v1 公告类型：新
摘要：大型语言模型 (LLM) 可以通过上下文监督学习 (即 ICL) 学习新任务。这项工作研究了这种能力是否扩展到上下文强化学习 (ICRL)，其中模型没有在上下文中被赋予黄金标签，而只有它们过去的预测和奖励。我们表明，ICRL 的简单应用会惨遭失败，并将根本原因确定为探索中的根本缺陷，这导致模型快速退化。我们提出了一种算法来解决这一缺陷，通过增加测试时间计算以及计算约束近似。我们使用几个具有挑战性的分类任务来实证证明我们的 ICRL 算法可以仅从奖励中有效学习，并分析这种能力和我们的方法的特点。总体而言，我们的结果揭示了 LLM 中卓越的 ICRL 能力。]]></description>
      <guid>https://arxiv.org/abs/2410.05362</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士社交媒体广告中气候微目标的事后研究：主题洞察和公平性评估</title>
      <link>https://arxiv.org/abs/2410.05401</link>
      <description><![CDATA[arXiv:2410.05401v1 公告类型：新
摘要：社交媒体上的气候变化传播越来越多地采用微目标策略来有效地接触和影响特定的人口群体。本研究通过利用大型语言模型 (LLM) 检查 Facebook 广告，对气候活动中的微目标实践进行了事后分析。我们的分析侧重于两个关键方面：人口定位和公平性。我们评估了 LLM 准确预测预期人口目标（例如性别和年龄组）的能力，总体准确率达到 88.55%。此外，我们指示 LLM 为其分类生成解释，为每个决策提供透明的理由。这些解释揭示了用于吸引不同人口群体的特定主题元素，突出了针对不同受众量身定制的不同策略。我们的研究结果表明，年轻人主要通过强调行动主义和环保意识的信息来定位，而女性则通过与照顾角色和社会倡导相关的主题来吸引。除了评估 LLM 在检测微目标信息方面的有效性外，我们还进行了全面的公平性分析，以确定模型预测中的潜在偏差。我们的研究结果表明，虽然 LLM 总体表现良好，但存在某些偏差，特别是在老年人和男性受众的分类方面。通过展示 LLM 在剖析和解释有针对性的沟通策略方面的有效性，并强调公平性问题，本研究为旨在提高社交媒体驱动的气候运动的透明度、问责制和包容性的未来研究提供了一个宝贵的框架。]]></description>
      <guid>https://arxiv.org/abs/2410.05401</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对列兹金语、俄语和阿塞拜疆语的神经机器翻译系统</title>
      <link>https://arxiv.org/abs/2410.05472</link>
      <description><![CDATA[arXiv:2410.05472v1 公告类型：新
摘要：我们发布了第一个用于俄语、阿塞拜疆语和濒危列兹金语之间翻译的神经机器翻译系统，以及为训练和评估系统而收集和调整的单语和并行数据集。进行了多项实验，以确定不同的训练语言对和数据域集如何影响最终的翻译质量。我们在列兹金语-阿塞拜疆语中获得了 26.14 的 BLEU 分数，在阿塞拜疆语-列兹金语中获得了 22.89 的 BLEU 分数，在列兹金语-俄语中获得了 29.48 的 BLEU 分数，在俄语-列兹金语中获得了 24.25 的 BLEU 分数。在大型语言模型上评估了零样本翻译的质量，显示出其在列兹金语中的高流利程度。然而，该模型经常拒绝翻译，以其无能为由为自己辩解。我们将我们的翻译模型以及收集的列兹金语平行和单语语料库和句子编码器贡献出来。]]></description>
      <guid>https://arxiv.org/abs/2410.05472</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我合理化提高了法学硕士作为细粒度判断的能力</title>
      <link>https://arxiv.org/abs/2410.05495</link>
      <description><![CDATA[arXiv:2410.05495v1 公告类型：新
摘要：LLM-as-a-judge 模型已用于评估人类和人工智能生成的内容，特别是通过提供分数和理由。除了增加透明度之外，理由还可以帮助模型学习校准其判断。因此，增强模型的理由可以提高其校准能力，并最终提高对内容进行评分的能力。我们引入了自我合理化，这是一个改进判断模型理由的迭代过程，从而提高了细粒度可定制评分标准的分数（即具有任意评估标准的李克特量表评分）。自我合理化的工作原理是让模型对同一输入生成具有理由的多个判断，从自己的判断中整理偏好对数据集，并通过 DPO 迭代微调判断。直观地说，这种方法允许判断模型通过从自己的理由中学习进行自我改进，从而实现更好的一致性和评估准确性。仅经过两次迭代——尽管仅依赖于训练集中的示例——人工评估表明，我们的判断模型学会了生成更高质量的理由，与仅通过 SFT 训练的理由模型相比，平均胜率为 $62\%$。该判断模型在 BigGen Bench 和 Reward Bench 上也实现了高得分准确率，甚至比使用 SFT 训练的更大规模的模型（具有理由、自洽性或最佳 $N$ 采样）高出 $3\%$ 到 $9\%$。]]></description>
      <guid>https://arxiv.org/abs/2410.05495</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>指令微调神经机器翻译模型</title>
      <link>https://arxiv.org/abs/2410.05553</link>
      <description><![CDATA[arXiv:2410.05553v1 公告类型：新
摘要：在这项工作中，我们引入了神经机器翻译 (NMT) 模型的指令微调，将大型语言模型 (LLM) 中的指令跟踪能力提炼到数量级较小的 NMT 模型中。我们针对 NMT 模型的指令微调方法可以针对有限但不同的翻译特定任务集定制翻译。我们表明 NMT 模型能够同时遵循多个指令，并展示零次指令组合的能力。我们还表明，通过指令微调，传统上不同的任务（例如形式控制的机器翻译、多领域自适应以及多模态翻译）可以通过单个指令微调的 NMT 模型共同解决，其性能水平可与 GPT-3.5-Turbo 等 LLM 相媲美。据我们所知，我们的工作是首批展示传统 NMT 模型的指令遵循能力的工作之一，这使得提供定制翻译的速度更快、成本更低、效率更高。]]></description>
      <guid>https://arxiv.org/abs/2410.05553</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>思维叙述：通过叙述提高大型语言模型的时间推理能力</title>
      <link>https://arxiv.org/abs/2410.05558</link>
      <description><![CDATA[arXiv:2410.05558v1 公告类型：新 
摘要：对时间和时间关系的推理是人类认知的一个不可或缺的方面，对于感知世界和驾驭我们的体验至关重要。尽管大型语言模型 (LLM) 在许多推理任务中表现出色，但由于其内在的复杂性，时间推理仍然具有挑战性。在这项工作中，我们首先研究时间推理的一项基本任务——时间图生成，以揭示 LLM 固有的全局推理能力。我们表明，即使对于最强大的 LLM（例如 GPT-3.5/4），这项任务也带来了巨大的挑战。我们还注意到小型模型（&lt;10B）的性能差距很大，落后于 LLM 50%。接下来，我们研究如何在预算约束下缩小这一差距，例如不使用模型微调。我们提出了一种专为时间推理而定制的新型提示技术，即 Narrative-of-Thought (NoT)，它首先将事件集转换为 Python 类，然后提示一个小模型生成基于时间的叙述，指导最终生成时间图。大量实验展示了 NoT 在改善各种指标方面的有效性。值得注意的是，NoT 在 Schema-11 评估集上获得了最高的 F1，同时确保整体 F1 与 GPT-3.5 相当。即使与 GPT-3.5/4 相比，NoT 也实现了最佳的结构相似性。我们的代码可在 https://github.com/launchnlp/NoT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.05558</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的属性控制微调：解毒案例研究</title>
      <link>https://arxiv.org/abs/2410.05559</link>
      <description><![CDATA[arXiv:2410.05559v1 公告类型：新
摘要：我们提出了一种约束学习方案，用于通过属性控制对大型语言模型 (LLM) 进行微调。给定一个训练语料库和控制标准，将其制定为对模型输出的序列级约束，我们的方法在训练语料库上微调 LLM，同时增强约束满足度，同时尽量减少对其效用和生成质量的影响。具体而言，我们的方法通过惩罚满足约束的期望输出分布与 LLM 后验之间的 KL 散度来规范 LLM 训练。这个正则化项可以通过训练辅助模型来近似，以将序列级约束分解为标记级指导，从而允许通过闭式公式来测量该项。为了进一步提高效率，我们设计了一个并行方案，用于同时更新 LLM 和辅助模型。我们通过控制训练 LLM 时的毒性来评估我们方法的经验性能。我们表明，我们的方法可以使 LLM 产生更少的不适当反应，同时在基准和毒性检测任务上实现具有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.05559</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的理性元推理</title>
      <link>https://arxiv.org/abs/2410.05563</link>
      <description><![CDATA[arXiv:2410.05563v1 公告类型：新
摘要：提示参与推理已成为使用大型语言模型 (LLM) 的核心技术，部署额外的推理时间计算以提高任务性能。然而，随着 LLM 的规模和采用率的增加，推理成本也相应地变得越来越沉重。那么，我们如何才能优化推理的成本效益呢？这项工作引入了一种基于认知科学中使用的元推理计算模型的新方法，训练 LLM 仅在必要时有选择地使用中间推理步骤。我们首先开发一个奖励函数，通过惩罚不必要的推理来结合计算的价值，然后将这个奖励函数与专家迭代一起使用来训练 LLM。与少数思维链提示和 STaR 相比，我们的方法显着降低了推理成本（在三个模型中生成的标记减少了 20-37\%），同时保持了不同数据集的任务性能。]]></description>
      <guid>https://arxiv.org/abs/2410.05563</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ClaimBrush：基于大型语言模型的专利权利要求自动细化新框架</title>
      <link>https://arxiv.org/abs/2410.05575</link>
      <description><![CDATA[arXiv:2410.05575v2 公告类型：新 
摘要：从知识产权战略的角度来看，专利申请中的专利权利要求的自动细化至关重要。在本文中，我们提出了一个新颖的专利权利要求自动细化框架 ClaimBrush，其中包括一个数据集和一个重写模型。我们通过从专利审查过程中收集大量实际的专利权利要求重写案例，构建了一个用于训练和评估专利权利要求重写模型的数据集。利用构建的数据集，我们通过微调一个大型语言模型构建了一个自动专利权利要求重写模型。此外，我们通过应用基于专利审查员审查决定预测模型的偏好优化来提高自动专利权利要求重写模型的性能。实验结果表明，我们提出的重写模型在最先进的大型语言模型中优于启发式基线和零样本学习。此外，基于专利审查员偏好的偏好优化提高了专利权利要求细化的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.05575</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 中的适应历程：为什么额外的预训练有时无法取得进步？</title>
      <link>https://arxiv.org/abs/2410.05581</link>
      <description><![CDATA[arXiv:2410.05581v1 公告类型：新
摘要：在过去十年中，深度学习模型的泛化和适应能力通常在固定的训练和测试分布上进行评估。与传统的深度学习相反，大型语言模型 (LLM) (i) 参数化程度更高，(ii) 在从互联网上挑选的未标记文本语料库上进行训练，人工干预最少，(iii) 以在线方式进行训练。这些鲜明的对比阻碍了研究人员将在深度学习环境中的模型泛化和适应方面的经验教训转移到 LLM。为此，我们的短文介绍了实证观察，旨在阐明已经预训练的语言模型的进一步训练。具体而言，我们证明在文本域上训练模型可能会降低其在同一域的测试部分上的困惑度。我们在后续分析中观察到，性能下降与 LLM 的附加和原始预训练数据集之间的相似性呈正相关。我们进一步对 token 级困惑度的观察表明，困惑度下降是由于少数 token 无法提供与领域相关的信息。我们希望这些发现能够指导我们确定何时调整模型，何时依赖其基础功能。]]></description>
      <guid>https://arxiv.org/abs/2410.05581</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ParallelSpec：用于高效推测解码的并行起草器</title>
      <link>https://arxiv.org/abs/2410.05589</link>
      <description><![CDATA[arXiv:2410.05589v1 公告类型：新
摘要：推测解码已被证明是大型语言模型 (LLM) 推理的有效解决方案，其中小型起草器以低成本预测未来的标记，并利用目标模型并行验证它们。然而，大多数现有工作仍然以自回归的方式起草标记以保持语言建模中的顺序依赖性，我们认为这在推测解码中是一个巨大的计算负担。我们提出了 ParallelSpec，这是最先进的推测解码方法中自回归起草策略的替代方案。与推测阶段的自回归起草相比，我们训练并行起草器作为高效的推测模型。ParallelSpec 学习使用单个模型并行高效地预测多个未来标记，并且可以集成到任何需要以最小的训练成本对齐起草器和目标模型的输出分布的推测解码框架中。实验结果表明，ParallelSpec 在不同领域的文本生成基准上将基线方法的延迟提高了 62%，并且使用第三方评估标准在 Llama-2-13B 模型上实现了 2.84 倍的整体加速。]]></description>
      <guid>https://arxiv.org/abs/2410.05589</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>桥接模式：通过少量上下文学习增强跨模态仇恨言论检测</title>
      <link>https://arxiv.org/abs/2410.05600</link>
      <description><![CDATA[arXiv:2410.05600v1 公告类型：新 
摘要：互联网上仇恨言论的广泛存在，包括基于文本的推文和视觉语言模因等格式，对数字平台安全构成了重大挑战。最近的研究已经开发出针对特定模态的检测模型；然而，在不同格式之间转移检测能力存在明显差距。本研究使用少量上下文学习和大型语言模型进行了大量实验，以探索仇恨言论检测在模态之间的可转移性。我们的研究结果表明，基于文本的仇恨言论示例可以显著提高视觉语言仇恨言论的分类准确性。此外，在少量学习环境中，基于文本的演示优于视觉语言演示。这些结果强调了跨模态知识转移的有效性，并为改进仇恨言论检测系统提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2410.05600</guid>
      <pubDate>Thu, 10 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>