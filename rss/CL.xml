<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 12 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>视觉-语言-动作模型综述</title>
      <link>https://arxiv.org/abs/2502.06851</link>
      <description><![CDATA[arXiv:2502.06851v1 公告类型：新
摘要：本文介绍了视觉-语言-动作 (VLA) 模型的 AI 生成评论，总结了关键方法、发现和未来方向。内容是使用大型语言模型 (LLM) 生成的，仅用于演示目的。这项工作并不代表原创研究，但强调了 AI 如何帮助实现文献综述的自动化。随着 AI 生成的内容变得越来越普遍，确保准确性、可靠性和正确的综合仍然是一个挑战。未来的研究将侧重于开发 AI 辅助文献综述的结构化框架，探索提高引用准确性、来源可信度和上下文理解的技术。通过研究 LLM 在学术写作中的潜力和局限性，本研究旨在为将 AI 集成到研究工作流程中的更广泛讨论做出贡献。这项工作是建立利用 AI 生成文献综述的系统方法的初步步骤，使学术知识综合更加高效和可扩展。]]></description>
      <guid>https://arxiv.org/abs/2502.06851</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自监督快速优化</title>
      <link>https://arxiv.org/abs/2502.06855</link>
      <description><![CDATA[arXiv:2502.06855v1 公告类型：新 
摘要：精心设计的提示对于增强大型语言模型 (LLM) 的推理能力至关重要，同时使其输出与不同领域的任务要求保持一致。但是，手动设计的提示需要专业知识和迭代实验。虽然现有的提示优化方法旨在自动化此过程，但它们严重依赖外部参考（例如基本事实或人类），限制了它们在无法获得此类数据或获取此类数据成本高昂的现实场景中的适用性。为了解决这个问题，我们提出了自监督提示优化 (SPO)，这是一个经济高效的框架，无需外部参考即可发现封闭式和开放式任务的有效提示。受提示质量直接体现在 LLM 输出中并且 LLM 可以有效评估对任务要求的遵守情况的观察结果的启发，我们纯粹从输出比较中得出评估和优化信号。具体来说，SPO 通过 LLM 评估器评估的成对输出比较来选择更优的提示，然后由 LLM 优化器将输出与任务要求对齐。大量实验表明，SPO 优于最先进的提示优化方法，以显著降低的成本（例如，现有方法的 1.1% 到 5.6%）和更少的样本（例如，三个样本）实现了相当或更优的结果。代码可在 https://github.com/geekan/MetaGPT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.06855</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 支持的自然语言到 Bash 翻译</title>
      <link>https://arxiv.org/abs/2502.06858</link>
      <description><![CDATA[arXiv:2502.06858v1 公告类型：新
摘要：Linux 系统的 Bourne-Again Shell (Bash) 命令行界面语法复杂，需要大量专业知识。使用大型语言模型 (LLM) 的自然语言到 Bash 命令 (NL2SH) 翻译功能进行命令组合可以避免这些问题。然而，由于测试数据不准确和用于确定 Bash 命令功能等价性的启发式方法不可靠，LLM 的 NL2SH 性能难以评估。我们提供了一个手动验证的 600 个指令命令对的测试数据集和一个 40,939 对的训练数据集，分别将以前的数据集的大小增加了 441% 和 135%。此外，我们提出了一种新颖的功能等价启发式方法，将命令执行与 LLM 对命令输出的评估相结合。我们的启发式方法可以以 95% 的置信度确定两个 Bash 命令的功能等效性，比以前的启发式方法提高了 16%。使用我们的测试数据集和启发式方法对流行的 LLM 进行评估表明，解析、上下文学习、权重学习和约束解码可以将 NL2SH 准确率提高多达 32%。我们的研究结果强调了数据集质量、基于执行的评估和翻译方法对于推进 NL2SH 翻译的重要性。我们的代码可在 https://github.com/westenfelder/NL2SH 上找到]]></description>
      <guid>https://arxiv.org/abs/2502.06858</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>知识图谱引导检索增强生成</title>
      <link>https://arxiv.org/abs/2502.06864</link>
      <description><![CDATA[arXiv:2502.06864v1 公告类型：新
摘要：检索增强生成 (RAG) 已成为一种有前途的技术，用于解决大型语言模型 (LLM) 生成的响应中的幻觉问题。现有的 RAG 研究主要集中于应用基于语义的方法来检索孤立的相关块，而忽略了它们的内在关系。在本文中，我们提出了一种新颖的知识图谱引导检索增强生成 (KG$^2$RAG) 框架，该框架利用知识图谱 (KG) 提供块之间的事实级关系，从而提高检索结果的多样性和连贯性。具体而言，在执行基于语义的检索以提供种子块后，KG$^2$RAG 采用 KG 引导的块扩展过程和基于 KG 的块组织过程，以组织良好的段落提供相关且重要的知识。在 HotpotQA 数据集及其变体上进行的大量实验证明了 KG$^2$RAG 与现有的基于 RAG 的方法相比在响应质量和检索质量方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.06864</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>禁忌科学：双重用途人工智能挑战基准和科学拒绝测试</title>
      <link>https://arxiv.org/abs/2502.06867</link>
      <description><![CDATA[arXiv:2502.06867v1 公告类型：新
摘要：为大型语言模型开发强大的安全基准需要开放、可重复的数据集，这些数据集可以衡量对有害内容的适当拒绝和对合法科学话语的潜在过度限制。我们提出了一个开源数据集和测试框架，用于评估主要受控物质查询的 LLM 安全机制，分析了四种主要模型对系统变化提示的响应。我们的结果揭示了不同的安全性：Claude-3.5-sonnet 展示了最保守的方法，拒绝率为 73%，允许率为 27%，而 Mistral 则试图回答 100% 的查询。GPT-3.5-turbo 表现出中等限制，拒绝率为 10%，允许率为 90%，Grok-2 记录了 20% 的拒绝和 80% 的允许。测试提示变化策略显示响应一致性下降，从单个提示的 85% 下降到五种变化的 65%。这一公开可用的基准能够系统地评估必要的安全限制与合法科学探究的潜在过度审查之间的关键平衡，同时为衡量人工智能安全实施的进展奠定基础。思路链分析揭示了安全机制中的潜在漏洞，凸显了在不过度限制理想和有效的科学论述的情况下实施强有力的保障措施的复杂性。]]></description>
      <guid>https://arxiv.org/abs/2502.06867</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>相关知识扰动问题：重新思考同一主题中的多个知识编辑</title>
      <link>https://arxiv.org/abs/2502.06868</link>
      <description><![CDATA[arXiv:2502.06868v1 公告类型：新
摘要：知识编辑已成为一种有效、精确更新大型语言模型 (LLM) 中嵌入知识的有前途的方法。在这项工作中，我们专注于同主题编辑，这涉及修改单个实体的多个属性，以确保对以实体为中心的知识进行全面和一致的更新。通过初步观察，我们发现了一个重大挑战：当前最先进的编辑方法在编辑同一主题的多个相关知识片段时会遇到困难。为了解决传统基准中缺乏相同主题的相关编辑数据的问题，我们引入了 $\text{S}^2\text{RKE}$(同主题相关知识编辑) 基准。我们的大量实验表明，只有主流的定位然后编辑方法（例如 ROME 和 MEMIT）才会出现“相关知识扰动”，即后续编辑会干扰先前的编辑。进一步分析表明，这些方法过度依赖主题信息，忽略了其他关键因素，导致编辑效果降低。]]></description>
      <guid>https://arxiv.org/abs/2502.06868</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向大型语言模型的可信检索增强生成：一项调查</title>
      <link>https://arxiv.org/abs/2502.06872</link>
      <description><![CDATA[arXiv:2502.06872v1 公告类型：新
摘要：检索增强生成 (RAG) 是一种先进的技术，旨在应对人工智能生成内容 (AIGC) 的挑战。通过将上下文检索集成到内容生成中，RAG 可提供可靠且最新的外部知识，减少幻觉并确保在各种任务中具有相关的上下文。然而，尽管 RAG 取得了成功并具有潜力，但最近的研究表明，RAG 范式也带来了新的风险，包括稳健性问题、隐私问题、对抗性攻击和问责问题。解决这些风险对于 RAG 系统的未来应用至关重要，因为它们直接影响其可信度。尽管已经开发了各种方法来提高 RAG 方法的可信度，但缺乏统一的研究视角和框架。因此，在本文中，我们旨在通过提供开发可信 RAG 系统的全面路线图来解决这一差距。我们的讨论围绕五个关键角度展开：可靠性、隐私、安全性、公平性、可解释性和责任制。对于每个角度，我们都提出了一个通用框架和分类法，提供了一种结构化的方法来理解当前的挑战、评估现有的解决方案并确定有希望的未来研究方向。为了鼓励更广泛的采用和创新，我们还重点介绍了可信赖的 RAG 系统具有重大影响的下游应用。]]></description>
      <guid>https://arxiv.org/abs/2502.06872</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过多跳心理治疗推理进行多模态认知重构治疗</title>
      <link>https://arxiv.org/abs/2502.06873</link>
      <description><![CDATA[arXiv:2502.06873v1 公告类型：新
摘要：先前的研究揭示了大型语言模型 (LLM) 支持认知重构疗法的潜力；然而，他们的重点主要放在基于文本的方法上，往往忽视了现实生活中治疗中至关重要的非语言证据的重要性。为了弥补这一差距，我们将文本认知重构扩展到多模态，并结合了视觉线索。具体来说，我们提出了一个名为多模态认知支持对话 (M2CoSC) 的新数据集，它将每个 GPT-4 生成的对话与反映虚拟客户面部表情的图像配对。为了更好地反映真实的心理治疗，其中面部表情导致对隐性情感证据的解释，我们提出了一种多跳心理治疗推理方法，明确识别和整合微妙的证据。我们对 LLM 和视觉语言模型 (VLM) 的全面实验表明，使用 M2CoSC 数据集，VLM 作为心理治疗师的表现得到了显着改善。此外，多跳心理治疗推理方法使 VLM 能够提供更周到、更有同理心的建议，优于标准提示方法。]]></description>
      <guid>https://arxiv.org/abs/2502.06873</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>群体推理排放估计网络</title>
      <link>https://arxiv.org/abs/2502.06874</link>
      <description><![CDATA[arXiv:2502.06874v1 公告类型：新 
摘要：准确的温室气体 (GHG) 排放报告对政府、企业和投资者至关重要。然而，由于实施成本高、排放因子数据库分散以及缺乏强大的行业分类方法，采用率仍然有限，尤其是在中小企业中。为了应对这些挑战，我们引入了群体推理排放估计网络 (GREEN)，这是一个人工智能驱动的碳核算框架，可标准化企业级排放估算，构建大规模基准数据集，并利用大型语言模型 (LLM) 的新型推理方法。具体来说，我们为 20,850 家具有经过验证的北美行业分类系统 (NAICS) 标签的公司编写了文本描述，并将其与碳强度因子的经济模型保持一致。通过将行业分类重新定义为信息检索任务，我们使用对比学习损失对 Sentence-BERT 模型进行微调。为了克服单阶段模型在处理数千个层次类别方面的局限性，我们提出了一种基于自然 NAICS 本体的 LLM 分类器集成群推理方法，将任务分解为多个子分类步骤。我们从理论上证明，这种方法可以降低分类不确定性和计算复杂性。对 1,114 个 NAICS 类别的实验产生了最先进的性能（83.68% Top-1、91.47% Top-10 准确率），对 20 家公司的案例研究报告平均绝对百分比误差 (MAPE) 为 45.88%。该项目可在以下网址获取：https://huggingface.co/datasets/Yvnminc/ExioNAICS。]]></description>
      <guid>https://arxiv.org/abs/2502.06874</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>混合数据还是合并模型？通过模型合并平衡大型语言模型的有用性、诚实性和无害性</title>
      <link>https://arxiv.org/abs/2502.06876</link>
      <description><![CDATA[arXiv:2502.06876v1 公告类型：新
摘要：实现大型语言模型 (LLM) 在有用性、诚实性和无害性方面的平衡对齐 (3H 优化) 是负责任的 AI 的基石，现有方法（如数据混合策略）面临着包括依赖专家知识和冲突的优化信号在内的限制。虽然模型合并通过集成专门的模型提供了一种有前途的替代方案，但其在 3H 优化方面的潜力仍未得到充分开发。本文建立了 3H 对齐 LLM 中模型合并的第一个综合基准，系统地评估了 10 个数据集中的 15 种方法（12 种无训练合并和 3 种数据混合技术），这些数据集与 5 个注释维度、2 个 LLM 系列和 2 个训练范式相关。我们的分析揭示了三个关键见解：（i）之前被忽视的 3H 维度之间的协作/冲突关系，（ii）模型合并在平衡对齐权衡方面始终优于数据混合方法，以及（iii）通过冗余组件修剪和异常值缓解解决参数级冲突的关键作用。基于这些发现，我们提出了 R-TSVM，这是一种重新加权增强的任务奇异向量合并方法，它结合了异常值感知参数权重和稀疏度自适应等级选择策略，以适应 LLM 的重尾参数分布和稀疏性，从而进一步改善了多次评估中的 LLM 对齐。我们的模型将在 https://huggingface.co/Jinluan 上提供。]]></description>
      <guid>https://arxiv.org/abs/2502.06876</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多智能体模拟器驱动法律密集型交互的语言模型</title>
      <link>https://arxiv.org/abs/2502.06882</link>
      <description><![CDATA[arXiv:2502.06882v1 公告类型：新
摘要：大型语言模型 (LLM) 具有显著先进的法律智能，但场景数据的稀缺阻碍了向交互式法律场景迈进。本文介绍了一种多智能体法律模拟驱动程序 (MASER)，通过模拟交互式法律场景来可扩展地生成合成数据。利用真实的法律案例来源，MASER 确保参与者之间的法律属性的一致性，并引入监督机制来协调参与者的性格和行为以及解决干扰。进一步构建了多阶段交互式法律评估 (MILE) 基准，以评估 LLM 在动态法律场景中的表现。大量实验证实了我们框架的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.06882</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究语境学习中语言模型的近侧发展区</title>
      <link>https://arxiv.org/abs/2502.06990</link>
      <description><![CDATA[arXiv:2502.06990v1 公告类型：新
摘要：在本文中，我们引入了一个学习分析框架，通过教育心理学中已建立的理论近侧发展区 (ZPD) 的视角来分析大型语言模型 (LLM) 的情境学习 (ICL) 行为。ZPD 描绘了学习者在没有支持的情况下能够做的事情与学习者即使在有支持的情况下也无法做的事情之间的空间。我们将这个概念应用于 ICL，根据有和没有 ICL 的单个示例上的模型性能来测量 LLM 的 ZPD。此外，我们提出了一个项目反应理论 (IRT) 模型来预测 LLM 的区域分布。我们的研究结果揭示了 ICL 的一系列复杂而多方面的行为，为理解和利用这项技术提供了新的见解。最后，我们展示了我们的框架如何在推理和微调场景中增强 LLM：（1）通过预测模型的近端发展区，我们有选择地将 ICL 应用于最有可能从演示中受益的查询，从而在推理成本和性能之间取得更好的平衡；（2）我们提出了一种类似人类的微调课程，该课程优先考虑模型 ZPD 中的示例。该课程可提高性能，我们通过分析 LLM 的训练动态来解释其有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.06990</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>揭开大型语言模型中奇异缺陷的神秘面纱</title>
      <link>https://arxiv.org/abs/2502.07004</link>
      <description><![CDATA[arXiv:2502.07004v1 公告类型：新
摘要：众所周知，大型 Transformer 模型会产生高范数 token。在视觉变换器 (ViT) 中，此类 token 已通过层的线性近似的奇异向量进行数学建模。然而，在大型语言模型 (LLM) 中，高范数 token 的根本原因仍未得到充分探索，其与 ViT 的不同属性需要新的分析框架。在本文中，我们提供了一系列近期模型的理论见解和实证验证，得出以下观察结果：i) 逐层奇异方向预测 LLM 中 token 范数的突然爆发。ii) 层的负特征值解释了其突然衰减。iii) 导致高范数 token 的计算路径在初始 token 和非初始 token 之间有所不同。 iv) 高范数标记由近似相应模块的矩阵的右前导奇异向量触发。我们展示了这些发现的两个实际应用：量化方案的改进和 LLM 签名的设计。我们的发现不仅促进了对 LLM 中奇异缺陷的理解，而且为其应用开辟了新的途径。我们期望这项工作将促进对 LLM 内部机制的进一步研究，因此将公开发布我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2502.07004</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>查找与 DIF 相关的单词：使用 LLM 和可解释 AI 预测差异项目功能</title>
      <link>https://arxiv.org/abs/2502.07017</link>
      <description><![CDATA[arXiv:2502.07017v1 公告类型：新
摘要：我们对几个基于编码器的 Transformer 大型语言模型 (LLM) 进行了微调和比较，以从项目文本中预测差异项目功能 (DIF)。然后，我们将可解释的人工智能 (XAI) 方法应用于这些模型，以识别与 DIF 相关的特定单词。数据包括 42,180 个针对 3 至 11 年级学生的英语语言艺术和数学总结性状态评估而设计的项目。在八个焦点组和参考组对中，预测 $R^2$ 的范围从 .04 到 .32。我们的研究结果表明，许多与 DIF 相关的词反映了测试蓝图中包含的次要子域，而不是应该从评估中删除的与构造无关的项目内容。这可能解释了为什么对 DIF 项目的定性审查往往会产生令人困惑或不确定的结果。我们的方法可用于在项目编写过程中筛选与 DIF 相关的单词以便立即修改，或通过突出显示文本中的关键词来帮助审查传统的 DIF 分析结果。这项研究的扩展可以提高评估计划的公平性，特别是那些缺乏资源来构建高质量项目的计划，以及在样本量不足以进行传统 DIF 分析的较小群体中。]]></description>
      <guid>https://arxiv.org/abs/2502.07017</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AIMS.au：用于分析公司报表中的现代奴隶制对策的数据集</title>
      <link>https://arxiv.org/abs/2502.07022</link>
      <description><![CDATA[arXiv:2502.07022v1 公告类型：新
摘要：尽管十多年来一直在通过立法努力解决大公司供应链中的现代奴隶制问题，但每年审查数千份声明的挑战仍然阻碍了政府监督的有效性。虽然大型语言模型 (LLM) 可以被视为自动分析和总结文档的成熟解决方案，但识别公司采取的具体现代奴隶制对策并将其与模糊的声明区分开来仍然是一项艰巨的任务。为了帮助评估和微调 LLM 以评估公司声明，我们引入了一个数据集，该数据集由 5,731 条现代奴隶制声明组成，这些声明取自澳大利亚现代奴隶制登记册并在句子级别进行注释。本文详细介绍了数据集的构建步骤，包括精心设计注释规范、选择和预处理语句以及创建高质量的注释子集以进行有效的模型评估。为了证明我们数据集的实用性，我们提出了一种机器学习方法来检测与《澳大利亚现代奴隶制法案》规定的强制性报告要求相关的句子。然后，我们按照这种方法在零样本和监督学习设置下对现代语言模型进行基准测试。]]></description>
      <guid>https://arxiv.org/abs/2502.07022</guid>
      <pubDate>Wed, 12 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>