<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 10 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>关于使用机器学习和深度学习模型进行精神疾病检测的教程</title>
      <link>https://arxiv.org/abs/2502.04342</link>
      <description><![CDATA[ARXIV：2502.04342V1公告类型：新 
摘要：社交媒体已成为理解心理健康的重要来源，为研究人员提供了一种检测诸如用户生成的职位抑郁症之类的疾病的方法。本教程提供了实用的指导，以应对在这些平台上应用机器学习和深度学习方法的共同挑战。它着重于使用不同数据集的策略，改善文本预处理以及解决诸如不平衡数据和模型评估等问题。现实世界中的例子和分步说明演示了如何有效地应用这些技术，重点是透明度，可重复性和道德考虑。通过共享这些方法，该教程旨在帮助研究人员建立更可靠且广泛适用的心理健康研究模型，从而有助于更好的早期检测和干预工具。]]></description>
      <guid>https://arxiv.org/abs/2502.04342</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Jingfang：一种传统的中药大型语言专家级医学诊断和基于综合症的治疗</title>
      <link>https://arxiv.org/abs/2502.04345</link>
      <description><![CDATA[ARXIV：2502.04345V1公告类型：新 
摘要：中医（TCM）在健康保护和疾病治疗中起着至关重要的作用，但其实际应用需要广泛的医学知识和临床经验。现有的TCM大语言模型（LLMS）表现出对医学咨询和诊断和基于不准确的综合症治疗的严重局限性。为了解决这些问题，这项研究建立了Jingfang（JF）：一种新型的TCM大语言模型，该模型展示了医学诊断和基于综合征分化的治疗的专家水平能力。我们为医疗咨询而创新了一个多代理动态协作链机制（MDCCTM），以有效且准确的诊断能力使JF能够实现JF。此外，开发了综合征剂和双阶段检索方案（DSR），以显着增强基于综合征分化的JF治疗的JF的能力。 Jingfang不仅促进了LLM的应用，而且还促进了TCM在人类健康保护和疾病治疗中的有效实践。]]></description>
      <guid>https://arxiv.org/abs/2502.04345</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用ML，DL和LLM中的Tweets/X中的多语言网络威胁检测：比较分析</title>
      <link>https://arxiv.org/abs/2502.04346</link>
      <description><![CDATA[ARXIV：2502.04346V1公告类型：新 
摘要：由于虚假信息和在社交媒体平台（例如Twitter（现在的X&#39;）上的有害内容的蔓延不断增长，网络威胁检测已成为当今数字时代的重点领域。这些网络威胁经常在推文中伪装，对个人，社区甚至国家构成了重大风险，强调需要有效检测系统。尽管以前的研究探讨了基于推文的威胁，但大部分工作仅限于特定的语言，域或位置，或依赖于单模方法，从而降低了它们对各种现实世界情景的适用性。为了解决这些差距，我们的研究重点是使用各种高级模型的多语言推文网络威胁检测。这项研究分为三个阶段：（1）我们采用了四种语言，中文，俄语和阿拉伯语收集并标记了Tweet数据集，采用基于手动和极性的标签方法来确保高质量的注释。 （2）使用机器学习（ML）和深度学习（DL）模型分别分析每个数据集，以评估其在不同语言上的性能。 （3）最后，我们将所有四个数据集组合到单个多语言数据集中，并应用的DL和大型语言模型（LLM）体系结构，以评估其在识别各种语言识别网络威胁方面的功效。我们的结果表明，在机器学习模型中，随机森林（RF）的性能最高。但是，BI-LSTM体系结构始终超过所有数据集的其他DL和LLM架构。这些发现强调了BI-LSTM在多语言网络威胁检测中的有效性。可以在此链接上找到本文的代码：https：//github.com/mmurrad/tweet-data-classification.git。]]></description>
      <guid>https://arxiv.org/abs/2502.04346</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SCALM：通过LLM检测智能合约中的不良做法</title>
      <link>https://arxiv.org/abs/2502.04347</link>
      <description><![CDATA[ARXIV：2502.04347V1公告类型：新 
摘要：随着以太坊平台继续成熟并获得广泛的用法，维持高标准的智能合同写作实践至关重要。尽管智能合约中的不良做法可能不会直接导致安全问题，但它们确实提高了遇到问题的风险。因此，为了理解并避免这些不良实践，本文介绍了对智能合约中不良实践的首次系统研究，并研究了35多个特定问题。具体而言，我们建议使用基于尺度的框架（LLMS）框架。它结合了逐步提示和检索成绩的一代（RAG），以有效地识别和解决各种不良实践。我们使用多个LLM和数据集进行的广泛实验表明，在检测智能合约中的不良实践方面，Scalm优于现有工具。]]></description>
      <guid>https://arxiv.org/abs/2502.04347</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的基于迅速的深度修剪</title>
      <link>https://arxiv.org/abs/2502.04348</link>
      <description><![CDATA[ARXIV：2502.04348V1公告类型：新 
摘要：深度修剪旨在通过简单地删除几个不太重要的变压器块，降低大语言模型的推理成本，而无需任何硬件特定的并发症。但是，我们的经验发现表明，变压器块的重要性可能是高度依赖于任务的 - 对于任务至关重要的块可以在不降低另一个任务的准确性的情况下删除任务。基于此观察结果，我们开发了一种动态的深度修剪算法，创造的布丁（及时路由的动态深度修剪），该算法决定了哪些基于输入提示符中要省略模型中的封锁。布丁通过训练轻型路由器来预测一组选项中的最佳遗漏，在此选项集中还以数据驱动的方式构建了此选项集。常识性推理基准的经验结果表明，布丁有效地加速了推理语言模型，并且比静态深度固化基线的静态深度更好。]]></description>
      <guid>https://arxiv.org/abs/2502.04348</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于LLM的对话数据捕获的动态基准测试框架</title>
      <link>https://arxiv.org/abs/2502.04349</link>
      <description><![CDATA[ARXIV：2502.04349V1公告类型：新 
摘要：大语言模型（LLM）的快速演变已经改变了对话剂，从而实现了复杂的人机相互作用。但是，评估框架通常集中在单个任务上，无法捕获多转对话的动态性质。本文介绍了一个动态的基准测试框架，以通过与合成用户的互动来评估基于LLM的对话代理。该框架集成了生成代理模拟以评估关键维度的性能：信息提取，上下文意识和自适应参与。通过模拟用户行为的各个方面，我们的工作提供了可扩展，自动化和灵活的基准测试方法。实验评估 - 在贷款应用程序用例中 - 在一次性和很少的提取条件下证明了该框架的有效性。结果表明，自适应策略提高了数据提取准确性，尤其是在处理模棱两可的响应时。未来的工作将将其适用性扩展到更广泛的领域，并包含其他指标（例如，对话连贯性，用户参与度）。这项研究为评估基于LLM的对话剂的结构化，可扩展的方法提供了促进现实世界的部署。]]></description>
      <guid>https://arxiv.org/abs/2502.04349</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CodeSteer：通过代码/文本指导的符号授权语言模型</title>
      <link>https://arxiv.org/abs/2502.04350</link>
      <description><![CDATA[ARXIV：2502.04350V1公告类型：新 
摘要：现有方法无法有效地引导大型语言模型（LLMS）在文本推理和代码生成之间，而符号计算功能未充分利用。我们介绍了CodeSteer，这是指导LLM代码/文本生成的有效方法。我们构建一个综合基准标准架，包括37个具有可调复杂性的符号任务，还合成了12K多轮指导/发电轨迹的数据集和5.5k指导比较对。我们使用新设计的多轮监督微调（SFT）和直接偏好优化（DPO）微调Llama-3-8B模型。所得的模型CodeSteerllm与所提出的符号和自我答案的检查器增强，有效地指导了较大模型的代码/文本生成。用CodeSteer增强GPT-4O的平均性能得分从53.3提高到86.4，甚至优于现有最佳LLM OpenAI O1（82.7），O1-Preview（74.8）和DeepSeek R1（76.8）（76.8）（76.8）看不见）。经过GPT-4O的培训，CodeSteer表现出卓越的概括性，为Claude，Mismtral和GPT-3.5提供了平均41.8的性能提高。 CodeSteer引导的LLM完全利用符号计算，以在高度复杂的任务上保持强劲的性能。型号，数据集和代码可在https://github.com/yongchao98/codesteer-v1.0上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.04350</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NER4ALL或上下文就是您所需要的：在历史文本上使用LLMS用于低及其高性能的NER。人文知情的方法</title>
      <link>https://arxiv.org/abs/2502.04351</link>
      <description><![CDATA[ARXIV：2502.04351V1公告类型：新 
摘要：命名实体识别（NER）是自动建立对人，地点，事件等所有参考的历史研究的核心任务。然而，要对来源的高语言和流派多样性，只有有限的拼写义务，所需的历史领域知识的水平以及带注释的培训数据的稀缺，已建立的自然语言处理方法（NLP）既昂贵又非常昂贵，而且都非常昂贵且既昂贵又非常昂贵在召回和精度方面，结果仅产生不令人满意的结果。我们的论文介绍了一种新方法。我们证明，在历史文档中，NER的F1分数较高7％至百分之二，而F1分数较高，则证明了易于获得的，最先进的LLM的表现如何显着优于两个领先的NLP框架，即Spacy和Flair。我们的消融研究表明，如何为任务提供历史背景以及一定的角色建模，这些角色建模将纯粹的语言方法转移到了成功的促进策略中的核心。我们还证明，与我们的期望相反，提供越来越多的示例的几个示例并不能提高回忆或精确度低于16杆的阈值。结果，我们的方法通过消除已建立的NLP工具所需的脚本语言和计算技能的障碍，而不是利用自然语言提示，消费者级工具和前端来使所有历史学家对NER的访问权限。]]></description>
      <guid>https://arxiv.org/abs/2502.04351</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大语言模型调查演绎推理的鲁棒性</title>
      <link>https://arxiv.org/abs/2502.04352</link>
      <description><![CDATA[ARXIV：2502.04352V1公告类型：新 
摘要：大型语言模型（LLM）已被证明可以为许多基于推理的自然语言处理（NLP）任务取得令人印象深刻的结果，这表明一定程度的演绎推理能力。但是，尚不清楚在非正式和自动化方法中，LLM在逻辑扣除任务上均具有强大的范围。此外，尽管已经提出了许多基于LLM的推论方法，但缺乏系统的研究来分析其设计组件的影响。在解决这两个挑战时，我们提出了第一个研究基于LLM的演绎推理方法的鲁棒性。我们设计了一个有两个扰动家族的框架：对抗性噪声和反事实陈述，共同生成七个扰动数据集。我们根据其推理格式，形式化语法和错误恢复的反馈来组织LLM推理者的景观。结果表明，对抗噪声会影响自动化，而反事实陈述影响所有方法。尽管降低了语法错误，但详细的反馈并不能提高整体准确性，这表明基于LLM的方法对有效自我校正的挑战。]]></description>
      <guid>https://arxiv.org/abs/2502.04352</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>认知：自动化艺术分析和解码审美元素的大型语言模型</title>
      <link>https://arxiv.org/abs/2502.04353</link>
      <description><![CDATA[ARXIV：2502.04353V1公告类型：新 
摘要：作为一种通用语言，艺术可以通过各种方式来解释，艺术品体现了深刻的含义和细微差别。大型语言模型（LLM）的出现以及多模式大语言模型（MLLM）的可用性提出了一个问题，即如何使用这些变革模型来评估和解释艺术品的艺术要素。据我们所知，尽管在该领域进行了研究，但尚未探讨对使用LLMS的艺术品的技术和表达特征深入详细的理解。在这项研究中，我们研究了正式的艺术分析框架的自动化，以迅速分析大量的艺术品，并检查其模式如何随着时间的流逝而发展。我们探讨了LLM如何解码艺术表达，视觉元素，组成和技术，从而揭示跨时期发展的新兴模式。最后，我们在这种情况下讨论了LLM的优势和局限性，强调了它们处理大量与艺术相关数据并产生有见地的解释的能力。由于结果的详尽和颗粒状性质，我们开发了交互式数据可视化，在线可用https://cognartive.github.io/，以增强理解和可及性。]]></description>
      <guid>https://arxiv.org/abs/2502.04353</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>恢复经典：大语言模型对齐方式的主动奖励建模</title>
      <link>https://arxiv.org/abs/2502.04354</link>
      <description><![CDATA[ARXIV：2502.04354V1公告类型：新 
摘要：从人类偏好中构建神经奖励模型是从人类反馈（RLHF）和大语言模型对准研究中学习的关键组成部分。鉴于人类注释的稀缺性和高成本，如何选择最有用的对注释是一个必不可少但具有挑战性的开放问题。在这项工作中，我们强调了一个见解，即用于奖励建模的理想比较数据集应平衡对表示空间的探索，并在对之间进行中等奖励差异之间的内容比较。从技术上讲，挑战在量化这两个目标并有效优先考虑要注释的比较时会出现挑战。为了解决这个问题，我们提出了基于Fisher信息的选择策略，调整经典实验设计文献的理论，并将其应用于基于深度神经网络的奖励建模任务的最终线性层。从经验上讲，与来自多个开源LLM和数据集中的深度学习和经典统计文献中的其他选择方法相比，我们的方法表现出显着的性能，高计算效率和稳定性。进一步的消融研究表明，在主动奖励建模中纳入交叉预测比较会显着提高标签效率，从而阐明了改善RLHF注释策略的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.04354</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM-PROS：分析大语言模型在竞争问题解决方面的表现</title>
      <link>https://arxiv.org/abs/2502.04355</link>
      <description><![CDATA[ARXIV：2502.04355V1公告类型：新 
摘要：大型语言模型的快速发展已为自动化复杂的问题解决任务（例如算法编码和竞争性编程）开辟了新的途径。本文介绍了一种新颖的评估技术LLM-PRO，以评估国际大学编程竞赛（ICPC）问题最先进的LLMS的表现。从2011年到2024年，使用166个世界决赛问题的策划数据集，我们将模型的推理，准确性和效率进行基准测试。我们评估了五种型号GPT-4O，Mistral大，Llama-3.1-405b和O1家族，由O1-Mini和O1-preiview组成，涉及正确性，资源利用和响应校准等关键指标。我们的结果揭示了模型能力概括，适应和解决新问题的能力的显着差异。我们还研究了培训方法，数据集污染和经过思考推理对模型性能的影响。这些发现为优化算法任务的LLM提供了新的见解，突出了当前模型的优势和局限性。]]></description>
      <guid>https://arxiv.org/abs/2502.04355</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医疗保健中的开放基础模型：挑战，悖论和Genai驱动的个性化处方</title>
      <link>https://arxiv.org/abs/2502.04356</link>
      <description><![CDATA[ARXIV：2502.04356V1公告类型：新 
摘要：回应专有大型语言模型（LLM）的成功，例如OpenAI的GPT-4，人们对开发开放的非专有LLMS和AI基础模型（AIFMS（AIFM）（用于透明）用于学术，科学，科学，科学，科学，科学，科学，科学，科学，和非商业应用程序。尽管他们无法与专有同行的精致功能相匹配，但开放模型仍具有彻底改变医疗保健应用的巨大潜力。在本文中，我们研究了开源LLM和AIFM的前景，用于开发医疗保健应用并做出两个关键贡献。首先，我们对当前最新的开源医疗保健LLM和AIFM进行了全面调查，并介绍了这些开放AIFM的分类法，从而在各种医疗保健任务中对其实用性进行了分类。其次，为了评估开放LLM在医疗保健中的通用应用，我们提出了一项有关个性化处方的案例研究。该任务尤其重要，因为它在提供量身定制的特定于患者药物方面的关键作用可以极大地改善治疗结果。此外，我们将开源模型的性能与具有和不检索发电机（RAG）的设置中的专有模型进行了比较。我们的发现表明，尽管不太精致，但开放式LLM与与抹布等接地技术配对时，可以实现与专有模型相当的性能。此外，为了强调LLMS授权个性化处方的临床意义，我们通过专业的临床医生进行主观评估。我们还详细阐述了与滥用强大的LLM和AIFM相关的道德考虑和潜在风险，强调了对医疗保健中谨慎和负责任的实施的需求。]]></description>
      <guid>https://arxiv.org/abs/2502.04356</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重复使用嵌入：在没有GPU的大型语言模型对齐中可重现的奖励模型研究</title>
      <link>https://arxiv.org/abs/2502.04357</link>
      <description><![CDATA[ARXIV：2502.04357V1公告类型：新 
摘要：大型语言模型（LLM）通过加强学习（RL）在结构化任务中取得了长足的进步，证明了数学推理和代码生成的水平。但是，将RL应用于聊天机器人和内容产生等较广泛的领域（通过称为增强人类反馈（RLHF）的加强学习过程）提出了独特的挑战。 RLHF中的奖励模型至关重要，充当代理，评估LLM输出与人类意图的一致性。尽管取得了进步，但奖励模型的发展仍受到诸如计算重型培训，昂贵的评估以及因此可重复性差的挑战所阻碍。我们主张在奖励模型研究中使用基于嵌入的输入作为对这些挑战的加速解决方案。通过利用嵌入式进行奖励建模，我们可以提高可重复性，减少对硬件的计算需求，提高训练稳定性，并大大降低培训和评估成本，从而促进该活跃研究领域的公平有效比较。然后，我们展示了使用基于嵌入的奖励模型重现现有奖励模型集成研究的案例研究。我们讨论了未来研究的途径，旨在为更安全，更有效的LLM部署做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2502.04357</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>位置：缩放LLM代理需要使用LLM原始素渐近分析</title>
      <link>https://arxiv.org/abs/2502.04358</link>
      <description><![CDATA[ARXIV：2502.04358V1公告类型：新 
摘要：将硬问题分解为子问题通常会使它们更容易，更有效地解决。借助大型语言模型（LLMS）越过关键的可靠性阈值，可以使越来越多的能力范围越来越多地将系统分解为基于LLM的代理人，每个人都可以将系统分解为基于LLM的代理。但是，这种分解（即使自动化）通常是直观的，例如，基于人类如何将角色分配给人类团队的成员。这些角色分解与最佳分解有多近？该立场论文认为，需要使用LLM原语的渐近分析来推理这种分解系统的效率，并且此类分析的见解将解锁扩展它们的机会。通过将LLM正向通行视为计算成本的原子单位，可以将特定LLM的（通常是不透明的）内部运作（通常是不透明的）内部工作与固有效率的固有效率分开，即如何策划一组LLM来解决硬性问题。换句话说，如果我们想将LLM的部署扩展到极限，而不是拟人化LLM，则应使用LLM原始素的渐近分析来推理和发展更强大的大问题分解为LLM Adents。]]></description>
      <guid>https://arxiv.org/abs/2502.04358</guid>
      <pubDate>Mon, 10 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>