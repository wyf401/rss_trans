<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 22 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>采用两阶段音译方法提高多语言 ASR 的性能</title>
      <link>https://arxiv.org/abs/2410.14709</link>
      <description><![CDATA[arXiv:2410.14709v1 公告类型：新
摘要：端到端自动语音识别 (ASR) 系统正迅速成为其他建模方法中最先进的系统。已经引入了几种技术来提高它们处理多种语言的能力。然而，由于不同语言的书写脚本存在差异，在解码声学上相似的单元时，它们并不总是映射到目标语言中的适当字素。这限制了模型在处理代码混合场景中的多种语言时的可扩展性和适应性。本文提出了一种构建语言无关的端到端模型的方法，该模型在通过将多语言字素数据投影到更通用的目标语言脚本中获得的字素集上进行训练。这种方法使声学模型免于重新训练以跨越更大的空间，并且可以轻松扩展到多种语言。两阶段音译过程实现了这种方法，并证明可以最大限度地减少语音类别混淆。我们针对两种印度语（尼泊尔语和泰卢固语）进行了端到端多语言语音识别系统的实验。这些语言的原始字素空间被投影到梵文脚本。与其他语言相关的建模方法相比，我们在音译空间中实现了单词错误率 (WER) 相对降低 20%，字符错误率 (CER) 相对降低 24%。]]></description>
      <guid>https://arxiv.org/abs/2410.14709</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 CycleQD 实现大型语言模型的代理技能获取</title>
      <link>https://arxiv.org/abs/2410.14735</link>
      <description><![CDATA[arXiv:2410.14735v1 公告类型：新
摘要：训练大型语言模型以获得特定技能仍然是一项具有挑战性的任务。传统的训练方法通常会遇到数据分布不平衡和目标函数不足的问题，这些不足与特定任务的性能不太匹配。为了应对这些挑战，我们引入了 CycleQD，这是一种新方法，它通过算法的循环调整以及基于模型合并的交叉和基于 SVD 的突变来利用质量多样性框架。在 CycleQD 中，每个任务的性能指标交替作为质量度量，而其他指标则作为行为特征。这种对单个任务的循环关注允许一次将精力集中在一个任务上，从而无需调整数据比率并简化目标函数的设计。 AgentBench 的实证结果表明，将 CycleQD 应用于基于 LLAMA3-8B-INSTRUCT 的模型不仅使它们能够超越编码、操作系统和数据库任务中的传统微调方法，而且在这些领域中实现了与 GPT-3.5-TURBO 相当的性能，后者可能包含更多参数。至关重要的是，这种增强的性能是在保留强大的语言能力的同时实现的，这一点从其在广泛采用的语言基准任务上的表现就可以看出。我们重点介绍了 CycleQD 中的关键设计选择，详细说明了这些选择如何有助于其有效性。此外，我们的方法是通用的，可以应用于图像分割模型，突出了其在不同领域的适用性。]]></description>
      <guid>https://arxiv.org/abs/2410.14735</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>引出思路链中的不确定性，以减轻对预测有害用户行为的偏见</title>
      <link>https://arxiv.org/abs/2410.14744</link>
      <description><![CDATA[arXiv:2410.14744v1 公告类型：新
摘要：对话预测任务是让模型预测正在进行的对话的结果。例如，它可以应用于社交媒体审核，以在有害用户行为发生之前进行预测，从而进行预防性干预。虽然大型语言模型 (LLM) 最近被提议作为一种有效的对话预测工具，但目前尚不清楚它们可能存在哪些偏见，尤其是在预测我们要求它们在审核期间预测的（潜在有害）结果方面。本文探讨了模型不确定性在多大程度上可以用作减轻潜在偏见的工具。具体来说，我们提出了三个主要研究问题：1) 当我们要求模型表示其不确定性时，LLM 预测准确性如何变化；2) 当我们要求模型表示其不确定性时，LLM 偏差如何变化；3) 我们如何使用不确定性表示来减少或完全减轻偏见，而无需太多训练数据点。我们针对 5 个开源语言模型解决了这些问题，这些模型在 2 个数据集上进行了测试，旨在评估社交媒体审核的对话预测。]]></description>
      <guid>https://arxiv.org/abs/2410.14744</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SemiEvol：针对 LLM 自适应的半监督微调</title>
      <link>https://arxiv.org/abs/2410.14745</link>
      <description><![CDATA[arXiv:2410.14745v1 公告类型：新
摘要：监督微调 (SFT) 对于将大型语言模型 (LLM) 适应特定领域或任务至关重要。然而，实际应用中可用的标记数据数量有限，这对 SFT 产生令人满意的结果提出了严峻挑战。因此，人们非常期待一个能够充分利用标记和未标记数据进行 LLM 微调的数据高效框架。为此，我们引入了一个半监督微调框架 SemiEvol，以从传播和选择的方式进行 LLM 适应。对于知识传播，SemiEvol 采用双层方法，通过权重和上下文方法将知识从标记数据传播到未标记数据。对于知识选择，SemiEvol 结合了协作学习机制，选择更高质量的伪响应样本。我们使用 GPT-4o-mini 和 Llama-3.1 在 7 个通用或领域特定数据集上进行了实验，结果表明模型在目标数据上的性能有显著提升。此外，我们还将 SemiEvol 与 SFT 和自进化方法进行了比较，突出了其在混合数据场景中的实用性。]]></description>
      <guid>https://arxiv.org/abs/2410.14745</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型不确定性估计中对谄媚行为的解释</title>
      <link>https://arxiv.org/abs/2410.14746</link>
      <description><![CDATA[arXiv:2410.14746v1 公告类型：新
摘要：有效的人机协作需要机器学习模型将不确定性外化，以便用户可以在必要时进行反思和干预。对于语言模型，这些不确定性的表示可能会受到谄媚偏见的影响：倾向于同意用户的意见，即使他们是错误的。例如，模型可能对用户建议的（不正确的）问题解决方案过于自信。我们首次研究了谄媚和不确定性估计之间的关系。我们提出了谄媚偏见定义的概括，以衡量对不确定性估计的下游影响，并提出了一种新算法（SyRoUP）来解释不确定性估计过程中的谄媚。与以前关于谄媚的研究不同，我们研究了广泛的用户行为，改变了用户建议的正确性和置信度，以了解模型答案（及其确定性）如何变化。我们在对话预测和问答任务中开展的实验表明，用户信心在调节谄媚效应方面起着关键作用，而 SyRoUP 可以更好地预测这些效应。从这些结果来看，我们认为，将模型和用户的不确定性外化有助于减轻谄媚偏见的影响。]]></description>
      <guid>https://arxiv.org/abs/2410.14746</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可控的意图发现：使用半监督对比学习的增量深度聚类</title>
      <link>https://arxiv.org/abs/2410.14755</link>
      <description><![CDATA[arXiv:2410.14755v1 公告类型：新
摘要：从对话式 AI 系统获取价值取决于用户将先验知识转化为配置的能力。在大多数情况下，发现一组相关的回合级说话者意图通常是关键步骤之一。纯无监督算法提供了一种解决发现问题的自然方法，但很难纳入约束，并且对结果的控制非常有限。先前的研究表明，半监督（深度）聚类技术可以让系统在意图发现过程中纳入先验知识和约束。然而，他们没有解决如何通过人工反馈进行控制的问题。在我们的可控意图发现 (CDI) 框架中，领域和先验知识是通过对未标记数据进行一系列无监督对比学习，然后对部分标记数据进行微调，最后通过重复聚类和伪标签微调迭代细化聚类和表示来整合的。此外，我们借鉴了持续学习文献，并使用“不遗忘学习”来防止在训练阶段发生灾难性遗忘。最后，我们展示了这种深度聚类过程如何成为人机参与的增量发现策略的一部分。我们报告了 CLINC 和 BANKING 数据集的结果。CDI 的表现远胜于之前的作品：分别为 10.26% 和 11.72%。]]></description>
      <guid>https://arxiv.org/abs/2410.14755</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现医学法学硕士 (LLM) 中偏见模式的可扩展评估</title>
      <link>https://arxiv.org/abs/2410.14763</link>
      <description><![CDATA[arXiv:2410.14763v1 公告类型：新
摘要：大型语言模型 (LLM) 在帮助解决众多医学挑战方面表现出了令人印象深刻的潜力。然而，在医学等高风险应用中部署 LLM 带来了许多担忧。一个主要关注的领域涉及 LLM 在医学应用中的偏见行为，导致对个人的不公平待遇。为了为负责任和有影响力地部署医学 LLM 铺平道路，严格的评估是一个关键的先决条件。由于不同医疗场景的巨大复杂性和多变性，该领域的现有工作主要依赖于使用手工制作的数据集进行偏见评估。在本研究中，我们提出了一种新方法，通过基于严格的医学证据自动生成测试用例来扩大此类偏见评估。我们专门针对以下挑战：a) 偏见表征的领域特异性，b) 在生成测试用例时产生幻觉，以及 c) 健康结果和敏感属性之间的各种依赖关系。为此，我们提供了新方法来应对这些挑战，这些方法与我们的生成流程集成，在我们的方法中使用医学知识图谱、医学本体和定制的通用 LLM 评估框架。通过一系列广泛的实验，我们表明，我们提出的方法生成的测试用例可以有效地揭示医学 LLM 中的偏差模式，其规模比人工制作的数据集更大、更灵活。我们使用我们的流程发布了一个大型偏差评估数据集，该数据集专用于一些医学案例研究。我们的晕影生成应用程序的现场演示可在 https://vignette.streamlit.app 上找到。我们的代码也可在 https://github.com/healthylaife/autofair 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.14763</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨文档事件键控摘要</title>
      <link>https://arxiv.org/abs/2410.14795</link>
      <description><![CDATA[arXiv:2410.14795v1 公告类型：新
摘要：事件键控摘要 (EKS) 需要根据文档和从中提取的事件表示生成关于文档中描述的特定事件的摘要。在这项工作中，我们将 EKS 扩展到跨文档设置 (CDEKS)，其中摘要必须综合来自多个来源提供的同一事件的描述信息。我们引入了 SEAMUS（跨多个来源的事件摘要），这是一个高质量的 CDEKS 数据集，基于专家对 FAMUS 数据集的重新注释，用于跨文档参数提取。我们在 SEAMUS 上提出了一套基线，涵盖了较小的、经过微调的模型，以及零样本和少量样本提示的 LLM，以及详细的消融和人工评估研究，表明 SEAMUS 是这项新任务的宝贵基准。]]></description>
      <guid>https://arxiv.org/abs/2410.14795</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自然语言的孤立因果效应</title>
      <link>https://arxiv.org/abs/2410.14812</link>
      <description><![CDATA[arXiv:2410.14812v1 公告类型：新
摘要：随着语言技术的普及，了解语言变化如何影响读者感知非常重要——形式化为某些焦点语言编码干预对外部结果的孤立因果效应。估计孤立效应的核心挑战是需要近似干预之外的所有非焦点语言。在本文中，我们引入了一个孤立因果效应的正式估计框架，并探讨了非焦点语言的不同近似如何影响效应估计。借鉴遗漏变量偏差原理，我们提出了沿保真度和重叠轴评估孤立效应估计和非焦点语言近似质量的指标。在半合成和真实世界数据的实验中，我们验证了我们的框架恢复地面真实孤立效应的能力，并证明了我们提出的指标作为孤立效应估计和非焦点语言近似质量度量的实用性。]]></description>
      <guid>https://arxiv.org/abs/2410.14812</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>软域转移和命名实体信息对欺骗检测的影响</title>
      <link>https://arxiv.org/abs/2410.14814</link>
      <description><![CDATA[arXiv:2410.14814v1 公告类型：新
摘要：在现代社会，大量的交流都发生在线上，很难知道写的东西是真实的还是欺骗性的。人们在网上欺骗的原因有很多（例如，金钱利益、政治利益），在没有任何身体互动的情况下检测这种行为是一项艰巨的任务。此外，欺骗发生在几个纯文本领域，目前还不清楚是否可以利用这些不同的来源来提高检测能力。为了解决这个问题，利用了来自不同领域的八个数据集来评估它们与通过微调的 BERT 模型的中间层连接进行迁移学习相结合时对分类器性能的影响。我们发现准确度比基线有所提高。此外，我们评估了数据集之间的多个距离测量，发现 Jensen-Shannon 距离与迁移学习性能有中等相关性。最后，评估了多种方法对 BERT 性能的影响，这些方法通过命名实体在数据集的文本中生成附加信息，我们发现准确率显著提高了 11.2%。]]></description>
      <guid>https://arxiv.org/abs/2410.14814</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用持续预训练和合成语料库将多语言 LLM 适配到低资源语言</title>
      <link>https://arxiv.org/abs/2410.14815</link>
      <description><![CDATA[arXiv:2410.14815v1 公告类型：新
摘要：多语言 LLM 支持多种语言；然而，它们的性能对于资源匮乏的语言来说并不理想。在这项工作中，我们强调了持续预训练多语言 LLM 以及使用基于翻译的合成预训练语料库来改进资源匮乏语言的 LLM 的重要性。我们在资源匮乏的印度语 Hindi 的背景下进行了研究。我们介绍了 Nemotron-Mini-Hindi 4B，这是一种基于 Nemotron-Mini 4B 的双语 SLM，支持 Hindi 和英语。该模型使用真实和合成的 Hindi + 英语标记进行训练，并对 400B 个标记进行持续预训练。我们证明基础模型和指导模型都在 Hindi 基准上取得了最先进的结果，同时在英语任务上保持竞争力。此外，我们观察到持续的预训练方法提高了模型的整体事实准确性。]]></description>
      <guid>https://arxiv.org/abs/2410.14815</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于复杂性的组合性理论</title>
      <link>https://arxiv.org/abs/2410.14817</link>
      <description><![CDATA[arXiv:2410.14817v1 公告类型：新
摘要：人们认为组合性是智能的基础。在人类中，它是思维、语言和高级推理结构的基础。在人工智能中，组合表示可以实现一种强大的分布外泛化形式，其中模型系统地适应已知概念的新组合。然而，虽然我们对组合性是什么有很强的直觉，但目前还没有可测量和数学的正式定义。在这里，我们提出了这样一个定义，我们称之为表征组合性，它解释并扩展了我们对组合性的直觉。这个定义在概念上很简单、定量、以算法信息论为基础，适用于任何表示。直观地说，表征组合性表明组合表示满足三个属性。首先，它必须具有表现力。其次，必须能够将表征重新描述为具有可重新组合部分的离散符号序列函数，类似于自然语言中的句子。第三，将这些符号序列与表征联系起来的函数（类似于自然语言中的语义）必须简单。通过对合成数据和现实世界数据进行实验，我们验证了组合性的定义，并展示了它如何统一人工智能和认知科学领域不同文献中的不同直觉。我们还表明，表征组合性虽然在理论上难以解决，但可以使用标准深度学习工具轻松估计。我们的定义有可能启发设计新颖的、理论驱动的模型，以更好地捕捉组合思维的机制。]]></description>
      <guid>https://arxiv.org/abs/2410.14817</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SPRIG：通过系统提示优化提高大型语言模型性能</title>
      <link>https://arxiv.org/abs/2410.14826</link>
      <description><![CDATA[arXiv:2410.14826v1 公告类型：新
摘要：大型语言模型 (LLM) 在许多场景中都表现出令人印象深刻的功能，但它们的性能在一定程度上取决于提示的选择。过去的研究主要集中在优化特定于任务的提示。然而，人们对优化提示中包含的一般指令（称为系统提示）的关注较少。为了解决这一差距，我们提出了 SPRIG，这是一种基于编辑的遗传算法，它从预先指定的组件迭代构建提示，以最大限度地提高模型在一般场景中的性能。我们在 47 种不同类型的任务集合上评估系统提示的性能，以确保通用性。我们的研究发现，单个优化的系统提示的性能与针对每个单独任务优化的任务提示相当。此外，结合系统和任务级优化可以进一步改进，这展示了它们的互补性。实验还表明，优化的系统提示可以有效地跨模型系列、参数大小和语言进行推广。这项研究深入了解了系统级指令在最大化 LLM 潜力方面的作用。]]></description>
      <guid>https://arxiv.org/abs/2410.14826</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DFlow：使用大型语言模型进行多样化对话流模拟</title>
      <link>https://arxiv.org/abs/2410.14853</link>
      <description><![CDATA[arXiv:2410.14853v1 公告类型：新
摘要：开发基于语言模型的对话代理需要有效的数据来训练可以遵循特定任务逻辑的模型。然而，大多数现有的数据增强方法都侧重于在话语层面增加语言、主题或对话行为的多样性，而很大程度上忽略了对话层面任务逻辑多样性的一个关键方面。本文提出了一种新颖的数据增强方法，旨在通过关注任务执行逻辑来增强合成对话的多样性。我们的方法使用 LLM 生成决策树结构的任务计划，从而可以为给定任务推导出不同的对话轨迹。每个轨迹（称为“对话流”）指导生成遵循独特轨迹的多轮对话。我们应用此方法生成一个面向任务的对话数据集，其中包含 15 个不同领域的 3,886 个对话流。我们使用下一步行动预测任务验证了此数据集的有效性，其中在我们的数据集上微调的模型优于包括 GPT-4 在内的强大基线。本文被接受后，我们计划公开发布代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2410.14853</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>哪些 LLM 难以检测？详细分析导致 LLM 文本检测困难的潜在因素</title>
      <link>https://arxiv.org/abs/2410.14875</link>
      <description><![CDATA[arXiv:2410.14875v1 公告类型：新
摘要：随着 LLM 的可访问性不断提高，LLM 生成的文本已在多个领域激增，例如科学、学术和创意写作。然而，LLM 并不是平等创建的；它们可能具有不同的架构和训练数据集。因此，一些 LLM 可能比其他 LLM 更难检测。使用两个涵盖四个写作领域的数据集，我们使用 LibAUC 库（一个用于训练具有不平衡数据集的分类器的深度学习库）训练 AI 生成的 (AIG) 文本分类器。我们在 Deepfake Text 数据集中的结果表明，AIG 文本检测在不同领域有所不同，科学写作相对具有挑战性。在专注于学生论文的 Rewritten Ivy Panda (RIP) 数据集中，我们发现 OpenAI 系列 LLM 对于我们的分类器来说很难与人类文本区分开来。此外，我们探讨了可能解释检测 OpenAI 生成的文本困难的可能因素。]]></description>
      <guid>https://arxiv.org/abs/2410.14875</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>