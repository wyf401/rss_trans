<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 04 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>CALF：以汉语考试为标杆的LFQA评估</title>
      <link>https://arxiv.org/abs/2410.01945</link>
      <description><![CDATA[arXiv:2410.01945v1 公告类型：新
摘要：长篇问答系统 (LFQA) 是指针对开放式问题生成深入的段落级答案。尽管已经开发了许多 LFQA 方法，但由于其复杂性和成本高，有效且高效地评估 LFQA 仍然具有挑战性。因此，到目前为止，还没有 LFQA 评估的标准基准。为了解决这一空白，我们首次尝试提出一个结构良好、基于参考的基准，即 LFQA 评估中文考试 (CALF)，旨在严格评估 LFQA 自动评估指标的性能。CALF 基准源自已翻译成英文的中文考试问题。它包括多达 1476 个由知识密集型和细致入微的答案组成的示例。我们的评估包括三种不同的设置，以全面分析自动指标的行为。我们对 7 个传统评估指标、3 个基于提示的指标和 3 个经过训练的评估指标进行了广泛的实验，并在代理系统上进行了 LFQA 评估测试。结果表明，当前的自动评估指标均无法与人类相比，这表明它们无法很好地捕获长格式响应中包含的密集信息。此外，我们还详细分析了自动评估指标在评估 LFQA 时失败的原因，为推进 LFQA 评估系统提供了宝贵的见解。数据集和相关代码可以在我们的 GitHub 存储库中访问。]]></description>
      <guid>https://arxiv.org/abs/2410.01945</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SciPrompt：基于知识的提示，实现科学主题的细粒度分类</title>
      <link>https://arxiv.org/abs/2410.01946</link>
      <description><![CDATA[arXiv:2410.01946v1 公告类型：新
摘要：基于提示的微调已成为从预训练语言模型中获取编码信息的重要方法，可用于各种任务，包括文本分类。对于多类分类任务，在资源匮乏的情况下，基于提示的微调可实现与完全微调方法相当的性能水平。先前的研究使用精心设计的提示模板和言语化器，从标签术语空间映射到类空间，将分类问题作为掩码语言建模任务来解决。然而，使用自动丰富的言语化器进行跨领域和细粒度的基于提示的微调仍未得到探索，这主要是因为手动为言语化器选择领域标签术语的难度和成本很高，这需要具有领域专业知识的人。为了应对这一挑战，我们引入了 SciPrompt，这是一个旨在自动检索科学主题相关术语的框架，用于资源匮乏的文本分类任务。为此，我们在科学文献的背景下选择了语义相关且领域特定的标签术语来增强语言化器。此外，我们提出了一种新的语言化策略，该策略使用相关性分数作为附加权重来增强模型调整期间语言模型的预测性能。在少数和零样本设置下的科学文本分类任务中，我们的方法优于最先进的基于提示的微调方法，尤其是在对细粒度和新兴科学主题进行分类时。]]></description>
      <guid>https://arxiv.org/abs/2410.01946</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TypedThinker：类型化思维改善大型语言模型推理</title>
      <link>https://arxiv.org/abs/2410.01952</link>
      <description><![CDATA[arXiv:2410.01952v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 的推理能力取得了重大进步，但缺乏多样化的推理解决方案常常使它们陷入有限的解决方案搜索领域。在本文中，我们提出了 TypedThinker，这是一个新颖的框架，通过结合多种推理类型（演绎、归纳、溯因和类比）来增强 LLM 的解决问题的能力。我们对四个基准的分析表明，不同的推理类型可以独特地解决不同的问题集，凸显了多样化思维方法的重要性。TypedThinker 解决了两个关键挑战：为给定的问题选择合适的推理类型并有效地实施特定的推理类型。通过对成功经验的自我训练，TypedThinker 学习了一种推理类型选择和应用的隐式策略。实验结果表明，与基线模型相比有显着改进，在四个推理基准上，Mistral 7B 的准确率提高了 3.4%，LLaMA3 8B 的准确率提高了 16.7%。值得注意的是，TypedThinker 表现出对新基准的有效泛化能力，并能进一步增强 GPT-4o 等强大模型的推理能力。代码发布于 https://github.com/dqwang122/ThinkHub。]]></description>
      <guid>https://arxiv.org/abs/2410.01952</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成然后细化：零样本意图检测的数据增强</title>
      <link>https://arxiv.org/abs/2410.01953</link>
      <description><![CDATA[arXiv:2410.01953v1 公告类型：新
摘要：在这篇短文中，我们提出了一种用于零资源域中意图检测的数据增强方法。现有的数据增强方法依赖于每个意图类别的少量标记示例，这在具有许多可能意图的设置中可能代价高昂。我们使用两阶段方法：首先，我们在零样本设置中使用开源大型语言模型为意图标签生成话语。其次，我们开发了一个较小的序列到序列模型（Refiner），以改进生成的话语。Refiner 在可见域上进行微调，然后应用于未知域。我们通过在生成的数据上训练意图分类器并在真实（人类）数据上对其进行评估来评估我们的方法。我们发现，与零样本 LLM 基线和常见基线方法相比，Refiner 显着提高了未知域的数据效用和多样性。我们的结果表明，零样本设置中的生成 LLM 和较小的序列到序列模型的两步方法可以为意图检测提供高质量的数据。]]></description>
      <guid>https://arxiv.org/abs/2410.01953</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对于对齐大型语言模型来说，人类反馈有多可靠？</title>
      <link>https://arxiv.org/abs/2410.01957</link>
      <description><![CDATA[arXiv:2410.01957v1 公告类型：新
摘要：当今大多数对齐研究都集中在使用 Anthropic-HH 等数据集设计新的学习算法，假设人类反馈数据本质上是可靠的。然而，很少有人关注人类反馈的定性不可靠性及其对对齐的影响。为了解决这一差距，我们进行了一项全面的研究并对人类反馈数据进行了深入分析。我们使用黄金奖励模型委员会评估反馈可靠性，结果显示超过 25% 的数据集与这些模型的一致性很低或不一致性，这意味着不可靠性程度很高。通过定性分析，我们确定了六个主要的不可靠性来源，例如错误标记、主观偏好、对有用性和无害性的不同标准和阈值等。最后，为了减轻不可靠性，我们提出了源感知清理，这是一种由我们的定性分析洞察力指导的自动数据清理方法，可显着提高数据质量。大量实验表明，在我们清理后的数据集 HH-Clean 上训练的模型比在原始数据集上训练的模型表现更好。我们发布 HH-Clean 是为了在未来支持更可靠的 LLM 对齐评估。]]></description>
      <guid>https://arxiv.org/abs/2410.01957</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型是好的分类器吗？科学文献修订中的编辑意图分类研究</title>
      <link>https://arxiv.org/abs/2410.02028</link>
      <description><![CDATA[arXiv:2410.02028v1 公告类型：新
摘要：分类是一种核心 NLP 任务架构，具有许多潜在应用。虽然大型语言模型 (LLM) 在文本生成方面取得了实质性进展，但它们在增强分类任务方面的潜力仍未得到充分开发。为了解决这一差距，我们提出了一个框架，用于彻底研究微调 LLM 进行分类，包括基于生成和基于编码的方法。我们在编辑意图分类 (EIC) 中实例化了这个框架，这是一个具有挑战性且尚未得到充分探索的分类任务。我们进行了广泛的实验，并与各种训练方法和具有代表性的 LLM 选择进行了系统比较，为它们在 EIC 中的应用提供了新的见解。我们研究了这些发现在另外五个分类任务中的普遍性。为了展示所提出的方法并解决实证编辑分析的数据短缺问题，我们使用我们表现最佳的 EIC 模型创建了 Re3-Sci2.0，这是一个新的大型数据集，包含 1,780 个科学文档修订，其中有超过 94k 个带标签的编辑。数据集的质量通过人工评估进行评估。新的数据集可以对学术写作中的人类编辑行为进行深入的实证研究。我们将实验框架、模型和数据公开。]]></description>
      <guid>https://arxiv.org/abs/2410.02028</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用反射树搜索和自学习改进自主 AI 代理</title>
      <link>https://arxiv.org/abs/2410.02052</link>
      <description><![CDATA[arXiv:2410.02052v1 公告类型：新
摘要：自主代理在自动执行复杂的多步骤决策任务方面表现出了巨大的潜力。然而，即使是最先进的视觉语言模型 (VLM)，如 GPT-4o，仍然达不到人类水平的性能，特别是在复杂的网络环境和长期规划任务中。为了解决这些限制，我们引入了反射蒙特卡洛树搜索 (R-MCTS)，这是一种新颖的测试时间算法，旨在增强 AI 代理（例如由 GPT-4o 提供支持）动态探索决策空间的能力。R-MCTS 通过 1) 结合对比反射来扩展传统的 MCTS，允许代理从过去的交互中学习并动态提高其搜索效率；2) 使用多代理辩论来提供可靠的状态评估。此外，我们通过自学习对 GPT-4o 进行微调，使用 R-MCTS 生成的树遍历（没有任何人工提供的标签）来提高代理的性能。在具有挑战性的 VisualWebArena 基准测试中，与之前的最先进技术相比，我们基于 GPT-4o 的 R-MCTS 代理在各种任务中实现了 6% 到 30% 的相对改进。此外，我们表明，从测试时搜索中获得的知识可以通过微调有效地转移回 GPT-4o。微调后的 GPT-4o 可达到 R-MCTS 性能的 97%，同时在测试时将计算使用量降低了四分之一。此外，定性结果表明，微调后的 GPT-4o 模型展示了探索环境、评估状态以及在检测到当前状态无法成功时回溯到可行状态的能力。此外，我们的工作展示了训练（使用 R-MCTS 收集数据）和测试时间中的计算扩展属性。这些结果指出了一个有前景的研究方向，即通过测试时间搜索和自学习来增强 VLM 对代理应用的推理和规划能力。]]></description>
      <guid>https://arxiv.org/abs/2410.02052</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RLEF：通过强化学习将代码 LLM 应用于执行反馈</title>
      <link>https://arxiv.org/abs/2410.02089</link>
      <description><![CDATA[arXiv:2410.02089v1 公告类型：新
摘要：部署为代理的大型语言模型 (LLM) 可通过多个步骤解决用户指定的任务，同时将所需的手动参与保持在最低限度。至关重要的是，此类 LLM 需要将其生成建立在任何获得的反馈之上，以可靠地实现预期结果。我们提出了一种端到端强化学习方法，用于教学模型利用代码合成领域的执行反馈，其中最先进的 LLM 与独立采样相比难以迭代改进代码。我们对竞争性编程任务进行了基准测试，我们利用小型（8B 参数）和大型（70B）模型实现了新的开创性结果，同时将所需的样本量减少了一个数量级。我们对推理时间行为的分析表明，我们的方法生成的 LLM 可有效利用多个步骤的自动反馈。]]></description>
      <guid>https://arxiv.org/abs/2410.02089</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>思绪万千：解释大型语言模型语境化错误</title>
      <link>https://arxiv.org/abs/2410.02102</link>
      <description><![CDATA[arXiv:2410.02102v1 公告类型：新
摘要：基于转换器的语言模型的巨大成功很大程度上可以归因于它们能够整合来自输入序列的相关上下文信息以生成响应或完成任务。但是，我们对模型用于实现此功能的算法知之甚少，也不了解它们的失败模式。例如，给出提示“约翰要去钓鱼，所以他走到银行。他可以进行 ATM 交易吗？”，如果模型没有正确地将“银行”语境化为地理特征而不是金融机构，则可能会错误地回答“是”。我们提出 LLM 竞争条件假设来解释这种形式的语境化错误。该假设确定了标记之间的依赖关系（例如，“银行”必须在最后一个标记“？”整合来自“银行”的信息之前正确语境化），并声称语境化错误是违反这些依赖关系的结果。利用机械可解释性的多种技术，我们提供支持该假设的相关性和因果证据，并建议推理时间干预来解决它。]]></description>
      <guid>https://arxiv.org/abs/2410.02102</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReGenesis：法学硕士可以通过自我提升成长为推理通才</title>
      <link>https://arxiv.org/abs/2410.02108</link>
      <description><![CDATA[arXiv:2410.02108v1 公告类型：新 
摘要：对具有明确推理轨迹的大型语言模型 (LLM) 进行后训练可以增强其推理能力。然而，获取这种高质量的轨迹数据通常需要人类或高级模型的细致监督，而这些监督可能既昂贵又受许可限制。在本文中，我们探讨了 LLM 可以在多大程度上通过将自合成推理路径作为训练数据而无需任何额外监督来提高其推理能力。现有的自合成方法（例如 STaR）对域外 (OOD) 推理任务的泛化能力较差。我们假设这是因为它们的自合成推理路径过于针对任务，缺乏通用的任务无关推理指导。为了解决这个问题，我们提出了通过自我改进实现推理通才 (ReGenesis)，这是一种通过从抽象到具体的过程将推理路径自我合成为后训练数据的方法。更具体地说，ReGenesis 通过将一般推理指南转换为特定于任务的指南，生成推理结构，然后将这些结构转换为推理路径来自我合成推理路径，而无需现有方法中使用的人为设计的特定于任务的示例。我们表明，与现有方法相比，ReGenesis 在所有测试的领域内和 OOD 设置上都实现了卓越的性能。具体来说，对于六个 OOD 任务，虽然以前的方法在后训练后平均性能下降了约 4.6%，但 ReGenesis 的性能提高了约 6.1%。我们还对我们的框架进行了深入分析，并表明 ReGenesis 在各种 LLM 和设计选择中都是有效的。]]></description>
      <guid>https://arxiv.org/abs/2410.02108</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>L-CiteEval：长上下文模型是否真正利用上下文来做出响应？</title>
      <link>https://arxiv.org/abs/2410.02115</link>
      <description><![CDATA[arXiv:2410.02115v1 公告类型：新
摘要：长上下文模型（LCM）近年来取得了长足的进步，为用户处理涉及长上下文的任务（例如文档摘要）提供了极大的便利。随着社区越来越重视生成结果的忠实性，仅仅确保 LCM 输出的准确性是不够的，因为人类很难从极其冗长的上下文中验证结果。然而，尽管已经做出了一些努力来评估 LCM 是否真正根据上下文做出响应，但这些工作要么仅限于特定任务，要么严重依赖 GPT-4 等外部评估资源。在这项工作中，我们引入了 L-CiteEval，这是一个全面的多任务基准，用于带有引文的长上下文理解，旨在评估 LCM 的理解能力和忠实性。L-CiteEval 涵盖了来自不同领域的 11 个任务，上下文长度从 8K 到 48K，并提供了一个完全自动化的评估套件。通过对11个最新的闭源和开源LCM进行测试，我们发现虽然这些模型在生成结果上表现出细微的差异，但开源模型在引用准确率和召回率方面远远落后于闭源模型。这表明当前的开源LCM倾向于根据其固有知识而不是给定上下文做出响应，这对实际应用中的用户体验构成了重大风险。我们还评估了RAG方法，发现RAG可以显著提高LCM的忠实度，尽管生成质量略有下降。此外，我们发现LCM的注意力机制与引用生成过程之间存在相关性。]]></description>
      <guid>https://arxiv.org/abs/2410.02115</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>矩阵与日语中的相对弱交叉：一项实验研究</title>
      <link>https://arxiv.org/abs/2410.02149</link>
      <description><![CDATA[arXiv:2410.02149v1 公告类型：新
摘要：本文提供了证据表明，矩阵从句和关系从句之间的弱交叉效应在本质上有所不同。福岛等人（2024）提供了类似的证据，表明当消除各种非结构因素时，英语使用者从不接受矩阵弱交叉情况，但经常接受相对弱交叉情况。然而，这些结果受到英语词序的限制，这导致不确定这种差异是由于线性优先还是句法结构的影响。在本文中，为了区分这两种可能性，我们使用日语进行了一项实验，日语缺乏英语所具有的词序混淆。我们发现的结果在质量上与福岛等人（2024）的结果一致，表明相关区别是结构性的，而不仅仅是基于优先性。]]></description>
      <guid>https://arxiv.org/abs/2410.02149</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于隐身检索毒害的自然对抗性文档的控制生成</title>
      <link>https://arxiv.org/abs/2410.02163</link>
      <description><![CDATA[arXiv:2410.02163v1 公告类型：新
摘要：最近的研究表明，基于嵌入相似性的检索（例如，用于检索增强生成）容易受到毒害：对手可以制作恶意文档，这些文档是响应广泛类别的查询而检索到的。我们证明，以前基于 HotFlip 的技术生成的文档非常容易通过困惑度过滤检测到。即使生成被限制为生成低困惑度文本，生成的文档也会被 LLM 识别为非自然的，并且可以自动从检索语料库中过滤掉。
我们设计、实施和评估一种新的受控生成技术，该技术将对抗性目标（嵌入相似性）与基于使用开源代理 LLM 计算的软分数的“自然度”目标相结合。所生成的对抗性文档 (1) 无法通过困惑度过滤和/或其他 LLM 自动检测，除非以检索语料库中出现大量假阳性为代价，但 (2) 实现了与使用 HotFlip 生成的易于检测的文档类似的毒害效果，并且 (3) 比之前的能量引导生成方法（如 COLD）更有效。]]></description>
      <guid>https://arxiv.org/abs/2410.02163</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>POSIX：大型语言模型的快速敏感度指数</title>
      <link>https://arxiv.org/abs/2410.02185</link>
      <description><![CDATA[arXiv:2410.02185v1 公告类型：新 
摘要：尽管大型语言模型 (LLM) 具有非凡的功能，但人们发现它们对提示的细微变化非常敏感，通常会根据提示的细微变化（例如拼写错误、措辞更改或提示模板）产生截然不同的输出。然而，在评估 LLM 的质量时，人们往往只关注其在下游任务中的表现，而很少或根本不关注提示敏感性。为了填补这一空白，我们提出了 POSIX——一种新颖的提示敏感性指数，作为提示敏感性的可靠衡量标准，从而提供对 LLM 性能的更全面评估。POSIX 背后的关键思想是在用不同的意图保留提示替换相应提示后，捕获给定响应的对数似然的相对变化。我们提供了详尽的实证证据，证明了 POSIX 在捕捉提示敏感度方面的有效性，随后用它来测量并比较各种开源 LLM 的提示敏感度。我们发现，仅仅增加参数数量或指令调整并不一定会降低提示敏感度，而添加一些少样本样本（即使只有一个）几乎总是会导致提示敏感度显著下降。我们还发现，在 MCQ 类型任务的情况下，对提示模板的修改会导致最高的敏感度，而在开放式生成任务中，释义会导致最高的敏感度。用于重现我们结果的代码在 https://github.com/kowndinyarenduchintala/POSIX 上开源。]]></description>
      <guid>https://arxiv.org/abs/2410.02185</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型能接受暗示吗？促使可控的情境化常识推理</title>
      <link>https://arxiv.org/abs/2410.02202</link>
      <description><![CDATA[arXiv:2410.02202v1 公告类型：新
摘要：在给定的故事上下文中生成常识性断言对于现代语言模型来说仍然是一项艰巨的任务。先前的研究通过将常识性推理与故事相结合并相应地训练语言生成模型来解决了这个问题。挑战之一是确定故事中的哪个主题或实体应该成为推断断言的焦点。先前的方法缺乏控制生成断言的特定方面的能力。在这项工作中，我们引入了“提示”，这是一种增强情境化常识推理的数据增强技术。“提示”采用前缀提示策略，使用硬提示和软提示来指导推理过程。为了证明其有效性，我们将“提示”应用于两个上下文常识推理数据集：ParaCOMET 和 GLUCOSE，评估其对一般和特定上下文推理的影响。此外，我们通过将同义词和反义词纳入提示来评估“提示”。我们的结果表明，“提示”不会损害上下文常识推理的性能，同时提供更高的可控性。]]></description>
      <guid>https://arxiv.org/abs/2410.02202</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>