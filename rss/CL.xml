<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 11 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>NLP 的小额索赔法庭：使用小型数据集判断法律文本分类策略</title>
      <link>https://arxiv.org/abs/2409.05972</link>
      <description><![CDATA[arXiv:2409.05972v1 公告类型：新
摘要：语言建模的最新进展大大减少了文本分类任务中对标记数据的需求。基于 Transformer 的模型，在未标记数据上进行预训练，可以胜过从头开始训练的模型在每项任务中的表现。然而，对于需要专家级注释者的领域（如法律领域），微调此类模型所需的标记数据量仍然相当高。本文研究了优化使用小型标记数据集和大量未标记数据的最佳策略，并在法律领域执行 50 个预定义主题的分类任务。更具体地说，我们使用巴西检察官办公室的要求记录，旨在分配其中一个主题的描述，目前需要深厚的法律知识才能手动填写。在这种情况下，优化分类器性能的任务尤其具有挑战性，因为葡萄牙语的可用资源很少，尤其是在法律领域。我们的结果表明，与 BERT 语言模型相比，逻辑回归和 SVM 等经典监督模型以及随机森林和梯度提升等集成模型以及使用 word2vec 提取的嵌入取得了更好的性能。后者在与模型本身作为分类器的架构相关的方面表现出色，在这方面超越了所有以前的模型。使用无监督数据增强 (UDA) 获得了最佳结果，它联合使用了 BERT、数据增强和半监督学习策略，在上述任务中的准确率为 80.7%。]]></description>
      <guid>https://arxiv.org/abs/2409.05972</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学人工智能 Lean4 中不同领域的数学形式化问题解决和定理证明</title>
      <link>https://arxiv.org/abs/2409.05977</link>
      <description><![CDATA[arXiv:2409.05977v1 公告类型：新
摘要：使用 Lean 4 等计算机化可验证形式语言来证明数学定理对数学形式化有重大影响。Lean 4 为推进数学推理提供了显著的潜力。然而，现有的努力仅限于大量在线语料库中的数学形式化语言，并致力于跟上快速发展的语言。为了弥合传统证明和计算机化证明之间的差距，我对形式化定理证明的方法包括使用基于自然语言 (NL) 证明的大型语言模型 (LLM) 生成形式步骤和完整证明。该方法是介绍基本结构和一般策略，确定人工智能如何协助数学形式化过程提高其性能，并给出与 NL 相比在 Lean 4 中解决问题的例子，主要是在 IMO 中，以及抽象代数中的示例定理证明。]]></description>
      <guid>https://arxiv.org/abs/2409.05977</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MessIRve：大规模西班牙语信息检索数据集</title>
      <link>https://arxiv.org/abs/2409.05994</link>
      <description><![CDATA[arXiv:2409.05994v1 公告类型：新
摘要：信息检索 (IR) 是根据用户查询查找相关文档的任务。尽管西班牙语是第二大母语，但当前的 IR 基准缺乏西班牙语数据，阻碍了西班牙语使用者的信息访问工具的开发。我们介绍了 MessIRve，这是一个大规模的西班牙语 IR 数据集，其中包含来自 Google 自动完成 API 的约 73 万个查询和来自维基百科的相关文档。MessIRve 的查询反映了不同的西班牙语地区，这与其他从英语翻译或不考虑方言变化的数据集不同。与较小的数据集不同，数据集的规模较大，可以涵盖各种各样的主题。我们对数据集进行了全面的描述、与现有数据集的比较以及对著名 IR 模型的基线评估。我们的贡献旨在推进西班牙 IR 研究并改善西班牙语使用者的信息访问。]]></description>
      <guid>https://arxiv.org/abs/2409.05994</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TransformerRanker：一种高效寻找最适合下游分类任务的语言模型的工具</title>
      <link>https://arxiv.org/abs/2409.05997</link>
      <description><![CDATA[arXiv:2409.05997v1 公告类型：新
摘要：NLP 中的分类任务通常通过从模型中心选择预训练语言模型 (PLM) 并针对手头的任务对其进行微调来解决。然而，鉴于目前可用的 PLM 数量非常多，一个实际挑战是确定其中哪一个最适合特定的下游任务。在本文中，我们介绍了 TransformerRanker，这是一个轻量级库，可以有效地对分类任务的 PLM 进行排名，而无需进行计算成本高昂的微调。我们的库实现了当前的可转移性估计方法（LogME、H-Score、kNN），结合了层聚合选项，我们通过经验证明这些方法可以产生最先进的 PLM 排名（Garbas 等人，2024 年）。我们设计的界面轻量级且易于使用，允许用户直接连接到 HuggingFace Transformers 和 Dataset 库。用户只需选择一个下游分类任务和一个 PLM 列表，即可创建最适合其任务的 PLM 排名。我们将 TransformerRanker 作为可通过 pip 安装的开源库提供 https://github.com/flairNLP/transformer-ranker。]]></description>
      <guid>https://arxiv.org/abs/2409.05997</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在真实的低资源环境中改进视觉提示关键词本地化</title>
      <link>https://arxiv.org/abs/2409.06013</link>
      <description><![CDATA[arXiv:2409.06013v1 公告类型：新
摘要：给定一个图像查询，视觉提示关键字定位 (VPKL) 旨在在语音集合中找到所描绘单词的出现。当资源匮乏的语言没有转录本时（例如，如果它不是书面的），这可能很有用。先前的研究表明，VPKL 可以使用在配对图像和未标记语音上训练的视觉基础语音模型来执行。但所有实验都是用英语进行的。此外，转录本用于获得对比损失的正对和负对。本文介绍了一种少量学习方案，可以在没有转录本的情况下自动挖掘对。在英语上，这只会导致性能略有下降。我们还首次考虑在真正的资源匮乏的语言约鲁巴语上使用 VPKL。虽然分数是合理的，但与使用地面实况对相比，我们看到性能下降幅度更大，因为挖掘在约鲁巴语中不太准确。]]></description>
      <guid>https://arxiv.org/abs/2409.06013</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过输出的语言变化识别 GPT 模型中的意识形态偏见来源</title>
      <link>https://arxiv.org/abs/2409.06043</link>
      <description><![CDATA[arXiv:2409.06043v1 公告类型：新
摘要：现有研究表明，GPT-3.5 和 4 等生成式 AI 模型延续了社会刻板印象和偏见。一个令人担忧但较少被探索的偏见来源是意识形态。GPT 模型是否对政治敏感话题采取意识形态立场？在本文中，我们提供了一种识别生成模型中意识形态偏见的原创方法，表明偏见可能源于训练数据和过滤算法。我们利用政治态度形成鲜明对比的国家的语言差异来评估这些语言中 GPT 对这些敏感政治话题的平均反应的偏见。首先，我们发现 GPT 输出在与保守社会很好地映射的语言（即波兰语）中更为保守，而在自由社会独有的语言（即瑞典语）中更为自由。这一结果为 GPT 模型中的训练数据偏差提供了有力证据。其次，尽管由于 OpenAI 的过滤政策，GPT-4 明显更加自由，但 GPT-3.5 中观察到的语言差异在 GPT-4 中仍然存在。我们的主要结论是，生成模型训练必须专注于高质量、精选的数据集以减少偏差，即使这需要在训练数据大小上做出妥协。训练后过滤响应只会引入新的偏差，而不会消除潜在的训练偏差。]]></description>
      <guid>https://arxiv.org/abs/2409.06043</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DetoxBench：对多任务欺诈和滥用检测的大型语言模型进行基准测试</title>
      <link>https://arxiv.org/abs/2409.06072</link>
      <description><![CDATA[arXiv:2409.06072v1 公告类型：新
摘要：大型语言模型 (LLM) 在自然语言处理任务中表现出了卓越的能力。然而，它们在高风险领域（例如欺诈和滥用检测）的实际应用仍然是一个需要进一步探索的领域。现有的应用程序通常只关注特定任务，例如毒性或仇恨言论检测。在本文中，我们提出了一个全面的基准测试套件，旨在评估 LLM 在各种现实场景中识别和缓解欺诈和辱骂性语言的性能。我们的基准测试涵盖了一系列不同的任务，包括检测垃圾邮件、仇恨言论、厌恶女性的语言等。我们评估了几种最先进的 LLM，包括来自 Anthropic、Mistral AI 和 AI21 系列的模型，以全面评估它们在这个关键领域的能力。结果表明，虽然 LLM 在单个欺诈和滥用检测任务中表现出了熟练的基线性能，但它们在不同任务中的表现差异很大，尤其是在处理需要细致入微的实用推理的任务时，例如识别各种形式的厌恶女性的语言。这些发现对于在高风险应用中负责任地开发和部署 LLM 具有重要意义。我们的基准套件可以作为研究人员和从业人员的工具，以系统地评估 LLM 的多任务欺诈检测，并推动创建更强大、更值得信赖、更符合道德规范的欺诈和滥用检测系统。]]></description>
      <guid>https://arxiv.org/abs/2409.06072</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ClarQ-LLM：任务导向对话中澄清和请求信息的模型基准</title>
      <link>https://arxiv.org/abs/2409.06097</link>
      <description><![CDATA[arXiv:2409.06097v1 公告类型：新
摘要：我们引入了 ClarQ-LLM，这是一个由英汉双语对话任务、对话代理和评估指标组成的评估框架，旨在作为评估代理在面向任务的对话中提出澄清问题的能力的强大基准。基准包括 31 种不同的任务类型，每种任务类型都有 10 个信息搜索者和提供者代理之间的独特对话场景。这些场景要求搜索者提出问题以解决不确定性并收集完成任务所需的信息。与基于固定对话内容评估代理的传统基准不同，ClarQ-LLM 包含一个提供者对话代理，以在基准中复制原始人类提供者。这允许当前和未来的搜索代理通过直接与我们的提供者代理交互来测试他们通过对话完成信息收集任务的能力。在测试中，LLAMA3.1 405B 搜索代理的最大成功率仅为 60.05%，这表明 ClarQ-LLM 对未来的研究提出了严峻的挑战。]]></description>
      <guid>https://arxiv.org/abs/2409.06097</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Doppelg_“anger 的守望：大型语言模型的分裂目标方法</title>
      <link>https://arxiv.org/abs/2409.06107</link>
      <description><![CDATA[arXiv:2409.06107v1 公告类型：新
摘要：在本文中，我们研究了大型语言模型中的“生成监督”问题，并提出了一种新颖的双腔结构，将监督信号与其核心能力有用性分开。Doppelg_“anger 是一个与底层语言模型并行的新模块，它监督每个标记的生成，并学习同​​时预测每个标记之前的序列的监督分数。在这项工作中，我们展示了理论发现，并将实验结果报告留待即将发表的出版物。]]></description>
      <guid>https://arxiv.org/abs/2409.06107</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>估计离散语音单元的完整性</title>
      <link>https://arxiv.org/abs/2409.06109</link>
      <description><![CDATA[arXiv:2409.06109v1 公告类型：新
摘要：用离散单元表示语音已广泛应用于语音编解码器和语音生成。然而，关于自监督离散单元有几个未经证实的说法，例如用 k 均值解开语音和说话人信息，或假设 k 均值后信息丢失。在这项工作中，我们从信息论的角度来回答残差矢量量化之前和之后存在多少信息（信息完整性）和可访问多少信息（信息可访问性）。我们展示了信息完整性的下限，并估计了残差矢量量化后离散化 HuBERT 表示的完整性。我们发现说话人信息在 HuBERT 离散单元中充分存在，语音信息在残差中充分存在，表明矢量量化不能实现解开。我们的结果对离散单元的选择进行了全面的评估，并表明应该挖掘而不是丢弃残差中的更多信息。]]></description>
      <guid>https://arxiv.org/abs/2409.06109</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 LFR 教学法加速大型语言模型预训练：学习、关注和复习</title>
      <link>https://arxiv.org/abs/2409.06131</link>
      <description><![CDATA[arXiv:2409.06131v1 公告类型：新
摘要：大型语言模型 (LLM) 预训练传统上依赖于对从网络规模数据集中随机采样的数据块进行自回归语言建模。我们从间隔重复等人类学习技术中汲取灵感，假设 LLM 的随机数据采样会导致高训练成本和低质量模型，这些模型往往会忘记数据。为了有效地将网络规模的信息提交到长期记忆中，我们提出了 LFR（学习、关注和复习）教学法，这是一种新的动态训练范式，它根据模型的学习速度和进度，以系统的间隔关注和反复复习复杂的数据块。LFR 记录不同数据块的模型困惑度，并经常重新访问困惑度更高、更容易被遗忘的块。我们使用 LFR 在 OpenWebText 数据集上从头开始预训练 GPT-2 模型（124M - 1.5B）。我们对语言建模、问答、翻译和问题解决领域的下游任务进行了测试，以达到比基线 OpenAI 模型更低的困惑度和更高的准确性，同时获得 20 倍的预训练速度提升。]]></description>
      <guid>https://arxiv.org/abs/2409.06131</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和大型语言模型用于音频和文本分析以预测中国心理支持热线中的自杀行为</title>
      <link>https://arxiv.org/abs/2409.06164</link>
      <description><![CDATA[arXiv:2409.06164v1 公告类型：新
摘要：自杀是一个紧迫的全球性问题，需要紧急有效的预防干预措施。在现有的各种策略中，心理支持热线已被证明是一种有效的干预方法。中国每年约有 200 万人自杀，许多人多次尝试。及时识别和干预高危人群对于防止悲剧发生至关重要。随着人工智能 (AI) 的快速发展，尤其是大规模语言模型 (LLM) 的发展，新的技术工具已被引入心理健康领域。这项研究包括 1284 名受试者，旨在验证深度学习模型和 LLM（使用支持热线的音频和转录文本）是否能有效预测自杀风险。我们提出了一个简单的基于 LLM 的流程，首先总结大约一小时的语音转录文本以提取关键特征，然后预测未来的自杀行为。我们在临床环境中将基于 LLM 的方法与传统的手动量表方法以及五种先进的深度学习模型进行了比较。令人惊讶的是，所提出的简单 LLM 流程在 46 个受试者的测试集上取得了出色的表现，与手动量表评分相结合时 F1 得分为 76%。这比最好的基于语音的深度学习模型高出 7%，与单独使用手动量表方法相比，F1 得分提高了 27.82%。我们的研究探索了 LLM 的新应用，并展示了它们未来在自杀预防工作中的应用潜力。]]></description>
      <guid>https://arxiv.org/abs/2409.06164</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更大的语言模型不关心你的想法：为什么思路链提示在主观任务中失败</title>
      <link>https://arxiv.org/abs/2409.06173</link>
      <description><![CDATA[arXiv:2409.06173v1 公告类型：新
摘要：大型语言模型 (LLM) 中的上下文学习 (ICL) 已成为执行自然语言任务的主要技术，因为它不需要使用基于梯度的方法更新模型参数。ICL 承诺“调整”LLM，以极具竞争力或最先进的水平执行当前任务，而计算成本仅为其一小部分。可以通过结合推理过程来增强 ICL，以在提示中明确得出最终标签，这种技术称为思维链 (CoT) 提示。然而，最近的研究发现，ICL 主要依赖于任务先验的检索，而较少依赖于“学习”来执行任务，尤其是对于情感和道德等复杂的主观领域，其中先验会使后验预测僵化。在这项工作中，我们研究了“启用”推理是否也会在 LLM 中产生相同的行为，其中 CoT 的格式检索推理先验，尽管提示中有证据，但这些先验相对保持不变。我们发现，令人惊讶的是，对于较大的语言模型，CoT 确实遭受与 ICL 相同的后验崩溃。代码可在 https://github.com/gchochla/cot-priors 上找到。]]></description>
      <guid>https://arxiv.org/abs/2409.06173</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型能否解锁新颖的科学研究思路？</title>
      <link>https://arxiv.org/abs/2409.06185</link>
      <description><![CDATA[arXiv:2409.06185v1 公告类型：新
摘要：“一个想法无非是旧元素的新组合”（Young，J.W.）。大型语言模型（LLM）和公开可用的 ChatGPT 的广泛采用标志着人工智能（AI）融入人们日常生活的重要转折点。本研究探讨了 LLM 根据研究论文中的信息产生新颖研究想法的能力。我们对五个领域（例如化学、计算机、经济学、医学和物理学）的 4 个 LLM 进行了彻底检查。我们发现 Claude-2 和 GPT-4 产生的未来研究想法比 GPT-3.5 和 Gemini 更符合作者的观点。我们还发现 Claude-2 比 GPT-4、GPT-3.5 和 Gemini 1.0 产生了更多样化的未来研究想法。我们进一步对生成的未来研究想法的新颖性、相关性和可行性进行了人工评估。这项调查深入了解了 LLM 在想法生成中不断演变的角色，突出了其能力和局限性。我们的工作有助于持续评估和利用语言模型来生成未来的研究想法。我们将数据集和代码公开。]]></description>
      <guid>https://arxiv.org/abs/2409.06185</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NOVI：面向具有 BERT 和 LLM 资格的大学新生的聊天机器人系统</title>
      <link>https://arxiv.org/abs/2409.06192</link>
      <description><![CDATA[arXiv:2409.06192v1 公告类型：新
摘要：为了缓解大学新生适应大学生活的困难，我们开发了基于 GPT-4o 的聊天机器人系统 NOVI。该系统利用大学社区网站 SKKU“Everytime”的帖子和评论数据。NOVI 使用 LangChain 开发，其性能已通过 BLEU 分数、Perplexity 分数、ROUGE-1 分数、ROUGE-2 分数、ROUGE-L 分数和 METEOR 分数进行评估。这种方法不仅限于帮助大学新生，还有望通过不同的数据帮助各种人适应新环境。本研究探讨了新教育技术工具的开发和潜在应用，有助于初学者更轻松地适应社会，并为未来攻读法学硕士奠定基础。]]></description>
      <guid>https://arxiv.org/abs/2409.06192</guid>
      <pubDate>Wed, 11 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>