<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 21 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>MAMASKPRUNE：基于面膜的LLM修剪，用于层均匀结构</title>
      <link>https://arxiv.org/abs/2502.14008</link>
      <description><![CDATA[ARXIV：2502.14008V1公告类型：新 
摘要：大型语言模型（LLM）在各种语言任务中的显着性能引起了极大的关注。但是，这些模型不断增加的规模给部署和推理带来了越来越多的挑战。结构化修剪是一种有效的模型压缩技术，由于其提高推理效率的能力，人们正在越来越多的关注。然而，大多数以前基于优化的结构化修剪方法牺牲了跨层的统一结构，以更大的灵活性来维持性能。异质结构阻碍了现成的推理加速技术的有效利用，并阻碍了有效的配置进行持续训练。为了解决这个问题，我们提出了一种基于最小值优化的新型掩蔽学习范式，以通过优化稀疏性正则化的口罩来获得统一的修剪结构。广泛的实验结果表明，我们的方法可以保持高性能，同时确保修剪模型结构的均匀性，从而超过现有的SOTA方法。]]></description>
      <guid>https://arxiv.org/abs/2502.14008</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>非人性化机器：减轻文本生成系统中的拟人化行为</title>
      <link>https://arxiv.org/abs/2502.14019</link>
      <description><![CDATA[ARXIV：2502.14019V1公告类型：新 
摘要：随着文本生成系统的输出越来越拟人化 - 被视为人类 - 学者们也引起了人们对此类输出如何导致有害结果的担忧，例如用户过度范围或对这些系统的情感依赖性。但是，如何干预此类系统输出以减轻拟人化行为及其随之而来的有害结果。通过这项工作，我们旨在为制定这种干预措施提供经验和理论基础。为此，我们编制了在先前的文献和众包研究中基于的干预措施的清单，其中参与者编辑了系统输出，以使其类似于人类。利用此库存，我们还开发了一个概念框架，以帮助表征可能的干预措施的景观，表达不同类型的干预措施之间的区别，并为评估不同干预措施的有效性提供了理论基础。]]></description>
      <guid>https://arxiv.org/abs/2502.14019</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>差采样：增强神经文本生成的多样性和准确性</title>
      <link>https://arxiv.org/abs/2502.14037</link>
      <description><![CDATA[ARXIV：2502.14037V1公告类型：新 
摘要：尽管其性能提高，但大型语言模型仍然倾向于再现训练数据，产生多个重复，并专注于最常见的语法结构和单词。一个可能的原因是采用了解码策略：最常见的策略要么仅考虑最可能的令牌，减少产出多样性，要么以产出准确性和正确性为代价增加了象征性的可能性。在本文中，我们通过利用令牌概率分布的数学分析来提出三种新解码方法的家族。特别是，可以使用连续排序的概率之间的差异来避免令牌不正确，并增加了低概率但准确的单词的机会。有关数学问题解决，极端摘要和不同关联任务的实验表明，我们的方法至少在质量和多样性方面始终如一以及当前的替代方案。]]></description>
      <guid>https://arxiv.org/abs/2502.14037</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义分解和选择性上下文过滤 - 基于上下文感知NLP的系统的文本处理技术</title>
      <link>https://arxiv.org/abs/2502.14048</link>
      <description><![CDATA[ARXIV：2502.14048V1公告类型：新 
摘要：在本文中，我们提出了两种用于上下文感知系统的技术：语义分解，它们将输入提示依次分解为结构化和层次的信息架构，在其中系统可以轻松地解析和处理性上下文过滤，并启用系统为了系统地滤除通过系统基于NLP的管道提供的上下文信息的特定不相关部分。我们将探讨如何利用这两种技术来实现动态LLM到系统接口，提高LLM生成更高上下文上上下文具有凝聚力的用户面向用户的响应并优化复杂的自动化工作流程和管道的能力。]]></description>
      <guid>https://arxiv.org/abs/2502.14048</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过稀疏自动编码器调整语言模型的多样性驱动数据选择</title>
      <link>https://arxiv.org/abs/2502.14050</link>
      <description><![CDATA[ARXIV：2502.14050V1公告类型：新 
摘要：当前的预训练的大语言模型通常需要进行指导调整以与人类的偏好保持一致。但是，由于数据收集大量和快速模型迭代，指令调整数据通常被数量饱和，而核心数据选择很重要，但没有充满驱动。另一方面，现有的质量驱动数据选择方法，例如Lima（Neurips 2023（Zhou等，2024））和Alpagasus（ICLR 2024（Chen等人））通常忽略数据多样性和复杂性的同等重要性。在这项工作中，我们旨在设计一种多样性吸引的数据选择策略，并创造性地建议使用稀疏的自动编码器来应对数据多样性度量的挑战。此外，稀疏的自动编码器还可以提供更多的模型行为的解释性，例如选择最长响应的令人惊讶的有效性（ICML 2024（Zhao等人））。使用有效的数据选择，我们在实验上证明了对我们所选数据训练的模型可以优于模型能力，降低培训成本以及可能对模型行为获得更多控制的其他方法。]]></description>
      <guid>https://arxiv.org/abs/2502.14050</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ROCKETKV：通过两阶段的KV缓存压缩加速长篇小说LLM推理</title>
      <link>https://arxiv.org/abs/2502.14051</link>
      <description><![CDATA[ARXIV：2502.14051V1公告类型：新 
摘要：基于变压器的大型语言模型严格依赖KV缓存来在解码阶段有效处理扩展上下文。然而，KV缓存的大小随输入长度的成比例增长，随着解码的进展，内存带宽和容量都会为内存带宽和容量负担。为了应对这一挑战，我们提出了RocketKV，这是一种专门旨在减少DECODE阶段中KV CACHE的内存带宽和容量需求的无训练KV缓存策略。 RocketKV连续两个阶段。在第一阶段，它使用Snapkv ++对输入序列令牌执行粗粒kV缓存驱逐，通过引入自适应池大小和完整的兼容性，可以改善SNAPKV的方法，并具有分组疑问。在第二阶段，它采用了一种混合注意方法来进行细粒度的TOP-K稀疏注意，从而通过利用头部和序列尺寸减少来近似注意力评分。 RocketKV结合了这两个阶段，可以实现显着的KV高速缓存，以获取带宽和存储节省，同时保持了与完全KV高速缓存的可比精度。我们表明，RocketKV在NVIDIA H100 GPU上的Decode阶段中最多可提供3 $ \ times $的端到端速度，而峰值记忆的峰值将最多减少31％，而与完整的KV缓存基线相比各种长篇文章任务的准确性损失。]]></description>
      <guid>https://arxiv.org/abs/2502.14051</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>规则是要违反吗？将多语言道德推理理解为单位的计算管道</title>
      <link>https://arxiv.org/abs/2502.14083</link>
      <description><![CDATA[arxiv：2502.14083v1公告类型：新 
摘要：道德推理是一个由个人经验和文化背景塑造的复杂认知过程，并为计算分析带来了独特的挑战。尽管自然语言处理（NLP）为研究这种现象提供了有希望的工具，但当前的研究缺乏凝聚力，采用了不和谐的数据集和任务来检查道德推理的孤立方面。我们弥合了这个差距，该差距是一个统一的数据集，该数据集以心理扎根和社会媒体衍生的道德困境为基础，并带有标签，以进行行动选择，道德原则，贡献因素和后果，并与注释者的道德和文化概况一起。认识到道德推理的文化相对性，单语言涵盖了六种语言，阿拉伯语，中文，英语，印地语，俄语和西班牙语，捕捉了多种社会文化背景。我们通过对四个任务的三个大语言模型（LLM）进行基准评估来证明单型实用性：行动预测，道德类型学分类，因素归因分析和后果生成。主要发现表明，虽然隐式嵌入道德背景增强了LLM的道德推理能力，但仍需要对日益专业的方法进行迫切需要，以进一步推进这些模型的道德推理。]]></description>
      <guid>https://arxiv.org/abs/2502.14083</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>导航语义关系：抽象常识性推理中语言模型的挑战</title>
      <link>https://arxiv.org/abs/2502.14086</link>
      <description><![CDATA[ARXIV：2502.14086V1公告类型：新 
摘要：大型语言模型（LLM）在生成类似人类的文本和解决中等复杂性的推理任务时取得了显着的性能，例如提问和数学问题解决。但是，它们在需要更深入认知技能的任务中的能力，例如常识性理解和抽象推理，仍然不足。在本文中，我们使用概念网知识图系统地评估了LLM中的抽象常识推理。我们提出了两种提示方法：指示提示，在其中模型根据提供的定义预测了合理的语义关系，并且很少射击提示，其中模型使用示例作为指导来识别关系。我们对GPT-4O-MINI模型进行的实验表明，在指示提示中，在对多重关系进行排名时会获得一致的性能，但是当模型仅限于仅预测一个关系时，会大幅下降。在很少的提示中，当从五个关系中选择而不是完整集合时，模型的准确性显着提高，尽管对某些关系有明显的偏见。这些结果表明，即使在商业使用的LLMS的抽象常识推理能力中，与人类水平的理解相比，仍然存在明显的差距。但是，这些发现还强调了基于选择性检索的仔细及时工程的承诺，以获得更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.14086</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在几次学习中检索与理解提取证据</title>
      <link>https://arxiv.org/abs/2502.14095</link>
      <description><![CDATA[Arxiv：2502.14095V1公告类型：新 
摘要：对齐的关键方面是正确使用文档内证据来构建文档级别的决策。我们分析了在几个弹奏环境中的大型语言模型的检索和解释之间的关系。具体而言，我们使用两个流行的封闭的专有模型来衡量模型预测误差与证据检索错误有关的五个数据集的人类宣传的提取证据的程度。我们进行两项消融研究，以调查标签预测和证据检索错误何时可以归因于相关证据的质量。我们发现，模型预测与证据检索错误之间存在很强的经验关系，但是证据检索错误主要与证据解释错误无关 - 这是基于该机制构建的下游应用的充满希望的迹象。]]></description>
      <guid>https://arxiv.org/abs/2502.14095</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向上下文bust llms：一种封闭式的微调方法</title>
      <link>https://arxiv.org/abs/2502.14100</link>
      <description><![CDATA[ARXIV：2502.14100V1公告类型：新 
摘要：大型语言模型（LLM）通过外部环境增强，例如通过检索型发电（RAG），通常在处理不完善的证据时面临挑战。他们倾向于过度依靠外部知识，使它们容易受到误导性和无助的背景。为了解决这一问题，我们提出了上下文持续llms的概念，该概念可以有效地平衡内部知识与外部背景，类似于人类的认知过程。具体而言，上下文持续的LLM只有在缺乏内部知识，确定内部和外部知识之间的矛盾并忽略无用的上下文时才能依靠外部上下文。为了实现这一目标，我们介绍了GRFT，这是一种轻巧和插件的封闭式表示方法。 GRFT由两个关键组成部分组成：检测和过滤有问题输入的门控机制，以及低级别表示适配器以调整隐藏的表示。通过训练仅在少于200个示例的模型大小的0.0004 \％的轻量级干预功能，GRFT可以有效地使LLMS适应上下文 - 弹性行为。]]></description>
      <guid>https://arxiv.org/abs/2502.14100</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>含义超出真理条件：通过Anaphora可访问性评估话语水平的理解</title>
      <link>https://arxiv.org/abs/2502.14119</link>
      <description><![CDATA[ARXIV：2502.14119V1公告类型：新 
摘要：我们提出了自然语言理解能力的层次结构，并主张超越评估在词汇和句子水平上的理解的重要性。我们提出了用于评估话语理解的诊断性访问性的任务，为此，提出了受动态语义理论研究启发的评估数据集。我们在数据集中评估了人类和LLM的性能，发现LLM和人类在某些任务上保持一致并在其他任务上分歧。与人类对结构抽象的敏感性相反，LLMS对语言理解过程中对特定词汇的依赖可以解释这种差异。]]></description>
      <guid>https://arxiv.org/abs/2502.14119</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>政治科学的基准LLM：联合国的观点</title>
      <link>https://arxiv.org/abs/2502.14122</link>
      <description><![CDATA[ARXIV：2502.14122V1公告类型：新 
摘要：大型语言模型（LLM）在自然语言处理方面取得了重大进步，但它们具有高级政治决策的潜力仍然在很大程度上没有探索。本文通过关注LLM在联合国（联合国）决策过程中的应用来解决差距，在联合国（联合国）的决策过程中，股份特别高，政治决策可能会带来深远的后果。我们介绍了一个新颖的数据集，其中包括1994年至2024年公开可用的联合国安全委员会（UNSC）记录，包括决议草案，投票记录和外交演讲。使用此数据集，我们提出了联合国基准（Unbenchen），这是第一个旨在评估四个相互联系的政治学任务的LLM的全面基准：共同持有人的判断，代表性投票模拟，采用预测草案和代表性陈述生成。这些任务涵盖了联合国决策过程的三个阶段（制止，投票和讨论），并旨在评估LLMS理解和模拟政治动态的能力。我们的实验分析表明，将LLM应用于该领域的潜力和挑战，提供了对他们在政治学中的优势和局限性的见解。这项工作有助于人工智能和政治学的日益增长，为全球治理中的研究和实际应用开辟了新的途径。可以通过以下网址访问Unbench存储库：https：//github.com/yueqingliang1/unbench。]]></description>
      <guid>https://arxiv.org/abs/2502.14122</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>其中哪个最能描述使用LLM的多项选择评估？ a）强迫b）有缺陷c）可固定d）上述所有</title>
      <link>https://arxiv.org/abs/2502.14127</link>
      <description><![CDATA[ARXIV：2502.14127V1公告类型：新 
摘要：多项选择问答（MCQA）由于其简单性和类似人类的测试而在LLM评估中很受欢迎，但我们主张其改革。我们首先揭示了MCQA格式的缺陷，因为它努力：1）测试/主观性； 2）匹配LLM用例； 3）完全测试知识。相反，我们主张基于人类测试的生成格式 -  llms构建llms并解释答案，以捕获用户的需求和知识，同时又易于评分。然后，我们显示MCQA是一种有用的格式，其数据集也会遭受：泄漏；无法选择；捷径；和饱和。在每个问题中，我们都会提供教育的修复，例如专栏来指导MCQ写作；为bridle猜测的评分方法；和项目响应理论以建立更难的MCQ。最后，我们讨论了MCQA合作社，偏见和不忠解释的LLM错误，以表达我们先前的解决方案如何更好地衡量或解决这些问题。虽然我们不需要抛弃MCQA，但我们鼓励根据教育测试来完善任务，并进行评估。]]></description>
      <guid>https://arxiv.org/abs/2502.14127</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>社区笔记可以取代专业事实检查器吗？</title>
      <link>https://arxiv.org/abs/2502.14132</link>
      <description><![CDATA[ARXIV：2502.14132V1公告类型：新 
摘要：在社交媒体上打击错误信息兴起的两种常用策略是（i）专业组织的事实检查以及（ii）平台用户的社区审核。 Twitter/X以及最近的Meta的政策变化表明，与事实检查组织的伙伴关系转变，并越来越依赖对众包社区票据的依赖。但是，事实检查和有用的社区笔记之间依赖关系的程度和性质尚不清楚。为了解决这些问题，我们使用语言模型来注释大量的Twitter/X社区注释，具有诸如主题，引用的资料之类的属性，以及它们是否驳斥与更广泛的错误信息叙述相关的主张。我们的分析表明，社区笔记引用了事实检验来源的数量，是先前报道的五倍。事实核对对于与更广泛的叙述相关的帖子的注释尤其至关重要，与其他来源相比，参考事实检查来源的可能性是两倍。总而言之，我们的结果表明，成功的社区节制在很大程度上依赖于专业事实检查。]]></description>
      <guid>https://arxiv.org/abs/2502.14132</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于可控的LLM分类的潜在空间解释的自我重新定义</title>
      <link>https://arxiv.org/abs/2502.14133</link>
      <description><![CDATA[arxiv：2502.14133v1公告类型：新 
摘要：现代文本分类方法在很大程度上依赖于大语言模型（LLMS）的上下文嵌入。与人工设计的特征相比，这些嵌入为分类模型培训提供了自动有效的表示。但是，他们还引入了一个挑战：我们失去了手动删除意外功能的能力，例如敏感或任务 - 默认功能，以确保法规合规性或提高分类模型的普遍性。由于LLM嵌入不透明且难以解释，因此会出现此限制。在本文中，我们提出了一个新颖的框架，以识别和规范LLM潜在空间中的意外功能。具体而言，我们首先预先培训稀疏的自动编码器（SAE），以从LLM潜在空间中提取可解释的功能。为了确保SAE可以捕获特定于任务的功能，我们将其在特定于任务的数据集中进行了微调。在训练分类模型时，我们提出了一个简单有效的正规器，通过最大程度地降低分类器权重和确定的意外功能之间的相似性，以消除这些意外特征对分类的影响。我们在三个现实世界任务上评估了提议的框架，包括有毒的聊天检测，奖励建模和疾病诊断。结果表明，提出的框架可以通过使与每个任务无关的那些功能正规化那些特征来显着改善分类器的普遍性。这项工作是通过利用解释的功能来应对概括，公平性和隐私挑战的利用LLM潜在空间的可控文本分类。我们将发布一旦接受的代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2502.14133</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>