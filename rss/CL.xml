<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 26 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>自然语言行动空间的政策学习：一种因果方法</title>
      <link>https://arxiv.org/abs/2502.17538</link>
      <description><![CDATA[ARXIV：2502.17538V1公告类型：新 
摘要：本文介绍了一个新型的因果框架，用于自然语言动作空间中多阶段决策，其中仅在一系列动作之后才观察到结果。尽管近端政策优化（PPO）等最新方法可以在高维操作空间中处理此类延迟奖励设置，但它们通常需要多个模型（策略，价值和奖励）和实质性的培训数据。我们的方法采用Q学习来通过单个模型来估算动态治疗方案（DTR），从而通过语言嵌入来实现数据有效的策略学习。我们方法的主要技术贡献是一种解码策略，将优化的嵌入重新转换为连贯的自然语言。我们评估了我们的心理健康干预，仇恨言论和情感转移任务的方法，表明对多个指标的竞争基准的重大改进。值得注意的是，我们的方法在通过人类评估中验证的，在保持内容保存和流利度的同时，可以达到卓越的传递强度。我们的工作为在培训数据受到限制的复杂语言任务中学习最佳政策提供了实用的基础。]]></description>
      <guid>https://arxiv.org/abs/2502.17538</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>旨在调节用户控制的临床文本生成</title>
      <link>https://arxiv.org/abs/2502.17571</link>
      <description><![CDATA[ARXIV：2502.17571V1公告类型：新 
摘要：尽管大型语言模型（LLMS）的进步仍在继续表现出幻觉和事实不一致，但必须在临床环境中部署自然语言生成系统仍然具有挑战性。本文使用LLMS作为人类代理来探讨自动数据集的增强，以调节临床医生控制的LLM，而不会增加认知工作。在bionlp acl&#39;24上，我释放了我！共同的任务，我们通过更有效的培训实现了比先前提交的更简单的方法实现新的最新结果，从而实现了9 \％的相对改进，而无需增强培训，并且使用DataSet Exmentation获得了最高34 \％。初步的人类评估进一步支持了我们方法的有效性，突出了增强临床文本生成以控制相关性，准确性和事实一致性的潜力。]]></description>
      <guid>https://arxiv.org/abs/2502.17571</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过视觉链中的视觉链中的端到端图表汇总</title>
      <link>https://arxiv.org/abs/2502.17589</link>
      <description><![CDATA[ARXIV：2502.17589V1公告类型：新 
摘要：自动图表汇总对于增强数据可访问性和从视觉数据中提取有效的信息至关重要。尽管视觉语言模型（VLM）的最新进展已经证明了有望，但现有方法通常会遇到限制，将生成的摘要与图表数据以及有关复杂图表模式的推理相匹配。本文介绍了用于图表摘要的端到端视觉链（V-COT），这是一种针对大型视觉模型（LVLM）优化的新方法。我们的方法直接训练LVLM来处理图表图像并以端到端方式生成文本摘要，从而消除了对明确图表解析模块的需求。我们通过教学进行微调结合了视觉链的机制，暗中指导LVLM在摘要生成过程中执行视觉推理步骤。在大规模的图表和QA数据集上进行了评估，我们的V-COT方法在包括BLEU，BLEURT，CIDER和CS在内的一系列自动指标上的最先进基准均显着优于最先进的基准，并在人类评估中表现出了卓越的匹配程度和推理的正确性。消融研究和详细分析进一步验证了我们提出的方法的有效性和鲁棒性，为端到端图表摘要建立了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2502.17589</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的主动隐私失忆：保护PII对模型实用程序的影响微不足道</title>
      <link>https://arxiv.org/abs/2502.17591</link>
      <description><![CDATA[ARXIV：2502.17591V1公告类型：新 
摘要：随着大语言模型（LLM）的兴起，不断增长的研究已经认识到在恶意攻击下泄漏个人身份信息（PII）的风险。尽管已努力保护LLM中的PII，但现有的方法努力平衡隐私保护与维护模型效用。在本文中，受认知科学的健忘症的启发，我们提出了一种新颖的方法，即主动的隐私失忆症（PPA），以保护LLMS的PII，同时保留其效用。该机制通过积极地识别和忘记序列中与PII最紧密相关的关键记忆，然后使用合适的替代记忆来维持LLM的功能。我们对多种模型进行评估，以保护普通PII，例如电话号码和物理地址，以防止普遍的PII靶向攻击，这证明了我们方法与其他现有防御技术相比的优势。结果表明，我们的PPA方法完全消除了电话号码暴露的风险100％，并显着将物理地址暴露的风险降低了9.8％-87.6％，同时保持了可比的模型公用事业性能。]]></description>
      <guid>https://arxiv.org/abs/2502.17591</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MEDA：动态KV缓存分配，用于有效的多模式长篇小说推断</title>
      <link>https://arxiv.org/abs/2502.17599</link>
      <description><![CDATA[ARXIV：2502.17599V1公告类型：新 
摘要：综合长文本图像和文本视频模式的长篇文化多模式大型语言模型（MLLM），需要大量资源，因为其多模式键值（KV）缓存随着输入长度的增加而增长，具有挑战性的推理效率。在纯文本和多模式LLM中，现有的KV缓存压缩方法已忽略了各个层的注意力密度变化，因此通常采用均匀或渐进的减少策略来分配层。在这项工作中，我们提出了MEDA，这是一种动态层的KV缓存分配方法，用于有效的多模式长篇下说推断。作为其核心，MEDA利用跨模式的注意熵来确定每个MLLMS层处的KV缓存尺寸。鉴于每一层动态分配的KV高速缓存大小，MEDA还采用KV对选择方案来确定要选择哪种KV对以及合并所选和非选择的KV对合并策略，以从整个上下文中保留信息。 MEDA可实现高达72％的KV缓存内存减少和2.82倍的解码速度，同时在长篇小说设置中维持或增强各种多模式任务的性能，包括多映射和长时间视频场景。我们的代码在https://github.com/aiot-mlsys-lab/meda上发布。]]></description>
      <guid>https://arxiv.org/abs/2502.17599</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>picaso：带有状态空间模型的置换不变上下文组成</title>
      <link>https://arxiv.org/abs/2502.17605</link>
      <description><![CDATA[ARXIV：2502.17605V1公告类型：新 
摘要：在推理时间为大型语言模型提供相关的上下文知识已被证明可以大大提高其世代的质量。这通常是通过准备文本或“上下文”的信息段落来实现的，这些文本从外部知识库中检索到其输入。但是，在线处理其他上下文会产生大量的计算成本，这些计算成本随其长度而扩展。状态空间模型（SSM）通过允许将上下文的数据库映射到固定维状态，从而从中开始生成。当试图利用跨多个上下文中存在的信息时，就会产生一个关键的挑战，因为没有直接的方法来调节现有SSM中多个独立状态的生成。为了解决这个问题，我们利用从SSM动力学得出的简单数学关系将多个状态构成一个状态，该状态有效地近似于串联文本上下文的效果。由于上下文的时间顺序通常可能是不信息的，因此我们通过在所有可能的上下文顺序中通过我们的组成算法获得的有效平均态来实施置换不变性。我们在零射门和微调设置上在Wikitext和MSMARCO上评估了我们的最终方法，并表明我们可以匹配性能最强的基线，同时平均享受5.4倍的速度。]]></description>
      <guid>https://arxiv.org/abs/2502.17605</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估检索增强对社会偏见的影响</title>
      <link>https://arxiv.org/abs/2502.17611</link>
      <description><![CDATA[ARXIV：2502.17611V1公告类型：新 
摘要：检索增强发电（RAG）已成为一种方便地纳入大型语言模型（LLM）基于基于的自然语言生成（NLG）系统的新事实的方法。但是，LLMS已知可以编码重要水平的不公平社会偏见。 RAG在NLG系统中对这些偏见的调节尚不清楚。在本文中，我们系统地研究了抹布系统的不同组成部分与跨三种语言（即英语，日语和中文）和四种社会偏见类型（即性别，种族，年龄，年龄和宗教）产生的文本中所产生的社会偏见之间的关系。具体而言，使用偏见问题回答（BBQ）基准数据集，我们评估了带有不同刻板印象偏见水平的文档收集的抹布回答中的社会偏见，采用了多个用作生成器的LLM。我们发现，即使生成的LLM表现出低水平的偏见，也会在生成的响应中放大文档收集中的偏见。我们的发现引起了人们对将抹布用作将新事实注入NLG系统的技术的担忧，并呼吁在现实世界部署之前仔细评估RAG应用程序中潜在的社会偏见。]]></description>
      <guid>https://arxiv.org/abs/2502.17611</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>倾向于类型性的重新纠正，以减轻低资源语言的不忠</title>
      <link>https://arxiv.org/abs/2502.17664</link>
      <description><![CDATA[ARXIV：2502.17664V1公告类型：新 
摘要：已知多语言大语模型（LLM）更频繁地产生资源受限语言的非信仰输出（Guerreiro等，2023-ARXIV：2303.16104），可能是因为这些类型上多样的语言在其培训数据中的代表性不足。为了减轻这种情况下的不忠，我们建议使用计算轻度辅助模型来营救较大架构的输出。为了证明这种方法的可行性，我们表明单语的4层BERT模型在不到700 MB的数据的情况下从头开始，而无需微调的数据可以识别出三种遗传上不相关的语言中平均准确性为88.33％的忠实摘要，而其形态的复杂性（越南人和Georgian）在其形态复杂性上有所不同。此外，相同的超参数组合也概括了其他三个任务，这表明了申请的申请，超越了忠诚。为了告知类型性意识的模型选择，我们还研究了形态复杂性如何与正则化，模型深度和训练目标相互作用，最终证明，形态上复杂的语言更有可能从辍学中受益，而跨语言的跨语言则通过浅层建筑以及使用标准BERT BERT目标训练。]]></description>
      <guid>https://arxiv.org/abs/2502.17664</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向人类认知：视觉上下文指导融合编码模型中的句法启动</title>
      <link>https://arxiv.org/abs/2502.17669</link>
      <description><![CDATA[ARXIV：2502.17669V1公告类型：新 
摘要：我们介绍了Prismatic，这是第一个多模式结构启动数据集，并提出了无参考评估指标，该指标评估启动效应而无需预定义的目标句子。使用此指标，我们构建了具有不同多模式编码体系结构（双重编码器和融合编码器）的模型，以研究其结构保存能力。我们的发现表明，具有两种编码方法的模型表明了可比的句法启动效应。但是，只有融合编码的模型在启动效应和视觉相似性之间表现出强大的正相关性，这表明认知过程与人类心理语言模式更加一致。这项工作为评估和了解如何在多模式模型中处理句法信息提供了新的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.17669</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义驱动日耳曼语强动词范式的类比变化：一项系统发育研究</title>
      <link>https://arxiv.org/abs/2502.17670</link>
      <description><![CDATA[ARXIV：2502.17670V1公告类型：新 
摘要：关于形态范式的大量研究表明，同种异体的不规则形态学模式在标志着重要的功能区别时，更可能出现和持续存在。更具体地说，已经观察到，在某些日耳曼语言中，过去分词表达了叙事的过去时态，对词干形式和过去的分词共享的茎同种异体具有更大的亲和力，而对当前形式的排除（所谓的ABB模式）具有更大的亲和力，因为它可以增强当前和过去和过去的Biarnity Sminantic Sminantic Rosistion。使用来自14种古老和当代日耳曼语言的107个同源动词的数据，以及一种新型的层次系统发育模型，我们表明，在叙事的过去时态已扩展到过去的分分表中，对这种交替模式存在更大的长期偏爱，从而证实了这一假设。我们进一步阐明了这种关联的基础机制，表明这种关联是因为具有ABB模式的动词更有可能在标志着重要的二元语义反对的情况下保存它。但是，有更少的证据表明，在相同情况下，ABB模式扩展到具有不同模式的动词。这些结果基于争论，即我们在跨语言上观察到的不规则性分布主要是由于（1）保存不规则模式，还是（2）在某些情况下朝着不规则化的主动驱动，并且更符合第一个假设。]]></description>
      <guid>https://arxiv.org/abs/2502.17670</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有全面答案的弥合信息差距：提高后续问题的多样性和信息性</title>
      <link>https://arxiv.org/abs/2502.17715</link>
      <description><![CDATA[ARXIV：2502.17715V1公告类型：新 
摘要：有效的对话系统有望动态生成上下文后续问题，以在保持对话流程的同时引起新信息。尽管人类通过直观地评估获得的信息和丢失的信息来提出各种各样的信息问题，但现有模型通常在这项任务上的人类表现不足。为了减轻这种情况，我们提出了一种基于针对未解决的信息，使用假设的LLM生成的“综合答案”来产生各种各样的信息问题。我们的方法用于增加现有的后续问题数据集。实验结果表明，在增强数据集上微调的语言模型产生了质量和多样性明显更高的后续问题。这种有希望的方法可以有效地采用以后的工作，以增加降低歧义并提高LLM答案准确性的信息寻求信息对话。]]></description>
      <guid>https://arxiv.org/abs/2502.17715</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>训练轮的知识蒸馏</title>
      <link>https://arxiv.org/abs/2502.17717</link>
      <description><![CDATA[ARXIV：2502.17717V1公告类型：新 
摘要：在生成语言建模中，使用知识蒸馏来利用较大的教师模型来培训较小的学生模型，从而改善了学生模型的功能。在本文中，我们制定了一个更一般的知识蒸馏框架，学生在培训期间向老师学习，并学会在测试时间遵循指定测试时间限制的规则时寻求老师的帮助。在此方面，我们首先将知识蒸馏作为熵登记的价值优化问题。采用路径一致性学习来解决这一问题，从而导致新的知识蒸馏算法使用政策和非政策示范。我们使用约束的强化学习将其扩展到一个框架，该框架将使用教师模型用作测试时间参考的框架内。在这种情况下，类似于人类学习者，模型不仅需要学习学习材料，而且还需要学习不同部分的相对难度以优先寻求教师帮助。我们通过在翻译和汇总任务中进行实验检查方法的功效，观察准确性和教师使用的趋势，并指出我们的方法解锁了流行的投机解码方法不可用的操作点。]]></description>
      <guid>https://arxiv.org/abs/2502.17717</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型中的自发给予和计算的贪婪</title>
      <link>https://arxiv.org/abs/2502.17720</link>
      <description><![CDATA[ARXIV：2502.17720V1公告类型：新 
摘要：大型语言模型在接受强化学习培训时，通过推理技术（例如思想链和反思）展示了先进的问题解决能力。但是，目前尚不清楚这些推理能力如何扩展到社会智能。在这项研究中，我们研究了推理如何影响社会困境中的模型结果。首先，我们研究了公共物品游戏中的思想链和反思技术的影响。然后，我们将分析扩展到有关合作和惩罚的六项经济游戏，比较现成的非争议和推理模型。我们发现推理模型减少了合作和规范执行，优先考虑个人合理性。因此，具有更多推理模型的小组通过反复的相互作用表现出较少的合作和降低的收益。这些行为平行于“自发给予和计算贪婪”的人类趋势。我们的结果表明，需要将社会智能与推理能力结合起来，以确保AI支持而不是破坏人类合作直觉。]]></description>
      <guid>https://arxiv.org/abs/2502.17720</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM推理通过有效的操作融合加速</title>
      <link>https://arxiv.org/abs/2502.17728</link>
      <description><![CDATA[ARXIV：2502.17728V1公告类型：新 
摘要：近年来，基于变压器的大语言模型（LLM）的快速发展与它们不断增长且已经巨大的大小紧密相关。许多LLM都包含数百亿个参数，并且需要专门的硬件资源进行培训和推理。变压器体系结构固有的关键挑战之一是支持许多涉及归一化的非线性变换。例如，每个解码器块通常包含至少一个SoftMax操作和两个分层。相应的归一化缩放因子的计算成为主要瓶颈，因为它需要空间集体操作。换句话说，当涉及用于软磁性和分层的分母计算时，所有向量元素都必须汇总到一个单个位置，需要大量的通信。这些集体操作对变形金刚的推断降低了约20％，打败了分布式内存中计算的全部目的。在这项工作中，我们提出了一种极其有效的技术，该技术可以完全掩盖此类集体运营造成的开销。请注意，每个SoftMax和LayerNorm操作通常都在线性层之后。由于非线性和线性操作是在不同的硬件引擎上执行的，因此一旦代数允许这种换向，它们就很容易平行。通过利用线性操作的固有属性，我们可以将上述软磁性和上层的归一化推迟到计算线性层之后。现在，我们可以与矩阵乘法同时计算集体缩放因子，并完全隐藏后者后面的潜伏期。这种并行化可以保留数值的精度，同时显着改善了硬件利用率并降低了整体延迟。]]></description>
      <guid>https://arxiv.org/abs/2502.17728</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>森林：空间推理任务中的参考评估框架</title>
      <link>https://arxiv.org/abs/2502.17775</link>
      <description><![CDATA[ARXIV：2502.17775V1公告类型：新 
摘要：空间推理是人类智力的基本方面。空间认知中的一个关键概念是参考框架（for），它标识了空间表达的观点。尽管具有重要意义，但由于需要空间智能的AI模型，因此受到了有限的关注。缺乏对该领域的大语言模型（LLM）的专用基准和深入评估。为了解决这个问题，我们在空间推理任务（Forest）基准中介绍了参考评估框架，旨在评估LLMS中的理解。我们评估了LLM，以回答使用森林中文本到图像模型中理解和布局产生的问题。我们的结果揭示了各种LLM中不同类别的不同性能差距，影响了它们为文本到图像生成的准确布局的能力。这重点介绍了理解的关键缺点。为了改善理解，我们提出了空间引导的提示，这提高了LLMS提取基本空间概念的能力。我们提出的方法改善了空间推理任务的整体性能。]]></description>
      <guid>https://arxiv.org/abs/2502.17775</guid>
      <pubDate>Wed, 26 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>