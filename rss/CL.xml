<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 06 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>（WhyPHI）微调 PHI-3 以用于多项选择题回答：方法、结果和挑战</title>
      <link>https://arxiv.org/abs/2501.01588</link>
      <description><![CDATA[arXiv:2501.01588v1 公告类型：新
摘要：大型语言模型 (LLM) 因其在理解和生成类似人类的文本方面令人印象深刻的能力而成为各个领域必不可少的工具。准确回答多项选择题 (MCQ) 的能力在教育中具有重要价值，特别是在自动辅导系统和评估平台中。然而，由于幻觉和不明确的提示，调整 LLM 以有效处理 MCQ 任务仍然具有挑战性。这项工作探索了 Microsoft 的 PHI-3\cite{Abdin2024}（一种紧凑而高效的 LLM）在 MCQ 回答中的潜力。我们的贡献包括在 TruthfulQA 数据集上微调模型，设计优化的提示以提高模型性能，以及使用困惑度和传统指标（如准确度和 F1 分数）进行评估。结果显示，经过微调后，PHI-3.5 的 MCQ 处理能力显著提高，困惑度从 4.68 降至 2.27，准确度从 62% 升至 90.8%。这项研究强调了高效模型在自适应学习系统和教育评估中的重要性，为更广泛地融入课堂铺平了道路，特别是在考试准备、学生反馈和个性化学习等领域。]]></description>
      <guid>https://arxiv.org/abs/2501.01588</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PSYCHE：用于评估精神病评估对话代理的多方面患者模拟框架</title>
      <link>https://arxiv.org/abs/2501.01594</link>
      <description><![CDATA[arXiv:2501.01594v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展加速了能够产生类似人类反应的对话代理的开发。由于精神病学评估通常涉及精神科医生和患者之间复杂的对话互动，因此人们越来越有兴趣开发基于 LLM 的精神科评估对话代理 (PACA)，旨在模拟精神科医生在临床评估中的作用。然而，用于对 PACA 与患者互动的临床适用性进行基准测试的标准化方法仍然未得到充分探索。在这里，我们提出了 PSYCHE，这是一个新颖的框架，旨在实现 1) 临床相关性、2) 道德安全、3) 成本效益和 4) PACA 的定量评估。这是通过基于多方面的精神病学结构模拟精神病患者来实现的，该结构定义了模拟患者的个人资料、历史和行为，而 PACA 有望对其进行评估。我们通过对 10 名经过委员会认证的精神科医生进行的研究来验证 PSYCHE 的有效性，并通过对模拟患者话语的深入分析来支持这一验证。]]></description>
      <guid>https://arxiv.org/abs/2501.01594</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ICPC：上下文提示压缩和更快推理</title>
      <link>https://arxiv.org/abs/2501.01625</link>
      <description><![CDATA[arXiv:2501.01625v1 公告类型：新 
摘要：尽管大型语言模型 (LLM) 最近取得了成功，但由于 LLM 输入的大小固定，向 LLM 提供长提示仍然具有挑战性。作为一种补救措施，提示压缩通过删除提示中的冗余标记成为一种有前途的解决方案。然而，在现有工作中使用 LLM 需要额外的计算资源并导致内存开销。为了解决这个问题，我们提出了 ICPC（上下文提示压缩），这是一种新颖且可扩展的提示压缩方法，可以自适应地减少提示长度。ICPC 的关键思想是使用编码器计算每个单词出现在提示中的概率，并通过信息函数计算每个单词所携带的信息，从而有效减少提示压缩过程中的信息损失并提高压缩速度。从经验上讲，我们证明 ICPC 可以有效地压缩不同类别的长文本，从而在不同类型的 NLP 任务上获得更好的性能和速度。]]></description>
      <guid>https://arxiv.org/abs/2501.01625</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于理解大型语言模型中新兴能力的非遍历框架</title>
      <link>https://arxiv.org/abs/2501.01638</link>
      <description><![CDATA[arXiv:2501.01638v1 公告类型：新
摘要：大型语言模型具有在规模上出乎意料的新兴能力，但我们需要一个理论框架来解释它们出现的原因和方式。我们证明语言模型实际上是非遍历系统，同时提供了一个基于 Stuart Kauffman 的相邻可能理论 (TAP) 的数学框架来解释能力的出现。我们的资源受限的 TAP 方程展示了架构、训练和上下文约束如何通过语义空间中的相变相互作用来塑造模型能力。我们通过对三种不同语言模型的实验证明，能力是通过约束交互和路径相关探索引导的离散转换而出现的。该框架为理解语言模型的出现提供了理论基础，并指导了可以指导能力出现的架构的开发。]]></description>
      <guid>https://arxiv.org/abs/2501.01638</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强生物医学知识图谱中的多模态对比表征学习</title>
      <link>https://arxiv.org/abs/2501.01644</link>
      <description><![CDATA[arXiv:2501.01644v1 公告类型：新
摘要：生物医学知识图谱 (BKG) 整合了各种数据集，以阐明生物医学领域内的复杂关系。对这些图进行有效的链接预测可以发现有价值的联系，例如潜在的新型药物-疾病关系。我们引入了一种新颖的多模态方法，将专门的语言模型 (LM) 的嵌入与图对比学习 (GCL) 统一起来，以增强实体内关系，同时采用知识图谱嵌入 (KGE) 模型来捕获实体间关系，以实现有效的链接预测。为了解决现有 BKG 的局限性，我们提出了 PrimeKG++，这是一个丰富的知识图谱，包含多模态数据，包括每种实体类型的生物序列和文本描述。通过将语义和关系信息结合在统一的表示中，我们的方法表现出很强的通用性，即使对于看不见的节点也能进行准确的链接预测。 PrimeKG++ 和 DrugBank 药物靶标相互作用数据集上的实验结果证明了我们的方法在各种生物医学数据集中的有效性和稳健性。我们的源代码、预训练模型和数据均可在 https://github.com/HySonLab/BioMedKG 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2501.01644</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MIRAGE：探索大型语言模型在复杂社交互动环境中的表现</title>
      <link>https://arxiv.org/abs/2501.01652</link>
      <description><![CDATA[arXiv:2501.01652v1 公告类型：新
摘要：大型语言模型 (LLM) 在环境感知、基于推理的决策和模拟复杂人类行为方面表现出卓越的能力，特别是在交互式角色扮演环境中。本文介绍了多元宇宙交互式角色扮演能力综合评估 (MIRAGE)，这是一个全面的框架，旨在评估 LLM 通过谋杀悬疑游戏描绘高级人类行为的能力。MIRAGE 具有八个精心制作的脚本，涵盖不同的主题和风格，提供丰富的模拟。为了评估 LLM 的表现，MIRAGE 采用了四种不同的方法：信任倾向指数 (TII) 来衡量信任和怀疑的动态，线索调查能力 (CIC) 来衡量 LLM 传递信息的能力，互动能力指数 (ICI) 来评估角色扮演能力和脚本合规指数 (SCI) 来评估 LLM 理解和遵循指令的能力。我们的实验表明，即使是像 GPT-4 这样的流行模型，在应对 MIRAGE 带来的复杂性时也面临着重大挑战。数据集和模拟代码可在 \href{https://github.com/lime728/MIRAGE}{github} 中找到。]]></description>
      <guid>https://arxiv.org/abs/2501.01652</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 CoT 的合成器：通过答案合成提高 LLM 性能</title>
      <link>https://arxiv.org/abs/2501.01668</link>
      <description><![CDATA[arXiv:2501.01668v1 公告类型：新
摘要：当前的推理扩展方法，例如自一致性和 Best-of-N，已被证明可有效提高 LLM 在复杂推理任务上的准确性。然而，这些方法严重依赖于候选答案的质量，并且当所有候选答案都不正确时，它们无法产生正确答案。在本文中，我们提出了一种新颖的推理扩展策略，即基于 CoT 的合成器，它利用 CoT 推理通过分析来自多个候选答案的互补信息来合成优质答案，即使所有候选答案都有缺陷。为了实现轻量级且经济高效的实施，我们引入了一个自动数据生成管道，可以创建多样化的训练数据。这使得基于这些数据训练的小型 LLM 能够提高大型模型（包括基于 API 的 LLM）的推理准确性。在四个基准数据集上使用七个策略模型进行的实验结果表明，我们的方法显著提高了性能，在 MATH 数据集上，Llama3-8B 的性能提升了 11.8%，GPT-4o 的性能提升了 10.3%。相应的训练数据和代码可在 https://github.com/RUCKBReasoning/CoT-based-Synthesizer 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2501.01668</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用预训练语言模型进行机器翻译的自适应少量提示</title>
      <link>https://arxiv.org/abs/2501.01679</link>
      <description><![CDATA[arXiv:2501.01679v1 公告类型：新 
摘要：最近，具有上下文学习的大型语言模型 (LLM) 在处理神经机器翻译方面表现出了巨大的潜力。然而，现有证据表明，LLM 对提示敏感，将固定提示应用于下游机器翻译任务的任何输入都是次优的。为了解决这个问题，我们提出了一个自适应的少样本提示 (AFSP) 框架，自动为各种源输入句子选择合适的翻译演示，以进一步引出 LLM 的翻译能力，从而实现更好的机器翻译。首先，我们基于 LLM 的嵌入构建了一个翻译演示检索模块，从对齐的平行翻译语料库中检索前 k 个语义相似的翻译演示。我们没有使用其他嵌入模型进行语义演示检索，而是基于部署的 LLM 的嵌入层构建了一个混合演示检索模块，以构建更好的输入表示，以检索更多与语义相关的翻译演示。然后，为了确保源输入和目标输出之间更好的语义一致性，我们强制部署的 LLM 本身在翻译演示的帮助下生成多个目标语言的输出候选并重新排序这些候选。此外，为了更好地评估我们的 AFSP 框架对最新语言的有效性并扩展神经机器翻译的研究边界，我们构建了一个高质量的外交中英平行数据集，该数据集由 5,528 个平行中英句子组成。最后，在提出的外交中英平行数据集和联合国平行语料库（中英部分）上进行的大量实验证明了我们提出的 AFSP 的有效性和优越性。]]></description>
      <guid>https://arxiv.org/abs/2501.01679</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>心智理论中情境理解的本质：基于故事人物问答的研究</title>
      <link>https://arxiv.org/abs/2501.01705</link>
      <description><![CDATA[arXiv:2501.01705v1 公告类型：新
摘要：心智理论 (ToM) 是一种基本的心理能力，它使人类能够理解和解释他人的心理状态。人类通过整合因果线索和来自广泛背景信息的间接线索来推断他人的想法，这些信息通常来自过去的互动。换句话说，人类的 ToM 严重依赖于对他人背景和生活故事的理解。不幸的是，由于机器使用没有全局背景的短篇叙述，现有的评估机器 ToM 能力的基准在很大程度上忽视了这一方面。在本文中，我们验证了理解 ToM 中长期个人背景的重要性，并评估了 LLM 在这种现实评估场景中的表现。为了实现这一点，我们引入了一个新颖的基准 CharToM-QA，它包含 1,035 个基于经典小说人物的 ToM 问题。我们的人类研究揭示了表现上的显著差异：同一组受过教育的参与者在阅读小说时的表现比没有阅读小说时要好得多。与此同时，我们对最先进的 LLM（包括最新的 o1 模型）进行的实验表明，尽管 LLM 在预训练期间已经看过这些故事，但它们的表现仍然明显不如人类。这凸显了当前 LLM 在捕捉 ToM 推理所需的细微背景信息方面的局限性。]]></description>
      <guid>https://arxiv.org/abs/2501.01705</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 实现法律概念解释自动化：检索、生成和评估</title>
      <link>https://arxiv.org/abs/2501.01743</link>
      <description><![CDATA[arXiv:2501.01743v1 公告类型：新
摘要：法律文章通常包含模糊的概念以适应不断变化的社会。对这些概念进行详细的解释是法律从业人员的一项关键任务，这需要法律专家进行细致而专业的注释，诚然，大规模收集这些注释既费时又费钱。在本文中，我们介绍了一种新颖的检索增强生成框架 ATRI，用于自动从过去的司法判例中检索相关信息并解释模糊的法律概念。我们进一步提出了一个新的基准，即法律概念蕴涵，以自动评估生成的概念解释，而无需专家参与。自动评估表明，我们生成的解释可以有效地帮助大型语言模型 (LLM) 理解模糊的法律概念。法律专家的多方面评估表明，我们的概念解释的质量与人类专家撰写的解释相当。我们的工作对于利用 LLM 支持法律从业人员解释模糊的法律概念及其他方面具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2501.01743</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>字里行间：关于为什么有些文本比其他文本更难的数据集和研究</title>
      <link>https://arxiv.org/abs/2501.01796</link>
      <description><![CDATA[arXiv:2501.01796v1 公告类型：新
摘要：我们的研究旨在更好地理解是什么让特定智力障碍的受众难以阅读文本，更具体地说，是那些认知功能受限的人，例如阅读和理解能力、智商低于 70 以及概念领域存在挑战的人。我们引入了一种基于心理学实证研究以及翻译研究的困难注释方案。本文描述了注释数据集，主要来自在线提供的平行文本（标准英语和易读英语翻译）。我们对四种不同的预训练转换器模型进行了微调，以执行多类分类任务，以预测简化所需的策略。我们还研究了当该语言模型旨在预测句子难度时解释其决策的可能性。资源可从 https://github.com/Nouran-Khallaf/why-tough 获得]]></description>
      <guid>https://arxiv.org/abs/2501.01796</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用梯度缓存的端到端长文档摘要</title>
      <link>https://arxiv.org/abs/2501.01805</link>
      <description><![CDATA[arXiv:2501.01805v1 公告类型：新
摘要：由于训练期间的二次内存消耗，训练基于 Transformer 的编码器-解码器模型进行长文档摘要是一项重大挑战。已经提出了几种方法来在测试时延长输入长度，但使用这些方法进行训练仍然很困难，需要截断输入文档并导致训练和测试条件不匹配。在这项工作中，我们提出了 CachED（用于 E ncoder-D ecoder 模型的梯度缓存），这种方法可以对现有的基于 Transformer 的编码器-解码器模型进行端到端训练，使用整个文档而不进行截断。具体来说，我们对输入文档应用不重叠的滑动窗口，然后在解码器中进行融合。在反向传播期间，梯度被缓存在解码器中，并通过重新计算隐藏向量以块的形式通过编码器，类似于梯度检查点。在长文档摘要实验中，我们将 BART 扩展为 CachED BART，在训练期间处理超过 500K 个标记，并且无需使用任何其他参数即可获得卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.01805</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>杏仁饼干就是证据</title>
      <link>https://arxiv.org/abs/2501.01827</link>
      <description><![CDATA[arXiv:2501.01827v1 公告类型：新
摘要：本文介绍了一个案例研究，介绍如何处理烹饪食谱（更广泛地说，操作说明），以便机器人或人工智能烹饪助手能够在厨房中为人类厨师提供支持。这样的人工智能助手将对社会大有裨益，因为它们可以帮助维持老年人或身体有障碍的人的自主权，或者可以减轻专业厨房的压力。我们提出了一种新颖的计算食谱理解方法，该方法模仿了基于叙述的人类理解过程。以杏仁新月饼干的英文食谱为例，我们展示了如何通过整合语言处理、本体和心理模拟等各种知识源将食谱建模为丰富的叙事结构。我们展示了如何使用此类叙述结构来（a）应对菜谱语言的挑战，例如零重复，（b）优化机器人的规划过程，（c）衡量人工智能系统对当前任务的理解程度，以及（d）让菜谱注释变得独立于语言。]]></description>
      <guid>https://arxiv.org/abs/2501.01827</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于描述性字幕生成的时间序列语言模型</title>
      <link>https://arxiv.org/abs/2501.01832</link>
      <description><![CDATA[arXiv:2501.01832v1 公告类型：新 
摘要：自动生成时间序列数据中可观察模式的代表性自然语言描述可增强可解释性，简化分析并提高时间数据的跨域效用。虽然预先训练的基础模型在自然语言处理 (NLP) 和计算机视觉 (CV) 方面取得了长足的进步，但它们在时间序列分析中的应用受到数据稀缺的阻碍。尽管已经提出了几种基于大型语言模型 (LLM) 的时间序列预测方法，但在 LLM 的背景下，时间序列字幕尚未得到充分探索。在本文中，我们介绍了 TSLM，一种专为时间序列字幕设计的新型时间序列语言模型。TSLM 作为编码器-解码器模型运行，利用文本提示和时间序列数据表示来捕获跨多个阶段的细微时间模式并生成时间序列输入的精确文本描述。 TSLM 首先利用上下文提示合成数据生成，然后通过应用于时间序列-字幕对的新型跨模态密集检索评分对生成的数据进行去噪，从而解决了时间序列字幕中的数据稀缺问题。在各种时间序列字幕数据集上的实验结果表明，TSLM 的表现远胜于现有的多种数据模态的先进方法。]]></description>
      <guid>https://arxiv.org/abs/2501.01832</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用逻辑反驳自己:通过对比问题探究模型防御</title>
      <link>https://arxiv.org/abs/2501.01872</link>
      <description><![CDATA[arXiv:2501.01872v1 公告类型：新
摘要：尽管付出了巨大努力将大型语言模型与人类价值观和道德准则相结合，但这些模型仍然容易受到利用其推理能力的复杂越狱攻击。传统的安全机制通常侧重于检测明确的恶意意图，而没有解决更深层次的漏洞。在这项工作中，我们引入了一种越狱技术 POATE（极点相反查询生成、对抗模板构建和细化），它利用对比推理来引发不道德的反应。POATE 生成具有语义相反意图的提示，并将它们与对抗模板相结合，巧妙地引导模型产生有害响应。我们对六个不同参数大小的语言模型系列进行了广泛的评估，包括 LLaMA3、Gemma2、Phi3 和 GPT-4，以证明攻击的稳健性，与现有方法相比，攻击成功率明显更高（~44%）。我们评估了我们针对七种安全防御措施的攻击，揭示了它们在解决基于推理的漏洞方面的局限性。为了解决这个问题，我们提出了一种防御策略，通过思路链提示和逆向思维来提高推理的稳健性，减轻推理驱动的对抗性攻击。]]></description>
      <guid>https://arxiv.org/abs/2501.01872</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>