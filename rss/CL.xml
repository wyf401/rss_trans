<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 03 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>VSP：评估 VLM 空间规划任务中感知和推理的双重挑战</title>
      <link>https://arxiv.org/abs/2407.01863</link>
      <description><![CDATA[arXiv:2407.01863v1 公告类型：新
摘要：视觉语言模型 (VLM) 是一种令人兴奋的新兴语言模型 (LM) 类，它将经典 LM 功能与图像处理系统的功能融合在一起。然而，这些功能的结合方式并不总是直观的，需要直接研究。VLM 中一项研究不足的能力是视觉空间规划——理解物体的空间排列并制定行动计划以在视觉场景中实现预期结果的能力。在我们的研究中，我们引入了 VSP，这是一个基准，它 1) 评估这些模型中的空间规划能力，2) 将视觉规划任务分解为更细粒度的子任务，包括感知和推理，并测量 LM 在这些子任务中的能力。我们的评估表明，开源和私有 VLM 都无法为即使是简单的空间规划任务生成有效的计划。对细粒度分析任务的评估进一步揭示了模型视觉感知的根本缺陷和推理能力的瓶颈，这解释了它们在一般空间规划任务中表现较差的原因。我们的工作为提高 VLM 的空间规划能力指明了未来方向。我们的基准测试可在 https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2407.01863</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:15 GMT</pubDate>
    </item>
    <item>
      <title>无忧比较：具有代际可分离性的可靠偏好评估</title>
      <link>https://arxiv.org/abs/2407.01878</link>
      <description><![CDATA[arXiv:2407.01878v1 公告类型：新
摘要：通过成对偏好判断对生成的语言进行人类评估是普遍存在的。然而，在常见情况下，例如当模型对的代数非常相似时，或者当随机解码导致代数变化很大时，会导致偏好评级不一致。我们通过引入元评估度量可分离性来解决这些挑战，该度量可估计测试实例对成对偏好评估的适用程度。对于候选测试实例，可分离性从一对模型中抽取多个代数样本，并测量两组代数的可区分程度。我们的实验表明，具有高可分离性值的实例会从人类和自动评分者那里获得更一致的偏好评级。此外，可分离性的分布可以洞察哪些测试基准对于比较模型更有价值。最后，我们将可分离性纳入 ELO 评级，考虑每个测试实例对可靠地排名 LLM 的适用程度。总体而言，可分离性对于人工和自动评分者对 LLM 进行一致、高效和稳健的偏好评估具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2407.01878</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:15 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型知识蒸馏综述：方法、评估与应用</title>
      <link>https://arxiv.org/abs/2407.01885</link>
      <description><![CDATA[arXiv:2407.01885v1 公告类型：新
摘要：大型语言模型（LLM）在各个领域都展示了卓越的能力，引起了学术界和工业界的极大兴趣。尽管它们的性能令人印象深刻，但LLM的庞大规模和计算需求对实际部署构成了相当大的挑战，特别是在资源有限的环境中。在保持准确性的同时压缩语言模型的努力已成为研究的焦点。在各种方法中，知识蒸馏已成为一种有效的技术，可以在不大幅影响性能的情况下提高推理速度。本文从方法、评估和应用三个方面进行了全面的概述，探索了专门针对LLM量身定制的知识蒸馏技术。具体来说，我们将方法分为白盒KD和黑盒KD，以更好地说明它们的差异。此外，我们还探讨了不同蒸馏方法之间的评估任务和蒸馏效果，并提出了未来的研究方向。通过深入了解最新进展和实际应用，该调查为研究人员提供了宝贵的资源，为该领域的持续进步铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2407.01885</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:15 GMT</pubDate>
    </item>
    <item>
      <title>LogEval：日志分析中大型语言模型的综合基准套件</title>
      <link>https://arxiv.org/abs/2407.01896</link>
      <description><![CDATA[arXiv:2407.01896v1 公告类型：新 
摘要：日志分析对于确保信息系统的有序和稳定运行至关重要，特别是在 IT 运营的人工智能 (AIOps) 领域。大型语言模型 (LLM) 在自然语言处理任务中表现出色。在 AIOps 领域，它们在异常检测、故障根本原因分析、操作和维护脚本生成以及警报信息汇总等任务中表现出色。然而，当前 LLM 在日志分析任务中的性能仍未得到充分验证。为了解决这一差距，我们引入了 LogEval，这是一个全面的基准测试套件，旨在首次评估 LLM 在各种日志分析任务中的能力。该基准测试涵盖了日志解析、日志异常检测、日志故障诊断和日志汇总等任务。LogEval 使用 4,000 个公开可用的日志数据条目评估每个任务，并为每个任务采用 15 个不同的提示，以确保评估全面而公平。通过严格评估领先的 LLM，我们展示了各种 LLM 技术对日志分析性能的影响，重点关注自洽性和少样本上下文学习等方面。我们还讨论了与模型量化、中英文问答评估和提示工程相关的发现。这些发现为多语言环境中 LLM 的优势和劣势以及不同提示策略的有效性提供了见解。针对不同的任务采用各种评估方法来准确衡量 LLM 在日志分析中的性能，确保进行全面的评估。从 LogEvals 评估中获得的见解揭示了 LLM 在日志分析任务中的优势和局限性，为研究人员和从业者提供了宝贵的指导。]]></description>
      <guid>https://arxiv.org/abs/2407.01896</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:15 GMT</pubDate>
    </item>
    <item>
      <title>广播警察通讯中的种族和隐私</title>
      <link>https://arxiv.org/abs/2407.01817</link>
      <description><![CDATA[arXiv:2407.01817v1 公告类型：新
摘要：无线电对于现代警察部门的运作至关重要，它们既是一种协作通信技术，又是一种社会技术系统。然而，之前很少有研究调查过它们的使用情况或它们与个人隐私以及种族在警务中的作用之间的联系，这两个话题在美国日益受到关注。作为一个案例研究，我们研究了芝加哥警察局 (CPD) 使用广播警察通信 (BPC) 来协调该市执法人员 (LEO) 的活动。从最近收集的与 CPD 运营相关的 80,775 小时 BPC 档案中，我们分析了 2018 年 8 月 10 日上午 9:00 至下午 5:00 在一个以黑人为主、一个以白人为主和一个以西班牙裔为主的城市地区 (24 小时音频) 播出的无线电传输的文本记录，以探讨三个研究问题：(1) BPC 是否反映了警务中报告的种族差异？ (2) BPC 中如何以及何时提及性别、种族/民族和年龄？(3) BPC 在多大程度上包含敏感信息，这种做法对谁的风险最大？(4) 大型语言模型 (LLM) 会在多大程度上加剧这种风险？我们探讨了警察在 BPC 中使用的词汇和言语行为，将个人特征与当地人口统计数据进行比较，通过 BPC 共享的个人信息以及由此带来的隐私问题。分析表明 (a) 无论背景如何，芝加哥市的警务专业人员都对黑人公众表现出不成比例的关注，(b) BPC 中关于事件信息主要提到性别、种族/民族和年龄等社会人口特征，以及 (c) 不成比例的关注会给黑人公众带来不成比例的隐私风险。]]></description>
      <guid>https://arxiv.org/abs/2407.01817</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:14 GMT</pubDate>
    </item>
    <item>
      <title>使用现成的情感相关推文分类器研究姓名中的国籍偏见和困惑</title>
      <link>https://arxiv.org/abs/2407.01834</link>
      <description><![CDATA[arXiv:2407.01834v1 公告类型：新
摘要：在本文中，我们应用一种方法来量化与来自不同国家的命名实体相关的偏见。我们在目标域数据上创建具有小扰动的反事实示例，而不是依赖模板或特定数据集进行偏见检测。在广泛用于主观性分析的分类器上，包括使用 Twitter 数据的情绪、情感、仇恨言论和冒犯性文本，我们的结果表明，在所有研究的分类器中，与一个国家使用的语言相关的积极偏见。值得注意的是，句子中某些国家名称的存在会强烈影响预测，仇恨言论检测的变化高达 23%，愤怒等负面情绪的预测变化高达 60%。我们假设这些偏见源于预训练语言模型 (PLM) 的训练数据，并发现英语和巴斯克语和毛利语等未知语言中情感预测与 PLM 可能性之间的相关性，揭示了具有加剧相关性的不同模式。此外，我们跟踪了同一句话中反事实示例之间的相关性，以消除句法成分，发现了有趣的结果，表明预训练数据的影响对于英语国家名称更为重要。我们的匿名代码是 [https://anonymous.4open.science/r/biases_ppl-576B/README.md]（可在此处获取）。]]></description>
      <guid>https://arxiv.org/abs/2407.01834</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:14 GMT</pubDate>
    </item>
    <item>
      <title>紫队法学硕士与对抗性防御者训练</title>
      <link>https://arxiv.org/abs/2407.01850</link>
      <description><![CDATA[arXiv:2407.01850v1 公告类型：新
摘要：现有的保护 LLM 的努力仅限于主动暴露目标 LLM 的漏洞并随时适应新出现的安全风险。为了解决这个问题，我们提出了带有对抗性防御者训练 (PAD) 的紫队 LLM，这是一种通过新颖地结合红队（攻击）和蓝队（安全训练）技术来保护 LLM 的管道。在 PAD 中，我们以自对战的方式自动收集涵盖特定安全风险的 LLM 漏洞的对话数据，其中攻击者旨在引发不安全的响应，而防御者则对这些攻击生成安全响应。然后，我们以生成对抗网络风格更新这两个模块，通过训练攻击者引发更多不安全的响应并更新防御者以识别它们并解释不安全的原因。实验结果表明，PAD 在发现有效攻击和建立强大的安全护栏方面都明显优于现有基线。此外，我们的研究结果表明，PAD 在安全性和整体模型质量之间取得了良好的平衡。我们还揭示了保护 LLM 的关键挑战，包括防御多轮攻击以及需要更细致的策略来识别特定风险。]]></description>
      <guid>https://arxiv.org/abs/2407.01850</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:14 GMT</pubDate>
    </item>
    <item>
      <title>通过语言自然和多样化的数据集改进多语言教学微调</title>
      <link>https://arxiv.org/abs/2407.01853</link>
      <description><![CDATA[arXiv:2407.01853v1 公告类型：新
摘要：大型语言模型 (LLM) 的进步显著增强了指令遵循能力。然而，大多数指令微调 (IFT) 数据集主要是英语，限制了其他语言的模型性能。创建多语言 IFT 数据集的传统方法，例如翻译现有的英语 IFT 数据集或通过模板将现有的 NLP 数据集转换为 IFT 数据集，难以捕捉语言细微差别并确保及时（指令）多样性。为了解决这个问题，我们提出了一种收集多语言 IFT 数据集的新方法，该方法保留了语言的自然性并确保了及时的多样性。这种方法利用以英语为重点的 LLM、单语语料库和评分函数来创建多种语言的高质量、多样化的 IFT 数据集。实验表明，使用这些 IFT 数据集进行微调的 LLM 在生成和判别任务中均表现出显著的改进，表明 LLM 在非英语语境中的语言理解能力有所增强。具体来说，在多语言摘要任务中，使用我们的 IFT 数据集的 LLM 比使用基于翻译和基于模板的数据集进行微调的 LLM 分别提高了 17.57% 和 15.23%。]]></description>
      <guid>https://arxiv.org/abs/2407.01853</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:14 GMT</pubDate>
    </item>
    <item>
      <title>NLPGuard：用于减轻 NLP 分类器使用受保护属性的框架</title>
      <link>https://arxiv.org/abs/2407.01697</link>
      <description><![CDATA[arXiv:2407.01697v1 公告类型：新
摘要：预计人工智能法规将禁止机器学习模型在训练期间使用敏感属性。然而，最新的自然语言处理 (NLP) 分类器依赖于深度学习，它们作为黑盒系统运行，使此类滥用的检测和补救变得复杂。NLP 中的传统偏见缓解方法旨在根据性别或种族等属性在不同群体之间实现可比的表现，但未能解决对受保护属性的依赖这一根本问题。为了部分解决这个问题，我们引入了 NLPGuard，这是一个用于减轻 NLP 分类器对受保护属性的依赖的框架。NLPGuard 将未标记的数据集、现有的 NLP 分类器及其训练数据作为输入，生成修改后的训练数据集，该数据集在不影响准确性的情况下显着减少对受保护属性的依赖。NLPGuard 应用于三个分类任务：识别有毒语言、情绪分析和职业分类。我们的评估表明，当前的 NLP 分类器严重依赖受保护的属性，其中最有预测性的单词与这些属性相关，高达 $23\%$。然而，NLPGuard 有效地将这种依赖性降低了高达 $79\%$，同时略微提高了准确性。]]></description>
      <guid>https://arxiv.org/abs/2407.01697</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>DiscoveryBench：利用大型语言模型实现数据驱动的发现</title>
      <link>https://arxiv.org/abs/2407.01725</link>
      <description><![CDATA[arXiv:2407.01725v1 公告类型：新
摘要：使用大型语言模型 (LLM) 在代码生成、函数调用和数据分析方面的快速发展是否有助于仅从一组提供的数据集中自动搜索和验证假设？为了评估这个问题，我们提出了 DiscoveryBench，这是第一个将数据驱动发现的多步骤过程形式化的综合基准。该基准旨在系统地评估当前模型在发现任务中的能力，并提供有用的资源来改进它们。我们的基准包含 264 个任务，这些任务来自 6 个不同的领域，例如社会学和工程学，通过从已发表的论文中手动推导发现工作流程来近似研究人员面临的现实挑战，其中每个任务都由数据集、其元数据和自然语言中的发现目标定义。我们还提供了 903 个合成任务，以跨任务复杂性进行受控评估。此外，我们对数据驱动发现的结构化形式化可以进行基于方面的评估，从而为不同的故障模式提供有用的见解。我们在 DiscoveryBench 上使用开放和封闭的 LLM 作为基准，评估了几种流行的基于 LLM 的推理框架，发现即使是最好的系统也只能获得 25% 的分数。因此，我们的基准说明了自主数据驱动发现的挑战，并成为社区取得进步的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2407.01725</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>分析模因文本中的说服策略：语言模型与释义丰富功能的融合</title>
      <link>https://arxiv.org/abs/2407.01784</link>
      <description><![CDATA[arXiv:2407.01784v1 公告类型：新
摘要：本文介绍了我们对 meme 文本中说服技巧的分层多标签检测方法。我们的模型是作为最近的 SemEval 任务的一部分开发的，基于对单个语言模型（BERT、XLM-RoBERTa 和 mBERT）的微调，并利用基于均值的集成模型，以及通过 ChatGPT 的释义生成进行数据集增强。研究范围包括通过创新的训练技术和数据增强策略来提高模型性能。要解决的问题是如何有效识别和分类 meme 文本中的多种说服技巧，这项任务因此类内容的多样性和复杂性而变得复杂。本文的目的是通过改进模型训练方法和检查平衡与不平衡训练数据集的影响来提高检测准确性。结果和讨论的新颖之处在于，发现使用释义进行训练可以提高模型性能，但平衡的训练集比更大的不平衡训练集更有优势。此外，分析揭示了不加区分地合并来自不同分布的释义的潜在缺陷，这可能会引入大量噪音。SemEval 2024 数据的结果证实了这些见解，表明使用所提出的方法可以提高模型效率。]]></description>
      <guid>https://arxiv.org/abs/2407.01784</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>让每一句话都扎实：通过交错引用声明生成改进检索增强型 LLM</title>
      <link>https://arxiv.org/abs/2407.01796</link>
      <description><![CDATA[arXiv:2407.01796v1 公告类型：new 
摘要：检索增强生成（RAG）已被广泛用于增强知识密集型任务中的大型语言模型（LLM）。最近，属性文本生成（ATG）受到越来越多的关注，它在RAG中提供引用来支持模型的响应，从而增强LLM生成内容的可信度并方便验证。现有方法主要采用粗粒度的归因，链接到段落级参考文献或提供段落级引用。然而，这些方法在可验证性方面仍然存在不足，并且需要一定的时间成本进行事实核查。本文提出了一种细粒度的ATG方法ReClaim（Refer＆Claim），该方法逐步交替生成参考文献和答案。与传统的粗粒度归因不同，ReClaim允许模型在长篇问答任务中为每个答案句子添加句子级的细粒度引用。我们的实验涵盖了各种训练和推理方法以及多个 LLM，验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.01796</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>JailbreakZoo：越狱大型语言和视觉语言模型的调查、概况和前景</title>
      <link>https://arxiv.org/abs/2407.01599</link>
      <description><![CDATA[arXiv:2407.01599v1 公告类型：新
摘要：通过大型语言模型 (LLM) 和视觉语言模型 (VLM) 的发展，人工智能 (AI) 迅速发展，为各个技术领域带来了重大进步。虽然这些模型增强了自然语言处理和视觉交互任务的能力，但它们的日益普及引发了对安全和道德一致性的关键担忧。这项调查对新兴的越狱领域（故意规避 LLM 和 VLM 的道德和操作界限）以及随之而来的防御机制的发展进行了广泛的回顾。我们的研究将越狱分为七种不同的类型，并详细阐述了解决这些漏洞的防御策略。通过这项全面的检查，我们发现了研究差距并提出了未来研究的方向，以增强 LLM 和 VLM 的安全框架。我们的研究结果强调了统一的观点的必要性，该观点将越狱策略和防御解决方案相结合，为下一代语言模型营造一个强大、安全和可靠的环境。更多详细信息请访问我们的网站：\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}。]]></description>
      <guid>https://arxiv.org/abs/2407.01599</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>纯注意力 Hardmax Transformer 中的聚类及其在情绪分析中的作用</title>
      <link>https://arxiv.org/abs/2407.01602</link>
      <description><![CDATA[arXiv:2407.01602v1 公告类型：新
摘要：Transformer 是极为成功的机器学习模型，但其数学特性仍不太为人所知。在这里，我们严格描述具有硬最大自注意力和规范化子层的 Transformer 的行为，因为层数趋于无穷大。通过将此类 Transformer 视为描述欧几里得空间中点的演化的离散时间动态系统，并得益于基于超平面分离的自注意力机制的几何解释，我们表明 Transformer 输入渐近收敛到由称为领导者的特殊点确定的聚类平衡。然后，我们利用这种理论理解，使用完全可解释的 Transformer 模型来解决语言处理中的情感分析问题，该模型通过将无意义的单词聚集在具有最多含义的领导者单词周围来有效地捕捉“上下文”。最后，我们概述了弥合 Transformer 的数学分析与其实际实现之间的差距的剩余挑战。]]></description>
      <guid>https://arxiv.org/abs/2407.01602</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>解读影响思维链有效性的因素：概率、记忆和噪声推理</title>
      <link>https://arxiv.org/abs/2407.01687</link>
      <description><![CDATA[arXiv:2407.01687v1 公告类型：新
摘要：思维链 (CoT) 提示已被证明可以增强大型语言模型 (LLM) 的多步骤推理能力。然而，关于 LLM 在给出 CoT 提示时是表现出抽象概括还是依赖浅层启发式方法的争论仍然存在。为了了解影响 CoT 推理的因素，我们提供了一个详细的案例研究，即解码移位密码的符号推理任务，其中字母在字母表中向前移动一定步数。GPT-4 在标准提示下对大多数移位密码的准确率达到零，但使用 CoT 时，其准确率提高到平均 32%。通过专注于一个相对简单的任务，我们能够确定系统地影响 CoT 性能的三个因素：任务预期输出的概率（概率）、模型在预训练期间隐式学习的内容（记忆）以及推理中涉及的中间操作的数量（嘈杂推理）。我们表明这些因素可以极大地影响任务的准确性；例如，改变输出的发生概率可以将准确率从 26% 提高到 70%。我们还表明，模型必须明确地生成中间步骤作为输出，以便对其进行调节以提高正确答案的概率。我们的实验表明，只要模型这样做，提示中演示的有效性就无关紧要了。总的来说，我们得出结论，CoT 提示表现既反映了记忆，也反映了真实推理的概率版本。]]></description>
      <guid>https://arxiv.org/abs/2407.01687</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:12 GMT</pubDate>
    </item>
    </channel>
</rss>