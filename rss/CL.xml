<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 11 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>跨语言词汇对齐的本地测量：领域和词级视角</title>
      <link>https://arxiv.org/abs/2410.07239</link>
      <description><![CDATA[arXiv:2410.07239v1 公告类型：新
摘要：到目前为止，NLP 关于词汇表示空间相互对齐的研究主要集中在整个语言空间的对齐上。然而，认知科学长期以来一直专注于局部视角，研究翻译等价物是否真正具有相同的含义，或者文化和地域影响导致含义变化的程度。随着最近的技术进步和可用数据的增加，跨语言词汇对齐这一长期存在的问题现在可以以更加数据驱动的方式来处理。然而，为这项任务制定指标需要一些方法来比较指标的有效性。我们解决了这一差距，并提出了一种分析合成验证和一种使用亲属关系领域词汇差距的新型自然验证的方法。我们进一步提出了基于语境化嵌入的新指标，这是这项任务迄今为止尚未探索过的。我们的分析涵盖了 16 种不同的语言，表明使用较新的语言模型有很大的改进空间。我们的研究为更准确、更细致的跨语言词汇对齐方法和评估铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.07239</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DA-Code：大型语言模型的代理数据科学代码生成基准</title>
      <link>https://arxiv.org/abs/2410.07331</link>
      <description><![CDATA[arXiv:2410.07331v1 公告类型：新
摘要：我们引入了 DA-Code，这是一个专门用于评估基于代理的数据科学任务的 LLM 的代码生成基准。该基准具有三个核心要素：首先，DA-Code 中的任务本质上具有挑战性，与传统的代码生成任务不同，并且要求在基础和规划方面具有高级编码技能。其次，DA-Code 中的示例都基于真实而多样的数据，涵盖了广泛的复杂数据整理和分析任务。第三，为了解决这些任务，模型必须使用复杂的数据科学编程语言来执行复杂的数据处理并得出答案。我们在可控且可执行的环境中设置基准，该环境与实际数据分析场景相一致并且可扩展。注释者精心设计评估套件，以确保评估的准确性和稳健性。我们开发了 DA-Agent 基线。实验表明，尽管基准测试结果优于其他现有框架，但使用当前最好的 LLM 也只能达到 30.5% 的准确率，还有很大的改进空间。我们在 [https://da-code-bench.github.io](https://da-code-bench.github.io) 发布了基准测试。]]></description>
      <guid>https://arxiv.org/abs/2410.07331</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SparseGrad：一种高效微调 MLP 层的选择性方法</title>
      <link>https://arxiv.org/abs/2410.07383</link>
      <description><![CDATA[arXiv:2410.07383v1 公告类型：新
摘要：通过增加参数数量和处理文本的长度，Transformer 模型的性能得到了增强。因此，对整个模型进行微调成为一个内存密集型过程。参数高效微调 (PEFT) 的高性能方法通常与 Attention 块一起使用，并且经常忽略包含大约一半模型参数的 MLP 块。我们提出了一种新的选择性 PEFT 方法，即 SparseGrad，它在 MLP 块上表现良好。我们将层梯度转移到一个只有大约 1\% 的层元素保持显著的空间。通过将梯度转换为稀疏结构，我们减少了更新参数的数量。我们应用 SparseGrad 对 NLU 任务的 BERT 和 RoBERTa 进行微调，对问答任务的 LLaMa-2 进行微调。在这些实验中，在内存要求相同的情况下，我们的方法优于 LoRA 和 MeProp，以及流行且最先进的 PEFT 方法。]]></description>
      <guid>https://arxiv.org/abs/2410.07383</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提倡字符错误率作为多语言 ASR 评估指标</title>
      <link>https://arxiv.org/abs/2410.07400</link>
      <description><![CDATA[arXiv:2410.07400v1 公告类型：新
摘要：自动语音识别 (ASR) 系统传统上是使用英语数据集进行评估的，其中单词错误率 (WER) 是主要指标。WER 的简单性和易于解释性促使其被广泛采用，尤其是对于英语。然而，随着 ASR 系统扩展到多语言环境，WER 以各种方式失败，特别是对于形态复杂的语言或没有明确单词边界的语言。我们的工作记录了 WER 作为评估指标的局限性，并主张将字符错误率 (CER) 作为多语言 ASR 评估的主要指标。我们表明 CER 避免了 WER 面临的许多挑战，并且在书写系统中表现出更大的一致性。我们通过对三种语言的 ASR 转录进行人工评估来支持我们的主张：马拉雅拉姆语、英语和阿拉伯语，这三种语言表现出不同的形态特征。我们表明，即使对于英语，CER 与人类判断的相关性也比 WER 更密切。为了便于进一步研究，我们发布了人工评估数据集，以便将来对 ASR 指标进行基准测试。我们的研究结果表明，在多语言 ASR 评估中，应优先考虑 CER，或至少对其进行补充，以考虑不同语言的不同语言特征。]]></description>
      <guid>https://arxiv.org/abs/2410.07400</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>C4 数据集是否适合剪枝？LLM 剪枝校准数据调查</title>
      <link>https://arxiv.org/abs/2410.07461</link>
      <description><![CDATA[arXiv:2410.07461v1 公告类型：新 
摘要：网络剪枝已成为一种潜在的解决方案，可以降低 LLM 的部署成本。然而，现有的 LLM 剪枝方法普遍依赖 C4 数据集作为计算剪枝分数的校准数据，而其最优性尚未得到探索。在本研究中，我们评估了 LLM 剪枝中校准数据的选择，涵盖了 LLM 训练和评估中最常用的各种数据集，包括四个相关数据集以及包含九个数据集的三类下游任务。每个下游数据集分别以上下文学习 (ICL) 和思维链 (CoT) 提示。除了已经很有趣的观察结果，即校准数据的选择会显着影响剪枝后的 LLM 的性能之外，我们的结果还揭示了一些微妙且常常出乎意料的发现，总结如下：(1) 即使在常用的预训练数据集中，C4 也不是 LLM 剪枝的最佳选择； (2) 算术数据集用作校准数据时，其性能与预训练数据集相当甚至更好；(3) 与预训练数据相比，使用下游数据集进行修剪并不一定有助于相应的下游任务；(4) ICL 对所有数据类别都有广泛益处，而 CoT 仅对某些任务有用。我们的研究结果揭示了仔细选择 LLM 修剪校准数据的重要性，并为在实际应用中更有效地部署这些强大的模型铺平了道路。我们的代码发布在：https://github.com/abx393/llm-pruning-calibration-data。]]></description>
      <guid>https://arxiv.org/abs/2410.07461</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>归因文本生成中事实不一致的定位</title>
      <link>https://arxiv.org/abs/2410.07473</link>
      <description><![CDATA[arXiv:2410.07473v1 公告类型：新 
摘要：人们对以不同粒度级别手动和自动检测模型生成文本中的幻觉的兴趣日益浓厚。但是，大多数现有方法都无法准确指出错误。在这项工作中，我们引入了 QASemConsistency，这是一种用于在细粒度级别定位可归因文本生成中事实不一致的新形式。从新戴维森形式语义学中汲取灵感，我们建议将生成的文本分解为最小的谓词-论元级命题，以简单的问答 (QA) 对表示，并评估每个单独的 QA 对是否由可信参考文本支持。由于每个 QA 对对应于谓词和论元之间的单一语义关系，因此 QASemConsistency 可以有效地定位不受支持的信息。我们首先通过收集众包的粒度一致性错误注释，同时实现注释者之间的高度一致性 ($\kappa &gt; 0.7)，证明了 QASemConsistency 方法对人工注释的有效性。然后，我们实现了几种自动检测局部事实不一致的方法，同时使用了监督蕴涵模型和开源 LLM。]]></description>
      <guid>https://arxiv.org/abs/2410.07473</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MoDEM：领域专家模型的混合</title>
      <link>https://arxiv.org/abs/2410.07490</link>
      <description><![CDATA[arXiv:2410.07490v1 公告类型：新
摘要：我们提出了一种新方法，通过将领域提示路由与领域专业模型相结合来提高大型语言模型 (LLM) 的性能和效率。我们引入了一个系统，该系统利用基于 BERT 的路由器将传入的提示定向到最合适的领域专家模型。这些专家模型专门针对健康、数学和科学等领域进行了调整。我们的研究表明，这种方法可以显著优于同等规模的通用模型，从而在各种基准测试中实现卓越的性能成本比。这项研究的意义表明 LLM 开发和部署可能会发生范式转变。人工智能的未来可能在于开发更小、高度专业化的模型生态系统以及复杂的路由系统，而不是仅仅专注于创建越来越大的通用模型。这种方法可以提高资源利用率、降低计算成本并实现卓越的整体性能。]]></description>
      <guid>https://arxiv.org/abs/2410.07490</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语音转文本应用的传感器一致性正则化</title>
      <link>https://arxiv.org/abs/2410.07491</link>
      <description><![CDATA[arXiv:2410.07491v1 公告类型：新
摘要：一致性正则化是一种常用的做法，它鼓励模型从扭曲的输入特征中生成一致的表示并提高模型的泛化能力。它在用交叉熵标准优化的各种语音应用中显示出显着的改进。然而，对于基于传感器的方法，应用一致性正则化并不是一件容易的事，由于具有竞争力的性能和流式特性，这些方法被广泛应用于语音应用。主要的挑战来自于传感器优化标准的广阔的对齐空间，并不是空间内的所有对齐都对模型优化有同等的贡献。在本研究中，我们提出了传感器一致性正则化 (TCR)，一种用于传感器模型的一致性正则化方法。我们应用诸如规范增强和 dropout 之类的扭曲来创建不同的数据视图并最小化分布差异。我们利用职业概率为传感器输出分布赋予不同的权重，因此只有接近 oracle 对齐的对齐才会对模型学习做出贡献。我们的实验表明，所提出的方法优于其他一致性正则化实现，与 \textsc{Librispeech} 数据集上的强基线相比，可以有效地将字错误率 (WER) 降低 4.3\%。]]></description>
      <guid>https://arxiv.org/abs/2410.07491</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PublicHearingBR：用于长篇文档摘要的巴西葡萄牙语公开听证会记录数据集</title>
      <link>https://arxiv.org/abs/2410.07495</link>
      <description><![CDATA[arXiv:2410.07495v1 公告类型：新
摘要：本文介绍了 PublicHearingBR，这是一个用于总结长篇文档的巴西葡萄牙语数据集。该数据集由巴西众议院举行的公开听证会的记录组成，并附有新闻文章和结构化摘要，其中包含参加听证会的个人及其陈述或意见。该数据集支持葡萄牙语长文档摘要系统的开发和评估。我们的贡献包括数据集、为未来研究建立基线的混合摘要系统，以及对涉及大型语言模型的摘要评估指标的讨论，解决了生成的摘要中出现幻觉的挑战。作为此次讨论的结果，该数据集还提供了可用于葡萄牙语自然语言推理任务的带注释数据。]]></description>
      <guid>https://arxiv.org/abs/2410.07495</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用法学硕士 (LLM) 发现法律因素</title>
      <link>https://arxiv.org/abs/2410.07504</link>
      <description><![CDATA[arXiv:2410.07504v1 公告类型：新
摘要：因素是法律分析和法律推理计算模型的基础组成部分。这些基于因素的表示使律师、法官以及人工智能和法律研究人员能够推理法律案件。在本文中，我们介绍了一种利用大型语言模型 (LLM) 来发现有效代表法律领域的因素列表的方法。我们的方法以原始法庭意见作为输入，并生成一组因素和相关定义。我们证明，半自动化方法结合最少的人为参与，可以生成能够预测案件结果的因素表示，即使还没有专家定义的因素那么好，也能取得中等程度的成功。]]></description>
      <guid>https://arxiv.org/abs/2410.07504</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Thought2Text：使用大型语言模型 (LLM) 从 EEG 信号生成文本</title>
      <link>https://arxiv.org/abs/2410.07507</link>
      <description><![CDATA[arXiv:2410.07507v1 公告类型：新
摘要：以可理解的形式解码和表达大脑活动是人工智能领域一个具有挑战性的前沿。本文介绍了 Thought2Text，它使用经过指令调整的大型语言模型 (LLM) 并通过 EEG 数据进行微调来实现这一目标。该方法涉及三个阶段：(1) 训练 EEG 编码器进行视觉特征提取，(2) 在图像和文本数据上微调 LLM，实现多模态描述生成，以及 (3) 在 EEG 嵌入上进一步微调以在推理过程中直接从 EEG 生成文本。在为六个受试者收集的带有图像刺激的公共 EEG 数据集上进行的实验证明了多模态 LLM (LLaMa-v3、Mistral-v0.3、Qwen2.5) 的有效性，并使用传统语言生成评估指标、基于 GPT-4 的评估和人类专家的评估进行了验证。这种方法标志着便携式、低成本“思想到文本”技术的重大进步，在神经科学和自然语言处理（NLP）领域都有潜在的应用。]]></description>
      <guid>https://arxiv.org/abs/2410.07507</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新闻记者：广播电视新闻的多语言法学硕士框架</title>
      <link>https://arxiv.org/abs/2410.07520</link>
      <description><![CDATA[arXiv:2410.07520v1 公告类型：新
摘要：大型语言模型 (LLM) 已迅速成为许多对话聊天机器人的必备工具，因为它们能够为各种查询提供连贯的答案。用于训练这些 LLM 的数据集通常是通用样本和合成样本的混合，因此缺乏为电视新闻提供正确且可验证答案所需的验证。
我们收集并分享了从美国各个新闻频道的新闻录音记录中提取的大量 QA 对。然后使用生成的 QA 对来微调现成的 LLM 模型。我们的模型在几个开放的 LLM 基准上超越了类似大小的基础模型。我们进一步整合并提出了一种 RAG 方法来改善我们答案的语境化，并将其指向可验证的新闻记录。]]></description>
      <guid>https://arxiv.org/abs/2410.07520</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DemoShapley：情境学习演示评估</title>
      <link>https://arxiv.org/abs/2410.07523</link>
      <description><![CDATA[arXiv:2410.07523v1 公告类型：新
摘要：利用上下文学习 (ICL) 的大型语言模型 (LLM) 为各种任务中的小样本学习设定了新的基准，而无需针对特定任务进行微调。然而，大量研究表明，ICL 的有效性受到演示的选择和排序的显著影响。考虑到演示选择在 ICL 中的关键作用，我们引入了 DemoShapley，它受到 Data Shapley 估值定理的启发。这种方法评估了单个演示实例的影响，区分了那些有积极贡献的实例和那些可能阻碍性能的实例。我们的研究结果表明，DemoShapley 不仅在准确性和公平性方面提高了模型性能，而且还概括了来自不同于上下文演示的领域的查询，突出了其在优化 ICL 演示选择方面的多功能性和有效性。最后但并非最不重要的是，DemoShapley 展示了它帮助识别演示集中的噪声数据的能力。]]></description>
      <guid>https://arxiv.org/abs/2410.07523</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将大型语言模型升级为专家混合模型</title>
      <link>https://arxiv.org/abs/2410.07524</link>
      <description><![CDATA[arXiv:2410.07524v1 公告类型：新
摘要：将预训练的密集语言模型升级为稀疏混合专家 (MoE) 模型是一种提高已训练模型模型容量的有效方法。然而，大规模升级的最佳技术仍不清楚。在这项工作中，我们对十亿参数规模语言模型的升级方法和超参数进行了广泛的研究。我们提出了一种新颖的“虚拟组”初始化方案和权重缩放方法，以实现升级为细粒度的 MoE 架构。通过消融，我们发现升级优于持续的密集模型训练。此外，我们表明 softmax-then-topK 专家路由比 topK-then-softmax 方法有所改进，更高粒度的 MoE 可以帮助提高准确性。最后，我们在 1T 标记上升级了 Nemotron-4 15B，并将其与在相同 1T 标记上连续训练的同一模型版本进行了比较：连续训练模型实现了 65.3% MMLU，而升级后的模型实现了 67.6%。我们的结果提供了见解和最佳实践，可有效利用升级来构建 MoE 语言模型。]]></description>
      <guid>https://arxiv.org/abs/2410.07524</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MKGL：掌握三词语言</title>
      <link>https://arxiv.org/abs/2410.07526</link>
      <description><![CDATA[arXiv:2410.07526v1 公告类型：新
摘要：大型语言模型 (LLM) 在一系列自然语言处理 (NLP) 任务中具有显着的先进性能。然而，它们在知识图谱 (KG) 中的应用仍然是一个未被充分探索的领域，知识图谱以三元组的形式描述事实并允许最小的幻觉。在本文中，我们通过引入专门的 KG 语言 (KGL) 来研究 LLM 与 KG 的集成，其中一个句子精确地由一个实体名词、一个关系动词组成，并以另一个实体名词结尾。尽管 KGL 的词汇对 LLM 来说很陌生，但我们通过量身定制的词典和说明性句子来促进其学习，并通过实时 KG 上下文检索和 KGL 标记嵌入增强来增强上下文理解。我们的结果表明，LLM 可以在 KGL 中实现流畅性，与传统的 KG 嵌入方法相比，大大减少了 KG 完成时的错误。此外，我们增强的 LLM 表现出卓越的能力，能够从初始实体生成准确的三字句子，并从 KG 中解释新的未见术语。]]></description>
      <guid>https://arxiv.org/abs/2410.07526</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>