<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Decompose-ToM：通过模拟和任务分解增强大型语言模型中的心智理论推理</title>
      <link>https://arxiv.org/abs/2501.09056</link>
      <description><![CDATA[arXiv:2501.09056v1 公告类型：新
摘要：心智理论 (ToM) 是理解和反思他人心理状态的能力。尽管这种能力对于人际交往至关重要，但在大型语言模型 (LLM) 上进行的测试表明，它们对此只有基本的了解。尽管最强大的闭源 LLM 在某些 ToM 任务上的表现已经接近人类，但它们在涉及更结构化推理的复杂任务变体上仍然表现不佳。在这项工作中，我们利用认知心理学中的“假装游戏”或“模拟理论”概念来提出“分解 ToM”：一种基于 LLM 的推理算法，可提高复杂 ToM 任务的模型性能。我们递归模拟用户视角，并将 ToM 任务分解为一组更简单的功能：主题识别、问题重构、世界模型更新和知识可用性。我们在高阶 ToM 任务和对话环境中测试 ToM 能力的任务上测试了该算法，结果表明，与基线方法相比，我们的方法在各个模型中显示出显着的改进，同时只需要在任务之间进行最少的快速调整，并且不需要额外的模型训练。]]></description>
      <guid>https://arxiv.org/abs/2501.09056</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SteLLA：使用 LLM 和 RAG 进行结构化评分的系统</title>
      <link>https://arxiv.org/abs/2501.09092</link>
      <description><![CDATA[arXiv:2501.09092v1 公告类型：新
摘要：大型语言模型 (LLM) 在许多应用中表现出强大的通用能力。然而，如何使它们成为某些特定任务（例如自动简答评分 (ASAG)）的可靠工具仍然是一个挑战。我们提出了 SteLLA（使用带有 RAG 的 LLM 的结构化评分系统），其中 a) 使用检索增强生成 (RAG) 方法通过从基于讲师提供的参考答案和评分标准的高度相关和可靠的外部知识中提取结构化信息，专门为 ASAG 任务增强 LLM 的能力，b) LLM 对学生答案进行结构化和基于问答的评估，以提供分析成绩和反馈。从大学水平的生物学课程中收集了一个包含学生考试答案的真实数据集。实验表明，我们提出的系统可以与人类评分者达成基本一致，同时提供问题中检查的所有知识点的细分成绩和反馈。对 GPT4 生成的反馈进行定性和错误分析表明，GPT4 擅长捕捉事实，但在评分任务中可能容易从给定的文本中推断出过多的含义，这为 ASAG 系统中 LLM 的使用提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2501.09092</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在开放式回答评估中使用大型语言模型生成和提炼来增强人工注释的训练数据</title>
      <link>https://arxiv.org/abs/2501.09126</link>
      <description><![CDATA[arXiv:2501.09126v1 公告类型：新 
摘要：像 GPT-4o 这样的大型语言模型 (LLM) 可以帮助以低成本和规模自动化文本分类任务。然而，人们对 LLM 输出的有效性和可靠性存在重大担忧。相比之下，人工编码通常更可靠，但大规模采购成本高昂。在这项研究中，我们提出了一种混合解决方案来利用两者的优势。我们结合人工编码的数据和合成的 LLM 生成的数据来微调经典的机器学习分类器，将两者提炼成一个较小的 BERT 模型。我们在人工编码的测试集上评估我们的方法，作为 LLM 输出质量的有效性度量。在三个实验中，我们系统地改变 LLM 生成的样本的大小、种类和一致性，并参考 LLM 调整的最佳实践。我们的研究结果表明，用合成样本扩充数据集可以提高分类器的性能，在 80% 的合成数据和 20% 的人工编码数据比率下可获得最佳结果。较低的温度设置为 0.3，对应于 LLM 生成中较小的变化，产生了更稳定的改进，但也限制了从增强样本中学习模型。相反，较高的温度设置（0.7 及以上）会给性能估计带来更大的变化，有时还会导致性能下降。因此，LLM 可能会产生更均匀的输出，而分类器会过拟合，或者产生更多样化的输出，这可能会因与预测任务无关的信息而降低模型性能。过滤掉不一致的合成样本并不能提高性能。我们得出结论，整合人工和 LLM 生成的数据来改进评估中的文本分类模型是一种可扩展的解决方案，既能利用人工编码的准确性，又能利用 LLM 输出的多样性。]]></description>
      <guid>https://arxiv.org/abs/2501.09126</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多语言法学硕士在双语文字处理中努力将正字法和语义联系起来</title>
      <link>https://arxiv.org/abs/2501.09127</link>
      <description><![CDATA[arXiv:2501.09127v1 公告类型：新
摘要：双语词汇处理是由两种语言的音系、正字法和语义特征在综合心理词汇中复杂的相互作用形成的。在人类中，这体现在同源词（正字法形式和含义都相似的词（例如，blind，在英语和德语中意为“盲人”））的处理容易度上，而跨语言同形异义词（正字法形式相同但含义不同（例如，gift，在英语中意为“礼物”，但在德语中意为“毒药”））的处理难度则更大。我们研究了多语言大型语言模型 (LLM) 如何处理此类现象，重点关注英语-西班牙语、英语-法语和英语-德语同源词、非同源词和跨语言同形异义词。具体而言，我们评估它们消除歧义和做出语义判断的能力，无论是当这些词类单独出现还是在句子上下文中出现时。我们的研究结果表明，虽然某些 LLM 在识别孤立同源词和非同源词方面表现出色，但它们在消除语言间同形异义词歧义方面表现出很大困难，通常表现低于随机基线。这表明 LLM 在解释语言间同形异义词时往往严重依赖正字法相似性而不是语义理解。此外，我们发现 LLM 在检索词义方面表现出困难，孤立消歧任务中的表现与语义理解无关。最后，我们研究了 LLM 如何处理不一致句子中的语言间同形异义词。我们发现模型在理解英语和非英语同形异义词时选择不同的策略，突显出缺乏统一的方法来处理跨语言歧义。]]></description>
      <guid>https://arxiv.org/abs/2501.09127</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>波罗的海和北欧语言的多语言法学硕士评估：立陶宛历史研究</title>
      <link>https://arxiv.org/abs/2501.09154</link>
      <description><![CDATA[arXiv:2501.09154v1 公告类型：新
摘要：在这项工作中，我们在多项选择问答任务中评估了多语言大型语言模型 (LLM) 的立陶宛语和一般历史知识。这些模型在立陶宛国家和一般历史问题的数据集上进行了测试，这些问题被翻译成波罗的海语、北欧语和其他语言（英语、乌克兰语、阿拉伯语），以评估文化和历史相关群体的知识共享。我们评估了 GPT-4o、LLaMa3.1 8b 和 70b、QWEN2.5 7b 和 72b、Mistral Nemo 12b、LLaMa3 8b、Mistral 7b、LLaMa3.2 3b 和北欧微调模型（GPT-SW3 和 LLaMa3 8b）。
我们的结果表明，GPT-4o 在各个语言组中的表现始终优于所有其他模型，波罗的海语和北欧语的结果略好。较大的开源模型（如 QWEN2.5 72b 和 LLaMa3.1 70b）表现良好，但与波罗的海语言的对齐较弱。较小的模型（Mistral Nemo 12b、LLaMa3.2 3b、QWEN 7B、LLaMa3.1 8B 和 LLaMa3 8b）与波罗的海语言的 LT 相关对齐存在差距，但在北欧和其他语言上表现更好。北欧微调模型并没有超越多语言模型，这表明仅靠共同的文化或历史背景并不能保证更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.09154</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估 GenAI 在教育领域简化文本的效果：提高准确性和一致性，增强可读性</title>
      <link>https://arxiv.org/abs/2501.09158</link>
      <description><![CDATA[arXiv:2501.09158v1 公告类型：新
摘要：生成人工智能 (GenAI) 作为支持个性化学习的工具前景广阔。教师需要工具来有效提高教育文本内容的可读性，以便它们与个别学生的阅读水平相匹配，同时保留关键细节。大型语言模型 (LLM) 显示出满足这一需求的潜力，但之前的研究指出当前方法存在多个缺点。在本研究中，我们引入了一种通用方法和指标，用于系统评估 LLM、提示技术和一种新颖的多代理架构的准确性和一致性，以简化 60 个信息阅读段落，将每个段落从十二年级水平降低到八年级、六年级和四年级水平。我们计算了每个 LLM 和提示技术准确达到每个段落目标年级水平的程度、字数的百分比变化以及保持关键字和关键短语（语义相似性）的一致性。单样本 t 检验和多元回归模型表明，在四项指标中，表现最佳的 LLM 和提示技术存在显著差异。在尝试将内容分级到四年级阅读水平时，LLM 和提示技术在年级准确性和关键词和关键短语的一致性方面都表现出不同的效用。这些结果证明了 LLM 在高效、精确的自动文本简化方面的应用前景，证明了当前模型和提示方法在实现各种评估标准的理想平衡方面的不足，以及一种可推广的评估未来系统的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.09158</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Veln(ia)s 体现在细节中：评估拉脱维亚语和立陶宛语简答题的 LLM 判断</title>
      <link>https://arxiv.org/abs/2501.09164</link>
      <description><![CDATA[arXiv:2501.09164v1 公告类型：新
摘要：在这项工作中，我们解决了评估大型语言模型 (LLM) 在拉脱维亚语和立陶宛语简答匹配任务中的挑战。我们引入了由 502 个拉脱维亚语和 690 个立陶宛语问答对组成的新数据集。对于每个问答对，我们使用一组专门设计用于在文本中引入细小但有意义的更改的更改规则生成匹配和不匹配的答案。这些生成的答案作为测试用例，以评估 LLM 检测原始答案匹配中细微差异的能力。对数据集的一个子集进行了手动质量和准确性验证。我们的结果表明，虽然较大的 LLM（例如 QWEN2.5 72b 和 LLaMa3.1 70b）在区分匹配和不匹配答案方面表现出近乎完美的性能，但较小的模型显示出更多的差异。例如，LLaMa3.1 8b 和 EuroLLM 9b 受益于少量样本，而 Mistral Nemo 12b 在检测细微文本改动方面表现不佳，尤其是在立陶宛语中，即使有更多样本也是如此。QWEN2.5 7b 和 Mistral 7b 在零样本和少量样本实验中能够获得与更大的 70b 模型相当的强大性能。此外，Mistral 7b 在少量样本实验中的表现较弱。]]></description>
      <guid>https://arxiv.org/abs/2501.09164</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FineMedLM-o1：从监督微调到测试时间训练，增强 LLM 的医学推理能力</title>
      <link>https://arxiv.org/abs/2501.09213</link>
      <description><![CDATA[arXiv:2501.09213v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展已在疾病诊断和治疗计划等医疗应用中显示出良好的前景。然而，大多数现有的医学 LLM 都难以应对复杂临床场景所需的高级推理，例如鉴别诊断或个性化治疗建议。我们提出了 FineMedLM-o1，它利用高质量的合成医疗数据和长格式推理数据进行监督微调 (SFT) 和直接偏好优化 (DPO)，实现高级对话和深度推理能力。此外，我们首次在医疗领域引入了测试时间训练 (TTT)，促进领域适应并确保可靠、准确的推理。实验结果表明，FineMedLM-o1 在关键医疗基准上的平均性能比以前的模型提高了 23%。此外，TTT 的引入又提高了 14% 的性能，凸显了其在增强医疗推理能力方面的有效性。为了支持这一过程，我们还提出了一种合成医学对话的新方法。与其他开源数据集相比，我们的数据集在质量和复杂性方面都表现出色。该项目和数据将在 GitHub 上发布。]]></description>
      <guid>https://arxiv.org/abs/2501.09213</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用多源信息探索和双层对比学习提升短文本分类能力</title>
      <link>https://arxiv.org/abs/2501.09214</link>
      <description><![CDATA[arXiv:2501.09214v1 公告类型：新
摘要：短文本分类作为自然语言处理的一个研究子课题，由于语义稀疏性和实际场景中标记样本不足而更具挑战性。本文提出了一种用于短文本分类的新模型MI-DELIGHT。具体而言，它首先进行多源信息（即统计信息、语言信息和事实信息）探索以缓解稀疏性问题。然后，采用图学习方法学习短文本的表示，并以图形形式呈现。此外，我们引入了一个双层（即实例级和集群级）对比学习辅助任务，以有效捕获大量未标记数据中不同粒度的对比信息。同时，以前的模型仅并行执行主任务和辅助任务，而不考虑任务之间的关系。因此，我们引入了一个分层架构来明确地模拟任务之间的相关性。我们在各种基准数据集上进行了广泛的实验，结果表明 MI-DELIGHT 明显优于之前的竞争模型。它甚至在多个数据集上的表现优于流行的大型语言模型。]]></description>
      <guid>https://arxiv.org/abs/2501.09214</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于短文本分类的简单图对比学习框架</title>
      <link>https://arxiv.org/abs/2501.09219</link>
      <description><![CDATA[arXiv:2501.09219v1 公告类型：新
摘要：短文本分类由于其普遍性和实际应用而在信息时代引起了广泛关注。图形学习与对比学习相结合的最新进展在解决短文本分类中的语义稀疏性和有限标记数据挑战方面显示出有希望的结果。然而，现有的模型有一定的局限性。它们依赖于显式的数据增强技术来生成对比视图，导致语义损坏和噪声。此外，这些模型只关注学习生成的视图之间的内在一致性，而忽略了来自其他潜在视图的有价值的判别信息。为了解决这些问题，我们提出了一个用于短文本分类的简单图形对比学习框架 (SimSTC)。我们的方法涉及对多个文本相关的组件图执行图形学习以获得多视图文本嵌入。随后，我们直接对这些嵌入应用对比学习。值得注意的是，我们的方法消除了数据增强操作来生成对比视图的需要，同时仍然利用了多视图对比学习的好处。尽管我们的模型很简单，但它却取得了出色的性能，超越了各种数据集上的大型语言模型。]]></description>
      <guid>https://arxiv.org/abs/2501.09219</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的基础</title>
      <link>https://arxiv.org/abs/2501.09223</link>
      <description><![CDATA[arXiv:2501.09223v1 公告类型：新
摘要：这是一本关于大型语言模型的书。正如标题所示，它主要关注基础概念，而不是全面涵盖所有前沿技术。本书分为四个主要章节，每章探索一个关键领域：预训练、生成模型、提示技术和对齐方法。它面向自然语言处理和相关领域的大学生、专业人士和从业者，可以作为对大型语言模型感兴趣的任何人的参考。]]></description>
      <guid>https://arxiv.org/abs/2501.09223</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>延迟融合：将大型语言模型集成到端到端语音识别的首遍解码中</title>
      <link>https://arxiv.org/abs/2501.09258</link>
      <description><![CDATA[arXiv:2501.09258v1 公告类型：新
摘要：本文介绍了一种使用大型语言模型 (LLM) 进行端到端自动语音识别 (E2E-ASR) 的有效解码方法。虽然浅层融合是将语言模型纳入 E2E-ASR 解码的最常见方法，但我们在使用 LLM 时面临两个实际问题。 (1) LLM 推理计算成本高昂。 (2) ASR 模型和 LLM 之间可能存在词汇不匹配。为了解决这种不匹配，我们需要重新训练 ASR 模型和/或 LLM，这在最好的情况下很耗时，而且在许多情况下是不可行的。我们提出了“延迟融合”，它在解码过程中延迟将 LLM 分数应用于 ASR 假设，并使在 ASR 任务中更容易使用预训练的 LLM。这种方法不仅可以减少 LLM 评分的假设数量，还可以减少 LLM 推理调用的数量。如果 ASR 和 LLM 采用不同的标记化方法，它还允许在解码过程中重新标记 ASR 假设。我们证明，使用 LibriHeavy ASR 语料库和三个公共 LLM（OpenLLaMA 3B &amp; 7B 和 Mistral 7B），延迟融合比浅融合和 N-best 重新评分提供了更高的解码速度和准确性。]]></description>
      <guid>https://arxiv.org/abs/2501.09258</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型解决主观任务的视角转换</title>
      <link>https://arxiv.org/abs/2501.09265</link>
      <description><![CDATA[arXiv:2501.09265v1 公告类型：新
摘要：大型语言模型（LLM）彻底改变了自然语言处理领域，使各种任务取得了显著进展。与常识推理和算术问答等客观任务不同，LLM 在主观任务上的表现仍然有限，其中对特定问题的视角对于更好地解释上下文和给出适当的回应起着至关重要的作用。例如，在某些情况下，LLM 从专家角色角度回答时可能会表现更好，可能会引出他们相关的领域知识。相反，在某些情况下，LLM 从第三人称角度回答时可能会提供更准确的答案，从而能够更全面地理解问题并可能减轻固有偏见。在本文中，我们提出了通过视角转换推理（RPT），这是一种基于上下文学习的方法，使 LLM 能够动态地在直接、角色和第三人称视角之间选择最佳方式来解决相应的主观问题。通过使用闭源和开源 LLM（包括 GPT-4、GPT-3.5、Llama-3 和 Qwen-2）对总共 12 个主观任务进行大量实验，我们的方法优于广泛使用的基于单一固定视角的方法（例如思路链提示和专家提示），突出了 LLM 可以调整其视角以针对不同问题提供细致入微且适合上下文的响应的复杂方式。]]></description>
      <guid>https://arxiv.org/abs/2501.09265</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检索还是不检索？动态检索增强生成的不确定性检测</title>
      <link>https://arxiv.org/abs/2501.09292</link>
      <description><![CDATA[arXiv:2501.09292v1 公告类型：新
摘要：检索增强生成使大型语言模型具备检索外部知识的能力，从而通过整合超出模型内在能力的信息来减轻幻觉。然而，大多数先前的研究都集中在确定性地调用检索，这使得它不适合诸如长篇问答之类的任务。相反，只有当底层 LLM 缺乏所需知识时才调用检索来动态执行检索可以更有效。在这种情况下，我们通过探索多种不确定性检测方法，深入探讨“检索还是不检索？”这个问题。我们评估了这些方法在长篇问答任务中的应用，采用了动态检索，并进行了比较。我们的研究结果表明，不确定性检测指标（例如度矩阵 Jaccard 和偏心率）可以将检索调用次数减少近一半，而问答准确性仅略有降低。]]></description>
      <guid>https://arxiv.org/abs/2501.09292</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于上下文学习的文本到 SQL 错误研究</title>
      <link>https://arxiv.org/abs/2501.09310</link>
      <description><![CDATA[arXiv:2501.09310v1 公告类型：新
摘要：大型语言模型 (LLM) 已被用于执行文本到 SQL 任务，利用其上下文学习 (ICL) 功能将自然语言问题转换为结构化查询语言 (SQL)。然而，这种技术面临正确性问题，需要有效的修复解决方案。在本文中，我们对文本到 SQL 错误进行了首次全面研究。我们的研究涵盖了四种基于 ICL 的代表性技术、五种基本修复方法、两个基准和两个 LLM 设置。我们发现文本到 SQL 错误很普遍，并总结了 7 个类别的 29 种错误类型。我们还发现，现有的修复尝试以高计算开销和许多错误修复为代价，限制了正确性的提高。基于这些发现，我们提出了 MapleRepair，一种新颖的文本到 SQL 错误检测和修复框架。评估表明，MapleRepair 的性能优于现有解决方案，可修复 13.8% 以上的查询，且错误修复可忽略不计，并且开销减少了 67.4%。]]></description>
      <guid>https://arxiv.org/abs/2501.09310</guid>
      <pubDate>Fri, 17 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>