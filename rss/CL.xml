<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 16 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>精心设计叙事结尾：使用 SSM Mamba 进行零样本学习以生成短篇故事结尾</title>
      <link>https://arxiv.org/abs/2410.10848</link>
      <description><![CDATA[arXiv:2410.10848v1 公告类型：新
摘要：写故事是一项引人入胜但又充满挑战性的工作。作者经常会遇到创作瓶颈，他们的叙述前进的道路变得模糊不清。本文旨在通过提供一种创新的解决方案来解决此类问题：一种根据给定提示完成故事的工具。通过输入一个短篇故事提示，用户可以收到他们的故事的结论，用一句话或多句话表达出来，从而通过人工智能驱动的创造力增强讲故事的过程。这个工具不仅旨在帮助作者克服写作瓶颈，而且还为任何人提供一种有趣且互动的方式，让他们自发地扩展故事创意。通过这篇论文，我们探索了人工智能与创意写作的交集，突破了故事创作和结局的界限。为了创建最终的文本生成模型，我们使用了预先训练的 GPT-3.5 模型和新创建的微调 SSM-Mamba 模型，这两个模型在包括 BERT 分数、METEOR、BLEU、ROUGE 和 Perplexity 在内的一系列指标上均表现良好。SSM 模型也已作为开源贡献向 HuggingFace 模型上的 NLP 社区公开，这暂时是 HuggingFace 上故事生成任务中首个此类状态空间模型。]]></description>
      <guid>https://arxiv.org/abs/2410.10848</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型对错误提示和人口统计学提示的可靠性</title>
      <link>https://arxiv.org/abs/2410.10850</link>
      <description><![CDATA[arXiv:2410.10850v1 公告类型：新
摘要：我们调查并观察了大型语言模型 (LLM) 支持的聊天机器人在气候变化和心理健康领域中处理带有人口统计信息的错误提示和问题的行为和性能。通过定量和定性方法的结合，我们评估了聊天机器人辨别陈述真实性、对事实的遵守以及其回答中是否存在偏见或错误信息的能力。我们使用真/假问题进行的定量分析表明，这些聊天机器人可以可靠地对这些封闭式问题给出正确的答案。然而，从领域专家那里收集到的定性见解表明，人们仍然担心隐私、道德影响以及聊天机器人引导用户获得专业服务的必要性。我们得出的结论是，虽然这些聊天机器人具有巨大的前景，但它们在敏感领域的部署需要仔细考虑、道德监督和严格改进，以确保它们能够有益地增强人类的专业知识，而不是一个自主的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2410.10850</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SafeLLM：大型语言模型领域特定安全监控：海上风电维护案例研究</title>
      <link>https://arxiv.org/abs/2410.10852</link>
      <description><![CDATA[arXiv:2410.10852v1 公告类型：新
摘要：海上风电 (OSW) 行业正在经历显着扩张，导致运营和维护 (O\&amp;M) 成本增加。智能报警系统提供了快速检测组件故障和过程异常的前景，从而能够及时和准确地进行干预，从而减少资源支出以及计划内和计划外的停机时间。本文介绍了一种利用大型语言模型 (LLM) 应对这一挑战的创新方法。我们提出了一个专门的对话代理，它结合统计技术来计算句子之间的距离，以检测和过滤幻觉和不安全的输出。这可能会使代理能够更好地解释警报序列并生成更安全的修复操作建议。初步研究结果介绍了应用于 ChatGPT-4 生成的测试句子的方法。讨论了使用 ChatGPT-4 的局限性以及通过使用专门的 OSW 数据集重新训练来增强该代理的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.10852</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型中的知识图谱和向量存储的集成来减轻幻觉，以增强心理健康支持</title>
      <link>https://arxiv.org/abs/2410.10853</link>
      <description><![CDATA[arXiv:2410.10853v1 公告类型：新
摘要：本研究深入探讨了大型语言模型 (LLM) 中幻觉的表现及其对心理健康领域应用的影响。主要目标是找出减少幻觉发生的有效策略，从而增强 LLM 在促进心理健康干预（如治疗、咨询和相关信息传播）方面的可靠性和安全性。通过严格的调查和分析，本研究旨在阐明 LLM 中幻觉的潜在机制，并随后提出有针对性的干预措施以缓解其发生。通过解决这一关键问题，该研究致力于为 LLM 在心理健康环境中的使用建立一个更强大的框架，确保其在辅助治疗过程方面的有效性和可靠性，并向寻求心理健康支持的个人提供准确的信息。]]></description>
      <guid>https://arxiv.org/abs/2410.10853</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>常识推理多项选择题基准中的疑似问题</title>
      <link>https://arxiv.org/abs/2410.10854</link>
      <description><![CDATA[arXiv:2410.10854v1 公告类型：新
摘要：涉及日常情况的常识推理的问题通常会允许许多$\textit{possible}$或$\textit{plausible}$答案。相比之下，常识推理的多项选择题 (MCQ) 基准要求对单个正确答案进行艰难选择，原则上，该答案应该代表$\textit{most}$合理的答案选择。在从两个常识推理基准中抽样的 $250$ 个 MCQ 项目中，我们收集了 $5,000$ 个独立的答案选择合理性判断。我们发现，对于超过 20% 的抽样 MCQ，被评为最合理的答案选择与基准黄金答案不匹配；经过人工检查，我们确认这个子集表现出更高的问题发生率，例如问题和答案选项之间的歧义或语义不匹配。使用 LLM 进行的实验表明，子集上的准确度较低且性能变化较大，这表明我们的合理性标准可能有助于确定更可靠的常识评估基准项目。]]></description>
      <guid>https://arxiv.org/abs/2410.10854</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CogDevelop2K：多模态大型语言模型中的逆向认知发展</title>
      <link>https://arxiv.org/abs/2410.10855</link>
      <description><![CDATA[arXiv:2410.10855v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 是随机鹦鹉吗？它们真的理解并能够执行它们擅长的任务吗？本文旨在探索 MLLM 的基本基础，即人类智能建立在感知、理解和推理之上的核心认知能力。为此，我们提出了 CogDevelop2K，这是一个全面的基准，涵盖了 12 个子概念，从对象永久性和边界等基础知识到意向性理解等高级推理，通过人类思维的发展轨迹构建而成。我们在基准上评估了 46 个 MLLM。综合而言，我们进一步评估了评估策略和提示技术的影响。令人惊讶的是，与人类相比，我们观察到了相反的认知发展轨迹。]]></description>
      <guid>https://arxiv.org/abs/2410.10855</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>镜像一致性：利用多数投票中的不一致性</title>
      <link>https://arxiv.org/abs/2410.10857</link>
      <description><![CDATA[arXiv:2410.10857v1 公告类型：新
摘要：自我一致性是一种广泛使用的解码策略，它显著提高了大型语言模型 (LLM) 的推理能力。然而，它依赖于多数投票规则，该规则关注最常见的答案，而忽略所有其他少数人的回答。这些不一致的少数人观点往往会揭示模型生成过程中的不确定领域。为了解决这一限制，我们提出了镜像一致性，这是标准自我一致性方法的增强。我们的方法将“反射镜”纳入自集成解码过程，使 LLM 能够批判性地检查多代之间的不一致性。此外，正如人类使用镜子来更好地了解自己一样，我们建议使用镜像一致性来增强基于样本的置信度校准方法，这有助于缓解过度自信的问题。我们的实验结果表明，与自我一致性相比，镜像一致性在推理准确性和置信度校准方面都表现出色。]]></description>
      <guid>https://arxiv.org/abs/2410.10857</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推理路径优化：学习从不同的路径进行推理和探索</title>
      <link>https://arxiv.org/abs/2410.10858</link>
      <description><![CDATA[arXiv:2410.10858v1 公告类型：新
摘要：OpenAI o1 等高级模型通过逐步推理展现出令人印象深刻的解决问题的能力。然而，它们在更复杂的问题上仍可能失败，犯下破坏其推理路径的错误。我们将其归因于广阔的解决方案空间，其中每一步都有出现错误的风险。为了增强语言模型推理能力，我们引入了一个称为推理路径优化 (RPO) 的专门训练框架，它可以学习从不同的路径进行推理和探索。我们的方法鼓励每个推理步骤中的有利分支，同时惩罚不利分支，从而提高模型的整体解决问题性能。推理路径优化不依赖于大规模人工注释的理由或闭源模型的输出，使其具有可扩展性和数据效率。我们专注于多步骤推理任务，例如数学应用题和基于科学的考试问题。实验表明，我们的框架显著提升了大型语言模型的推理性能，在 GSM8K 和 MMLU (STEM) 上分别提升了 3.1% 和 4.3%。我们的数据和代码可以在 https://reasoning-paths.github.io 找到。]]></description>
      <guid>https://arxiv.org/abs/2410.10858</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FAME：面向事实多任务模型编辑</title>
      <link>https://arxiv.org/abs/2410.10859</link>
      <description><![CDATA[arXiv:2410.10859v1 公告类型：新
摘要：大型语言模型 (LLM) 嵌入了大量知识并利用它在各种任务中表现出色。然而，LLM 中过时的知识或事实错误可能会导致误导或不正确的响应，从而在实际应用中造成重大问题。为了在不需要昂贵的模型再训练的情况下纠正致命缺陷，已经提出了各种模型编辑方法，以经济高效的方式纠正 LLM 中的不准确知识。为了评估这些模型编辑方法，以前的工作引入了一系列数据集。然而，大多数以前的数据集只包含单一格式的虚构数据，这与现实世界的模型编辑场景不同，引起了人们对它们在实践中的可用性的怀疑。为了促进模型编辑在现实场景中的应用，我们提出了实用性的挑战。为了解决这些挑战并有效增强 LLM 的功能，我们提出了 FAME，这是一个事实性、全面性和多任务的数据集，旨在提高模型编辑的实用性。然后，我们提出了 SKEME，这是一种模型编辑方法，它使用一种新颖的缓存机制来确保与现实世界同步。实验表明，SKEME 在各种任务和场景中均表现出色，证实了它的实用性。]]></description>
      <guid>https://arxiv.org/abs/2410.10859</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>构建合规房地产聊天机器人的秘诀</title>
      <link>https://arxiv.org/abs/2410.10860</link>
      <description><![CDATA[arXiv:2410.10860v1 公告类型：新
摘要：近年来，人们付出了巨大的努力来将大型语言模型与人类偏好相结合。这项工作的重点是开发专门用于房地产领域的聊天机器人，重点是纳入合规行为，以确保它可以在不延续操纵和红线等歧视性做法的情况下使用，这些做法在历史上一直困扰着美国的房地产行业。在前人工作的基础上，我们提出了一种生成合成通用指令遵循数据集以及安全数据的方法。通过广泛的评估和基准测试，我们对 llama-3-8B-instruct 模型进行了微调，并证明我们可以显着提高其性能以匹配 GPT-4o 等大型闭源模型，同时使其更安全、更合规。我们开源模型、数据和代码，以支持社区的进一步开发和研究。]]></description>
      <guid>https://arxiv.org/abs/2410.10860</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Translation Canvas：一个可解释的界面，用于精确定位和分析翻译系统</title>
      <link>https://arxiv.org/abs/2410.10861</link>
      <description><![CDATA[arXiv:2410.10861v2 公告类型：新
摘要：随着机器翻译研究的快速发展，评估工具包已成为对系统进展进行基准测试的必不可少的工具。COMET 和 SacreBLEU 等工具提供的单一质量分数评估对于成对系统比较非常有效。然而，这些工具对于细粒度的系统级比较和实例级缺陷分析提供的见解有限。为了解决这些限制，我们引入了 Translation Canvas，这是一个可解释的界面，旨在查明和分析翻译系统的性能：1）Translation Canvas 通过识别常见错误（其频率和严重程度）并基于各种评估指标分析不同系统之间的关系，帮助机器翻译研究人员理解系统级模型性能。2）它通过突出显示带有解释的错误范围并有选择地显示系统的预测来支持细粒度分析。根据人工评估，Translation Canvas 在可玩性和可理解性标准下表现出优于 COMET 和 SacreBLEU 软件包的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.10861</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>表面安全一致性假说</title>
      <link>https://arxiv.org/abs/2410.10862</link>
      <description><![CDATA[arXiv:2410.10862v1 公告类型：新
摘要：随着大型语言模型 (LLM) 越来越多地集成到各种应用程序中，确保它们生成安全且一致的响应是迫切需要的。以前对对齐的研究主要集中在一般的指令遵循上，但往往忽略了安全对齐的独特属性和挑战，例如安全机制的脆弱性。为了弥补这一差距，我们提出了表面安全对齐假设 (SSAH)，该假设安全对齐应该教会原本不安全的模型选择正确的推理方向——解释为专门的二元分类任务——并结合拒绝机制和多个​​保留的后备选项。此外，通过 SSAH，我们假设 LLM 中的安全护栏只需少数基本组件即可建立。为了验证这一点，我们进行了一项消融研究，并成功识别了安全对齐的 LLM 中的四种属性关键组件：专用安全单元 (ESU)、专用实用单元 (EUU)、复杂单元 (CU) 和冗余单元 (RU)。我们的研究结果表明，在微调期间冻结某些安全关键组件 7.5% 可使模型在适应新任务的同时保留其安全属性。此外，我们表明，利用预训练模型中 20% 的冗余单元作为“对齐预算”可以有效地最大限度地减少对齐税，同时实现对齐目标。综合考虑，本文得出结论，LLM 中安全的原子功能单元处于神经元级别，并强调安全对齐不应该很复杂。我们相信这项工作为未来 LLM 高效且可扩展的安全对齐奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2410.10862</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>是什么让你的模特成为一个缺乏同理心或热情的人：探索法学硕士中人格的起源</title>
      <link>https://arxiv.org/abs/2410.10863</link>
      <description><![CDATA[arXiv:2410.10863v1 公告类型：新
摘要：大型语言模型 (LLM) 已表现出生成类似人类的文本和表现出与人类相似的性格特征的卓越能力。然而，LLM 编码和表达诸如亲和性和冲动性等特征的机制仍然不太清楚。借鉴社会决定论，我们研究长期背景因素（如家庭环境和文化规范）如何与短期压力（如外部指示）相互作用，塑造和影响 LLM 的性格特征。通过利用模型中的可解释特征来控制 LLM 的输出，我们探索这些背景和压力因素如何导致模型特征的变化而无需进一步微调。此外，我们从个性的角度提出了这些因素对模型安全性的潜在影响。]]></description>
      <guid>https://arxiv.org/abs/2410.10863</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>填补空白：利用合成数据进行模型校准和泛化</title>
      <link>https://arxiv.org/abs/2410.10864</link>
      <description><![CDATA[arXiv:2410.10864v1 公告类型：新
摘要：随着机器学习模型的快速发展，在实际和广泛实施之前，校准其性能已成为主要关注点。由于验证数据缺乏多样性，大多数现有的校准方法通常会对模型准确性产生负面影响，从而降低通用性。为了解决这个问题，我们提出了一种在不影响准确性的情况下结合合成数据的校准方法。我们使用可能近似正确 (PAC) 学习框架推导出预期校准误差 (ECE) 界限。大型语言模型 (LLM) 以其模拟真实数据和生成具有混合类标签的文本的能力而闻名，被用作合成数据生成策略来降低 ECE 界限并提高真实测试数据的模型准确性。此外，我们提出了有效校准的数据生成机制。在四种不同的自然语言处理任务上测试我们的方法时，我们观察到准确度平均提高了 34%，ECE 降低了 33%。]]></description>
      <guid>https://arxiv.org/abs/2410.10864</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成用于小样本快速调优的合成数据集</title>
      <link>https://arxiv.org/abs/2410.10865</link>
      <description><![CDATA[arXiv:2410.10865v1 公告类型：新
摘要：提示调整的一个主要限制是它依赖于大型标记训练数据集。在少样本学习设置下，提示调整远远落后于全模型微调，限制了其应用范围。在本文中，我们利用强大的 LLM 来合成特定于任务的标记数据以训练软提示。我们首先引入一种分布对齐的加权生成器调整 (DawGen) 方法来鼓励生成与少样本真实数据一致的分布内数据。然后，我们使用梯度手术方法在合成和真实数据集上训练软提示，从而消除来自不同数据源的冲突梯度。在七个句子对分类数据集上的实验证明了我们提出的方法在少样本学习设置中提升提示调整的有效性。 QQP、MRPC 和 SICK 数据集上的结果甚至可以与大型真实数据集的迁移学习性能相媲美，显示出合成数据作为增强软提示调整的替代方案的前景。]]></description>
      <guid>https://arxiv.org/abs/2410.10865</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>