<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 23 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过联邦学习和差异隐私增强性侵犯者早期检测中的隐私保护</title>
      <link>https://arxiv.org/abs/2501.12537</link>
      <description><![CDATA[arXiv:2501.12537v1 公告类型：新
摘要：COVID-19 大流行导致屏幕时间增加和孤立，导致网络诱骗案件大幅增加，网络诱骗是指犯罪分子使用策略引诱儿童进行性剥削。之前在工业界和学术界检测诱骗的努力包括通过集中训练的模型访问和监控私人对话，或将私人对话发送到全球服务器。在这项工作中，我们实施了一条隐私保护管道，用于早期发现性侵犯者。我们利用联邦学习和差异隐私，在尊重儿童隐私的同时为儿童创造更安全的在线空间。我们研究了各种隐私保护实现方式，并讨论了它们的优点和缺点。我们使用真实数据进行的广泛评估证明，隐私和效用可以共存，而效用只会略有降低。]]></description>
      <guid>https://arxiv.org/abs/2501.12537</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学术病例报告缺乏多样性：评估与 COVID-19 后状况相关的社会人口和行为因素的存在及其多样性</title>
      <link>https://arxiv.org/abs/2501.12538</link>
      <description><![CDATA[arXiv:2501.12538v1 公告类型：新 
摘要：了解弱势群体中 COVID-19 后病症 (PCC) 的流行程度、差异和症状变化对于改善护理和解决交叉不平等至关重要。本研究旨在通过利用 NLP 技术分析 PCC 病例报告中 SDOH 表示的差异和变化，开发一个将健康社会决定因素 (SDOH) 整合到 PCC 研究中的综合框架。在构建 PCC 病例报告语料库（包含来自 LitCOVID 存储库的 7,000 多份病例报告）后，使用预先训练的命名实体识别 (NER) 模型、人工审查和数据增强对 709 份报告的子集进行了注释，其中包含 26 种核心 SDOH 相关实体类型，以提高实体类型的质量、多样性和表示。开发了一个集成 NER、自然语言推理 (NLI)、三元组和频率分析的 NLP 管道来提取和分析这些实体。仅编码器的 Transformer 模型和基于 RNN 的模型都针对 NER 目标进行了评估。
经过微调的仅编码器 BERT 模型在对不同句子结构的通用性和更大的类稀疏性方面优于传统的基于 RNN 的模型。探索性分析揭示了实体丰富度的变化，其中普遍存在的实体包括病情、年龄和获得护理的机会，而敏感类别（如种族和住房状况）的代表性不足。三元分析强调了实体之间的频繁共现，包括年龄、性别和病情。NLI 目标（蕴涵和矛盾分析）显示，“经历过暴力或虐待”和“有医疗保险”等属性具有较高的蕴涵率（82.4%-80.3%），而“女性身份”、“已婚”和“患有绝症”等属性表现出较高的矛盾率（70.8%-98.5%）。]]></description>
      <guid>https://arxiv.org/abs/2501.12538</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用欧洲主要语言和阿拉伯语数据集进行情感分析的比较方法</title>
      <link>https://arxiv.org/abs/2501.12540</link>
      <description><![CDATA[arXiv:2501.12540v1 公告类型：新
摘要：本研究探索了基于 Transformer 的模型，例如 BERT、mBERT 和 XLM-R，用于跨不同语言结构的多语言情感分析。主要贡献包括确定 XLM-R 在形态复杂的语言中具有出色的适应性，准确率达到 88% 以上。该研究重点介绍了微调策略，并强调了它们对于改善代表性不足的语言的情感分类的重要性。]]></description>
      <guid>https://arxiv.org/abs/2501.12540</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>类似人类的概念表征源自语言预测</title>
      <link>https://arxiv.org/abs/2501.12547</link>
      <description><![CDATA[arXiv:2501.12547v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展为解决概念如何在大脑中表示和组织这一长期存在的问题提供了新的机会，这对于揭示人类认知的本质至关重要。在这里，我们重新构建了经典的反向词典任务，以模拟人类在上下文中的概念推理，并研究了 LLM 中类似人类的概念表征的出现。我们发现 LLM 能够从定义描述中推断概念，并构建向共享的、独立于上下文的结构收敛的表示空间。这些表征有效地预测了人类的行为判断，并与人类大脑中的神经活动模式很好地吻合，为生物学合理性提供了证据。这些发现表明，即使没有现实世界的基础，类似人类的概念表征和组织也可以自然地从语言预测中出现。我们的工作支持这样的观点，即 LLM 是理解复杂人类认知的宝贵工具，并为人工智能和人类智能之间的更好协调铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2501.12547</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>O1-Pruner：用于 O1 类推理修剪的长度协调微调</title>
      <link>https://arxiv.org/abs/2501.12570</link>
      <description><![CDATA[arXiv:2501.12570v1 公告类型：新 
摘要：最近，长期思考推理的LLM，例如OpenAI的O1，采用了类似于人类思考复杂问题的扩展推理过程。这种推理范式显着增强了模型的解决问题的能力，并取得了令人鼓舞的结果。然而，长期思考的推理过程导致推理时间大幅增加。一个紧迫的挑战是在确保准确性的同时减少长期思考LLM的推理开销。在本文中，我们通过实验证明长期思考推理模型难以根据问题难度和推理冗余有效地分配令牌预算。为了解决这个问题，我们提出了长度协调微调（O1-Pruner），旨在在保持准确性的同时最小化推理开销。这种有效的微调方法首先通过预采样估计 LLM 的基线性能，然后使用 RL 风格的微调来鼓励模型在精度约束下生成更短的推理过程。这使得模型在保持精度的同时实现低冗余的高效推理。在各种数学推理基准上的实验表明，O1-Pruner 不仅显著降低了推理开销，而且实现了更高的精度，为这一挑战提供了一种新颖且有前途的解决方案。我们的代码即将在 https://github.com/StarDewXXX/O1-Pruner 上发布]]></description>
      <guid>https://arxiv.org/abs/2501.12570</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BLR-MoE：增强型语言路由专家混合，实现领域稳健的多语言 E2E ASR</title>
      <link>https://arxiv.org/abs/2501.12602</link>
      <description><![CDATA[arXiv:2501.12602v1 公告类型：新 
摘要：最近，混合专家 (MoE) 架构（例如 LR-MoE）通常用于减轻语言混淆对多语言 ASR (MASR) 任务的影响。然而，它仍然面临语言混淆问题，尤其是在不匹配的领域场景中。在本文中，我们将 LR-MoE 中的语言混淆解耦为自注意力和路由器中的混淆。为了缓解自注意力中的语言混淆，我们基于 LR-MoE，提出将注意力-MoE 架构应用于 MASR。在我们的新架构中，MoE 不仅用于前馈网络 (FFN)，还用于自注意力。此外，为了提高基于 LID 的路由器对语言混淆的鲁棒性，我们提出了专家修剪和路由器增强方法。结合以上内容，我们得到了增强语言路由 MoE (BLR-MoE) 架构。我们在 10,000 小时 MASR 数据集中验证了所提出的 BLR-MoE 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.12602</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>T2ISafety：评估图像生成中的公平性、毒性和隐私的基准</title>
      <link>https://arxiv.org/abs/2501.12612</link>
      <description><![CDATA[arXiv:2501.12612v1 公告类型：新
摘要：文本转图像 (T2I) 模型发展迅速，能够从各个领域的文本提示生成高质量的图像。然而，这些模型存在显著的安全问题，包括生成有害、有偏见或私人内容的风险。目前对评估 T2I 安全性的研究仍处于早期阶段。虽然已经做出了一些努力来评估特定安全维度上的模型，但许多关键风险仍未得到探索。为了弥补这一差距，我们引入了 T2ISafety，这是一个安全基准，可评估三个关键领域的 T2I 模型：毒性、公平性和偏见。我们基于这三个领域构建了 12 个任务和 44 个类别的详细层次结构，并精心收集了 70K 个相应的提示。基于此分类法和提示集，我们构建了一个包含 68K 手动注释图像的大规模 T2I 数据集，并训练了一个评估器，该评估器能够检测出以前的工作未能识别的关键风险，包括甚至像 GPT 这样的超大型专有模型都无法正确检测的风险。我们在 T2ISafety 上评估了 12 个著名的传播模型，并发现了几个问题，包括种族公平性问题持续存在、产生有害内容的倾向以及各个模型之间隐私保护的显著差异，即使采用概念擦除等防御方法也是如此。数据和评估器发布在 https://github.com/adwardlee/t2i_safety 下。]]></description>
      <guid>https://arxiv.org/abs/2501.12612</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的提炼量化</title>
      <link>https://arxiv.org/abs/2501.12619</link>
      <description><![CDATA[arXiv:2501.12619v1 公告类型：新 
摘要：模型蒸馏是一种将知识从大型语言模型（LLM）转移到较小模型的技术，旨在创建资源高效但性能高的模型。然而，过度蒸馏会导致同质化，降低模型之间的多样性，削弱它们处理复杂或新任务的能力。这些限制强调了系统地量化蒸馏过程及其影响的必要性。在本文中，我们提出了一个评估和量化模型蒸馏的框架。我们的方法解决了两个关键方面：（1）识别身份认知矛盾以评估模型感知和表示身份相关信息的差异，以及（2）分析跨模型的多粒度响应相似性以衡量同质化的程度。实验结果表明了两个关键见解：（1）除 Claude、Doubao 和 Gemini 外，著名的闭源和开源 LLM 通常表现出较高的蒸馏度。 (2) 与对齐的 LLM 相比，基础 LLM 的蒸馏程度更高。通过提供系统的方法提高 LLM 数据蒸馏的透明度，我们呼吁 LLM 具有更多的独立开发和更透明的技术报告，以提高 LLM 的稳健性和安全性。代码和数据可在 https://github.com/Aegis1863/LLMs-Distillation-Quantification 下找到。]]></description>
      <guid>https://arxiv.org/abs/2501.12619</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>政治播客中的毒性动态</title>
      <link>https://arxiv.org/abs/2501.12640</link>
      <description><![CDATA[arXiv:2501.12640v1 公告类型：新
摘要：数字媒体中的毒性带来了重大挑战，但很少有人关注其在快速增长的播客媒体中的动态。本文通过分析政治播客数据来研究毒性的出现和传播，重点研究播客记录中的对话链结构回复模式，从而解决了这一差距。利用最先进的转录模型和先进的对话分析技术，我们系统地研究了美国 30 多个流行政治播客中的毒性话语。我们的主要贡献包括：(1) 创建转录和日记化的政治播客综合数据集，使用 Google 的 Perspective API 识别数千个有毒实例，(2) 发现令人担忧的趋势，即大多数剧集至少包含一个有毒实例，(3) 引入有毒对话链并分析其结构和语言属性，揭示诸如持续时间较长、重复模式、比喻性语言和与愤怒和烦恼相关的情绪线索等特征，(4) 识别与需求相关的词，如“想要”、“喜欢”和“知道”作为毒性的前兆，以及 (5) 开发预测模型，根据注释的变化点预测毒性变化。我们的研究结果为播客毒性提供了重要的见解，并为未来研究实时监测和干预机制奠定了基础，以促进这一有影响力的媒体中更健康的讨论。]]></description>
      <guid>https://arxiv.org/abs/2501.12640</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用预训练语言模型作为认知科学理论的潜力和缺陷</title>
      <link>https://arxiv.org/abs/2501.12651</link>
      <description><![CDATA[arXiv:2501.12651v1 公告类型：新
摘要：许多研究已经评估了预训练语言模型 (PLM) 的认知一致性，即它们与一系列认知领域的成人表现的对应关系。最近，重点已经扩展到这些模型的发展一致性：确定训练期间模型性能改进与儿童思维发展改进的阶段。然而，将 PLM 用作认知科学理论面临许多挑战，包括不同的架构、不同的训练数据模式和规模以及有限的模型可解释性。在本文中，我们总结了将 PLM 视为认知科学和发展科学模型而不是工程工件的经验教训。我们回顾了研究人员用来将 PLM 性能指标映射到人类性能指标的假设。我们确定了这种理解人类思维的方法的潜在缺陷，最后列举了使用 PLM 作为认知和认知发展的可靠解释的标准。]]></description>
      <guid>https://arxiv.org/abs/2501.12651</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过知识提炼提取低资源语言的通用 Transformer</title>
      <link>https://arxiv.org/abs/2501.12660</link>
      <description><![CDATA[arXiv:2501.12660v1 公告类型：新
摘要：在本文中，我们建议使用简单的知识蒸馏从大规模多语言转换器 (MMT) 中生成更小、更高效的单语言转换器，以减轻在资源匮乏的环境中使用此类转换器所带来的权衡。以他加禄语为例，我们表明这些较小的单语言模型在各种基准测试任务中的表现与强大的基线相当，而且效率更高。此外，我们研究了蒸馏过程中的其他步骤，以改进目标语言的软监督，并提供了许多分析和消融来证明所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.12660</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过人工智能反馈训练对话系统，改善整体对话印象</title>
      <link>https://arxiv.org/abs/2501.12698</link>
      <description><![CDATA[arXiv:2501.12698v1 公告类型：新
摘要：为了提高用户与对话系统对话时的参与度，我们必须在整个对话过程中改进个人对话反应和对话印象，例如一致性、个性和同理心。虽然此类对话系统在大型语言模型 (LLM) 的帮助下发展迅速，但从人工智能反馈中强化学习 (RLAIF) 引起了人们的关注，以使基于 LLM 的对话模型与此类对话印象保持一致。在 RLAIF 中，基于另一个 LLM 的奖励模型用于使用零样本/少量样本提示技术为基于 LLM 的对话模型创建训练信号。然而，仅通过提示 LLM 来评估整个对话具有挑战性。在本研究中，LLM 的监督微调 (SFT) 准备了与整个对话印象相关的 12 个指标相对应的奖励模型，用于评估对话响应。我们使用奖励模型信号作为反馈来调整我们的对话模型，以改善系统的印象。自动和人工评估的结果表明，使用与对话印象相对应的奖励模型调整对话模型可以改善对单个指标的评估和对话响应的自然度。]]></description>
      <guid>https://arxiv.org/abs/2501.12698</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EvidenceMap：利用证据分析释放小型语言模型的力量，用于生物医学问答</title>
      <link>https://arxiv.org/abs/2501.12746</link>
      <description><![CDATA[arXiv:2501.12746v1 公告类型：新
摘要：当前基于 LLM 的方法通过利用模型的内部推理能力或结合外部知识来提高问答性能。然而，当人类解决专业问题时，明确分析来自多个部分和不同证据来源的多方面关系以获得更好的答案至关重要。在本研究中，我们提出了一种用于生物医学领域的新型生成式问答框架，名为 EvidenceMap，它明确学习并将证据分析与小型语言模型 (SLM) 结合起来。该框架为每个问题描述了一个证据图，并充分利用 SLM 来得出支持性评估的表示、逻辑相关性和相关证据的总结，这有助于以自回归的方式与另一个 SLM 进行分析增强生成。大量实验表明，引入证据分析学习过程可以显著胜过更大的模型和流行的 LLM 推理方法。]]></description>
      <guid>https://arxiv.org/abs/2501.12746</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NExtLong：无需长文档即可实现有效的长上下文训练</title>
      <link>https://arxiv.org/abs/2501.12766</link>
      <description><![CDATA[arXiv:2501.12766v1 公告类型：新
摘要：具有扩展上下文窗口的大型语言模型 (LLM) 取得了重大进展，但由于长文档稀缺，这仍然是一个挑战。现有方法倾向于合成长上下文数据，但缺乏明确的机制来强化长距离依赖关系建模。为了解决这一限制，我们提出了 NExtLong，这是一种通过负文档扩展合成长上下文数据的新框架。NExtLong 将文档分解为多个元块，并通过交错从预训练语料库中检索到的硬负干扰项来扩展上下文。这种方法迫使模型区分长距离依赖上下文和干扰内容，从而增强其建模长距离依赖关系的能力。大量实验表明，与现有的长上下文合成方法和领先模型（在非合成长文档上训练）相比，NExtLong 在 HELMET 和 RULER 基准上实现了显着的性能改进。这些发现凸显了 NExtLong 减少对非合成长文档的依赖的能力，使其成为开发高级长上下文 LLM 的有效框架。]]></description>
      <guid>https://arxiv.org/abs/2501.12766</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为事实知识库的局限性及解决方案</title>
      <link>https://arxiv.org/abs/2501.12774</link>
      <description><![CDATA[arXiv:2501.12774v1 公告类型：新
摘要：LLM 的知识来源是数据快照，其中包含在不同时间戳和不同媒体类型（例如 wiki、社交媒体等）收集的有关实体的事实信息。这种非结构化知识可能会随着从过去到现在的更新而发生变化。同样重要的是不同信息源中出现的不一致和不准确性。因此，在对快照序列进行训练或在推理时，模型关于实体的知识可能会受到干扰，从而导致模型性能不一致和不准确。在这项工作中，我们研究了大型语言模型 (LLM) 作为事实知识存储库的适用性。我们考虑了 24 个最先进的 LLM，它们要么是封闭的，要么是部分（权重）或完全（权重和训练数据）开源的。我们评估了它们在提示受到干扰时，在准确性和一致性方面回答时间敏感的事实问题的可靠性。我们进一步评估了最先进的方法在提高 LLM 的准确性和一致性方面的有效性。然后，我们提出了“实体感知微调”（ENAF），这是一种软神经符号方法，旨在在微调过程中提供实体的结构化表示，以提高模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.12774</guid>
      <pubDate>Thu, 23 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>