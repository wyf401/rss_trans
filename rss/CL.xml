<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 13 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>针对大型语言模型的目标驱动攻击</title>
      <link>https://arxiv.org/abs/2411.07268</link>
      <description><![CDATA[arXiv:2411.07268v1 Announce Type: new 
摘要：当前的大型语言模型（LLM）为大规模面向用户的自然语言任务提供了坚实的基础。许多用户可以通过用户界面轻易地注入对抗性文本或指令，从而导致语言模型无法给出正确答案等LLM模型安全挑战。虽然目前有大量关于黑盒攻击的研究，但这些黑盒攻击大多采用随机和启发式策略，尚不清楚这些策略与攻击成功率有何关系，从而有效提高模型鲁棒性。为了解决这个问题，我们提出了目标驱动的黑盒攻击方法，最大化干净文本和攻击文本的条件概率之间的KL散度，以重新定义攻击的目标。我们将距离最大化问题转化为两个基于攻击目标的凸优化问题，以求解攻击文本并估计协方差。此外，投影梯度下降算法求解攻击文本对应的向量。我们的目标驱动黑盒攻击方法包括两种攻击策略：token 操纵和错误信息攻击。在多个大型语言模型和数据集上的实验结果证明了我们的攻击方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.07268</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>富裕国家的更丰富产出：揭示生成的故事和旅行建议中的地理差异</title>
      <link>https://arxiv.org/abs/2411.07320</link>
      <description><![CDATA[arXiv:2411.07320v1 公告类型：新
摘要：虽然大量研究检查语言模型是否存在与性别、种族、职业和宗教有关的偏见，但地理偏见的研究相对较少。最近的一些研究对大型语言模型编码地理空间知识的程度进行了基准测试。然而，编码的地理知识（或缺乏地理知识）对现实世界应用的影响尚未得到记录。在这项工作中，我们研究了两种需要地理知识的常见场景的大型语言模型：（a）旅行推荐和（b）地理锚定故事生成。具体来说，我们研究了四种流行的语言模型，在大约 10 万美元的旅行请求和 20 万美元的故事生成中，我们观察到与较贫穷国家相对应的旅行建议不太独特，位置参考较少，与富裕国家相比，这些地区的故事更经常传达艰辛和悲伤的情绪。]]></description>
      <guid>https://arxiv.org/abs/2411.07320</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SetLexSem 挑战：使用集合运算评估语言模型的词汇和语义稳健性</title>
      <link>https://arxiv.org/abs/2411.07336</link>
      <description><![CDATA[arXiv:2411.07336v1 公告类型：新
摘要：集合论是数学的基础，当集合是有限的时，它是推理世界的基础。智能系统应该一致地执行集合运算，而不管操作数的表面变化如何。大型语言模型 (LLM) 最初是为面向语义的 NLP 任务设计的，现在正在算法任务上进行评估。由于集合由任意符号（例如数字、单词）组成，因此它们提供了一个机会来系统地测试 LLM 在简单的词汇或语义变化下的算法能力的不变性。为此，我们提出了 SetLexSem 挑战，这是一个综合基准，用于评估 LLM 在集合运算上的性能。SetLexSem 评估 LLM 在各种条件下的指令遵循能力的稳健性，重点关注集合运算以及集合成员的性质和构造。使用 SetLexSem 评估七个 LLM，我们发现它们对操作和操作数的变化都表现出较差的鲁棒性。通过该框架沿词汇和语义维度对集合成员进行系统抽样，我们表明 LLM 不仅对这些维度的变化不具有鲁棒性，而且在易于创建的“欺骗性”集合语义分组中表现出独特的故障模式。我们发现严格测量语言模型对频率和长度变化的鲁棒性具有挑战性，并提出了一种独立测量它们的分析。用于重现本文结果和生成 SetLexSem Challenge 数据集的代码可在 \href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge} 中找到。]]></description>
      <guid>https://arxiv.org/abs/2411.07336</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于多头跨度的科学论文 AI 生成片段检测器</title>
      <link>https://arxiv.org/abs/2411.07343</link>
      <description><![CDATA[arXiv:2411.07343v1 公告类型：新
摘要：本文介绍了一种系统，该系统旨在区分第四届科学文献处理研讨会主办的 DAGPap24 竞赛中人工智能生成的科学摘录和人类编写的科学摘录。在本次竞赛中，任务是在科学领域的文档中查找人工生成的标记级文本片段。我们的工作重点是使用具有两个头部的多任务学习架构。这种方法的应用是由任务的特殊性证明的，其中类跨度在几百个字符上是连续的。我们考虑了不同的编码器变体来获得序列中每个标记的状态向量，以及将片段拆分成标记的变体，以进一步输入到基于变换的编码器的输入中。这种方法使我们能够使用平均宏 F1 分数实现相对于开发集上的基线解决方案分数 9% 的质量改进（从 0.86 到 0.95），以及在竞赛数据集的封闭测试部分上获得 0.96 的分数。]]></description>
      <guid>https://arxiv.org/abs/2411.07343</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BeeManc 参加 TAC-2024 的 PLABA 赛道：任务 1 使用 RoBERTa，任务 2 使用 LLaMA3.1 和 GPT-4o</title>
      <link>https://arxiv.org/abs/2411.07381</link>
      <description><![CDATA[arXiv:2411.07381v1 公告类型：新 
摘要：本报告是 BeeManc 团队对共享任务“生物医学摘要的通俗语言改编 (PLABA) 2024”的系统描述。本报告包含两个部分，分别对应 PLABA 2024 中的两个子任务。在任务一中，我们应用微调的 ReBERTa-Base 模型来识别和分类生物医学摘要中的难词、术语和首字母缩略词，并报告 F1 分数。由于时间限制，我们没有完成替换任务。在任务二中，我们利用 Llamma3.1-70B-Instruct 和 GPT-4o 以及一次性提示完成了摘要改编，并报告了 BLEU、SARI、BERTScore、LENS 和 SALSA 中的分数。根据 PLABA-2024 对任务 1A 和 1B 的官方评估，我们的 \textbf{小得多的微调 RoBERTa-Base} 模型在两个子任务中分别排名第 3 和第 2，并且在 9 个评估系统中的 \textbf{两个任务的平均 F1 分数上排名第一}。我们在 \url{https://github.com/HECTA-UoM/PLABA2024} 上分享了我们的微调模型和相关资源]]></description>
      <guid>https://arxiv.org/abs/2411.07381</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>等时控制语音到文本的翻译：汉藏语到印欧语的翻译研究</title>
      <link>https://arxiv.org/abs/2411.07387</link>
      <description><![CDATA[arXiv:2411.07387v1 公告类型：新
摘要：端到端语音翻译 (ST) 是将源语言语音直接翻译成目标语言文本的技术，近年来引起了广泛关注。许多 ST 应用需要严格的长度控制，以确保翻译时长与源音频的长度相匹配，包括语音和停顿段。以前的方法通常控制机器翻译模型生成的单词或字符的数量以近似源句子的长度，而不考虑停顿和语音段的等时性，因为不同语言之间的持续时间可能不同。为了解决这个问题，我们对序列到序列 ST 模型的持续时间对齐组件进行了改进。我们的方法通过结合翻译过程预测语音和停顿的持续时间来控制翻译长度。这是通过向解码器提供时间信息来实现的，确保它在生成翻译时跟踪语音和停顿的剩余持续时间。 CoVoST 2 在 Zh-En 测试集上的评估表明，所提出的等时控制 ST 实现了 0.92 的语音重叠和 8.9 BLEU，与 ST 基线相比仅下降了 1.4 BLEU。]]></description>
      <guid>https://arxiv.org/abs/2411.07387</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现 RAG 的最佳搜索和检索</title>
      <link>https://arxiv.org/abs/2411.07396</link>
      <description><![CDATA[arXiv:2411.07396v1 公告类型：新
摘要：检索增强生成 (RAG) 是一种很有前途的方法，可以解决与大型语言模型 (LLM) 相关的一些内存相关挑战。两个独立的系统构成了 RAG 管道，即检索器和阅读器，每个系统对下游任务性能的影响尚不清楚。在这里，我们致力于了解如何针对常见任务（例如问答 (QA)）优化检索器以用于 RAG 管道。我们进行了实验，重点关注 QA 和归因 QA 中检索和 RAG 性能之间的关系，并揭示了许多对开发高性能 RAG 管道的从业者有用的见解。例如，降低搜索准确度对 RAG 性能的影响很小，但可能会提高检索速度和内存效率。]]></description>
      <guid>https://arxiv.org/abs/2411.07396</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越关键词：基于情境的混合方法挖掘与道德问题相关的应用评论</title>
      <link>https://arxiv.org/abs/2411.07398</link>
      <description><![CDATA[arXiv:2411.07398v1 公告类型：新
摘要：随着移动应用程序在我们日常生活中的日益普及，围绕道德的担忧也大幅增加。用户通常会在应用程序 (app) 评论中传达他们的反馈、报告问题并建议新功能，经常强调安全、隐私和问责问题。纳入这些评论对于开发成功的产品至关重要。然而，与道德问题相关的应用评论通常使用特定领域的语言，并使用更多样化的词汇来表达。因此，自动提取与道德问题相关的应用评论是一项具有挑战性且耗时的工作。
本研究提出了一种基于自然语言处理 (NLP) 的新型方法，该方法结合了自然语言推理 (NLI)（提供对语言细微差别的深刻理解）和仅解码器（类似 LLaMA）的大型语言模型 (LLM)，以大规模提取与道德问题相关的应用评论。利用来自心理健康领域的 43,647 条应用评论，所提出的方法 1）评估四种 NLI 模型以提取潜在的隐私评论，并将领域特定隐私假设的结果与一般隐私假设进行比较；2）评估四种 LLM 将应用评论归类为隐私问题；3）进一步使用最佳 NLI 和 LLM 模型从数据集中提取新的隐私评论。结果表明，具有领域特定假设的 DeBERTa-v3-base-mnli-fever-anli NLI 模型性能最佳，而 Llama3.1-8B-Instruct LLM 在应用评论分类方面表现最佳。然后，使用 NLI+LLM，又提取了 1,008 条新的隐私相关评论，这些评论在以前的研究中未通过基于关键字的方法识别出来，从而证明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.07398</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可控的情境敏感性及其背后的旋钮</title>
      <link>https://arxiv.org/abs/2411.07404</link>
      <description><![CDATA[arXiv:2411.07404v1 公告类型：新
摘要：在进行预测时，语言模型必须在其对上下文的依赖程度与对先验知识的依赖程度之间进行权衡。选择模型对上下文的敏感程度是一项基本功能，因为它使模型能够出色地完成检索增强生成和问答等任务。在本文中，我们寻找一个控制这种敏感度的旋钮，确定语言模型是根据上下文还是先验知识来回答。为了指导这一搜索，我们设计了一个可控上下文敏感度的任务。在这个任务中，我们首先向模型提供一个上下文（巴黎在英国）和一个问题（巴黎在哪里？）；然后我们指示模型使用其先验知识或上下文知识，并评估它是否为两种意图（法国或英国）生成了正确的答案。当针对此任务进行微调时，Llama-3.1、Mistral-v0.3 和 Gemma-2 的指令调整版本可以高精度（85-95%）解决此问题。通过分析这些高性能模型，我们使用一种新颖的线性时间算法缩小了对上下文敏感性可能重要的层的范围。然后，在每个模型中，我们在单个层中识别一个 1-D 子空间，该子空间编码模型是遵循上下文还是先验知识。有趣的是，虽然我们在微调模型中识别了这个子空间，但我们发现完全相同的子空间不仅在该模型中充当有效旋钮，而且在该模型系列的非微调指令和基础模型中也充当有效旋钮。最后，我们展示了模型的性能与它在这个子空间中将上下文一致答案与上下文忽略答案区分开的程度之间的强相关性。这些结果表明单个子空间有助于模型在上下文和先验知识之间进行选择，暗示了一种控制这种行为的简单基本机制。]]></description>
      <guid>https://arxiv.org/abs/2411.07404</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用生成式人工智能和多智能体提供自动反馈</title>
      <link>https://arxiv.org/abs/2411.07407</link>
      <description><![CDATA[arXiv:2411.07407v1 公告类型：新
摘要：本研究调查了生成式人工智能和多智能体系统在教育环境中提供自动反馈的使用情况，特别是针对学生在科学评估中构建的答案。该研究通过探索多智能体系统（称为 AutoFeedback）如何提高 GenAI 生成的反馈的质量来解决该领域的一个关键差距，克服了单智能体大型语言模型 (LLM) 中常见的过度赞扬和过度推理等已知问题。该研究开发了一个由两个人工智能代理组成的多智能体系统：一个用于生成反馈，另一个用于验证和改进反馈。该系统在 240 个学生答案的数据集上进行了测试，并将其性能与单智能体 LLM 的性能进行了比较。结果表明，AutoFeedback 显着减少了过度赞扬和过度推理错误的发生，提供了更准确和更符合教学法的反馈。研究结果表明，多智能体系统可以为教育环境中的自动反馈提供更可靠的解决方案，凸显了其在可扩展和个性化学习支持方面的潜力。这些结果对于寻求在形成性评估中利用人工智能的教育工作者和研究人员具有重要意义，为更有效的反馈机制提供了途径，从而提高了学生的学习成果。]]></description>
      <guid>https://arxiv.org/abs/2411.07407</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解读仇恨言论定义：跨文化和跨领域的语义成分分析</title>
      <link>https://arxiv.org/abs/2411.07417</link>
      <description><![CDATA[arXiv:2411.07417v1 公告类型：新
摘要：仇恨言论在很大程度上依赖于文化影响，导致不同的个人解释。为此，我们提出了一个语义成分分析 (SCA) 框架，用于跨文化和跨领域分析仇恨言论定义。我们创建了第一个定义数据集，该数据集来自五个领域：在线词典、研究论文、维基百科文章、立法和在线平台，随后将其分析为语义成分。我们的分析表明，各个定义的组成部分各不相同，但许多领域相互借用定义而不考虑目标文化。我们使用我们提出的数据集进行零样本模型实验，采用三种流行的开源 LLM 来了解不同定义对仇恨言论检测的影响。我们的研究结果表明，LLM 对定义很敏感：仇恨言论检测的响应会根据提示中使用的定义的复杂性而变化。]]></description>
      <guid>https://arxiv.org/abs/2411.07417</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高效精准的即时优化：范例引导反思中的记忆优势</title>
      <link>https://arxiv.org/abs/2411.07446</link>
      <description><![CDATA[arXiv:2411.07446v1 公告类型：新
摘要：自动提示工程旨在提高大型语言模型 (LLM) 的生成质量。最近的研究利用错误案例生成的反馈来指导提示优化。在推理过程中，它们可能会进一步检索几个语义相关的示例并将它们连接到优化的提示以提高性能。然而，这些工作只利用当前步骤的反馈，忽略了可能有益的历史和未选择的反馈。此外，示例的选择只考虑一般的语义关系，在任务性能和与优化提示的匹配方面可能不是最佳的。在这项工作中，我们提出了一种带记忆机制的示例引导反射 (ERM)，以实现更高效、更准确的提示优化。具体来说，我们设计了一种示例引导的反射机制，其中反馈生成还由生成的示例引导。我们进一步构建了两种记忆，以充分利用历史反馈信息并支持更有效的示例检索。实证评估表明，我们的方法以更少的优化步骤超越了以前的最先进技术，例如，在 LIAR 数据集上将 F1 分数提高了 10.1，并在 ProTeGi 上减少了一半的优化步骤。]]></description>
      <guid>https://arxiv.org/abs/2411.07446</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DecoPrompt：解码提示可减少大型语言模型遇到错误前提时产生的幻觉</title>
      <link>https://arxiv.org/abs/2411.07457</link>
      <description><![CDATA[arXiv:2411.07457v1 公告类型：新
摘要：虽然大型语言模型 (LLM) 已经显示出越来越强大的能力，但它们也要求研究其偏离事实正确陈述的幻觉输出。在本文中，我们重点关注一个重要的错误前提场景，其中 LLM 被不一致的主张所分散注意力，尽管该模型拥有准确回答原始问题所需的事实知识。受错误前提提示的熵与其引发幻觉产生的可能性密切相关的观察启发，我们提出了一种名为 DecoPrompt 的新提示算法来减轻幻觉。DecoPrompt 利用 LLM 来“解码”错误前提提示，而无需真正从 LLM 中引发幻觉输出。我们对两个数据集进行了实验，证明 DecoPrompt 可以有效减少不同 LLM 输出的幻觉。此外，DecoPrompt 表现出跨模型可转移性，这有助于它应用于大尺寸的 LLM 或不可用的模型逻辑等场景。]]></description>
      <guid>https://arxiv.org/abs/2411.07457</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>识别我：一个具有挑战性的长上下文提及解析基准</title>
      <link>https://arxiv.org/abs/2411.07466</link>
      <description><![CDATA[arXiv:2411.07466v1 公告类型：新
摘要：最近对 LLM 共指解析的评估表明，传统的输出格式和评估指标不能完全捕捉模型的指称理解。为了解决这个问题，我们引入了 IdentityMe，这是一个以多项选择题 (MCQ) 格式呈现的提及解析的新基准，通常用于评估 LLM。IdentifyMe 具有长篇叙述，并采用启发式方法来排除容易识别的提及，从而创建了更具挑战性的任务。基准还包含不同提及类型和相应实体的精选混合，允许对模型性能进行细粒度分析。我们在 IdentityMe 上评估了闭源和开源 LLM，并观察到最先进的 10B 以下开放模型与闭源模型之间存在显着的性能差距（20-30%）。我们观察到，代词提及具有有限的表面信息，通常比名词提及更难被模型解析。此外，我们发现，当 LLM 的提及在嵌套结构中重叠时，它们通常会混淆实体。得分最高的模型 GPT-4o 的准确率达到了 81.9%，这凸显了最先进的 LLM 强大的参考能力，同时也表明了进一步改进的空间。]]></description>
      <guid>https://arxiv.org/abs/2411.07466</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多语言模型中的句法知识控制评估</title>
      <link>https://arxiv.org/abs/2411.07474</link>
      <description><![CDATA[arXiv:2411.07474v1 公告类型：新
摘要：语言模型 (LM) 能够获取类似人类的句法知识元素。有针对性的句法评估测试已用于衡量它们对英语等高资源语言中的句法现象形成概括的能力。然而，我们仍然缺乏对 LM 在低资源语言中进行句法概括的能力的透彻了解，而低资源语言是造成全球句法模式多样性的主要原因。在本研究中，我们针对三种低资源语言（巴斯克语、印地语和斯瓦希里语）开发了有针对性的句法评估测试，并使用它们来评估五种开放获取的多语言 Transformer LM 系列。我们发现，一些句法任务对于 LM 来说相对容易，而其他一些任务（巴斯克语中包含间接宾语的句子中的一致性、斯瓦希里语中介词短语的一致性）则具有挑战性。我们还发现了公开可用的 Transformers 存在的问题，包括多语言 BERT 中对印地语习惯方面的偏见，以及与 XGLM-4.5B 中类似大小的模型相比表现不佳。]]></description>
      <guid>https://arxiv.org/abs/2411.07474</guid>
      <pubDate>Wed, 13 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>