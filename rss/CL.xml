<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>法律领域的自然语言处理：任务、数据集、模型和挑战的调查</title>
      <link>https://arxiv.org/abs/2410.21306</link>
      <description><![CDATA[arXiv:2410.21306v1 公告类型：新
摘要：自然语言处理正在彻底改变法律专业人士和外行在法律领域的运作方式。自然语言处理在法律领域的巨大潜力，特别是在开发用于各种法律流程的计算工具方面，多年来一直引起研究人员的兴趣。这项调查遵循系统评价和荟萃分析的首选报告项目框架，审查了 148 项研究，经过人工筛选后最终选定了 127 项。它探讨了法律领域与自然语言处理相关的基础概念，说明了处理法律文本的独特方面和挑战，例如广泛的文档长度、复杂的语言和有限的开放法律数据集。我们概述了特定于法律文本的自然语言处理任务，例如法律文件摘要、法律命名实体识别、法律问答、法律文本分类和法律判决预测。在法律语言模型部分，我们分析了已开发的语言模型以及将通用语言模型应用于法律领域的方法。此外，我们还确定了 15 项开放研究挑战，包括人工智能应用中的偏见、对更强大和可解释模型的需求，以及提高可解释性以处理法律语言和推理的复杂性。]]></description>
      <guid>https://arxiv.org/abs/2410.21306</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解码扩散：使用自然语言提示对潜在空间偏差和表征进行无监督分析的可扩展框架</title>
      <link>https://arxiv.org/abs/2410.21314</link>
      <description><![CDATA[arXiv:2410.21314v1 公告类型：新
摘要：图像生成领域的最新进展使扩散模型成为创建高质量图像的强大工具。然而，它们的迭代去噪过程使得理解和解释它们的语义潜在空间比其他生成模型（如 GAN）更具挑战性。最近的方法试图通过识别潜在空间内语义上有意义的方向来解决这个问题。然而，它们通常需要人工解释，或者可以训练的向量数量有限，从而限制了它们的范围和效用。本文提出了一种用于无监督探索扩散潜在空间的新框架。我们直接利用自然语言提示和图像标题来映射潜在方向。这种方法可以自动理解隐藏的特征，并支持更广泛的分析，而无需训练特定的向量。我们的方法提供了对扩散模型中编码的语义知识的更具可扩展性和可解释性的理解，促进了对这些模型学习的潜在偏差和细微表示的全面分析。实验结果表明，我们的框架可以揭示各个领域的隐藏模式和关联，为扩散模型潜在空间的可解释性提供新的见解。]]></description>
      <guid>https://arxiv.org/abs/2410.21314</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GraphLSS：集成词汇、结构和语义特征以实现长文档提取摘要</title>
      <link>https://arxiv.org/abs/2410.21315</link>
      <description><![CDATA[arXiv:2410.21315v1 公告类型：新
摘要：异构图神经网络最近因长文档摘要而受到关注，将提取建模为节点分类任务。虽然有效，但这些模型通常需要外部工具或额外的机器学习模型来定义图形组件，从而产生高度复杂且不太直观的结构。我们提出了 GraphLSS，一种用于长文档提取摘要的异构图构造，结合了词汇、结构和语义特征。它定义了两个级别的信息（单词和句子）和四种类型的边（句子语义相似性、句子出现顺序、句子中的单词和单词语义相似性），而无需任何辅助学习模型。在两个基准数据集上的实验表明，GraphLSS 与表现最佳的基于图的方法相比具有竞争力，优于最近的非图模型。我们在 GitHub 上发布了我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2410.21315</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学推导图：总结 STEM 手稿中方程依赖关系的任务</title>
      <link>https://arxiv.org/abs/2410.21324</link>
      <description><![CDATA[arXiv:2410.21324v1 公告类型：新
摘要：自然语言处理 (NLP) 的最新进展，特别是大型语言模型 (LLM) 的出现，大大增强了文本分析领域。然而，虽然这些发展在文本数据分析方面取得了实质性进展，但将分析应用于数学方程及其在文本中的关系却产生了好坏参半的结果。在本文中，我们迈出了理解 STEM 文章中数学表达式之间依赖关系的初步步骤。我们的数据集来自 arXiv 语料库的随机抽样，包含对 107 篇已发表的 STEM 手稿的分析，这些手稿的方程间依赖关系已手工标记，从而产生了一个新对象，我们称之为派生图，它总结了手稿的数学内容。我们详尽地评估了分析模型和基于 NLP 的模型，以评估它们识别和提取每篇文章的派生关系的能力，并将结果与​​基本事实进行比较。我们进行的全面测试发现，无论是分析模型还是 NLP 模型（包括 LLM），从文章中提取派生图的 F1 得分都达到了 $\sim$40-50%，这表明与更简单的分析模型相比，NLP 领域的最新进展在理解数学文本方面并没有取得重大进展。虽然目前的方法为提取数学信息提供了坚实的基础，但还需要进一步研究来提高该领域的准确性和深度。]]></description>
      <guid>https://arxiv.org/abs/2410.21324</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 生物医学问答中对错误信息的鲁棒性</title>
      <link>https://arxiv.org/abs/2410.21330</link>
      <description><![CDATA[arXiv:2410.21330v1 公告类型：新
摘要：检索增强生成 (RAG) 方法用于通过检索和提供来自外部知识源的额外上下文（例如，通过将上下文添加到提示）来减少大型语言模型 (LLM) 在问答中的虚构。但是，注入不正确的信息可能会误导 LLM 生成不正确的答案。
在本文中，我们评估了四个 LLM（Gemma 2、GPT-4o-mini、Llama~3.1 和 Mixtral）在回答生物医学问题时对错误信息的有效性和稳健性。我们在三种情况下评估是非和自由形式问题的答案准确性：原始 LLM 答案（未提供上下文）、“完美”增强生成（提供正确的上下文）和提示注入攻击（提供不正确的上下文）。我们的结果表明，Llama 3.1（70B 参数）在 vanilla（0.651）和“完美” RAG（0.802）场景中均实现了最高的准确率。然而，在“完美” RAG 中，模型之间的准确率差距几乎消失，这表明它有可能缓解 LLM 与规模相关的有效性差异。
我们进一步评估了 LLM 一方面生成恶意上下文的能力，另一方面评估了 LLM 抵御提示注入攻击的鲁棒性，使用攻击成功率 (ASR)、攻击准确率和准确率下降等指标。作为对手，我们使用相同的四个 LLM（Gemma 2、GPT-4o-mini、Llama 3.1 和 Mixtral）来生成注入目标模型提示中的不正确上下文。有趣的是，Llama 被证明是最有效的对手，在目标模型中，普通答案的准确率下降高达 0.48，而“完美” RAG 的准确率下降高达 0.63。我们的分析表明，稳健性排名因评估指标而异，这凸显了评估 LLM 对对抗性攻击的适应力的复杂性。]]></description>
      <guid>https://arxiv.org/abs/2410.21330</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>微调大型语言模型 (LLM)：改进快速注入攻击检测</title>
      <link>https://arxiv.org/abs/2410.21337</link>
      <description><![CDATA[arXiv:2410.21337v1 公告类型：新
摘要：大型语言模型 (LLM) 正在成为一种流行的工具，因为它们在处理各种基于语言的任务方面的能力有了显著的提高。然而，LLM 应用程序极易受到提示注入攻击，这是一个严重的问题。这些攻击通过使用精心设计的输入提示来转移 LLM 应用程序，使模型偏离原始指令，从而执行意外操作。这些操作构成了严重的安全威胁，可能导致数据泄露、输出偏差或有害响应。该项目探讨了与提示注入攻击相关的安全漏洞。为了检测提示是否存在漏洞，我们遵循两种方法：1) 预训练的 LLM，2) 微调的 LLM。然后，我们对分类性能进行彻底的分析和比较。首先，我们使用预训练的 XLM-RoBERTa 模型在未经任何微调的情况下使用测试数据集检测提示注入，并通过零样本分类对其进行评估。然后，本研究将使用 huggingface 中 deepset 中的特定任务标记数据集，对这个预训练的 LLM 进行监督微调，经过严格的实验和评估，这个微调模型取得了令人印象深刻的结果，准确率为 99.13%，精确率为 100%，召回率为 98.33%，F1 得分为 99.15%。我们观察到，我们的方法在检测即时注入攻击方面非常有效。]]></description>
      <guid>https://arxiv.org/abs/2410.21337</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FinTeamExperts：财务分析专业 MOE 角色</title>
      <link>https://arxiv.org/abs/2410.21338</link>
      <description><![CDATA[arXiv:2410.21338v1 公告类型：新 
摘要：大型语言模型 (LLM)，例如 ChatGPT、Phi3 和 Llama-3，正在引领人工智能的重大飞跃，因为它们可以将训练中的知识推广到新任务而无需微调。然而，它们在金融领域的应用仍然相对有限。金融领域本质上很复杂，需要从宏观、微观经济趋势到定量分析等各个角度深入了解。受这种复杂性的启发，针对特定金融领域量身定制的专家 LLM 组合可以为复杂的金融任务提供更全面的理解。在本文中，我们介绍了 FinTeamExperts，这是一个角色专门化的 LLM 框架，其结构为用于财务分析的专家混合 (MOE)。该框架通过训练每个模型以专注于不同的角色来模拟协作团队设置：宏观分析师、微观分析师和定量分析师。这种针对特定角色的专业化增强了模型整合其领域特定专业知识的能力。我们通过在不同的语料库上训练三个 80 亿参数模型来实现这一目标，每个模型都致力于在特定的金融相关角色中表现出色。然后，我们在下游任务上对 FinTeamExperts 进行指导调整，以适应实际的金融任务。实验结果表明，在四个数据集中的三个上，FinTeamExperts 的表现优于所有相同规模和更大的模型。在第四个数据集上，它代表了更复杂的任务，FinTeamExperts 仍然超越了所有相同规模的模型。这凸显了我们基于角色的专业化方法和 FinTeamExperts 持续培训方法的成功。]]></description>
      <guid>https://arxiv.org/abs/2410.21338</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医疗任务中的大型语言模型基准</title>
      <link>https://arxiv.org/abs/2410.21348</link>
      <description><![CDATA[arXiv:2410.21348v1 公告类型：新
摘要：随着大型语言模型 (LLM) 在医学领域的应用越来越广泛，使用基准数据集评估这些模型的性能变得至关重要。本文对医学 LLM 任务中使用的各种基准数据集进行了全面调查。这些数据集涵盖多种模态，包括文本、图像和多模态基准，重点关注医学知识的不同方面，如电子健康记录 (EHR)、医患对话、医学问答和医学图像字幕。调查按模态对数据集进行分类，讨论它们的意义、数据结构以及对 LLM 开发的影响，用于临床任务，例如诊断、报告生成和预测决策支持。主要基准包括 MIMIC-III、MIMIC-IV、BioASQ、PubMedQA 和 CheXpert，它们促进了医疗报告生成、临床总结和合成数据生成等任务的进步。本文总结了利用这些基准推进多模态医学智能的挑战和机遇，强调需要具有更高语言多样性的数据集、结构化组学数据和创新的综合方法。这项工作还为未来 LLM 在医学领域的应用研究奠定了基础，为不断发展的医学人工智能领域做出了贡献。]]></description>
      <guid>https://arxiv.org/abs/2410.21348</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMCBench：对大型语言模型压缩进行基准测试以实现高效部署</title>
      <link>https://arxiv.org/abs/2410.21352</link>
      <description><![CDATA[arXiv:2410.21352v1 Announce Type: new 
摘要：虽然大型语言模型（LLM）已经展示了其强大的智能能力，但对计算和存储的高需求阻碍了它们的实际应用。为此，提出了许多模型压缩技术来提高LLM的效率。然而，目前的研究只在有限的模型、数据集、指标等方面验证了它们的方法，仍然缺乏在更一般场景下的全面评估。所以在具体情况下我们应该使用哪种模型压缩方法仍然是一个问题。为了弥补这一差距，我们提出了大型语言模型压缩基准（LLMCBench），这是一个严格设计的基准，对LLM压缩算法进行了深入分析。我们首先分析实际的模型生产需求，精心设计评估轨道和指标。然后，我们使用多种主流的LLM压缩方法进行了广泛的实验和比较。最后，我们根据评估进行了深入分析，并为LLM压缩设计提供了有用的见解。我们希望我们的LLMCBench能够为LLM压缩算法设计提供有见地的建议，并为未来的研究奠定基础。我们的代码可以在 https://github.com/AboveParadise/LLMCBench 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.21352</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>因果路径上的因果干预：将 GPT-2 的推理从句法映射到语义</title>
      <link>https://arxiv.org/abs/2410.21353</link>
      <description><![CDATA[arXiv:2410.21353v1 公告类型：新
摘要：虽然可解释性研究揭示了基于 Transformer 的 LLM 所使用的一些内部算法，但自然语言推理具有深层的语境性和模糊性，难以进行简单的分类。因此，制定清晰且有激励性的电路分析问题是一项挑战，这些问题依赖于因果干预所需的明确定义的域内和域外示例。尽管大量工作已经研究了特定任务的电路，例如间接对象识别 (IOI)，但由于其固有的复杂性，通过电路解密自然语言推理仍然很困难。在这项工作中，我们采取初步措施来表征 LLM 中的因果推理，方法是分析明确的因果句子，例如“我打开了雨伞，因为开始下雨”，其中因果干预可能通过使用 GPT-2 small 精心设计的场景来实现。我们的研究结果表明，因果句法集中在前 2-3 层，而后几层中的某些头句对因果句的无意义变体表现出高度的敏感性。这表明模型可以通过 (1) 检测句法线索和 (2) 在关注语义关系的最后几层中隔离不同的头句来推断推理。]]></description>
      <guid>https://arxiv.org/abs/2410.21353</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于能量的文本生成扩散语言模型</title>
      <link>https://arxiv.org/abs/2410.21357</link>
      <description><![CDATA[arXiv:2410.21357v1 公告类型：新
摘要：尽管自回归语言模型取得了显著进展，但人们仍在积极探索除从左到右生成之外的替代生成范式。具有并行生成能力的离散扩散模型最近已成为一种有前途的替代方案。不幸的是，这些模型的表现仍然不如自回归模型，而且在减少采样步骤数时，性能差距会越来越大。我们的分析表明，这种退化是由于扩散模型使用的近似值不完美造成的。在这项工作中，我们提出了基于能量的扩散语言模型 (EDLM)，这是一种在每个扩散步骤的全序列级别上运行的基于能量的模型，旨在改进扩散模型使用的底层近似值。更具体地说，我们引入了残差形式的 EBM，并表明可以通过利用预训练的自回归模型或通过噪声对比估计微调双向变压器来获得其参数。我们还提出了一种通过并行重要采样实现的有效生成算法。在语言建模基准上进行的全面实验表明，我们的模型可以持续显著超越最先进的扩散模型，并且接近自回归模型的困惑度。我们进一步表明，在没有任何生成性能下降的情况下，我们的框架比现有的扩散模型提供了 1.3$\times$ 的采样速度。]]></description>
      <guid>https://arxiv.org/abs/2410.21357</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器能像人类一样思考吗？独裁者博弈中 LLM-Agent 的行为评估</title>
      <link>https://arxiv.org/abs/2410.21359</link>
      <description><![CDATA[arXiv:2410.21359v1 公告类型：新
摘要：随着基于大型语言模型 (LLM) 的代理越来越多地承担现实世界的任务并与人类社会互动，我们对它们的行为了解多少？本研究 (1) 调查了 LLM 代理的亲社会行为（一种基本的社会规范）如何由不同的角色诱导并与人类行为进行基准测试；(2) 引入了一种行为方法来评估 LLM 代理在复杂决策场景中的表现。我们探讨了不同的角色和实验框架如何影响这些 AI 代理在独裁者游戏中的利他行为，并比较了它们在同一个 LLM 家族、不同家族以及人类行为中的行为。我们的研究结果揭示了 LLM 之间存在显着的差异和不一致性，以及与人类行为相比的显着差异。仅仅为 LLM 分配类似人类的身份并不能产生类似人类的行为。尽管这些 AI 代理接受了大量人类生成的数据的训练，但它们无法准确预测人类的决策。 LLM 代理无法捕捉人类决策的内部过程，并且它们与人类行为的一致性具有高度可变性，并且依赖于特定的模型架构和提示公式；更糟糕的是，这种依赖性并不遵循清晰的模式。]]></description>
      <guid>https://arxiv.org/abs/2410.21359</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型时代文本可信度信号的自动可信度评估综述</title>
      <link>https://arxiv.org/abs/2410.21360</link>
      <description><![CDATA[arXiv:2410.21360v1 公告类型：新 
摘要：在当今社交媒体和生成式人工智能时代，自动评估在线社交媒体内容可信度的能力至关重要。可信度评估从根本上讲是基于将可信度信号（指的是内容真实性、偏见或说服技巧的存在等小信息单位）聚合成总体可信度分数。与目前占主导地位的假新闻检测（利用各种（主要是潜在）特征）相比，可信度信号提供了更细粒度、更容易解释和广泛利用的信息。越来越多的关于自动可信度评估和可信度信号检测的研究可以归结为高度分散和缺乏相互联系。由于缺乏对自动可信度评估研究工作的最新概述，这个问题更加突出。在本综述中，我们对 175 篇研究论文进行了系统而全面的文献综述，重点关注文本可信度信号和自然语言处理 (NLP)，由于大型语言模型 (LLM) 的出现，该领域取得了重大进展。在将 NLP 研究置于其他多学科研究工作的背景下时，我们研究了可信度评估方法以及 9 类可信度信号（我们对其中 3 类进行了彻底分析，即：1）事实性、主观性和偏见，2）说服技巧和逻辑谬误，3）主张和真实性）。在描述现有方法、数据集和工具之后，我们确定了未来的挑战和机遇，同时特别关注生成式人工智能的近期快速发展。]]></description>
      <guid>https://arxiv.org/abs/2410.21360</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CT2C-QA：针对中文文本、表格和图表的多模态问答</title>
      <link>https://arxiv.org/abs/2410.21414</link>
      <description><![CDATA[arXiv:2410.21414v1 公告类型：新
摘要：多模态问答 (MMQA) 至关重要，因为它通过整合来自表格、图表和文本等不同数据表示的见解，实现全面理解和准确响应。MMQA 中的大多数现有研究仅关注图像文本问答、表格文本问答和图表文本问答两种模态，而研究文本、表格和图表的联合分析的研究仍然非常稀缺。在本文中，我们提出了 C$\text{T}^2$C-QA，这是一个开创性的基于推理的中文问答数据集，其中包含大量文本、表格和图表，这些文本、表格和图表是从 200 个选择性来源的网页精心编制而成的。我们的数据集模拟了真实的网页，是对模型分析和推理多模态数据的能力的一个很好的测试，因为问题的答案可能出现在各种模态中，甚至可能根本不存在。此外，我们还介绍了 AED（\textbf{A}llocating、\textbf{E}xpert 和 \textbf{D}esicion），这是一个通过不同代理之间的协作部署、信息交互和集体决策实现的多代理系统。具体来说，分配代理负责选择和激活专家代理，包括精通文本、表格和图表的代理。决策代理负责根据这些专家代理提供的分析见解做出最终裁决。我们进行了全面的分析，将 AED 与 MMQA 中的各种最新模型（包括 GPT-4）进行了比较。实验结果表明，包括 GPT-4 在内的当前方法尚未达到我们数据集设定的基准。]]></description>
      <guid>https://arxiv.org/abs/2410.21414</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UFT：通过广义隐式奖励函数统一 SFT 和 RLHF/DPO/UNA 的微调</title>
      <link>https://arxiv.org/abs/2410.21438</link>
      <description><![CDATA[arXiv:2410.21438v1 公告类型：新
摘要：通过对数万亿个 token 进行预训练，LLM 获得了文本生成能力。然而，为了增强其实用性并减少潜在危害，SFT 和对齐被顺序应用于预训练模型。由于 SFT 和对齐的性质和目标函数不同，灾难性遗忘已成为一个重大问题。为了解决这个问题，我们引入了统一微调 (UFT)，它通过隐式奖励函数将 SFT 和对齐集成到单个训练阶段，使用相同的目标和损失函数。我们的实验结果表明，UFT 在单独的指令调整数据上优于 SFT。此外，当将指令调整数据与对齐数据相结合时，UFT 有效地防止了这两个阶段的灾难性遗忘，并且比顺序应用 SFT 和对齐显示出明显的优势。这在 \textbf{ifeval} 任务的指令遵循和 \textbf{truthful-qa} 任务的事实性方面观察到的显著改进中可见一斑。所提出的通用微调框架 UFT 为 LLM 培训建立了有效且高效的预训练-UFT 范例。]]></description>
      <guid>https://arxiv.org/abs/2410.21438</guid>
      <pubDate>Wed, 30 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>