<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 16 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>FuseChat：聊天模型的知识融合</title>
      <link>https://arxiv.org/abs/2408.07990</link>
      <description><![CDATA[arXiv:2408.07990v1 公告类型：新
摘要：虽然从头开始训练大型语言模型 (LLM) 确实可以产生具有不同能力和优势的模型，但它会产生大量成本并可能导致能力冗余。知识融合旨在通过轻量级的持续训练将现有的不同架构和能力的 LLM 集成到更强大的 LLM 中，从而减少对昂贵的 LLM 开发的需求。在这项工作中，我们提出了一个通过两个主要阶段进行聊天 LLM 知识融合的新框架，从而产生了 FuseChat。首先，我们对不同结构和规模的源聊天 LLM 进行成对知识融合，通过轻量级微调创建具有相同结构和大小的多个目标 LLM。在此过程中，引入了基于统计的标记对齐方法作为融合不同结构的 LLM 的基石。其次，我们在参数空间内合并这些目标 LLM，并提出了一种新方法，根据微调前后参数更新的幅度来确定合并系数。我们使用六种具有不同架构和规模的著名聊天 LLM 实现并验证了 FuseChat，包括 OpenChat-3.5-7B、Starling-LM-7B-alpha、NH2-SOLAR-10.7B、InternLM2-Chat-20B、Mixtral-8x7B-Instruct 和 Qwen-1.5-Chat-72B。在两个指令跟踪基准测试 AlpacaEval 2.0 和 MT-Bench 上的实验结果表明，FuseChat-7B 优于各种规模的基线。我们的模型甚至可以与更大的 Mixtral-8x7B-Instruct 相媲美，并且在 MT-Bench 上接近 GPT-3.5-Turbo-1106。我们的代码、模型权重和数据在 \url{https://github.com/fanqiwan/FuseAI} 上公开。]]></description>
      <guid>https://arxiv.org/abs/2408.07990</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>利用网络爬取数据进行高质量微调</title>
      <link>https://arxiv.org/abs/2408.08003</link>
      <description><![CDATA[arXiv:2408.08003v1 公告类型：新
摘要：大多数大型语言模型都是使用昂贵的人工注释数据或 GPT-4 生成的数据进行微调的，而这些数据无法保证在某些领域的性能。我们认为，尽管网络爬取的数据经常存在格式错误导致语义不准确，但它仍然可以作为特定领域高质量监督微调的宝贵来源，而无需依赖 GPT-4 等高级模型。为此，我们通过将网络爬取的数据与一组较小的高质量数据对齐来自动创建配对训练数据集。通过在该数据集上训练语言模型，我们可以将格式不规则的网络数据转换为高质量数据。我们的实验表明，使用模型转换后的数据进行训练可以产生更好的结果，在中国数学问题中比仅使用高质量数据的训练平均高出 9.4%。此外，我们的 7B 模型表现优于几个大于 32B 的开源模型，并超越了 GPT-3.5 等知名的闭源模型，凸显了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.08003</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>RAGChecker：用于诊断检索增强生成的细粒度框架</title>
      <link>https://arxiv.org/abs/2408.08067</link>
      <description><![CDATA[arXiv:2408.08067v1 公告类型：新
摘要：尽管检索增强生成 (RAG) 在利用外部知识方面表现出良好的能力，但由于 RAG 的模块化特性、对长格式响应的评估和测量的可靠性，对 RAG 系统的全面评估仍然具有挑战性。在本文中，我们提出了一个细粒度的评估框架 RAGChecker，它结合了一套针对检索和生成模块的诊断指标。元评估验证了 RAGChecker 与人类判断的相关性明显优于其他评估指标。使用 RAGChecker，我们评估了 8 个 RAG 系统并对其性能进行了深入分析，揭示了 RAG 架构设计选择中的深刻模式和权衡。RAGChecker 的指标可以指导研究人员和从业人员开发更有效的 RAG 系统。]]></description>
      <guid>https://arxiv.org/abs/2408.08067</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型预测肺癌患者预后</title>
      <link>https://arxiv.org/abs/2408.07971</link>
      <description><![CDATA[arXiv:2408.07971v1 公告类型：新
摘要：预后预测对于确定肺癌患者的最佳治疗方案至关重要。传统上，这种预测依赖于从回顾性患者数据开发的模型。最近，大型语言模型 (LLM) 因其基于大量学习知识处理和生成文本的能力而受到关注。在本研究中，我们评估了 GPT-4o mini 和 GPT-3.5 在预测肺癌患者预后方面的潜力。我们收集了两个预后数据集，即生存期和术后并发症数据集，并设计了多个任务来全面评估模型的性能。还开发了逻辑回归模型作为比较的基线。实验结果表明，尽管不使用额外的患者数据，但与数据驱动的逻辑回归模型相比，LLM 在肺癌预后预测方面可以实现具有竞争力的性能，并且在某些任务中表现更出色。这些发现表明，LLM 可以成为肺癌预后预测的有效工具，特别是在患者数据有限或不可用的情况下。]]></description>
      <guid>https://arxiv.org/abs/2408.07971</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>ArabLegalEval：用于评估大型语言模型中的阿拉伯语法律知识的多任务基准</title>
      <link>https://arxiv.org/abs/2408.07983</link>
      <description><![CDATA[arXiv:2408.07983v1 公告类型：新
摘要：大型语言模型 (LLM) 的快速发展已导致各种自然语言处理任务的显着改进。然而，对 LLM 的法律知识的评估，特别是对阿拉伯语等非英语语言的法律知识的评估，仍然未得到充分探索。为了解决这一差距，我们引入了 ArabLegalEval，这是一个用于评估 LLM 的阿拉伯语法律知识的多任务基准数据集。受 MMLU 和 LegalBench 数据集的启发，ArabLegalEval 由来自沙特法律文件和综合问题的多个任务组成。在这项工作中，我们旨在分析解决阿拉伯语法律问题所需的能力，并对最先进的 LLM 的性能进行基准测试。我们探索了上下文学习的影响并研究了各种评估方法。此外，我们还探索了生成具有自动验证的问题的工作流程，以提高数据集的质量。我们分别对多语言和以阿拉伯语为中心的 LLM（例如 GPT-4 和 Jais）进行了基准测试。我们还分享了创建数据集和验证的方法，这些方法可以推广到其他领域。我们希望通过发布 ArabLegalEval 数据集和代码来加速阿拉伯法律领域的 AI 研究：https://github.com/Thiqah/ArabLegalEval]]></description>
      <guid>https://arxiv.org/abs/2408.07983</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>MAG-SQL：具有软模式链接和迭代子 SQL 细化的文本到 SQL 多代理生成方法</title>
      <link>https://arxiv.org/abs/2408.07930</link>
      <description><![CDATA[arXiv:2408.07930v1 公告类型：新
摘要：最近基于上下文学习的方法在文本到 SQL 任务中取得了显著的成功。然而，这些模型的性能与人类在具有复杂数据库模式和困难问题的数据集（如 BIRD）上的性能仍然存在很大差距。此外，现有工作忽略了在使用问题分解方法迭代解决问题时监督中间步骤，并且这些工作中使用的模式链接方法非常简陋。为了解决这些问题，我们提出了 MAG-SQL，一种具有软模式链接和迭代 Sub-SQL 细化的多代理生成方法。在我们的框架中，使用基于表摘要的实体方法来选择数据库中的列，并引入了一种新颖的目标条件分解方法来分解这些复杂问题。此外，我们构建了一个迭代生成模块，其中包括一个 Sub-SQL 生成器和 Sub-SQL 细化器，为生成的每个步骤引入外部监督。通过一系列消融研究，我们框架中每个代理的有效性都得到了证明。在使用 GPT-4 的 BIRD 基准上进行评估时，MAG-SQL 的执行准确率为 61.08%，而 vanilla GPT-4 的基线准确率为 46.35%，MAC-SQL 的基线准确率为 57.56%。此外，我们的方法在 Spider 上也取得了类似的进展。]]></description>
      <guid>https://arxiv.org/abs/2408.07930</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>GERestaurant：用于基于方面的情绪分析的德国带注释餐厅评论数据集</title>
      <link>https://arxiv.org/abs/2408.07955</link>
      <description><![CDATA[arXiv:2408.07955v1 公告类型：新
摘要：我们介绍了 GERestaurant，这是一个新颖的数据集，包含 3,078 条德语餐厅评论，这些评论已手动注释为基于方面的情绪分析 (ABSA)。所有评论均来自 Tripadvisor，涵盖了各种各样的餐厅，包括具有各种烹饪风格的区域和国际美食。注释涵盖了隐式和显式方面，包括所有方面术语、它们对应的方面类别以及对它们表达的情绪。此外，我们为四个 ABSA 任务方面类别检测、方面类别情绪分析、端到端 ABSA 和目标方面情绪检测提供了基线分数，作为未来发展的参考点。该数据集填补了德语资源的空白，并促进了餐厅领域 ABSA 的探索。]]></description>
      <guid>https://arxiv.org/abs/2408.07955</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>指导大型语言模型逐步生成科学文献调查</title>
      <link>https://arxiv.org/abs/2408.07884</link>
      <description><![CDATA[arXiv:2408.07884v1 公告类型：新 
摘要：摘要。自动生成科学文献调查是一项有价值的任务，可以显著提高研究效率。然而，文献调查中信息的多样性和复杂性对生成模型提出了巨大的挑战。在本文中，我们设计了一系列提示来系统地利用大型语言模型 (LLM)，从而能够通过循序渐进的方式创建全面的文献调查。具体来说，我们设计提示来指导 LLM 依次生成文献调查的标题、摘要、分层标题和主要内容。我们认为这种设计能够从高级角度生成标题。在内容生成过程中，此设计通过限制 LLM 查询中输入和输出内容的长度，有效地利用相关信息，同时最大限度地降低成本。我们基于 Qwen-long 的实现在 NLPCC 2024 科学文献综述生成评估任务中取得了第三名，总分仅比第二名团队低 0.03%。此外，我们的软标题召回率为 95.84%，在提交的论文中排名第二。得益于高效的提示设计和 Qwen-long API 的低成本，我们的方法将生成每个文献综述的费用降低到 0.1 元人民币，提高了我们方法的实用价值。]]></description>
      <guid>https://arxiv.org/abs/2408.07884</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>在医学问答中使用人类启发式学习策略对大型语言模型进行微调</title>
      <link>https://arxiv.org/abs/2408.07888</link>
      <description><![CDATA[arXiv:2408.07888v1 公告类型：新
摘要：训练大型语言模型 (LLM) 会产生大量与数据相关的成本，这促使人们通过优化数据排序和选择来开发数据高效的训练方法。人类启发式学习策略（例如课程学习）通过根据常见的人类学习实践组织数据，为高效训练提供了可能性。尽管有证据表明，通过课程学习进行微调可以提高 LLM 在自然语言理解任务中的表现，但其有效性通常使用单一模型来评估。在这项工作中，我们扩展了以前的研究，通过评估多个 LLM 中的基于课程和非基于课程的学习策略，使用人为定义和自动化的数据标签来回答医学问题。我们的结果表明，使用人类启发式学习策略对微调 LLM 产生了中等影响，每个模型的最大准确率提高了 1.77%，每个数据集的最大准确率提高了 1.81%。至关重要的是，我们证明了这些策略的有效性在不同的模型数据集组合中存在显著差异，强调了特定人类启发的 LLM 微调策略的好处并不具有普遍性。此外，我们发现证据表明，使用 LLM 定义的问题难度的课程学习优于人类定义的难度，这凸显了使用模型生成的度量进行最佳课程设计的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.07888</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>评估语言模型的世界观对小说生成的影响</title>
      <link>https://arxiv.org/abs/2408.07904</link>
      <description><![CDATA[arXiv:2408.07904v1 公告类型：新
摘要：大型语言模型 (LLM) 的使用已经变得无处不在，在计算创造力方面有着丰富的应用。其中一个应用是虚构故事的生成。小说是一种发生在与我们略有不同的故事世界中的叙述。随着 LLM 成为写作伙伴，我们质疑他们是否适合创作小说。本研究调查了 LLM 维持创作小说所必需的世界状态的能力。通过对九位 LLM 提出一系列问题，我们发现只有两个模型表现出一致的世界观，而其余的都是自相矛盾的。随后对四个模型生成的故事的分析揭示了一种惊人统一的叙事模式。模型之间的这种一致性进一步表明缺乏小说所必需的“状态”。我们强调了当前 LLM 在小说写作方面的局限性，并提倡未来的研究来测试和创建适合 LLM 居住的故事世界。所有代码、数据集和生成的响应都可以在 https://github.com/tanny411/llm-reliability-and-consistency-evaluation 中找到。]]></description>
      <guid>https://arxiv.org/abs/2408.07904</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>语言至关重要：利用大型语言模型减少在线对话中有关物质使用的耻辱感</title>
      <link>https://arxiv.org/abs/2408.07873</link>
      <description><![CDATA[arXiv:2408.07873v1 公告类型：新
摘要：对于患有物质使用障碍 (SUD) 的人来说，耻辱是治疗的障碍，这导致治疗参与率显著降低。只有 7% 的患者接受任何形式的帮助，社会耻辱不仅阻止了 SUD 患者寻求帮助，而且孤立了他们，阻碍了他们的康复之旅，并延续了羞耻和自我怀疑的循环。这项研究调查了耻辱在社交媒体上的表现，尤其是 Reddit，匿名性会加剧歧视行为。我们分析了超过 120 万个帖子，发现 3,207 个帖子表现出对使用物质的人 (PWUS) 的污名化语言。使用知情和风格化的 LLM，我们开发了一个模型，将这些表达去污名化为富有同情心的语言，从而产生了 1,649 个改革​​后的短语对。我们的论文提出了一个用于分析污名和消除网络内容污名的计算框架，并深入研究了传播针对 PWUS 的污名的语言特征，为该领域做出了贡献。我们的工作不仅增强了对污名在网上表现形式的理解，还提供了实用工具，为受 SUD 影响的人营造一个更具支持性的数字环境。代码和数据将在论文被接受后公开。]]></description>
      <guid>https://arxiv.org/abs/2408.07873</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:12 GMT</pubDate>
    </item>
    <item>
      <title>ONSEP：一种基于大型语言模型的新型在线神经符号事件预测框架</title>
      <link>https://arxiv.org/abs/2408.07840</link>
      <description><![CDATA[arXiv:2408.07840v1 公告类型：新
摘要：在事件预测领域，时间知识图谱预测 (TKGF) 是一种关键技术。以前的方法面临的挑战是在测试期间不利用经验并依赖单一的短期历史，这限制了对不断变化的数据的适应。在本文中，我们介绍了在线神经符号事件预测 (ONSEP) 框架，该框架通过集成动态因果规则挖掘 (DCRM) 和双重历史增强生成 (DHAG) 进行创新。DCRM 从实时数据中动态构建因果规则，从而可以快速适应新的因果关系。同时，DHAG 融合了短期和长期历史背景，利用双分支方法来丰富事件预测。我们的框架在不同数据集上都表现出显著的性能提升，其中 Hit@k（k=1,3,10）显著提升，展示了其增强大型语言模型 (LLM) 进行事件预测的能力，而无需进行大量重新训练。ONSEP 框架不仅推动了 TKGF 领域的发展，还凸显了神经符号方法在适应动态数据环境方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.07840</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>SER 评估：语音情感识别的域内和域外基准测试</title>
      <link>https://arxiv.org/abs/2408.07851</link>
      <description><![CDATA[arXiv:2408.07851v1 公告类型：新
摘要：随着强大的自监督学习 (SSL) 模型的出现，语音情感识别 (SER) 取得了重大进展。然而，将这些模型推广到不同的语言和情感表达仍然是一个挑战。我们提出了一个大规模基准来评估最先进的 SER 模型在域内和域外环境中的稳健性和适应性。我们的基准包括一组多样化的多语言数据集，重点关注不太常用的语料库以评估对新数据的泛化。我们采用 logit 调整来考虑不同的类分布并建立单个数据集集群进行系统评估。令人惊讶的是，我们发现主要用于自动语音识别的 Whisper 模型在跨语言 SER 中的表现优于专用的 SSL 模型。我们的结果强调了对更强大和更通用的 SER 模型的需求，我们的基准是推动未来研究的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2408.07851</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>在知识图谱上训练语言模型：对幻觉及其可检测性的洞察</title>
      <link>https://arxiv.org/abs/2408.07852</link>
      <description><![CDATA[arXiv:2408.07852v1 公告类型：新
摘要：虽然语言模型 (LM) 的许多功能随着训练预算的增加而提高，但规模对幻觉的影响尚未完全了解。幻觉有多种形式，没有普遍接受的定义。因此，我们专注于研究那些在训练集中逐字出现正确答案的幻觉。为了完全控制训练数据内容，我们构建了一个基于知识图谱 (KG) 的数据集，并使用它来训练一组越来越大的 LM。我们发现，对于固定的数据集，更大、训练时间更长的 LM 幻觉较少。然而，对 $\leq5$% 的训练数据产生幻觉需要一个数量级更大的模型，因此计算量也比 Hoffmann 等人 (2022) 报告的最佳值大一个数量级。考虑到这种成本高昂，我们研究了幻觉检测器如何依赖于规模。虽然我们看到探测器尺寸提高了固定 LM 输出的性能，但我们发现 LM 的规模与其幻觉的可检测性之间存在反比关系。]]></description>
      <guid>https://arxiv.org/abs/2408.07852</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>语言驱动的切片发现和错误纠正</title>
      <link>https://arxiv.org/abs/2408.07832</link>
      <description><![CDATA[arXiv:2408.07832v1 公告类型：新
摘要：错误切片发现将结构化模式与模型错误相关联。现有方法通过将具有相似模式的易错样本聚类或为每个样本分配离散属性以进行事后分析来发现错误切片。虽然这些方法旨在通过重新加权或重新平衡来实现可解释性和更容易的缓解，但由于属性不完整或缺失，它们可能无法捕获错误模式的全部复杂性。与现有方法相反，本文利用大型语言模型 (LLM) 的推理能力来分析复杂的错误模式并生成可测试的假设。本文提出了 LADDER：语言驱动的切片发现和错误纠正。它首先将模型的表示投影到语言对齐的特征空间（\eg CLIP）中，以保留原始模型特征空间中的语义。这确保准确检索突出模型错误的句子。接下来，LLM 利用句子并生成假设来发现错误切片。最后，我们通过使用假设创建组平衡数据集来微调分类头，从而减轻错误。我们的整个方法不需要任何属性注释，无论是显式注释还是通过外部标记模型。我们使用 \textbf{five} 个图像分类数据集验证了我们的方法。代码可从\footnote{\url{https://github.com/batmanlab/Ladder}} 获取]]></description>
      <guid>https://arxiv.org/abs/2408.07832</guid>
      <pubDate>Fri, 16 Aug 2024 06:20:10 GMT</pubDate>
    </item>
    </channel>
</rss>