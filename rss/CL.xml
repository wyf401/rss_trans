<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 04 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Eeyore：通过监督和偏好优化的现实抑郁模拟</title>
      <link>https://arxiv.org/abs/2503.00018</link>
      <description><![CDATA[ARXIV：2503.00018V1公告类型：新 
摘要：大型语言模型（LLMS）以前已经进行了用于心理保健培训和治疗客户模拟的探索，但它们在真正捕获多样化的客户特征和心理状况方面仍然缺乏。我们介绍了\ textbf {eeyore}，这是一种通过结构化对齐框架进行了为逼真的抑郁模拟优化的8B模型，在每个阶段都结合了专家输入。首先，我们系统地策划了现实世界中与抑郁症相关的对话，提取抑郁特征来指导数据过滤和心理概况构建，并将此数据集使用该数据集来指导Eyeyore以遵守个人资料依从性。接下来，为了进一步增强现实主义，Eeyore经历了迭代偏好优化 - 首先利用模型生成的偏好，然后使用一小部分专家宣布的偏好进行校准。在整个管道中，我们积极地与域专家合作，开发交互式界面以验证特质提取，并迭代地完善结构化心理概况，以实现临床上有意义的角色扮演定制。尽管模型尺寸较小，但Eyeore抑郁型模拟在语言真实性和概况依从性方面均超过了GPT-4O，促使SOTA提示策略。]]></description>
      <guid>https://arxiv.org/abs/2503.00018</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对文本对图像（T2I）Gen AI模型安全性中使用的开放数据集的系统评价</title>
      <link>https://arxiv.org/abs/2503.00020</link>
      <description><![CDATA[ARXIV：2503.00020V1公告类型：新 
摘要：针对文本图像图像（T2I）生成AI安全性的新颖研究通常依赖于公开可用的数据集用于培训和评估，从而使这些数据集的质量和组成至关重要。本文对T2I研究中使用的关键数据集进行了全面的评论，详细介绍了提示的收集方法，组成，语义和句法多样性以及数据集中危害类型的质量，覆盖范围和分布。通过强调数据集的优势和局限性，本研究使研究人员能够找到用例使用数据集分布的用例，尤其是在模型安全性方面，并确定未来研究的数据集覆盖率和质量差异，因此，研究人员能够找到最相关的数据集，批判性地评估其工作的下游影响。]]></description>
      <guid>https://arxiv.org/abs/2503.00020</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>kvcrush：使用相似性的键值缓存尺寸减少</title>
      <link>https://arxiv.org/abs/2503.00022</link>
      <description><![CDATA[ARXIV：2503.00022V1公告类型：新 
摘要：键值（KV）缓存已成为一种至关重要的优化技术，用于加速大型语言模型（LLMS）。通过允许注意力运行线性地扩展而不是二次地缩放，KV缓存显着增强了生成吞吐量。但是，由于现代LLM中的上下文长度较大，KV的内存足迹是直接影响模型批次大小的模型部署的巨大瓶颈，阻碍了其提供高通量的能力。现有的研究使用多种技术解决了这一挑战，例如丢弃低意见令牌，量化和矩阵近似，这通常会对模型准确性产生负面影响。
  在本文中，我们提出了KVCrush技术，该技术可以与许多KV压缩技术结合使用，以提高在较小的内存下的模型准确性。 KVCrush为钥匙值状态提供了替代表示方案，以及一个低空的令牌修剪算法，该算法是KV CACHE中的令牌分布，这又允许较小的足迹，同时保持模型的准确性。根据我们的结果，KVCrush将Longbench KV高速缓存的大小降低了4倍，精度下降少于1％，并实现了最先进的平均准确性，而最小的开销则少于0.5％的总推断潜伏期。 KVCrush不仅要优于最先进的基于重要性的令牌保留方案的准确性，而且还与使用KV缓存计划方案（例如VLLM）和混合精度量化的典型实用LLM部署兼容。]]></description>
      <guid>https://arxiv.org/abs/2503.00022</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>情绪真的影响论点令人信服吗？基于LLM的操作检查的动态方法</title>
      <link>https://arxiv.org/abs/2503.00024</link>
      <description><![CDATA[ARXIV：2503.00024V1公告类型：新 
摘要：情绪已被证明在论证令人信服中发挥了作用，但是在自然语言处理（NLP）社区中，这一方面却没有反应。与先前使用静态分析的研究，专注于单个文本领域或语言，或将情感视为众多因素之一，我们引入了一个动态框架，灵感来自心理学和社会科学中常用的操纵检查；该框架利用基于LLM的操作检查检查了感知的情绪强度影响感知到的令人信服的程度。通过人类对跨不同语言，文本领域和主题的论点的评估，我们发现，尽管有感知到的情感强度有所不同，但在超过一半的情况下，令人信服的判断仍然没有改变。当情绪确实产生影响时，它们常常会增强而不是弱化。我们进一步分析了11个LLM在同一情况下的行为方式，发现尽管LLM通常反映了人类的模式，但它们却难以捕捉单个判断中的细微情感影响。]]></description>
      <guid>https://arxiv.org/abs/2503.00024</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在西班牙医学实习生（MIR）考试2024/2025评估大型语言模型：临床推理和知识应用的比较分析</title>
      <link>https://arxiv.org/abs/2503.00025</link>
      <description><![CDATA[ARXIV：2503.00025V1公告类型：新 
摘要：这项研究对西班牙医学实习生的22种大语言模型进行了比较评估，其中2024年和2025年的MIR考试重点侧重于临床推理领域特定的专业知识和多模式处理能力。MiR考试由210个多模式考试组成，由210个多项选择性问题组成，有些多种选择问题，这些问题既需要进行临床问题，又可以评估临床问题。克劳德·拉玛（Claude Llama）和双子座（Gemini）以及像Miri Pro这样的专业微调系统，它利用了西班牙医疗保健数据在医疗环境中脱颖而出
  Recent market entries Deepseek and Grok have further enriched the evaluation landscape particularly for tasks that demand advanced visual and semantic analysis The findings indicate that while general purpose LLMs perform robustly overall fine tuned models consistently achieve superior accuracy especially in addressing nuanced domain specific challenges A modest performance decline observed between the two exam cycles appears attributable to the implementation of modified questions designed to mitigate reliance on memorization
  结果强调了领域特定的微调和多模式整合在进行医学AI应用方面的变革潜力]]></description>
      <guid>https://arxiv.org/abs/2503.00025</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过语言特征分析检测LLM生成的韩语文本</title>
      <link>https://arxiv.org/abs/2503.00032</link>
      <description><![CDATA[ARXIV：2503.00032V2公告类型：新 
摘要：大语言模型（LLMS）的快速发展增加了区分人写和LLM生成的文本的困难。检测LLM生成的文本对于维护学术完整性，防止窃，保护版权和确保道德研究实践至关重要。大多数关于检测LLM生成的文本的先前研究主要集中在英语文本上。但是，具有独特形态和句法特征的语言需要专门的检测方法。它们的独特结构和使用模式可能会阻碍主要针对英语设计的方法的直接应用。在此类语言中，我们专注于韩语，韩语具有相对灵活的间距规则，丰富的形态学系统，并且与英语相比，使用频率较低。我们介绍了Katfish，这是第一个用于检测LLM生成的韩语文本的基准数据集。数据集由人类编写的文本组成，并由四种类型的四个LLM生成。
  通过检查间距模式，言论的一部分多样性和逗号的使用，我们阐明了人文和LLM生成的韩语文本之间的语言差异。在这些观察结果的基础上，我们提出了Katfishnet，这是一种专门为韩语设计的检测方法。与表现最佳的现有检测方法相比，Katfishnet平均AUROC平均增加了19.78％。我们的代码和数据可在https://github.com/shinwoo-park/detecting_llm_generated_korean_korean_text_through_linguistic_analysis中获得。]]></description>
      <guid>https://arxiv.org/abs/2503.00032</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过编辑锚压缩来限制顺序模型编辑</title>
      <link>https://arxiv.org/abs/2503.00035</link>
      <description><![CDATA[ARXIV：2503.00035V1公告类型：新 
摘要：大型语言模型（LLMS）由于虚假或过时的知识而与幻觉作斗争。鉴于对这些模型的资源要求很高，因此越来越关注开发模型编辑。但是，跨下游任务的LLM的一般能力在顺序编辑过程中容易出现显着降解。从统计学上讲，本文的编辑后参数矩阵与先前的状态相比，随着编辑数量的增加，参数矩阵表现出明显的偏差。这种严重的偏差会影响LLM内的原始知识协会，并导致其一般能力的退化。为此，提出了一个称为编辑锚压缩（EAC）的框架来限制在顺序编辑过程中参数矩阵的偏差。它通过选择编辑锚来压缩编辑信息，这些锚对编码新关系很重要而不偏离原始矩阵，从而保留了一般能力。在四个任务上，将EAC应用于三个LLM的两种流行编辑方法的实验。评估结果表明，EAC可以有效地最大程度地减少模型编辑引起的不合理偏差，从而保留了超过70％的一般能力，同时与原始对应方法相比，可以更好地保留编辑知识。]]></description>
      <guid>https://arxiv.org/abs/2503.00035</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过LVLMS中固有的多模式比对，针对有毒图像的零射击防御</title>
      <link>https://arxiv.org/abs/2503.00037</link>
      <description><![CDATA[ARXIV：2503.00037V1公告类型：新 
摘要：大型视觉模型（LVLM）在多模式理解方面取得了重大步伐，这要归功于大型视觉数据集的大量预训练和微调。但是，尽管具有强大的文本安全机制，但它们仍然容易受到有害视觉投入的影响。现有的保障措施通常依赖于预过滤或微调的高成本和整体效用。为了解决这个关键的漏洞，我们引入了SafeClip，这是一种轻巧的方法，该方法利用LVLMS固有的多模式对齐方式来零射击有毒图像检测。 By projecting CLIPs discarded CLS token into its text space and matching it with toxic descriptors, SafeCLIP detects harmful content without any architectural changes-adding minimal latency and enabling dynamic safety corrections during inference and fine-tuning.Experiments show that SafeCLIP achieves a 66.9% defense success rate with only 3.2% false positive rate and 7.2% overhead.相比之下，最先进的方法取得了52.9％的成功，但假阳性率为10.7％，开销210％。我们的工作表明，利用固有的多模式比对可以产生有效的低成本LVLM安全性。代码可在匿名。]]></description>
      <guid>https://arxiv.org/abs/2503.00037</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从良性进口有毒：通过对抗隐喻越狱语言模型</title>
      <link>https://arxiv.org/abs/2503.00038</link>
      <description><![CDATA[ARXIV：2503.00038V1公告类型：新 
摘要：当前的研究暴露了大语模型（LLMS）的风险，该模型通过越狱攻击产生有害内容。但是，他们忽略了从头开始的有害内容的直接生成比诱导LLM校准良性含量为有害形式更加困难。在我们的研究中，我们介绍了一个新颖的攻击框架，该攻击框架利用对抗性隐喻（Avatar）诱导LLM校准恶意隐喻以犯有越狱。具体而言，要回答有害的查询，阿凡达（Avatar）会自适应地识别一组良性但与逻辑相关的隐喻为初始种子。然后，在这些隐喻的驱动下，目标LLM被诱导进行推理并校准隐喻内容，从而通过直接输出有害响应或校准隐喻和专业有害内容之间的残留物来越狱。实验结果表明，阿凡达（Avatar）可以有效，可转让的越狱LLM，并在多个高级LLM中获得最先进的攻击成功率。]]></description>
      <guid>https://arxiv.org/abs/2503.00038</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估基于LLMS的隐藏状态作为以人为中心的NLP任务的作者表示</title>
      <link>https://arxiv.org/abs/2503.00124</link>
      <description><![CDATA[ARXIV：2503.00124V1公告类型：新 
摘要：与大多数NLP一样，以人为中心的NLP任务模型 - 试图评估作者级信息的任务 - 主要使用来自基于变压器LLMS的隐藏状态的表示形式。但是，LM的哪个组件用于表示形式差异很大。此外，需要隐式对作者建模并提供用户级隐藏状态的人类语言模型（HULMS）。在这里，我们系统地评估使用不同的LM和HULM体系结构来表示文档和用户的不同方式，以预测任务成果，因为动态变化的状态和平均特质的类型用户级属性，唤醒，同情和困扰。我们发现将文档表示为代币隐藏状态的平均表现最好。此外，虽然用户级隐藏状态本身很少是最佳表示，但我们发现它包含在模型中，可以增强用于得出文档和用户级表示形式的代币或文档嵌入，从而导致最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2503.00124</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Annocaselaw：一个富裕的数据集，用于基准可解释的法律判断预测</title>
      <link>https://arxiv.org/abs/2503.00128</link>
      <description><![CDATA[ARXIV：2503.00128V1公告类型：新 
摘要：全球法律制度继续在压倒性的案件量，有限的司法资源和法律程序中日益增长的复杂性中挣扎。人工智能（AI）提供了一个有前途的解决方案，并具有法律判断预测（LJP） - 从案件事实中预测法院的判决的做法 - 成为关键研究领域。但是，现有数据集通常是不切实际地制定LJP的任务，而不是反映其真正的难度。他们还缺乏对法律推理和解释性必不可少的高质量注释。为了解决这些缺点，我们介绍了Annocaselaw，这是471个精心注释的美国上诉法院疏忽案件的首个数据集。每种情况都充满了全面，专家标记的注释，这些注释突出了司法决策的关键组成部分以及相关的法律概念。我们的数据集为更加人类，可解释的LJP模型奠定了基础。我们定义了三个与法律相关的任务：（1）判断预测； （2）概念识别； （3）自动化案例注释，并使用行业领先的大语言模型（LLM）建立绩效基线。我们的结果表明，LJP仍然是一项艰巨的任务，法律先例的应用也特别困难。代码和数据可从https://github.com/anonymonpolar1/annocaselaw获得。]]></description>
      <guid>https://arxiv.org/abs/2503.00128</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS的个性化因果图推理：饮食建议的案例研究</title>
      <link>https://arxiv.org/abs/2503.00134</link>
      <description><![CDATA[ARXIV：2503.00134V1公告类型：新 
摘要：大型语言模型（LLMS）有效地利用常识性知识来进行一般推理，但是当负责解释多因素个人数据时，它们在个性化的推理中挣扎。这种限制限制了它们在需要对个人量身定制的上下文意识决策的领域中的适用性。本文将个性化因果图推理作为一个代理框架，通过合并从个人数据数据中得出的个人因果图来增强LLM推理。这些图提供了指导LLM推理过程的基础。我们在针对营养方向的饮食建议的案例研究中对其进行了评估，该建议需要个人推理，这是由于隐含的独特饮食影响。我们提出了反事实评估，以估计LLM备受推测的食品用于葡萄糖管理的效率。结果表明，该提出的方法有效地提供了个性化的饮食建议，以减少在三个时间窗口中的平均葡萄糖IAUC，这表现优于先前的方法。 LLM-AS-A-A-A-Gudge评估结果表明，我们提出的方法在推理过程中增强了个性化。]]></description>
      <guid>https://arxiv.org/abs/2503.00134</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>得分：大语言模型的系统一致性和鲁棒性评估</title>
      <link>https://arxiv.org/abs/2503.00137</link>
      <description><![CDATA[ARXIV：2503.00137V1公告类型：新 
摘要：大语言模型（LLMS）的典型评估报告每个数据集的单个度量，通常代表在精心选择的设置下模型的最佳性能。不幸的是，这种方法忽略了现实应用程序中的模型鲁棒性和可靠性。例如，在MMLU-PRO数据集上的提示简单释义会导致高达10 \％的准确性波动，同时在Agieval DataSet中重新排序答案选择会导致高达6.1 \％的准确性差异。尽管一些研究讨论了LLM鲁棒性的问题，但没有用于评估语言模型鲁棒性的统一或集中式框架。为了解决这一差距并巩固了现有的模型鲁棒性研究，我们提出得分（$ \ Mathbf {s} $ semtyatic $ \ mathbf {co} $ nsistency和$ \ MathBf {r} $ ubustness $ \ mathbf {e}得分框架通过在各种设置中的相同基准上重复测试模型来评估模型，以对其准确性和一致性进行现实估计。我们公开发布该代码，并启动LLM鲁棒性排行榜，以促进进一步的发展和研究。]]></description>
      <guid>https://arxiv.org/abs/2503.00137</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>棕榈：阿拉伯语LLM的文化包含和语言多样性的数据集</title>
      <link>https://arxiv.org/abs/2503.00151</link>
      <description><![CDATA[ARXIV：2503.00151V1公告类型：新 
摘要：随着大型语言模型（LLM）越来越多地整合到日常生活中，确保其文化敏感性和包容性至关重要。我们介绍了我们的数据集，这是一个为期一年的社区驱动项目，涵盖了所有22个阿拉伯国家。该数据集包括现代标准阿拉伯语（MSA）和方言阿拉伯语（DA）中的说明（输入，响应对），涵盖了20个不同的主题。由阿拉伯世界的44名研究人员组成的团队，他们都是本文的作者，我们的数据集提供了广泛而包容的观点。我们使用数据集评估了几种边境LLM的文化和方言能力，从而揭示了显着的局限性。例如，虽然封闭源LLM通常表现出很强的性能，但它们并非没有缺陷，而较小的开源型号则面临更大的挑战。此外，某些国家（例如，埃及，阿联酋）似乎比其他国家更好（例如伊拉克，毛里塔尼亚，也门）。我们的注释指南，代码和可重复性的数据公开可用。]]></description>
      <guid>https://arxiv.org/abs/2503.00151</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型的不确定性估计方法的调查</title>
      <link>https://arxiv.org/abs/2503.00172</link>
      <description><![CDATA[ARXIV：2503.00172V1公告类型：新 
摘要：大型语言模型（LLMS）在各种任务中都表现出了显着的功能。但是，这些模型可以提供偏见，幻觉或非事实的反应，以伪装它们的流利性和现实外观。不确定性估计是应对这一挑战的关键方法。尽管不确定性估计的研究工作正在加剧，但对LLM不确定性估计缺乏全面和专用的调查。这项调查提出了LLM不确定性估计的四个主要途径。此外，我们对多种方法和数据集进行了广泛的实验评估。最后，我们为LLM不确定性估计提供了关键和有希望的未来方向。]]></description>
      <guid>https://arxiv.org/abs/2503.00172</guid>
      <pubDate>Tue, 04 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>