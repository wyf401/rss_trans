<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>微笑背后的匕首：用幸福结局的故事愚弄法学硕士</title>
      <link>https://arxiv.org/abs/2501.13115</link>
      <description><![CDATA[arXiv:2501.13115v1 公告类型：新
摘要：大型语言模型 (LLM) 的广泛采用引起了 \textit{越狱} 攻击的极大关注，其中通过优化或手动设计制作的对抗性提示利用 LLM 生成恶意内容。然而，基于优化的攻击效率和可转移性有限，而手动设计要么容易被检测到，要么需要与 LLM 进行复杂的交互。在本文中，我们首先指出了越狱攻击的一个新视角：LLM 对 \textit{positive} 提示更敏感。基于此，我们部署了 Happy Ending Attack (HEA) 来将恶意请求包装在一个场景模板中，该场景模板主要通过 \textit{happy ending} 形成积极提示，从而欺骗 LLM 立即越狱或在后续恶意请求时越狱。这使得 HEA 既高效又有效，因为它只需要最多两个步骤就可以完全越狱 LLM。大量实验表明，我们的 HEA 可以成功越狱最先进的 LLM，包括 GPT-4o、Llama3-70b、Gemini-pro，平均攻击​​成功率达到 88.79%。我们还为 HEA 的成功提供了潜在的定量解释。]]></description>
      <guid>https://arxiv.org/abs/2501.13115</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MyGO Multiplex CoT：一种通过双思维链进行大型语言模型自我反思的方法</title>
      <link>https://arxiv.org/abs/2501.13117</link>
      <description><![CDATA[arXiv:2501.13117v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展已在各种推理和决策任务中展示了其令人印象深刻的能力。然而，推理过程的质量和连贯性仍然可以从增强的内省和自我反思中受益。在本文中，我们介绍了多重 CoT（思维链），这种方法通过启动双重思维链 (CoT) 思维，使 LLM 能够在推理时模拟一种自我审查形式。多重 CoT 利用迭代推理的力量，其中模型生成初始思维链，随后通过第二轮思维生成批评和改进这种推理。这种递归方法允许更连贯、更合乎逻辑和更强大的答案，从而改善整个决策过程。我们展示了如何在现有的 LLM 架构中使用简单的即时工程有效地实现此方法，从而实现与学习细化模型 (LRM) 类似的效果，而无需额外训练。此外，我们还提供了在 Google Colab 中实现该方法的实用指南，以便轻松集成到实际应用中。]]></description>
      <guid>https://arxiv.org/abs/2501.13117</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士设计的“不安分的土匪”奖励函数中的多语言性：对任务绩效和公平性的影响</title>
      <link>https://arxiv.org/abs/2501.13120</link>
      <description><![CDATA[arXiv:2501.13120v1 公告类型：新
摘要：多臂老虎机 (RMAB) 已成功应用于各种环境中的资源分配问题，包括公共卫生。随着强大的大型语言模型 (LLM) 的快速发展，它们越来越多地用于设计奖励函数以更好地匹配人类偏好。最近的研究表明，LLM 可用于根据语言提示根据社区需求定制自动分配决策。然而，这主要针对英语提示进行研究，并且仅关注任务绩效。这可能是一个问题，因为基层工人，特别是在印度等发展中国家的工人，更喜欢使用当地语言工作，其中一些语言资源匮乏。此外，考虑到问题的性质，用户无意的人口群体偏见也是不可取的。在这项工作中，我们研究了当 DLM 算法（最近使用 LLM 为 RMAB 设计奖励函数的研究）以非英语语言命令提示时对任务绩效和公平性的影响。具体来说，我们在合成环境中针对翻译成多种语言的各种提示运行该模型。提示本身的复杂程度各不相同。我们的结果表明，与其他语言相比，LLM 提出的奖励函数在用英语提示时明显更好。我们还发现，提示的确切措辞会影响任务性能。此外，随着提示复杂性的增加，所有语言的性能都会变差；然而，英语提示的性能比资源较少的语言更稳定。在公平性方面，我们发现资源较少的语言和更复杂的提示都很可能在意想不到的维度上造成不公平。]]></description>
      <guid>https://arxiv.org/abs/2501.13120</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的情景记忆生成和评估基准</title>
      <link>https://arxiv.org/abs/2501.13121</link>
      <description><![CDATA[arXiv:2501.13121v1 公告类型：新
摘要：情景记忆——回忆特定时间和空间事件的能力——是人类认知的基石，不仅可以实现连贯的故事讲述，还可以实现规划和决策。尽管大型语言模型 (LLM) 具有非凡的能力，但它们缺乏强大的情景记忆机制：我们认为，将情景记忆功能集成到 LLM 中对于推动 AI 向类似人类的认知发展至关重要，可以提高其一致推理的潜力并将其输出建立在现实世界的情景事件中，从而避免虚构。为了应对这一挑战，我们引入了一个全面的框架来建模和评估 LLM 情景记忆能力。从认知科学中汲取灵感，我们开发了一种结构化的方法来表示情景事件，封装时间和空间背景、涉及的实体和详细描述。我们合成了一个独特的、不受污染的情景记忆基准，并发布了开源代码和数据集，以评估 LLM 在各种回忆和情景推理任务中的表现。我们对最先进的模型（包括 GPT-4 和 Claude 变体、Llama 3.1 和 o1-mini）的评估表明，即使是最先进的 LLM 也会在情景记忆任务中遇到困难，尤其是在处理多个相关事件或复杂的时空关系时——即使在短至 10k-100k 个标记的上下文中也是如此。]]></description>
      <guid>https://arxiv.org/abs/2501.13121</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>零样本验证引导的思路链</title>
      <link>https://arxiv.org/abs/2501.13122</link>
      <description><![CDATA[arXiv:2501.13122v1 公告类型：新
摘要：先前的研究已经证明了思维链 (COT) 提示和验证器在引导大型语言模型 (LLM) 完成推理空间方面的有效性。然而，大多数此类研究要么使用经过微调的验证器，要么依赖于手工制作的少量样本。相比之下，在本文中，我们专注于通过完全零样本机制中的 COT 提示对基于 LLM 的自生成推理步骤进行自我验证。为了探索这种设置，我们设计了一个新的零样本提示，我们称之为 COT STEP，以帮助零样本分解推理步骤，并为基于 LLM 的验证器设计了两个新的零样本提示。我们评估了验证器对推理链正确性进行分类的能力，并探索了使用验证器分数指导不同 LLM 的各种数学和常识推理任务推理的不同方法。]]></description>
      <guid>https://arxiv.org/abs/2501.13122</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>辩论有助于从弱到强的概括</title>
      <link>https://arxiv.org/abs/2501.13124</link>
      <description><![CDATA[arXiv:2501.13124v1 公告类型：新
摘要：将已经具备能力的模型与所需行为对齐的常用方法依赖于人类提供监督的能力。然而，未来的超人模型将超越人类的能力。因此，人类只能对超人模型进行弱监督。这种预期的人类评估缺陷将削弱未来人工智能系统的安全性。可扩展监督和弱到强泛化是解决这一问题的两种互补方法。在本文中，我们试图结合这两种方法的优势来进一步改善对齐。具体来说，我们研究了使用强大的预训练模型改进人工监督的方法，然后使用增强的弱人工监督来监督强模型。为了取得迭代的经验进展，我们考虑了一个类比：我们可以使用强模型来改进弱模型监督，然后用它来监督强模型吗？我们通过在一个大型强模型的帮助下对一个小型弱模型在基本事实标签上进行微调，然后对由弱模型生成的标签上的强模型进行微调，从而对其进行了实证测试。我们发现辩论可以帮助弱模型从不可信的强模型中提取可信信息，这在训练弱模型时可以作为样本的背景。我们还表明，弱模型集合有助于利用强模型辩论者生成的长篇论据并获得更稳健的监督估计。在 OpenAI 弱到强 NLP 基准上进行的大量实验表明，组合方法可以实现更好的一致性，这表明辩论有可能帮助实现从弱到强的泛化。]]></description>
      <guid>https://arxiv.org/abs/2501.13124</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过学生选择预测为多项选择题生成合理的干扰项</title>
      <link>https://arxiv.org/abs/2501.13125</link>
      <description><![CDATA[arXiv:2501.13125v1 公告类型：新 
摘要：在教育设计多项选择题 (MCQ) 时，创建合理的干扰项对于识别学生的误解和知识差距以及准确评估他们的理解至关重要。然而，先前关于干扰项生成的研究并没有充分关注提高干扰项的难度，导致 MCQ 的有效性降低。本研究提出了一种训练模型以生成学生更有可能选择的干扰项的流程。首先，我们训练一个成对排序器来推理学生的误解并评估两个干扰项的相对合理性。使用该模型，我们创建了一个成对干扰项排名的数据集，然后通过直接偏好优化 (DPO) 训练干扰项生成器以生成更合理的干扰项。在计算机科学科目 (Python、DB、MLDL) 上的实验表明，我们的成对排序器可以有效识别学生的潜在误解，并实现与人类专家相当的排名准确度。此外，我们的干扰项生成器在生成合理的干扰项方面优于几个基线，并且产生具有更高项目辨别指数（DI）的问题。]]></description>
      <guid>https://arxiv.org/abs/2501.13125</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏好课程：法学硕士应始终接受其偏好数据的预训练</title>
      <link>https://arxiv.org/abs/2501.13126</link>
      <description><![CDATA[arXiv:2501.13126v1 Announce Type: new 
摘要：目前的大型语言模型（LLM）通常在整个预训练过程中使用一致的数据分布。然而，随着模型能力的提高，直观地应该使用差异化的数据进行预训练。为了实现它，我们提出了基于困惑度差异的偏好课程学习（PDPC）框架，它始终感知并使用LLM偏好的数据来训练和提升它们。首先，我们引入了PD指标来衡量强模型和弱模型对样本的拟合程度的差异。具有高PD的样本对于弱模型来说更难学习，更适合在预训练的后期进行安排。其次，我们提出了PD偏好函数来近似模型并随时预测LLM的数据偏好，从而离线完成整个数据的安排并确保不间断的持续训练。在1.3B和3B模型上的实验结果表明我们的PDPC显著超越了基线。值得注意的是，3B 模型取得了更显著的收益，在各个基准测试中平均准确率提高了 4.1% 以上。]]></description>
      <guid>https://arxiv.org/abs/2501.13126</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RAG-Reward：使用奖励建模和 RLHF 优化 RAG</title>
      <link>https://arxiv.org/abs/2501.13264</link>
      <description><![CDATA[arXiv:2501.13264v1 公告类型：新
摘要：检索增强生成 (RAG) 通过相关和最新的知识增强大型语言模型 (LLM)，从而提高其回答知识密集型问题的能力。事实证明，它可以提高生成质量和可信度。虽然许多工作都集中在改进检索、生成和评估上，但奖励模型在强化学习中用于优化 RAG 和建立自动基准测试管道的作用仍未得到充分探索。在本文中，我们介绍了 \textbf{RAG-Reward}，这是一个旨在实现 \textit{无幻觉、全面、可靠和高效的 RAG} 的数据集。我们定义了评估生成质量的四个关键指标，并开发了一个自动注释管道，该管道利用多个 LLM 在不同的 RAG 场景中生成输出。GPT-4o 用于评估和构建偏好数据。使用 \textbf{RAG-Reward}，我们训练奖励模型并应用带有人工反馈的强化学习 (RLHF) 来提高 LLM 在 RAG 中的有效性。实验结果表明，我们的奖励模型在保留的测试集上实现了最佳性能，证明了我们方法的有效性和数据集的质量。此外，训练后的策略模型的生成质量得到提高，凸显了使用 RLHF 增强 RAG 管道的可行性。]]></description>
      <guid>https://arxiv.org/abs/2501.13264</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用框架语义自动核查事实</title>
      <link>https://arxiv.org/abs/2501.13288</link>
      <description><![CDATA[arXiv:2501.13288v1 公告类型：新
摘要：我们提出了一种自动事实核查的新范式，利用框架语义来增强对声明的结构化理解，解决当今信息生态系统中错误信息带来的挑战。为了支持这种方法，我们引入了一个从 PolitiFact 中提取的真实世界声明的试点数据集，专门针对大规模结构化数据进行了注释。该数据集支持两个案例研究：第一个使用投票语义框架调查与投票相关的声明，而第二个探索来自经济合作与发展组织 (OECD) 的各种语义框架和数据源。我们的研究结果证明了框架语义在改进证据检索方面的有效性，表明自动事实核查能力取得了重大进步。最后，我们对事实核查声明中引起的框架进行了调查，确定了影响深远的框架以指导未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2501.13288</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RAMQA：检索增强多模态问答的统一框架</title>
      <link>https://arxiv.org/abs/2501.13297</link>
      <description><![CDATA[arXiv:2501.13297v1 公告类型：新
摘要：集成文本和图像的多模态检索增强问答 (MRAQA) 在信息检索 (IR) 和自然语言处理 (NLP) 中引起了广泛关注。传统的排名方法依赖于基于小型编码器的语言模型，这些模型与现代基于解码器的生成式大型语言模型 (LLM) 不兼容，后者已经推进了各种 NLP 任务。为了弥补这一差距，我们提出了 RAMQA，这是一个将学习排名方法与生成排列增强排名技术相结合的统一框架。我们首先使用 LLaVA 作为主干训练逐点多模态排名器。然后，我们应用指令调整来训练 LLaMA 模型，使用创新的自回归多任务学习方法对前 k 个文档进行重新排名。我们的生成排名模型从各种排列的文档候选中生成重新排名的文档 ID 和具体答案。在两个 MRAQA 基准 WebQA 和 MultiModalQA 上进行的实验表明，与强基线相比，我们的方法有显著的改进，凸显了我们方法的有效性。代码和数据可在以下位置获取：https://github.com/TonyBY/RAMQA]]></description>
      <guid>https://arxiv.org/abs/2501.13297</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用目标驱动和约束引导的 LLM 代理进行材料发现和设计的假设生成</title>
      <link>https://arxiv.org/abs/2501.13299</link>
      <description><![CDATA[arXiv:2501.13299v1 公告类型：新
摘要：材料发现和设计对于推动各个行业的技术进步至关重要，因为它可以促进特定应用材料的开发。最近的研究利用大型语言模型 (LLM) 来加速这一过程。我们探索了 LLM 生成可行假设的潜力，这些假设一旦得到验证，就可以加快材料发现。我们与材料科学专家合作，从最近的期刊出版物中整理出了一个新数据集，其中包含现实世界的目标、约束和设计现实世界应用程序的方法。使用此数据集，我们测试基于 LLM 的代理，这些代理会在特定约束下生成实现给定目标的假设。为了评估这些假设的相关性和质量，我们提出了一种新颖的可扩展评估指标，该指标模拟了材料科学家用来批判性地评估假设的过程。我们整理的数据集、提出的方法和评估框架旨在推动未来使用 LLM 加速材料发现和设计的研究。]]></description>
      <guid>https://arxiv.org/abs/2501.13299</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关注人工智能看门狗：人工智能安全调节分类器的公平性和稳健性分析</title>
      <link>https://arxiv.org/abs/2501.13302</link>
      <description><![CDATA[arXiv:2501.13302v1 公告类型：新
摘要：AI 安全审核 (ASM) 分类器旨在审核社交媒体平台上的内容，并充当护栏，防止大型语言模型 (LLM) 对不安全的输入进行微调。由于它们可能产生不同的影响，因此必须确保这些分类器：(1) 不会不公平地将少数群体用户的内容与多数群体用户的内容相比归类为不安全，以及 (2) 它们的行为在类似输入中保持稳健和一致。因此，在这项工作中，我们研究了四个广泛使用的闭源 ASM 分类器的公平性和稳健性：OpenAI Moderation API、Perspective API、Google Cloud Natural Language (GCNL) API 和 Clarifai API。我们使用人口统计奇偶性和条件统计奇偶性等指标来评估公平性，将它们的性能与 ASM 模型和公平基线进行比较。此外，我们通过测试分类器对小而自然的输入扰动的敏感度来分析稳健性。我们的研究结果揭示了潜在的公平性和稳健性差距，强调了在这些模型的未来版本中缓解这些问题的必要性。]]></description>
      <guid>https://arxiv.org/abs/2501.13302</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>按照我们的方式做，而不是按照你的想法做：大型语言模型的一致性</title>
      <link>https://arxiv.org/abs/2501.13381</link>
      <description><![CDATA[arXiv:2501.13381v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展彻底改变了智能代理领域，使协作多智能体系统能够解决各个领域的复杂问题。然而，这些系统中的一致性潜力（类似于人类群体动力学中的一致性偏见和群体思维等现象）仍未得到充分探索，这引发了人们对其集体解决问题的能力和可能的伦理影响的担忧。本文对 LLM 驱动的多智能体系统中的一致性进行了全面研究，重点关注三个方面：一致性的存在、影响一致性的因素以及潜在的缓解策略。特别是，我们引入了 BenchForm，这是一种新的面向一致性的基准，具有推理密集型任务和五种不同的交互协议，旨在探测 LLM 在协作场景中的行为。在 BenchForm 上评估了几个代表性的 LLM，使用一致性率和独立率等指标来量化一致性的影响。我们的分析深入研究了影响从众性的因素，包括交互时间和多数规模，并研究了主体代理如何合理化其从众行为。此外，我们探索了两种减轻从众效应的策略，即开发增强角色和实施反思机制。从实证结果和案例研究中得出了有关 LLM 从众性的几个有趣发现。我们希望这些见解可以为更强大、更符合道德规范的协作 AI 系统铺平道路。我们的基准和代码可在 BenchForm 上找到。]]></description>
      <guid>https://arxiv.org/abs/2501.13381</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型能够理解个性化推荐中的偏好吗？</title>
      <link>https://arxiv.org/abs/2501.13391</link>
      <description><![CDATA[arXiv:2501.13391v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种任务中表现出色，包括个性化推荐。现有的评估方法通常侧重于评分预测，依赖于实际评分和预测评分之间的回归误差。然而，用户评分偏差和项目质量是评分分数背后的两个影响因素，它们会掩盖用户-项目对数据中的个人偏好。为了解决这个问题，我们引入了 PerRecBench，将评估与这两个因素分离，并评估以分组排名方式捕获个人偏好的推荐技术。我们发现，通常擅长评分预测的基于 LLM 的推荐技术在通过对用户进行分组消除用户评分偏差和项目质量后，无法识别用户喜欢和不喜欢的项目。通过 PerRecBench 和 19 个 LLM，我们发现虽然较大的模型通常优于较小的模型，但它们在个性化推荐方面仍然举步维艰。我们的研究结果揭示了成对和列表排序方法优于逐点排序、PerRecBench 与传统回归指标的低相关性、用户资料的重要性以及预训练数据分布的作用。我们进一步探索了三种监督微调策略，发现合并单一格式训练的权重很有前景，但提高 LLM 对用户偏好的理解仍然是一个悬而未决的研究问题。代码和数据可在 https://github.com/TamSiuhin/PerRecBench 获得]]></description>
      <guid>https://arxiv.org/abs/2501.13391</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>