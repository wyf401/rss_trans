<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>多样性有助于突破大型语言模型</title>
      <link>https://arxiv.org/abs/2411.04223</link>
      <description><![CDATA[arXiv:2411.04223v1 公告类型：新
摘要：我们发现了一种强大的越狱技术，该技术利用大型语言模型偏离先前上下文的能力，使它们能够绕过安全约束并产生有害输出。通过简单地指示 LLM 偏离和混淆以前的攻击，我们的方法大大优于现有方法，在仅使用 13% 的查询的情况下，在入侵包括 GPT-4、Gemini 和 Llama 在内的九个领先聊天机器人方面实现了高达 62% 的成功率。这一发现暴露了当前 LLM 安全培训中的一个关键缺陷，表明现有方法可能只是掩盖了漏洞而不是消除它们。我们的发现敲响了紧急警钟，需要彻底改变测试方法，以确保强大而可靠的 LLM 安全性。]]></description>
      <guid>https://arxiv.org/abs/2411.04223</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不公平对齐：检查视觉语言模型中视觉编码器层之间的安全对齐</title>
      <link>https://arxiv.org/abs/2411.04291</link>
      <description><![CDATA[arXiv:2411.04291v1 公告类型：新
摘要：视觉语言模型 (VLM) 在多模态任务中取得了显着进步，但它们更复杂的架构使得它们的安全性对齐比大型语言模型 (LLM) 的对齐更具挑战性。在本文中，我们揭示了 VLM 视觉编码器各层之间的安全性分布不公平，与更强大的最终层相比，早期层和中间层更容易受到恶意输入的攻击。这种“跨层”漏洞源于该模型无法将其安全性训练从训练期间使用的默认架构设置推广到看不见或分布外的场景，从而使某些层暴露在外。我们通过投射来自各个中间层的激活进行全面分析，并证明这些层在暴露于恶意输入时更有可能产生有害输出。我们对 LLaVA-1.5 和 Llama 3.2 进行的实验表明，各层的攻击成功率和毒性评分存在差异，这表明当前专注于单个默认层的安全协调策略是不够的。]]></description>
      <guid>https://arxiv.org/abs/2411.04291</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究语言技术中的偏见和危害的能力方法</title>
      <link>https://arxiv.org/abs/2411.04298</link>
      <description><![CDATA[arXiv:2411.04298v1 公告类型：新
摘要：主流自然语言处理 (NLP) 研究忽略了世界上大多数语言。在从排除世界上大多数语言到盲目采用我们为英语所做的工作的过程中，我们首先冒着引入我们在英语中最多减轻和至少衡量过的相同危害的风险。然而，在评估和减轻在这种背景下采用新技术所带来的危害时，我们常常忽视 (1) 语言技术的实际社区需求，以及 (2) 社区背景下的偏见和公平问题。在这个扩展的摘要中，我们从能力方法的角度来考虑语言技术的公平性、偏见和包容性。能力方法关注的是人们在相互交织的社会、政治和经济背景下能够取得什么成就，而不是他们（理论上）可以获得什么资源。我们详细介绍了能力方法、它与多语言和多元文化评估的关系，以及该框架如何与社区成员进行有意义的合作，以定义和衡量语言技术的危害。]]></description>
      <guid>https://arxiv.org/abs/2411.04298</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提高语言模型的双语能力，支持教育领域的多样化语言实践</title>
      <link>https://arxiv.org/abs/2411.04308</link>
      <description><![CDATA[arXiv:2411.04308v1 公告类型：新
摘要：大型语言模型 (LLM) 有望生成教育内容、提供教师反馈并减少教师的评估工作量。虽然先前的研究主要集中在研究 LLM 驱动的学习分析，但有限的研究已经研究了 LLM 在双语环境中的有效性。在本文中，我们研究了多语言大型语言模型 (MLLM) 在单语（仅英语、仅西班牙语）和双语（西班牙语英语）学生写作中的有效性。我们提出了一个学习分析用例，详细说明了 LLM 在评估科学和社会科学概念的可接受和不可接受的解释方面的表现。我们的研究结果表明，与仅英语和仅西班牙语写作相比，预训练双语写作模型的评分表现存在显着偏差。在此之后，我们使用以英语、西班牙语和西班牙语英语生成的合成数据集对开源 MLLM（包括 Llama 3.1 和 Mistral NeMo）进行了微调。我们的实验表明，在使用双语数据进行微调后，模型在这三种语言上的表现明显更好。这项研究强调了提高 MLLM 有效性以支持双语学习者进行真实的语言练习的潜力。它还旨在说明将非英语语言纳入教育语言模型的设计和实施中的价值。]]></description>
      <guid>https://arxiv.org/abs/2411.04308</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型和可解释人工智能的低资源语言翻译多语言情感词典</title>
      <link>https://arxiv.org/abs/2411.04316</link>
      <description><![CDATA[arXiv:2411.04316v1 公告类型：新
摘要：南非和刚果民主共和国 (DRC) 呈现出复杂的语言环境，包括祖鲁语、塞佩迪语、南非荷兰语、法语、英语和奇卢巴语 (Ciluba) 等语言，由于缺乏准确标记的数据，这给人工智能驱动的翻译和情感分析系统带来了独特的挑战。本研究旨在通过开发专为法语和奇卢巴语设计的多语言词典来应对这些挑战，现在扩展到包括英语、南非荷兰语、塞佩迪语和祖鲁语的翻译。该词典通过整合特定于语言的情感分数来增强情感分类中的文化相关性。创建了一个全面的测试语料库来支持翻译和情感分析任务，并使用机器学习模型（例如随机森林、支持向量机 (SVM)、决策树和高斯朴素贝叶斯 (GNB)）来训练以预测低资源语言 (LRL) 中的情感。其中，随机森林模型表现尤为出色，能够有效捕捉情绪极性并处理特定语言的细微差别。此外，大型语言模型 (LLM) Transformers 的双向编码器表示 (BERT) 被用于预测基于上下文的情绪，准确率高达 99%，精确度高达 98%，优于其他模型。使用可解释人工智能 (XAI) 澄清了 BERT 预测，提高了透明度并增强了对情绪分类的信心。总体而言，研究结果表明，所提出的词典和机器学习模型显著增强了南非和刚果民主共和国 LRL 的翻译和情绪分析能力，为未来支持代表性不足的语言的人工智能模型奠定了基础，并将应用于多语言环境中的教育、治理和商业领域。]]></description>
      <guid>https://arxiv.org/abs/2411.04316</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平衡透明度和准确性：政治偏见分类中基于规则和深度学习模型的比较分析</title>
      <link>https://arxiv.org/abs/2411.04328</link>
      <description><![CDATA[arXiv:2411.04328v1 公告类型：新
摘要：数字信息的不受控制的传播，加上日益加剧的政治两极分化以及个人倾向于将自己与对立的政治观点隔离开来，促使研究人员开发自动检测媒体政治偏见的系统。社交媒体上的讨论进一步推动了这一趋势。我们探索对美国新闻文章中的偏见进行分类的方法，比较基于规则和深度学习的方法。该研究强调了现代自学系统对不受约束的数据摄取的敏感性，同时重新考虑了传统基于规则的系统的优势。将这两种模型应用于左倾（CNN）和右倾（FOX）新闻文章，我们评估了它们在原始训练和测试集之外的数据上的有效性。该分析强调了每个模型的准确性，为探索深度学习的可解释性提供了一个框架，并揭示了美国新闻媒体的政治偏见。我们将深度学习模型的不透明架构与语言规则型模型的透明度进行对比，结果表明基于规则的模型在不同数据条件下表现一致，并提供更高的透明度，而深度学习模型依赖于训练集并且难以处理看不见的数据。]]></description>
      <guid>https://arxiv.org/abs/2411.04328</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CodeTree：使用大型语言模型进行代码生成的代理引导树搜索</title>
      <link>https://arxiv.org/abs/2411.04329</link>
      <description><![CDATA[arXiv:2411.04329v1 公告类型：新
摘要：大型语言模型 (LLM) 经过大量代码和文本数据的预训练，在执行代码生成任务方面取得了显著成就。通过额外的基于执行的反馈，这些模型可以充当具有自我改进和自主改进生成代码能力的代理。然而，在具有极大搜索空间的具有挑战性的编码任务中，当前的代理方法仍然难以进行多阶段规划、生成和调试。为了解决这个问题，我们提出了 CodeTree，这是一个 LLM 代理框架，可以在代码生成过程的不同阶段有效地探索搜索空间。具体来说，我们采用统一的树结构来明确探索不同的编码策略，生成相应的编码解决方案，然后改进解决方案。在每个阶段，探索过程的关键决策（排名、终止、扩展）都由基于环境执行的反馈和 LLM 代理生成的反馈指导。我们在 7 个代码生成基准上全面评估了 CodeTree，并证明了 CodeTree 相对于强大基线的显着性能提升。使用 GPT-4o 作为基础模型，我们在 HumanEval 上始终取得最高成绩 95.1，在 MBPP 上取得 98.7，在 CodeContests 上取得 43.0。在具有挑战性的 SWEBench 基准测试中，我们的方法显著提高了性能。]]></description>
      <guid>https://arxiv.org/abs/2411.04329</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大型语言模型中测量简短形式的事实性</title>
      <link>https://arxiv.org/abs/2411.04368</link>
      <description><![CDATA[arXiv:2411.04368v1 公告类型：新
摘要：我们提出了 SimpleQA，这是一个评估语言模型回答简短、寻求事实的问题的能力的基准。在设计此评估时，我们优先考虑了两个属性。首先，SimpleQA 具有挑战性，因为它是针对 GPT-4 响应进行对抗性收集的。其次，响应很容易评分，因为问题的创建方式使得只有一个无可争议的答案。SimpleQA 中的每个答案都被评为正确、不正确或未尝试。具有理想行为的模型会尽可能多地回答正确问题，而不会尝试回答它不确定自己知道正确答案的问题。SimpleQA 是一种简单、有针对性的评估，用于评估模型是否“知道它们所知道的”，我们希望这个基准将在未来几代前沿模型中保持相关性。SimpleQA 可在 https://github.com/openai/simple-evals 找到。]]></description>
      <guid>https://arxiv.org/abs/2411.04368</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 评估器对胜率估计进行贝叶斯校准</title>
      <link>https://arxiv.org/abs/2411.04424</link>
      <description><![CDATA[arXiv:2411.04424v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展表明，使用 LLM 作为评估器来评估 LLM 文本生成质量的潜力。然而，由于 LLM 评估器固有的胜率估计偏差，天真地应用 LLM 评估器来比较或判断不同的系统可能会导致不可靠的结果。为了缓解这个问题，我们提出了两种校准方法，贝叶斯胜率抽样 (BWRS) 和贝叶斯 Dawid-Skene，这两种方法都利用贝叶斯推理来更准确地推断生成语言模型的真实胜率。我们在六个数据集上对我们的方法进行了实证验证，涵盖故事生成、总结和指令遵循任务。我们表明，我们的两种方法都能有效地提高使用 LLM 作为评估器的胜率估计的准确性，为可靠的自动文本质量评估提供了一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2411.04424</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DELIFT：数据高效语言模型指令微调</title>
      <link>https://arxiv.org/abs/2411.04425</link>
      <description><![CDATA[arXiv:2411.04425v1 公告类型：新
摘要：微调大型语言模型 (LLM) 对于提高其在特定任务上的性能至关重要，但由于数据冗余或信息量不足，通常会​​耗费大量资源。为了解决这种低效率问题，我们引入了 DELIFT（数据高效语言模型指令微调），这是一种新颖的算法，可系统地优化微调三个关键阶段的数据选择：（1）指令调整，（2）特定于任务的微调（例如，推理、问答）和（3）持续微调（例如，合并新数据版本）。与专注于单阶段优化或依赖计算密集型梯度计算的现有方法不同，DELIFT 在所有阶段都能有效运行。我们方法的核心是成对效用指标，它量化了数据样本对改善模型对其他样本的响应有多大益处，有效地衡量了相对于模型当前功能的信息价值。通过利用应用于该指标的不同子模块函数，DELIFT 选择了适用于所有微调阶段的多样化最佳子集。跨各种任务和模型规模的实验表明，DELIFT 可以在不影响性能的情况下将微调数据大小减少高达 70%，从而显著节省计算量，并在效率和功效方面均优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2411.04425</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一条鱼，两条鱼，但不是整个海洋：对齐减少了语言模型的概念多样性</title>
      <link>https://arxiv.org/abs/2411.04427</link>
      <description><![CDATA[arXiv:2411.04427v1 公告类型：新
摘要：社会科学和心理学的研究人员最近提出使用大型语言模型 (LLM) 代替人类进行行为研究。除了关于 LLM 是否准确捕捉人口水平模式的争论之外，这还引发了关于 LLM 是否捕捉到类似人类的概念多样性的问题。另外，人们争论的是训练后对齐 (RLHF 或 RLAIF) 是否会影响模型的内部多样性。受人类研究的启发，我们使用一种新方法来衡量合成生成的 LLM“种群”的概念多样性，即将模拟个体的内部变异性与种群水平的变异性联系起来。我们使用这种方法在具有丰富人类行为数据的两个领域评估非对齐和对齐的 LLM。虽然没有模型达到类似人类的多样性，但对齐模型通常比其指令微调模型表现出更少的多样性。我们的研究结果强调了增加模型的价值对齐和减少其概念表示多样性之间的潜在权衡。]]></description>
      <guid>https://arxiv.org/abs/2411.04427</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ACCIO：通过聚合对比学习增强表格理解</title>
      <link>https://arxiv.org/abs/2411.04443</link>
      <description><![CDATA[arXiv:2411.04443v1 公告类型：新
摘要：使用最近的自然语言模型对表格理解的关注度不断增长。然而，大多数相关工作往往侧重于直接学习表格的结构。正如人类通过比较句子来提高对句子的理解一样，他们也可以通过比较表格来增强理解。基于这个想法，在本文中，我们介绍了 ACCIO，即通过聚合对比学习增强的表格理解，这是一种通过对比学习将原始表格与其枢轴摘要进行对比来增强表格理解的新方法。ACCIO 训练编码器以使这些表格对更紧密地结合在一起。通过列类型注释的验证，ACCIO 与最先进的方法相比，实现了具有竞争力的性能，宏 F1 得分为 91.1。这项工作代表了首次尝试利用表格对进行表格嵌入，有望在表格理解方面取得重大进展。我们的代码可在 https://github.com/whnhch/ACCIO/ 获得。]]></description>
      <guid>https://arxiv.org/abs/2411.04443</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度定位改善语言模型的终身预训练</title>
      <link>https://arxiv.org/abs/2411.04448</link>
      <description><![CDATA[arXiv:2411.04448v1 公告类型：新
摘要：在网络规模文本语料库上训练的大型语言模型 (LLM) 已被证明可以在其参数中捕获世界知识。然而，语言模型存储不同类型知识的机制尚不清楚。在这项工作中，我们研究了两种与时间敏感实体相关的知识，并证明每种类型都局限于 LLM 内的不同参数集。我们假设，现有的持续学习方法缺乏对知识局部性的考虑，这导致了以下两种情况：无法吸收新信息，以及灾难性地遗忘了以前学到的信息。我们观察到，包含对更新和新提及实体的引用的序列在层子集中表现出更大的梯度范数。我们证明，针对这些相关层的参数更新可以提高对包含时间漂移的语言进行持续预训练的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.04448</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ML-Promise：用于企业承诺验证的多语言数据集</title>
      <link>https://arxiv.org/abs/2411.04473</link>
      <description><![CDATA[arXiv:2411.04473v1 公告类型：新
摘要：政客、企业领导人和公众人物的承诺对公众的看法、信任和机构声誉有重大影响。然而，此类承诺的复杂性和数量，加上核实其履行情况的困难，需要创新方法来评估其可信度。本文介绍了承诺验证的概念，这是一种系统的方法，涉及承诺识别、证据评估和验证时间评估等步骤。我们提出了第一个多语言数据集 ML-Promise，其中包括英语、法语、中文、日语和韩语，旨在促进对承诺的深入验证，特别是在环境、社会和治理 (ESG) 报告的背景下。鉴于企业对环境的贡献越来越受到重视，该数据集解决了评估企业承诺的挑战，尤其是考虑到漂绿等做法。我们的研究结果还探索了基于文本和图像的基线，检索增强生成 (RAG) 方法取得了令人鼓舞的结果。这项工作旨在促进跨多种语言和领域的公共承诺问责制的进一步讨论。]]></description>
      <guid>https://arxiv.org/abs/2411.04473</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Thanos：通过融合心智技能的大型语言模型增强对话代理</title>
      <link>https://arxiv.org/abs/2411.04496</link>
      <description><![CDATA[arXiv:2411.04496v1 公告类型：新
摘要：为了增加与对话者的社会联系，人类自然而然地获得了在特定情况下做出适当回应的能力，即通过考虑哪种对话技巧最适合回应 - 我们称之为心智技能的过程。对于基于大型语言模型 (LLM) 的对话代理，由于社交对话的复杂性，尤其是在交互式场景中，像人类一样规划适当的对话技巧具有挑战性。为了解决这个问题，我们提出了一个带有心智技能注释的对话数据集，名为多面心智技能，其中包括各种交互场景（例如，长期、咨询、面向任务）的多轮和多面对话技巧，以不同的社会背景（例如，人口统计、角色、经验法则）为基础。该数据集包含大约 100K 次对话。利用此数据集，我们引入了一个名为 Thanos 的新型 LLM 系列，该系列融合了思维技能，其模型大小为 1B、3B 和 8B 个参数。通过大量实验，这些模型成功展示了思维技能过程，并在推断各个领域的多方面技能方面表现出很强的通用性。此外，我们表明 Thanos 显著提高了基于 LLM 的对话代理生成的响应质量，并在人类评估中促进了亲社会行为。]]></description>
      <guid>https://arxiv.org/abs/2411.04496</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>