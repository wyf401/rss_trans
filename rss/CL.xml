<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 07 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>摄取并接地：使用 RAG 消除持续预训练的 LLM 中的幻觉</title>
      <link>https://arxiv.org/abs/2410.02825</link>
      <description><![CDATA[arXiv:2410.02825v1 公告类型：新
摘要：本文介绍了使用 LLM 和 RAG 提高隐私处理效率的新方法。为了减少幻觉，我们不断使用隐私特定的知识库对基础 LLM 模型进行预训练，然后使用语义 RAG 层对其进行扩充。我们的评估表明，这种方法通过用事实信息作为响应的基础，从而减少了不准确性，提高了模型在处理隐私相关查询时的性能（与开箱即用的 LLM 相比，指标提高了一倍）。]]></description>
      <guid>https://arxiv.org/abs/2410.02825</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>立场：法学硕士 (LLM) 忘却学习基准是衡量进步的薄弱指标</title>
      <link>https://arxiv.org/abs/2410.02879</link>
      <description><![CDATA[arXiv:2410.02879v1 公告类型：新
摘要：通过事后删除敏感或有害信息，取消学习方法有可能提高大型语言模型 (LLM) 的隐私和安全性。LLM 取消学习研究界越来越多地转向实证基准来评估此类方法的有效性。在本文中，我们发现现有基准对候选取消学习方法的有效性提供了过于乐观且可能具有误导性的看法。通过对许多流行基准进行简单、良性的修改，我们揭示了一些实例，其中所谓的取消学习信息仍然可以访问，或者取消学习过程使模型在保留信息上的性能下降到比原始基准所指示的程度更大的程度。我们发现现有基准特别容易受到修改的影响，这些修改会在遗忘和保留信息之间引入松散的依赖关系。此外，我们表明，现有基准中取消学习目标的模糊性很容易导致设计出过度拟合给定测试查询的方法。根据我们的研究结果，我们敦促社区在将基准结果解释为可靠的进度衡量标准时要谨慎，并提供一些建议来指导未来的 LLM 忘记学习研究。]]></description>
      <guid>https://arxiv.org/abs/2410.02879</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>艺术灵感的计算建模：利用语言和风格特征预测抒情诗审美偏好的框架</title>
      <link>https://arxiv.org/abs/2410.02881</link>
      <description><![CDATA[arXiv:2410.02881v1 公告类型：新
摘要：艺术灵感仍然是创作过程中最不为人理解的方面之一。它在创作与观众产生深刻共鸣的作品方面起着至关重要的作用，但激发灵感的审美刺激的复杂性和不可预测性一直未能得到系统研究。这项工作提出了一个新颖的框架，通过关键的语言和风格属性，以计算方式建模不同个体的艺术偏好，重点关注抒情内容。除了该框架之外，我们还引入了 \textit{EvocativeLines}，这是一个带注释的歌词数据集，分为“鼓舞人心”或“不鼓舞人心”，以方便在不同的偏好概况中评估我们的框架。我们的计算模型利用所提出的语言和诗意特征，并在其上应用校准网络，以准确预测不同创作个体的艺术偏好。我们的实验表明，我们的框架比现成的 LLaMA-3-70b（一种最先进的开源语言模型）高出近 18 分。总体而言，这项工作贡献了一个可解释且灵活的框架，可以适应分析任何类型的艺术偏好，这些偏好本质上是主观的，涵盖广泛的技能水平。]]></description>
      <guid>https://arxiv.org/abs/2410.02881</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FactCheckmate：提前检测并缓解 LM 中的幻觉</title>
      <link>https://arxiv.org/abs/2410.02899</link>
      <description><![CDATA[arXiv:2410.02899v1 公告类型：新摘要：语言模型 (LM) 会产生幻觉。我们询问：我们能否在幻觉发生之前检测并缓解它？这项工作对这个研究问题给出了肯定的回答，表明 LM 的内部表示提供了可用于此目的的丰富信号。我们引入了 FactCheckMate，它通过学习一个分类器来预先检测幻觉，该分类器基于模型在输入上产生的隐藏状态，在解码开始之前预测 LM 是否会产生幻觉。如果检测到幻觉，FactCheckMate 就会进行干预，通过调整 LM 的隐藏状态，使模型产生更多的事实输出。FactCheckMate 提供了新的见解，即 LM 的内部工作原理可以通过其隐藏状态揭示。实际上，FactCheckMate 中的检测和缓解模型都是轻量级的，几乎不会增加推理开销；与许多事后替代方案相比，FactCheckMate 是一种更有效的缓解幻觉的方法。我们评估了不同规模和模型系列（包括 Llama、Mistral 和 Gemma）的 LM 的 FactCheckMate，这些 LM 涉及来自不同领域的各种 QA 数据集。我们的结果证明了利用内部表示进行早期幻觉检测和缓解的有效性，实现了超过 70% 的先发制人检测准确率。平均而言，与没有干预的 LM 相比，有干预的 LM 生成的输出比没有干预的 LM 生成的输出更符合事实 34.4%。FactCheckMate 引入的推理时间平均开销差异约为 3.16 秒。]]></description>
      <guid>https://arxiv.org/abs/2410.02899</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过最小化贝叶斯风险实现更好的指令遵循</title>
      <link>https://arxiv.org/abs/2410.02902</link>
      <description><![CDATA[arXiv:2410.02902v1 公告类型：新
摘要：能够进行人类水平评估的通用 LLM 判断器不仅提供了一种可扩展且准确的评估指令跟随 LLM 的方法，而且还提供了监督和提高其性能的新途径。利用 LLM 判断器进行监督的一种有前途的方法是通过最小贝叶斯风险 (MBR) 解码，它使用基于参考的评估器从一组候选输出中选择高质量输出。在这项工作的第一部分，我们探索使用 MBR 解码作为一种提高指令跟随 LLM 测试时间性能的方法。我们发现，在 AlpacaEval 和 MT-Bench 上，使用基于参考的 LLM 判断器的 MBR 解码比贪婪解码、使用无参考判断器的 N 中最佳解码和使用基于词汇和嵌入的指标的 MBR 解码有显着改进。这些增益在具有多达 70B 个参数的 LLM 中是一致的，表明较小的 LLM 判断器可用于监督更大的 LLM。然后，为了保留 MBR 解码带来的改进，同时减少额外的测试时间成本，我们探索了对 MBR 解码输出进行迭代自训练。我们发现使用直接偏好优化进行自训练可显著提高性能，因此使用贪婪解码的自训练模型通常与使用 MBR 解码的基本模型的性能相当，有时甚至超过它们的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.02902</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NNetscape Navigator：无需演示器的 Web 代理的复杂演示</title>
      <link>https://arxiv.org/abs/2410.02907</link>
      <description><![CDATA[arXiv:2410.02907v1 公告类型：新
摘要：我们介绍了 NNetscape Navigator (NNetnav)，这是一种完全通过合成演示来训练 Web 代理的方法。这些演示是通过首先与浏览器交互来收集的，以生成轨迹展开，然后使用语言模型将其追溯标记为指令。大多数关于训练浏览器代理的工作都依赖于昂贵的人工监督，而之前对这种交互优先的合成数据技术的有限研究未能在指数探索空间中提供有效的搜索。相比之下，NNetnav 利用语言指令的层次结构使这种搜索更易于处理：复杂的指令通常可以分解为更简单的子任务，当中间轨迹无法用有意义的子任务注释时，允许 NNetnav 自动修剪交互事件。我们使用语言模型中的 NNetnav 演示对较小的语言模型策略进行监督微调，发现在 WebArena 上改进了 6 点，在 MiniWoB++ 上改进了 20 点以上，这两个环境都是 Web 代理的流行环境。值得注意的是，在 WebArena 上，我们观察到，当使用来自同一语言模型的 NNetnav 演示进行微调时，语言模型策略可以得到进一步增强。最后，我们在 WebArena 上收集并发布了一个包含超过 6000 个 NNetnav 演示的数据集，涵盖了一组多样化且复杂的指令。]]></description>
      <guid>https://arxiv.org/abs/2410.02907</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于长文档分类的双向信息传播图树融合模型</title>
      <link>https://arxiv.org/abs/2410.02930</link>
      <description><![CDATA[arXiv:2410.02930v1 公告类型：新
摘要：长文档分类由于其广泛的内容和复杂的结构，在捕获局部和全局依赖关系方面都面临挑战。现有方法通常会遇到标记限制问题，并且无法充分模拟文档内的层次关系。为了解决这些限制，我们提出了一种利用图树结构的新模型。我们的方法集成了用于句子编码的语法树和用于文档编码的文档图，它们分别捕获细粒度的句法关系和更广泛的文档上下文。我们使用树形转换器来生成句子编码，而图形注意网络则模拟句子间和句子内的依赖关系。在训练期间，我们实现了从单词到句子到文档的双向信息传播，反之亦然，从而丰富了上下文表示。我们提出的方法能够全面理解所有层次结构的内容，并有效地处理任意长的上下文，而不受标记限制的约束。实验结果证明了我们的方法在所有类型的长文档分类任务中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.02930</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 LLM 的工具链进行可视化编辑：一种适用于实时应用的有效提炼方法</title>
      <link>https://arxiv.org/abs/2410.02952</link>
      <description><![CDATA[arXiv:2410.02952v1 公告类型：新
摘要：我们提出了一种实用的蒸馏方法来微调 LLM，以便在实时应用程序中调用工具。我们专注于视觉编辑任务；具体来说，我们通过解释以自然语言（“黄金时间”）指定的用户风格请求来修改图像和视频，使用 LLM 选择合适的工具及其参数来实现所需的视觉效果。我们发现 GPT-3.5-Turbo 等专有 LLM 在该任务中显示出潜力，但它们的高成本和延迟使它们不适合实时应用。在我们的方法中，我们在（较大的）教师 LLM 和行为信号的指导下对（较小的）学生 LLM 进行微调。我们引入了离线指标来评估学生 LLM。在线和离线实验都表明，我们的学生模型能够与我们的教师模型（GPT-3.5-Turbo）的性能相匹配，从而显着降低了成本和延迟。最后，我们表明，使用增强技术，在低数据环境下微调效果提高了 25％。]]></description>
      <guid>https://arxiv.org/abs/2410.02952</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过认知提示解锁语言模型中的结构化思维</title>
      <link>https://arxiv.org/abs/2410.02953</link>
      <description><![CDATA[arXiv:2410.02953v1 公告类型：新
摘要：我们提出一种新方法，即通过结构化的、类似人类的认知操作（例如目标澄清、分解、过滤、抽象和模式识别）指导大型语言模型 (LLM) 中的问题解决。通过采用系统的、循序渐进的推理，认知提示使 LLM 能够有效地处理复杂的多步骤任务。我们评估了认知提示对 Meta 的 LLaMA 模型的有效性，使用 GSM8K 数据集和常识推理基准比较了算术推理任务的性能。我们的分析包括没有认知提示的模型、具有静态认知操作序列的模型和使用反射性认知提示的模型之间的比较，其中 LLM 动态地自我选择认知操作序列。结果表明，认知提示（尤其是在动态调整时）可显著提高大型模型（如 LLaMA3.1 70B）的性能，并增强其处理多步骤推理任务的能力。这种方法还提高了可解释性和灵活性，凸显了认知提示是通用 AI 推理的一种有前途的策略。]]></description>
      <guid>https://arxiv.org/abs/2410.02953</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>煤矿开采法学硕士问答</title>
      <link>https://arxiv.org/abs/2410.02959</link>
      <description><![CDATA[arXiv:2410.02959v1 公告类型：新 
摘要：在本文中，我们提出了一种使用大型语言模型 (LLM) 结合定制的提示工程技术进行煤炭开采问答 (QA) 的新方法。煤炭开采是一个复杂、高风险的行业，准确、情境感知的信息对于安全高效的运营至关重要。当前的 QA 系统难以处理与采矿相关的查询的技术和动态性质。为了应对这些挑战，我们提出了一个多轮提示工程框架，旨在指导 LLM（例如 GPT-4）以更高的精度和相关性回答煤炭开采问题。通过将复杂的查询分解为结构化组件，我们的方法使 LLM 能够更有效地处理细微的技术信息。我们从现实世界的采矿场景中手动整理了 500 个问题的数据集，并使用准确度 (ACC) 和基于 GPT-4 的评分指标评估了系统的性能。通过比较 ChatGPT、Claude2 和 GPT-4 的基线、思路链 (CoT) 和多轮提示方法，实验表明，我们的方法显著提高了准确率和上下文相关性，平均准确率提高了 15-18%，GPT-4 得分也显著提高。结果表明，我们的提示工程方法为煤矿开采等高风险环境中特定领域的问答提供了一种强大且适应性强的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2410.02959</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transformers 可以学习 $n$-gram 语言模型吗？</title>
      <link>https://arxiv.org/abs/2410.03001</link>
      <description><![CDATA[arXiv:2410.03001v1 公告类型：新
摘要：许多理论工作描述了 Transformer 表示形式语言的能力。然而，由于架构、学习算法和训练数据之间的复杂相互作用，将理论结果与经验性能联系起来并不简单。为了测试理论下限是否意味着形式语言的 \emph{可学习性}，我们转向最近将 Transformer 与 $n$-gram 语言模型 (LM) 联系起来的工作。我们研究了 Transformer 学习两种随机 $n$-gram 语言模型的能力：一种具有任意下一个符号概率，另一种是用共享参数定义的。我们发现，经典的 $n$-gram 语言模型估计技术（例如 add-$\lambda$ 平滑）在前者上优于 Transformer，而 Transformer 在后者上表现更好，优于专门设计用于学习 $n$-gram 语言模型的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.03001</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Tutor CoPilot：扩展实时专业知识的人机结合方法</title>
      <link>https://arxiv.org/abs/2410.03017</link>
      <description><![CDATA[arXiv:2410.03017v1 公告类型：新
摘要：生成式人工智能，尤其是语言模型 (LM)，有可能改变现实世界领域并产生社会影响，特别是在专家资源有限的情况下。例如，在教育领域，为新手教育者提供专家指导对于提高效率很重要，但成本高昂，为大规模提高教育质量设置了重大障碍。这一挑战对来自服务不足社区的学生造成了不成比例的伤害，而他们本可以从高质量的教育中获益最多。我们推出了 Tutor CoPilot，这是一种新颖的人机结合方法，它利用专家思维模型为辅导老师提供专家般的指导。这项研究是人机结合系统在现场辅导中的首次随机对照试验，涉及来自历史上服务不足社区的 900 名辅导老师和 1,800 名 K-12 学生。根据预先注册的分析计划，我们发现，与能够使用 Tutor CoPilot 的导师合作的学生掌握主题的可能性高出 4 个百分点 (p.p.) (p&lt;0.01)。值得注意的是，评分较低的导师的学生受益最大，掌握程度提高了 9 个百分点。我们发现 Tutor CoPilot 每位导师每年只需 20 美元。我们使用分类器分析了 550,000 多条消息以确定教学策略，发现能够使用 Tutor CoPilot 的导师更有可能使用高质量策略来促进学生理解（例如，提出引导性问题），而不太可能将答案透露给学生。导师访谈强调了 Tutor CoPilot 的指导如何帮助导师满足学生的需求，尽管他们标记了 Tutor CoPilot 中的问题，例如生成的建议不适合年级水平。总而言之，我们对 Tutor CoPilot 的研究表明了人机交互系统如何扩展现实世界领域的专业知识、弥补技能差距并创造一个所有学生都能接受高质量教育的未来。]]></description>
      <guid>https://arxiv.org/abs/2410.03017</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你的论文正在接受法学硕士的审阅吗？调查同行评审中的 AI 文本可检测性</title>
      <link>https://arxiv.org/abs/2410.03019</link>
      <description><![CDATA[arXiv:2410.03019v1 公告类型：新
摘要：同行评审是确保已发表科学研究完整性的关键过程。对这一过程的信心基于这样的假设：相关领域的专家会仔细考虑提交出版的手稿的优点。随着大型语言模型 (LLM) 语言能力的快速发展，同行评审过程面临的一个新潜在风险是，疏忽大意的审阅者将依赖 LLM 来执行通常耗时的论文审阅过程。在本研究中，我们调查了现有 AI 文本检测算法区分人类撰写的同行评审和不同最先进的 LLM 的能力。我们的分析表明，现有方法无法识别许多 GPT-4o 书面评论，同时也会产生大量的假阳性分类。为了解决这一缺陷，我们提出了一种新的检测方法，该方法在低水平假阳性分类下识别 GPT-4o 书面同行评审方面超越了现有方法。我们的工作揭示了在单个评论层面准确识别人工智能生成的文本的难度，凸显了迫切需要新的工具和方法来检测这种不道德的生成人工智能应用。]]></description>
      <guid>https://arxiv.org/abs/2410.03019</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>描述摘要中的情境影响和幻觉</title>
      <link>https://arxiv.org/abs/2410.03026</link>
      <description><![CDATA[arXiv:2410.03026v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 在许多下游任务中取得了显著的表现，但它们的普遍性也引发了两个重大担忧。一是 LLM 可能会通过生成与相关上下文信息相矛盾的内容而产生幻觉；二是 LLM 可能会由于输入反刍而无意中泄露私人信息。许多先前的研究已经对每个问题进行了广泛的独立研究，但没有一项研究同时对它们进行研究。此外，在注重隐私的开放式生成过程中，对提供的上下文的影响进行审核的研究不足。为此，我们全面描述了上下文信息在摘要过程中的影响和幻觉。我们引入了上下文影响和上下文影响解码 (CID) 的定义，然后我们表明，放大上下文（通过分解先验知识）和上下文相对于先验知识的分布不均会增加上下文对 LLM 的影响。此外，我们表明，上下文影响给出了 CID 的私人信息泄露的下限。我们通过实验评估证实了我们的分析结果，实验表明，与常规解码相比，将 LLaMA 3 的 CNN-DM 上的 F1 ROGUE-L 得分提高 $\textbf{10}$% 也会导致上下文影响增加 $\textbf{1.5x}$。此外，我们通过实证评估了上下文影响和幻觉如何受到 (1) 模型容量、(2) 上下文大小、(3) 当前响应的长度和 (4) 上下文的不同标记 $n$-gram 的影响。我们的代码可以在此处访问：https://github.com/james-flemings/context_influence。]]></description>
      <guid>https://arxiv.org/abs/2410.03026</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解开神经语音表征的文本和声学特征</title>
      <link>https://arxiv.org/abs/2410.03037</link>
      <description><![CDATA[arXiv:2410.03037v1 公告类型：新
摘要：神经语音模型构建深度纠缠的内部表示，以分布式编码捕获各种特征（例如，单词的基频、响度、句法类别或语义内容）。这种复杂性使得很难跟踪此类表示对文本和声学信息的依赖程度，或者难以抑制可能在关键的现实应用中造成隐私风险（例如，性别或说话者身份）的声学特征的编码。在本文中，我们基于信息瓶颈原理提出了一个解缠框架，将复杂的语音表示分为两个不同的部分：一个编码内容（即可转录为文本的内容），另一个编码与给定下游任务相关的声学特征。我们将我们的框架应用于情绪识别和说话者识别下游任务并进行评估，量化每个模型层的文本和声学特征的贡献。此外，我们探索了解缠框架作为一种归因方法的应用，从文本和声学角度识别最突出的语音框架表示。]]></description>
      <guid>https://arxiv.org/abs/2410.03037</guid>
      <pubDate>Mon, 07 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>