<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 04 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有紧凑且一致的下一个标记分布的语言模型的有效训练</title>
      <link>https://arxiv.org/abs/2407.02819</link>
      <description><![CDATA[arXiv:2407.02819v1 公告类型：新
摘要：最大化下一个标记的可能性是预训练语言模型的既定、统计上合理的目标。在本文中，我们表明，我们可以通过使用折叠的 $n$-gram 分布预聚合语料库来更快地训练更好的模型。先前的研究提出了语料库级 $n$-gram 统计数据作为正则化器；然而，如果简单地构建和查询此类 $n$-gram，则会证明成本高昂且会严重阻碍训练速度，从而限制它们在现代大型语言模型预训练中的应用。
我们引入了下一个标记分布的另一种紧凑表示，期望它与完整的 $n$-gram 分布一致，同时与标准下一个标记损失相比，显着降低了小批量之间的方差。从经验上讲，我们证明与现有方法相比，$n$-gram 正则化模型和我们的近似值都可以显着提高模型质量和收敛速度。此外，与简单的 $n$-gram 正则化方法相比，我们的近似有助于将收益扩展到更大的数据集和模型。]]></description>
      <guid>https://arxiv.org/abs/2407.02819</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>研究负责上下文和时间语义变化的语境化词嵌入维度</title>
      <link>https://arxiv.org/abs/2407.02820</link>
      <description><![CDATA[arXiv:2407.02820v1 公告类型：新
摘要：单词的含义会随着时间的推移和在不同语境中发生变化。感知语境化词嵌入 (SCWE)（例如 XL-LEXEME 通过在 Word-in-Context (WiC) 数据上微调掩码语言模型 (MLM) 生成的词嵌入）试图在语境化词嵌入 (CWE) 空间中对单词的这种语义变化进行编码。尽管 SCWE 在上下文/时间语义变化检测 (SCD) 基准测试中表现优异，但含义变化在嵌入空间中是如何编码的仍不清楚。为了研究这一点，我们在主成分分析 (PCA) 和独立成分分析 (ICA) 变换下比较了预训练的 CWE 及其在上下文和时间语义变化基准测试中的微调版本。我们的实验结果揭示了一些新颖的见解，例如 (a) 尽管在预训练的 CWE 空间中，负责单词语义变化的轴数量较少，但经过微调后，这些信息会分布在所有维度上；(b) 与之前研究 CWE 几何形状的工作相比，我们发现 PCA 比 ICA 更能表示语义变化。源代码可在 https://github.com/LivNLP/svp-dims 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.02820</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>MLKD-BERT：预训练语言模型的多级知识提炼</title>
      <link>https://arxiv.org/abs/2407.02775</link>
      <description><![CDATA[arXiv:2407.02775v1 公告类型：新
摘要：知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型 BERT 表现良好，但它们可以在两个方面进一步改进：可以进一步探索关系级知识以提高模型性能；学生注意力头数量的设置可以更灵活，以减少推理时间。因此，我们提出一种新颖的知识蒸馏方法 MLKD-BERT，以在师生框架中蒸馏多层知识。在 GLUE 基准和提取式问答任务上进行的大量实验表明，我们的方法优于 BERT 上最先进的知识蒸馏方法。此外，MLKD-BERT 可以灵活地设置学生注意力头数量，从而大幅减少推理时间，而性能几乎不会下降。]]></description>
      <guid>https://arxiv.org/abs/2407.02775</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>具有密度映射的量子有限态语言框架</title>
      <link>https://arxiv.org/abs/2407.02776</link>
      <description><![CDATA[arXiv:2407.02776v1 公告类型：新
摘要：量子有限状态自动机 (QFA) 是一种理论模型，旨在模拟具有有限内存的量子系统响应连续输入字符串的演化。我们将 QFA 的语言定义为从初始状态处理时导致 QFA 进入接受状态的字符串集。QFA 说明了量子计算如何实现比传统计算更高的效率。虽然 QFA 是最简单的量子模型之一，但由于在自动机上叠加幺正约束需要量子力学的初步知识，因此从头开始构建 QFA 仍然极具挑战性。此外，即使正确组装了 QFA，当前量子计算机的局限性也可能导致模拟结果出现波动，具体取决于组装的 QFA 如何转换为量子电路。
我们提出了一个框架，提供了一种简单直观的方法来构建 QFA 并最大限度地提高模拟精度。我们的框架依赖于两种方法：首先，它为识别特殊语言 MOD 和 EQU 的基础 QFA 类型提供了预定义构造。它们充当更复杂 QFA 的基本构建块。换句话说，人们可以使用标准语言操作从这些基础自动机中获得更复杂的 QFA。其次，我们通过将这些 QFA 转换为量子电路来提高模拟精度，从而使生成的电路在嘈杂的量子计算机上表现良好。
我们的框架可在 https://github.com/sybaik1/qfa-toolkit 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.02776</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>52B 到 1T：通过 Tele-FLM 系列学到的经验教训</title>
      <link>https://arxiv.org/abs/2407.02783</link>
      <description><![CDATA[arXiv:2407.02783v1 公告类型：新
摘要：大型语言模型 (LLM) 代表着向通用人工智能迈出的重大一步。由于缩放定律强调了增加模型大小的潜力，学术界已经加强了对容量超过 500 亿个参数的 LLM 的研究。本技术报告以我们之前对 Tele-FLM（也称为 FLM-2）的研究为基础，这是一个公开的 520 亿个参数模型。我们深入研究了两个主要领域：首先，我们讨论对 Tele-FLM-52B 上的监督微调 (SFT) 的观察，它支持 SFT 数据构建的“少即是多”方法；其次，我们展示了我们的实验和分析，这些实验和分析是关于逐步将模型从 520 亿个参数增加到 1020 亿个参数，随后增加到 1 万亿个参数的最佳实践。我们将开源一个 1T 模型检查点，即 Tele-FLM-1T，以推进进一步的培训和研究。]]></description>
      <guid>https://arxiv.org/abs/2407.02783</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>学习减少：提高结构化数据上大型语言模型的性能</title>
      <link>https://arxiv.org/abs/2407.02750</link>
      <description><![CDATA[arXiv:2407.02750v1 公告类型：新 
摘要：大型语言模型 (LLM) 在广泛的下游任务中都取得了出色的表现，但现有研究表明，对结构化数据进行推理对 LLM 来说具有挑战性。这是因为 LLM 需要理解长结构化数据或在推理之前选择最相关的证据，而这两种方法都并非易事。本文提出了一个框架，即 Learning to Reduce，它使用 On-Policy Learning 对语言模型进行微调，以生成输入结构化数据的简化版本。与 GPT-4 等最先进的 LLM 相比，Learning to Reduce 不仅在减少输入方面取得了出色的表现，而且在不同数据集上表现出色。我们进一步表明，使用我们的框架微调的模型有助于 LLM 在表格 QA 任务上表现更好，尤其是在上下文较长的情况下。]]></description>
      <guid>https://arxiv.org/abs/2407.02750</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>多模态对话中的情感与意图联合理解：基准数据集</title>
      <link>https://arxiv.org/abs/2407.02751</link>
      <description><![CDATA[arXiv:2407.02751v1 公告类型：新
摘要：多模态对话中的情感和意图联合理解 (MC-EIU) 旨在解码多模态对话历史中体现的语义信息，同时推断当前话语的情感和意图。MC-EIU 是许多人机界面的支持技术。然而，在注释、模态、语言多样性和可访问性方面缺乏可用的数据集。在这项工作中，我们提出了一个 MC-EIU 数据集，它具有 7 种情感类别、9 种意图类别、3 种模态（即文本、声学和视觉内容）和两种语言（即英语和普通话）。此外，它是完全开源的，可以免费访问。据我们所知，MC-EIU 是第一个全面而丰富的多模态对话情感和意图联合理解数据集。随着数据集的发布，我们还通过对多模态对话中情感和意图之间的深度关联进行建模，开发了一个情感和意图交互 (EI$^2$) 网络作为参考系统。通过比较实验和消融研究，我们证明了所提出的 EI$^2$ 方法在 MC-EIU 数据集上的有效性。数据集和代码将在以下网址提供：https://github.com/MC-EIU/MC-EIU。]]></description>
      <guid>https://arxiv.org/abs/2407.02751</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>通过基于规则的数据增强来促进生物医学概念提取</title>
      <link>https://arxiv.org/abs/2407.02719</link>
      <description><![CDATA[arXiv:2407.02719v1 公告类型：新
摘要：文档级生物医学概念提取是识别给定文档中提到的生物医学概念的任务。最近的进展已经为这项任务调整了预训练语言模型。然而，领域特定数据的稀缺和概念与其规范名称的偏差往往会阻碍这些模型的有效性。为了解决这个问题，我们使用现有的基于规则的概念映射系统 MetaMapLite 从 PubMed 和 PMC 生成额外的伪注释数据。注释数据用于增强有限的训练数据。通过大量实验，本研究证明了手工制作的概念映射工具在训练更好的概念提取模型方面的实用性。]]></description>
      <guid>https://arxiv.org/abs/2407.02719</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>e-Health CSIRO 参加 2024 年“出院！”活动：使用经过微调的语言模型生成出院摘要部分</title>
      <link>https://arxiv.org/abs/2407.02723</link>
      <description><![CDATA[arXiv:2407.02723v1 公告类型：新
摘要：临床文档是临床医生日常工作的一个重要方面，通常需要大量时间。BioNLP 2024 简化出院文档共享任务（Discharge Me！）旨在通过自动生成出院摘要部分（包括简短的医院病程和出院指导）来减轻这种文档负担，这些部分通常需要耗费大量时间进行合成和手动编写。我们通过微调多个开源语言模型（LM）来完成生成任务，包括仅解码器和编码器-解码器 LM，并对输入上下文进行各种配置。我们还研究了解码算法、模型集成或合并以及模型专业化的不同设置。我们的结果表明，在目标部分之前对出院摘要的内容进行条件调节对于生成任务是有效的。此外，我们发现较小的编码器-解码器 LM 可以与通过 LoRA 微调的基于较大解码器的 LM 一样好甚至略好。我们团队 (aehrc) 的模型检查点是公开的。]]></description>
      <guid>https://arxiv.org/abs/2407.02723</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>MentalAgora：通过多智能体辩论和属性控制实现心理健康高级个性化护理的门户</title>
      <link>https://arxiv.org/abs/2407.02736</link>
      <description><![CDATA[arXiv:2407.02736v1 公告类型：新
摘要：随着全球心理健康问题的不断升级，对先进的数字支持系统的需求巨大。我们推出了 MentalAgora，这是一个新颖的框架，它采用大型语言模型，通过多个代理之间的交互得到增强，以提供量身定制的心理健康支持。该框架通过三个阶段运行：战略辩论、量身定制的咨询师创建和响应生成，可根据个人用户偏好和治疗需求动态定制响应。我们利用与心理健康专业人士共同制作的高质量评估数据集 TherapyTalk 进行实验，表明 MentalAgora 生成了符合专家标准和用户偏好增强的响应。我们的评估（包括实验和用户研究）表明，MentalAgora 符合专业标准并有效满足用户偏好，为数字心理健康干预树立了新的标杆。]]></description>
      <guid>https://arxiv.org/abs/2407.02736</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>尼日利亚电影业：我们去看电影吧！</title>
      <link>https://arxiv.org/abs/2407.02631</link>
      <description><![CDATA[arXiv:2407.02631v1 公告类型：新
摘要：Nollywood 是基于印度宝莱坞的理念，是一系列源自尼日利亚的优秀电影。不幸的是，虽然这些电影是英文的，但由于讲英语的方言，许多母语人士很难理解。在本文中，我们实现了两个目标：（1）创建一个能够将尼日利亚英语语音翻译成美式英语的语音字幕模型；（2）使用最先进的毒性检测器来发现语音的毒性。我们的目标是突出显示这些视频中的文字，由于尼日利亚许多人在家中讲豪萨语等母语，这些文字常常因缺乏方言理解而被忽略。]]></description>
      <guid>https://arxiv.org/abs/2407.02631</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>改变我的框架：在 r/ChangeMyView 中重新构图</title>
      <link>https://arxiv.org/abs/2407.02637</link>
      <description><![CDATA[arXiv:2407.02637v1 公告类型：新
摘要：在文本风格转换的范围内，重构的最新工作迄今为止已经利用了脱离上下文、任务提示的话语来产生中性或乐观的重构。我们的工作旨在基于 subreddit r/ChangeMyView (CMV) 概括重构。我们构建了一个数据集，利用 CMV 社区的互动和惯例来识别产生观点变化的高价值、社区认可的话语。有了这些数据，我们扩大了重构方向的范围，因为观点的变化不仅发生在中性或积极的方向上。我们对基于变换器的模型进行微调，利用现代 LLM 来完善我们的数据集，并探索围绕这种重构类型的数据集创建和评估中的挑战。]]></description>
      <guid>https://arxiv.org/abs/2407.02637</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>通过知识图谱比较确保大型语言模型训练数据的负责任采购</title>
      <link>https://arxiv.org/abs/2407.02659</link>
      <description><![CDATA[arXiv:2407.02659v1 公告类型：新
摘要：鉴于最近出版商、报纸和其他版权语料库创建者对大型语言模型 (LLM) 开发人员提出的剽窃指控，我们提出了一种新系统，即剽窃检测系统的一种变体，用于评估知识源是否已用于大型语言模型的训练或微调。与当前方法不同，我们采用一种使用资源描述框架 (RDF) 三元组的方法，从源文档和该文档的 LLM 延续创建知识图。然后使用余弦相似度分析这些图的内容，并使用显示同构程度的图编辑距离的规范化版本分析这些图的结构。与专注于源语料库和目标语料库之间的内容匹配和关键字识别的传统系统不同，我们的方法通过关注思想及其组织与其他思想之间的关系，可以更广泛地评估相似性，从而更准确地比较源文档和 LLM 延续之间的相似性。此外，我们的方法不需要访问 LLM 指标（例如困惑度），这些指标在封闭的大型语言建模“黑盒”系统和训练语料库中可能无法获得。我们的系统原型可以在超链接的 GitHub 存储库中找到。]]></description>
      <guid>https://arxiv.org/abs/2407.02659</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>利用解缠技术探索语音表征学习的下一个前沿</title>
      <link>https://arxiv.org/abs/2407.02543</link>
      <description><![CDATA[arXiv:2407.02543v1 公告类型：新
摘要：流行的语音表示自监督学习框架主要集中于语音区域的帧级掩蔽预测。虽然这已显示出语音识别和相关任务的良好下游任务性能，但这在很大程度上忽略了在较粗层次上编码的语音因素，例如在整个语音表达过程中保持一致的说话者或声道的特征。在这项工作中，我们提出了一个学习解缠自监督（称为 Learn2Diss）语音表示的框架，该框架由帧级和话语级编码器模块组成。这两个编码器最初是独立学习的，其中帧级模型主要受到现有自监督技术的启发，从而学习伪音素表示，而话语级编码器受到池化嵌入的构造学习的启发，从而学习伪说话者表示。这两个模块的联合学习包括使用基于互信息的标准解缠两个编码器。通过几个下游评估实验，我们表明，提出的 Learn2Diss 在各种任务上都取得了最先进的结果，其中帧级编码器表示可以改善语义任务，而话语级表示可以改善非语义任务。]]></description>
      <guid>https://arxiv.org/abs/2407.02543</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:39 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 能说多种语言：为 LLM 解锁多语言偏好优化</title>
      <link>https://arxiv.org/abs/2407.02552</link>
      <description><![CDATA[arXiv:2407.02552v1 公告类型：新
摘要：偏好优化技术已成为训练最先进的大型语言模型 (LLM) 的标准最终阶段。然而，尽管被广泛采用，但迄今为止的绝大多数工作都集中在英语和中文等一流公民语言上。这只涵盖了世界上一小部分语言，但也不清楚当前最先进的研究的哪些方面可以转移到多语言环境中。在这项工作中，我们进行了一项详尽的研究，以在对齐多语言 LLM 方面取得新的进展。我们引入了一种新颖的可扩展方法来生成高质量的多语言反馈数据，以平衡数据覆盖范围。我们确定了跨语言迁移和增加数据集大小在偏好训练中的好处。我们的偏好训练模型在与 Aya 23 8B（其参数类别中目前最先进的多语言 LLM）的对战中取得了 54.4% 的胜率，在与 Gemma-1.1-7B-it、Llama-3-8B-Instruct、Mistral-7B-Instruct-v0.3 等广泛使用的模型的对战中取得了 69.5% 或更高的胜率。通过我们的研究，我们将对齐技术的前沿扩展到 23 种语言，覆盖了世界一半的人口。]]></description>
      <guid>https://arxiv.org/abs/2407.02552</guid>
      <pubDate>Thu, 04 Jul 2024 06:19:39 GMT</pubDate>
    </item>
    </channel>
</rss>