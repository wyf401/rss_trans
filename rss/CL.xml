<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 12 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于仇恨言论检测的分层情绪分析框架：实施二元和多类分类策略</title>
      <link>https://arxiv.org/abs/2411.05819</link>
      <description><![CDATA[arXiv:2411.05819v1 公告类型：新
摘要：在社交媒体上自动检测仇恨言论的一个重大挑战是将仇恨言论与常规和攻击性语言区分开来。这些确定了网络过滤器试图删除的重要内容类别。只有自动化方法才能管理如此大量的日常数据。为了解决这个问题，自然语言处理社区目前正在研究不同的仇恨言论检测方法。除此之外，以前的方法（例如卷积神经网络、多通道 BERT 模型和词汇检测）始终没有仔细处理情绪分析和情绪分类等其他相关任务，而取得了较低的精度。他们仍然喜欢将所有包含特定单词的消息归类为仇恨言论，因为这些术语经常与仇恨言论一起出现。在这项研究中，我们的论文提出了基于深度学习和机器学习的仇恨言论文本分类系统模型。在本文中，我们提出了一种新的多任务模型，该模型结合了共享的情感表征来检测英语中的仇恨言论。我们从 Hugging Face 和情绪分析中使用的基于 Transformer 的模型帮助我们避免了误报。结论。我们得出结论，利用情绪分析和基于 Transformer 的训练模型可以显著提高跨多个数据集的仇恨言论检测能力。]]></description>
      <guid>https://arxiv.org/abs/2411.05819</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>阿拉伯语语音识别中的方言覆盖和泛化</title>
      <link>https://arxiv.org/abs/2411.05872</link>
      <description><![CDATA[arXiv:2411.05872v1 公告类型：新
摘要：为阿拉伯语开发强大的自动语音识别 (ASR) 系统需要有效的策略来管理其复杂性，阿拉伯语是一种以丰富的方言多样性为特征的语言，通常被认为是语音技术中的低资源语言。本研究探讨了影响 ASR 性能的三个关键因素：方言覆盖在预训练中的作用、与多方言方法相比方言特定微调的有效性以及推广到看不见的方言的能力。通过对不同方言组合进行大量实验，我们的研究结果为推动阿拉伯语等多中心语言的 ASR 系统的发展提供了关键见解。]]></description>
      <guid>https://arxiv.org/abs/2411.05872</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型识别和分解膳食计划中的复合成分</title>
      <link>https://arxiv.org/abs/2411.05892</link>
      <description><![CDATA[arXiv:2411.05892v1 公告类型：新
摘要：本研究探讨了大型语言模型在膳食计划中的有效性，重点关注其识别和分解复合成分的能力。我们评估了三种模型 - GPT-4o、Llama-3 (70b) 和 Mixtral (8x7b) - 以评估它们识别和分解复杂成分组合的能力。初步结果表明，虽然 Llama-3 (70b) 和 GPT-4o 在准确分解方面表现出色，但所有模型在识别调味料和油等必需元素方面都遇到了困难。尽管整体性能强劲，但不同模型的准确性和完整性存在差异。这些发现强调了 LLM 增强个性化营养的潜力，但也强调了进一步改进成分分解的必要性。未来的研究应该解决这些局限性，以改善营养建议和健康结果。]]></description>
      <guid>https://arxiv.org/abs/2411.05892</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SSSD：简单可扩展的推测解码</title>
      <link>https://arxiv.org/abs/2411.05894</link>
      <description><![CDATA[arXiv:2411.05894v1 公告类型：新
摘要：在过去的一年里，推测解码作为一种加速大型语言模型推理的技术越来越受欢迎。虽然已经引入了几种方法，但大多数方法都难以在数据中心典型的批量大小 ($\geq 8$) 下提供令人满意的性能，而且通常涉及相当大的部署复杂性。在这项工作中，我们从理论角度解释了如何有效地利用推测解码处理更大的批量大小。我们还介绍了一种无缝集成到现有系统中的方法，无需额外的培训或部署小型 LLM 的复杂性。在连续批处理设置中，我们实现了 4 倍的吞吐量提升，而短上下文生成的延迟没有任何影响，并且较长上下文的延迟和吞吐量都提高了 1.7-2 倍。]]></description>
      <guid>https://arxiv.org/abs/2411.05894</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一大一小用于文档级事件参数提取</title>
      <link>https://arxiv.org/abs/2411.05895</link>
      <description><![CDATA[arXiv:2411.05895v1 公告类型：新 
摘要：由于输入长度增加，文档级事件参数提取 (EAE) 面临两个挑战：1) 难以区分事件之间的语义边界，2) 冗余信息的干扰。为了解决这些问题，我们提出了两种方法。第一种方法引入了基于小语言模型 (SLM) 的共现和结构事件参数提取模型 (CsEAE)。CsEAE 包含一个共现感知模块，该模块通过上下文标记和共现事件提示提取整合了当前输入中存在的所有事件的信息。此外，CsEAE 还包括一个结构感知模块，该模块通过在包含触发器的句子和文档中的其他句子之间建立结构关系来减少冗余信息的干扰。第二种方法引入了新的提示，将提取任务转换为适合大型语言模型 (LLM) 的生成任务，解决了在监督微调 (SFT) 条件下使用 LLM 的 EAE 性能方面的差距。我们还对多个数据集进行了微调，以开发在大多数数据集上表现更好的 LLM。最后，我们将 CsEAE 的见解应用于 LLM，从而进一步提高了性能。这表明在 SLM 上验证的可靠见解也适用于 LLM。我们在 Rams、WikiEvents 和 MLEE 数据集上测试了我们的模型。与基线 PAIE~\cite{PAIE} 相比，CsEAE 模型在 Arg-C F1 指标上实现了 2.1\%、2.3\% 和 3.2\% 的改进。对于 LLM，我们证明了它们在文档级数据集上的性能与 SLM 相当~\footnote{所有代码均可在 https://github.com/simon-p-j-r/CsEAE 上找到}。]]></description>
      <guid>https://arxiv.org/abs/2411.05895</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人类在复杂的临床决策中继续超越大型语言模型：一项使用医学计算器的研究</title>
      <link>https://arxiv.org/abs/2411.05897</link>
      <description><![CDATA[arXiv:2411.05897v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 已经通过医学执照考试评估了一般医学知识，但它们是否能够有效支持临床决策任务（例如选择和使用医学计算器）仍不确定。在这里，我们评估了医学实习生和 LLM 推荐医学计算器的能力，以应对各种多项选择临床场景，例如风险分层、预后和疾病诊断。我们评估了八个 LLM，包括开源、专有和领域特定模型，在 35 个临床计算器中使用 1,009 个问答对，并测量了 100 个问题子集上的人类表现。虽然表现最好的 LLM GPT-4o 的答案准确率为 74.3%（CI：71.5-76.9%），但人类注释者的平均准确率为 79.5%（CI：73.5-85.0%），优于 LLM。错误分析显示，表现最好的 LLM 仍然在理解（56.6%）和计算器知识（8.1%）方面犯错，但我们的研究结果强调，人类在计算器推荐等复杂的临床任务上继续超越 LLM。]]></description>
      <guid>https://arxiv.org/abs/2411.05897</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过重点学习减少长语境语言模型中的干扰</title>
      <link>https://arxiv.org/abs/2411.05928</link>
      <description><![CDATA[arXiv:2411.05928v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展显著增强了它们处理长上下文的能力。然而，由于干扰问题，有效利用这种长上下文仍然是一个挑战，其中无关信息主导着冗长的上下文，导致 LLM 失去对最相关片段的关注。为了解决这个问题，我们提出了一种新颖的训练方法，通过基于检索的数据增强和对比学习的独特组合来增强 LLM 辨别相关信息的能力。具体而言，在使用长上下文进行微调时，我们使用检索器来提取最相关的片段，作为增强输入。然后，我们引入一个辅助对比学习目标，以明确确保原始上下文和检索到的子上下文的输出紧密一致。在长单文档和多文档 QA 基准上进行的大量实验证明了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.05928</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BERTrend：用于新兴趋势检测的神经主题建模</title>
      <link>https://arxiv.org/abs/2411.05930</link>
      <description><![CDATA[arXiv:2411.05930v1 公告类型：新
摘要：检测和跟踪大型不断发展的文本语料库中的新兴趋势和弱信号对于监测科学文献、管理品牌声誉、监视关键基础设施以及更普遍的任何类型的基于文本的事件检测等应用至关重要。现有的解决方案通常无法捕捉细微的上下文或动态跟踪随时间演变的模式。BERTrend 是一种新方法，它使用在线环境中的神经主题建模来解决这些限制。它引入了一种新指标，通过考虑文档数量和更新频率来量化主题随时间推移的流行度。该指标将主题分类为噪声、弱信号或强信号，标记新兴、快速增长的主题以供进一步研究。在两个大型真实数据集上进行的实验证明了 BERTrend 能够准确检测和跟踪有意义的弱信号，同时滤除噪声，为监测大规模不断发展的文本语料库中的新兴趋势提供了全面的解决方案。该方法还可用于对过去事件的回顾性分析。此外，大型语言模型与 BERTrend 的使用为事件趋势的可解释性提供了有效的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.05930</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeKo：面向任务型专家的后识别生成校正大型语言模型</title>
      <link>https://arxiv.org/abs/2411.05945</link>
      <description><![CDATA[arXiv:2411.05945v1 公告类型：新
摘要：构建通用的后识别错误校正器提出了一个关键问题：我们如何才能最有效地在大量混合领域数据集上训练模型？答案在于学习特定于数据集的特征并在单个模型中消化它们的知识。以前的方法通过使用单独的校正语言模型来实现这一点，从而显著增加参数。在这项工作中，我们提出了混合专家作为一种解决方案，强调 MoE 不仅仅是一个可扩展性工具。我们提出了一种多任务校正 MoE，通过学习将每个数据集的标记路由到其映射的专家，我们训练专家成为语音到文本、语言到文本和视觉到文本数据集的“专家”。在 Open ASR Leaderboard 上的实验表明，我们通过实现平均相对 $5.0$% 的 WER 减少和语音和翻译任务的 BLEU 分数的大幅提高，探索了新的最先进性能。在零样本评估中，NeKo 在 Hyporadise 基准测试中以 $15.5$% 到 $27.6$% 的相对 WER 减少量超越 GPT-3.5 和 Claude-Opus。作为多任务模型，NeKo 在语法和 OCR 后校正方面表现出色。]]></description>
      <guid>https://arxiv.org/abs/2411.05945</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>社交媒体中网络欺凌数据的情绪分析</title>
      <link>https://arxiv.org/abs/2411.05958</link>
      <description><![CDATA[arXiv:2411.05958v1 公告类型：新
摘要：社交媒体已成为现代生活不可或缺的一部分，但它也带来了网络欺凌这一普遍存在的问题，这是当今数字时代的严重威胁。网络欺凌是社交网络上发生的一种骚扰形式，随着这些平台的发展而不断升级。情绪分析不仅在检测欺凌短语方面具有巨大潜力，而且在识别对自己或他人有高伤害风险的受害者方面也具有巨大潜力。我们的工作重点是利用深度学习和自然语言理解技术来检测社交媒体帖子中的欺凌痕迹。我们开发了一个具有长短期记忆 (LSTM) 单元的循环神经网络，使用不同的嵌入。一种方法利用 BERT 嵌入，而另一种方法用 OpenAI 最近发布的嵌入 API 替换嵌入层。我们对这两种方法进行了性能比较，以评估它们在 Formspring 网络欺凌数据情绪分析中的有效性。我们的代码可以在 https://github.com/ppujari/xcs224u 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.05958</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据清理对语言模型的实证影响</title>
      <link>https://arxiv.org/abs/2411.05978</link>
      <description><![CDATA[arXiv:2411.05978v1 公告类型：新
摘要：语言建模环境中的数据清理涉及识别敏感内容，例如个人身份信息 (PII)，并将其从数据集语料库中删除。这是自然语言处理 (NLP) 中用于维护隐私的常见做法。然而，数据清理对语言模型的语言理解能力的影响仍然研究较少。本文实证分析了数据清理对几个基准语言建模任务的影响，包括理解问答 (Q&amp;A)、蕴涵、情感分析和文本分类。我们的实验涵盖了广泛的范围，包括对原始数据集和清理后的数据集进行小规模语言模型的微调，以提示大型语言模型 (LLM)，并比较它们在任务中的表现。有趣的是，我们的结果表明，对于某些任务（例如情绪分析或蕴涵），编辑的影响非常低，通常在 1-5% 左右，而对于理解问答等任务，与原始查询相比，编辑查询的性能大幅下降了 25% 以上。对于影响较大的任务，我们会进行更深入的研究以检查任务关键实体的存在。最后，我们研究了性能与编辑实体数量之间的相关性，并提出了一种通过基于内容的子采样来修复已编辑数据集的策略。更多详细信息请访问 https://sites.google.com/view/datasan。]]></description>
      <guid>https://arxiv.org/abs/2411.05978</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FactLens：细粒度事实验证的基准测试</title>
      <link>https://arxiv.org/abs/2411.05980</link>
      <description><![CDATA[arXiv:2411.05980v1 公告类型：新
摘要：大型语言模型 (LLM) 在语言生成和理解方面表现出了令人印象深刻的能力，但它们产生幻觉和产生事实错误信息的倾向仍然是一个关键限制。为了验证 LLM 生成的内容和来自其他来源的声明，传统的验证方法通常依赖于整体模型，这些模型为复杂的声明分配单一的事实标签，可能会掩盖细微的错误。在本文中，我们提倡转向细粒度验证，其中复杂的声明被分解为较小的子声明以进行单独验证，从而可以更精确地识别不准确性，提高透明度并减少证据检索中的歧义。然而，生成子声明带来了挑战，例如保持上下文并确保与原始声明的语义等价性。我们引入了 FactLens，这是一个评估细粒度事实验证的基准，具有子声明质量的指标和自动评估器。基准数据是手动整理的，以确保高质量的事实真相。我们的结果表明，自动 FactLens 评估器与人类判断一致，并且我们讨论了子声明特征对整体验证性能的影响。]]></description>
      <guid>https://arxiv.org/abs/2411.05980</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用错误严重程度映射的机器翻译细粒度奖励优化</title>
      <link>https://arxiv.org/abs/2411.05986</link>
      <description><![CDATA[arXiv:2411.05986v1 公告类型：新
摘要：强化学习 (RL) 已被证明是一种有效且稳健的神经机器翻译系统训练方法，尤其是与能够准确评估翻译质量的强大奖励模型结合使用时。然而，大多数研究都集中在使用句子级反馈的 RL 方法上，这会导致由于奖励稀疏性问题而导致学习信号效率低下——模型对整个句子只有一个分数。为了解决这个问题，我们引入了一种新方法，该方法利用细粒度的 token 级奖励机制和 RL 方法。我们使用最先进的质量评估系统 xCOMET 作为我们的 token 级奖励模型。xCOMET 通过预测给定源-翻译对的细粒度错误跨度及其严重程度来提供详细的反馈。我们对小型和大型翻译数据集进行实验，以比较句子级和细粒度奖励信号对翻译质量的影响。我们的结果表明，根据自动和人工评估，使用 token 级奖励进行训练可以提高跨语言对的翻译质量。此外，token 级奖励优化还可以提高训练稳定性，这可以从训练期间平均奖励的稳步增加中看出。]]></description>
      <guid>https://arxiv.org/abs/2411.05986</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GUIDEQ：引导式提问框架，用于逐步收集和分类信息</title>
      <link>https://arxiv.org/abs/2411.05991</link>
      <description><![CDATA[arXiv:2411.05991v1 公告类型：新
摘要：问答 (QA) 是通过信息收集进行文本分类等任务的重要组成部分。它们在医疗保健、客户支持、法律服务等领域的应用越来越广泛，用于收集并将响应分类为可操作的类别。虽然 LLM 可以支持 QA 系统，但它们面临着分类信息不足或缺失的重大挑战。虽然 LLM 在推理方面表现出色，但模型依靠其参数知识来回答。然而，询问用户需要特定领域的信息来帮助收集准确的信息。我们的工作 GUIDEQ 提出了一个新颖的框架，用于提出引导性问题以进一步推进部分信息。我们利用从分类器模型中获得的可解释性以及 LLM 来提出引导性问题以进一步增强信息。这些进一步的信息有助于更准确地对文本进行分类。GUIDEQ 使用遮挡得出代表标签的最重要关键词。我们根据前 3 个分类器标签输出和重要单词开发了 GUIDEQ 的引导问题提示策略，以寻找具体和相关的信息，并有针对性地进行分类。通过我们的实验结果，我们证明了 GUIDEQ 优于其他基于 LLM 的基线，通过准确收集相关的进一步信息获得了更高的 F1 分数。我们进行了各种分析研究，并报告了与我们的方法相比更好的问题质量。]]></description>
      <guid>https://arxiv.org/abs/2411.05991</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的个性化说服的黑暗模式：揭示法学硕士回答中大五人格特质的说服性语言特征</title>
      <link>https://arxiv.org/abs/2411.06008</link>
      <description><![CDATA[arXiv:2411.06008v1 公告类型：新
摘要：本研究探讨了大型语言模型 (LLM) 如何调整语言特征以创建个性化的说服输出。虽然研究表明 LLM 可以个性化输出，但在理解其说服能力的语言特征方面仍然存在差距。我们确定了 13 个语言特征，这些特征对于影响大五人格模型不同层次的个性至关重要。我们分析了带有人格特质信息的提示如何影响五个模型系列中 19 个 LLM 的输出。研究结果表明，模型使用更多与焦虑相关的词语来表示神经质，增加与成就相关的词语来表示尽责，并使用更少的认知过程词语来表示开放性。一些模型系列擅长调整语言以适应开放性，另一些模型系列擅长调整尽责性，而只有一个模型调整语言以适应神经质。我们的研究结果表明，法学硕士 (LLM) 如何根据提示中的个性线索来定制回答，这表明他们有潜力创造出有说服力的内容，影响接受者的思想和幸福感。]]></description>
      <guid>https://arxiv.org/abs/2411.06008</guid>
      <pubDate>Tue, 12 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>