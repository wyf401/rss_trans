<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 19 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>利用大型语言模型从病理报告中提取结构化信息</title>
      <link>https://arxiv.org/abs/2502.12183</link>
      <description><![CDATA[ARXIV：2502.12183V1公告类型：新 
摘要：背景：从非结构化组织病理学报告中提取的结构化信息促进了临床研究的数据可访问性。专家的手动提取是耗时且昂贵的，限制了可扩展性。大型语言模型（LLMS）通过零射击提示提供有效的自动提取，仅需要自然语言说明，而无需标记数据或培训。与受过训练的人类注释者手动提取相比，我们评估了LLMS从乳腺癌组织病理学报告中提取结构化信息的准确性。
  方法：我们开发了医学报告信息提取器，这是一种利用LLMS自动提取的Web应用程序。我们开发了一个黄金标准提取数据集，以评估人类注释者与五个LLM一起评估包括GPT-4O（领先的专有模型GPT-4O）和Llama 3模型家族，该家族允许自托管数据隐私。我们的评估涉及111个来自乳腺癌（BCN）世代研究的组织病理学报告，研究了研究词典中指定的51个病理特征。
  结果：针对黄金标准数据集的评估表明，Llama 3.1 405b（准确度为94.7％）和GPT-4O（96.1％）的提取精度可与人类注释相当（95.4％; P = 0.146; P = 0.146和P = 0.106）。尽管美洲驼3.1 70b（91.6％）的表现低于人类的准确性（p &lt;0.001），但其计算要求降低使其成为自我托管的可行选择。
  结论：我们开发了一种用于结构化信息提取的开源工具，该工具可以由非程序员使用自然语言自定义。它的模块化设计可重用各种提取任务，从非结构化文本报告中产生标准化的结构化数据，从而通过提高可访问性和互操作性来促进分析。]]></description>
      <guid>https://arxiv.org/abs/2502.12183</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型用于制造过程的外推模型</title>
      <link>https://arxiv.org/abs/2502.12185</link>
      <description><![CDATA[ARXIV：2502.12185V1公告类型：新 
摘要：制造过程中参数关系的常规预测建模受到人类专业知识和直觉的主观性的限制，另一方面是实验数据生成的成本和时间。这项工作通过建立新的大型语言模型（LLM）框架来解决此问题。新颖性在于，基于少量的实验数据，将嵌入文献中嵌入的过程相关知识的自动提取与迭代模型改进结合在一起。对基于加工，变形和添加剂原理的三个不同的制造过程进行评估。结果表明，对于相同的小实验数据预算，我们的框架所产生的模型具有出乎意料的外反性性能，通常超过了传统机器学习的能力。此外，我们的方法消除了初始模型的手动生成或对文献的专业知识的解释。结果还揭示了从文献中提取的知识性质的重要性以及知识提取和模型改进成分的重要性。]]></description>
      <guid>https://arxiv.org/abs/2502.12185</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>幻觉是不可避免的，但统计学上可以忽略不计</title>
      <link>https://arxiv.org/abs/2502.12187</link>
      <description><![CDATA[ARXIV：2502.12187V1公告类型：新 
摘要：幻觉是一种语言模型（LM）产生非事实内容的现象，对LMS的实际部署构成了重大挑战。虽然已经提出了许多经验方法来减轻幻觉，但最近的一项研究确立了一个可计算性的理论结果，表明任何LM都将不可避免地会在一套无限的输入上产生幻觉，而不管培训数据的质量和数量如何模型架构，培训和推理算法。尽管可计算性理论结果似乎似乎是悲观的，但在实际观点中的重要性尚不清楚。相反，我们从概率的角度提出了积极的理论结果。具体而言，我们证明幻觉可以在统计上可以忽略不计，只要培训数据的质量和数量就足够。有趣的是，我们的积极结果与可计算性理论结果并存，这意味着，尽管无法完全消除一组无限投入的幻觉，但可以通过改善算法和培训数据来降低它们的概率。通过通过信息理论的角度评估两个看似矛盾的结果，我们认为我们的概率理论阳性结果更好地反映了实际考虑，而不是可计算性理论负面结果。]]></description>
      <guid>https://arxiv.org/abs/2502.12187</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我监督的属性 - 感知动态偏好排名对齐</title>
      <link>https://arxiv.org/abs/2502.12189</link>
      <description><![CDATA[ARXIV：2502.12189V1公告类型：新 
摘要：从人类反馈及其变体中学习的强化在与人类产生有益，无害和诚实回应的意图相符方面表现出色。但是，他们中的大多数都依赖于昂贵的人为认可的成对比较来进行监督对准，这不适合列表级别的情况，例如社区问题回答。另外，人类的偏好受到反应中多个内在因素的影响，导致决策不一致。因此，我们提出\ textbf {se} lf-supervisise \ textbf {a} ttribute-ware \ textbf {d} ynamic \ textbf {p} referct \ textbf {ra} nking，称为\ shortname。 \它根据属性 - 感知距离因子（APDF）量化响应之间的偏好差异，并动态确定列表的对齐顺序。此外，它可以实现细粒度的偏好差学习，并可以与最佳的偏好差学习。我们专门构建了一个名为STACOCOQA的具有挑战性的代码偏好数据集，并引入了更具成本效益和可扩展的首选项评估指标：预先交换和预验证。广泛的实验结果表明，Seadpra在八个流行领域的Stacocoqa和偏好数据集上表现出卓越的性能和概括性。]]></description>
      <guid>https://arxiv.org/abs/2502.12189</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI与法律：评估Chatgpt在法律分类方面的表现</title>
      <link>https://arxiv.org/abs/2502.12193</link>
      <description><![CDATA[ARXIV：2502.12193V1公告类型：新 
摘要：在刑事诉讼中使用CHATGPT分析和分类证据一直是持续讨论的话题。但是，据我们所知，这个问题尚未在波兰语言的背景下进行研究。这项研究通过评估Chatgpt在《波兰刑法》中分类法律案件中的有效性来解决这一研究差距。结果显示出极好的二元分类精度，所有正和负案例都正确分类。此外，定性评估证实，为每个案件提供的法律基础以及相关法律内容都是适当的。获得的结果表明，Chatgpt可以在应用适当的法律规则的同时有效地分析和分类证据。总之，Chatgpt有可能协助感兴趣的方分析证据，并为在该领域经验或知识较少的个人提供宝贵的法律资源。]]></description>
      <guid>https://arxiv.org/abs/2502.12193</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>仔细查看系统提示鲁棒性</title>
      <link>https://arxiv.org/abs/2502.12197</link>
      <description><![CDATA[ARXIV：2502.12197V1公告类型：新 
摘要：系统提示已成为指定LLM在聊天和代理设置中的行为的关键控制表面。开发人员依靠系统提示来指定重要的上下文，输出格式，个性，护栏，内容策略和安全对策，所有这些都需要模型才能坚持系统提示，尤其是在面对冲突或对抗性用户输入时。实际上，模型通常会忘记考虑相关的护栏或无法解决系统与用户之间的冲突需求。在这项工作中，我们根据从OpenAI的GPT商店和Huggingface的HuggingChat中收集的提示来创建现实的新评估和微调数据集，从而研究了改善系统的各种方法。我们的实验通过一组新的和现有的基准评估模型表明，通过现实的微调数据以及推理时间干预措施（例如无分类器指导）可以大大提高性能。最后，我们分析了OpenAI和DeepSeek最近发布的推理模型的结果，这些模型对我们研究的基准显示了令人兴奋但不均匀的改进。总体而言，当前技术无法确保系统迅速鲁棒性和进一步的研究。]]></description>
      <guid>https://arxiv.org/abs/2502.12197</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过及时分解和压缩外部产品进行高效有效的及时调整</title>
      <link>https://arxiv.org/abs/2502.12200</link>
      <description><![CDATA[ARXIV：2502.12200V1公告类型：新 
摘要：提示调整（PT）为微调大规模预训练的语言模型（PLM）提供了一种具有成本效益的替代方法，仅在输入文本之前添加了软提示令牌中的一些参数。但是，现有的PT方法面临两个重要的问题：（i）它们忽略了软及时令牌之间的内在语义关联，导致高离散性和有限的相互作用，从而降低了模型对复杂任务的理解和有效性。 （ii）由于下游任务的复杂性，需要长时间的软提示来提高性能，但及时长度与内存使用和计算成本呈正相关。达到高效率和性能仍然是一个持续的挑战。为了解决这些问题，我们提出了一种新型的低参数促使调整（LAMP）方法，该方法促使分解和压缩外产品。具体而言，及时分解模块采用截短的SVD来减少训练参数，并显着降低软提示参数空间的维度。然后，它利用压缩的外产品模块来促进及时令牌之间的多次相互作用，探索其内在关联以增强知识表示。最后，LAMP使用平均合并来减少记忆使用和训练/推理时间。跨六个架构和八个数据集进行的广泛实验表明，灯在性能和效率方面优于基于PT的最先进和基于LORA的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.12200</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器人：通过后门攻击，打破了类似O1的大型语言模型的长期思考过程</title>
      <link>https://arxiv.org/abs/2502.12202</link>
      <description><![CDATA[ARXIV：2502.12202V1公告类型：新 
摘要：更长的思想，更好的表现：具有深层推理能力的大型语言模型，尤其是类似O1的模型，通过在推理过程中产生广泛的思维过程表现出了出色的性能。这种权衡揭示了潜在的脆弱性：对手可以通过强迫无需思考过程的即时响应来损害模型绩效。为此，在本文中，我们介绍了一种新颖的攻击场景，以针对O1型模型的长时间思考过程并提出了Bot（Break Cot），该过程可以通过后门攻击选择性地破坏固有的推理机制。 Bot通过监督微调或直接偏好优化构建具有设计的触发器和注入后门的中毒数据集。当触发时，模型直接生成答案而无需思考过程，同时保持了正常的推理能力以进行清洁输入。包括最近的DeepSeek-R1在内的开源O1模型进行的广泛实验表明，机器人几乎取得了高攻击成功率，同时保持了清洁准确性，突出了当前模型的关键安全风险。此外，任务难度与帮助性之间的关系揭示了良好的潜在应用，使用户能够根据任务复杂性自定义模型行为。代码可在\ href {https://github.com/zihao-ai/bot} {https://github.com/zihao-ai-ai/bot}中获得。]]></description>
      <guid>https://arxiv.org/abs/2502.12202</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测交互式多主题协作的筛查访谈中的抑郁症</title>
      <link>https://arxiv.org/abs/2502.12204</link>
      <description><![CDATA[ARXIV：2502.12204V1公告类型：新 
摘要：自动抑郁症检测为临床医生的早期临床干预提供了线索。抑郁症检测的临床访谈涉及以多个主题为中心的对话。现有研究主要设计端到端神经网络模型，以捕获临床访谈对话的分层结构。但是，这些方法在建模临床访谈的主题内容时表现出缺陷：1）它们未能明确捕获主题内和主题相关性，而2）他们不允许临床医生干预并专注于感兴趣的主题。为了解决这些问题，本文介绍了一个交互式抑郁识别框架。该框架利用文化学习技术在临床访谈中识别主题，然后对主主题和主题之间的相关性进行建模。此外，它采用AI驱动的反馈来模拟临床医生的利益，从而可以对主题重要性进行互动调整。与抑郁症检测数据集DAIC-WOZ相比，PDIMC的绝对改进为35 \％和12 \％，这证明了建模主题相关性并纳入交互式外部反馈的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.12204</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过检索增强生成增强框架检测</title>
      <link>https://arxiv.org/abs/2502.12210</link>
      <description><![CDATA[ARXIV：2502.12210V1公告类型：新 
摘要：自然语言处理的最新进展显着改善了从非结构化文本中提取结构化语义表示的，尤其是通过框架语义角色标签（FSRL）。尽管取得了这种进步，但检索型生成（RAG）模型的框架检测的潜力仍然不足。在本文中，我们介绍了第一个基于抹布的框架检测方法称为RCIF（检索候选者并识别帧）。 RCIF也是不需要明确目标跨度的第一种操作方法，并包括三个主要阶段：（1）从各种表示形式生成帧嵌入； （2）给定输入文本的候选框架的检索； （3）识别最合适的帧。我们跨多种配置进行了广泛的实验，包括零射，很少射击和微调设置。我们的结果表明，我们的检索组件通过缩小搜索空间可大大降低任务的复杂性，从而使框架标识符能够完善并完成一组候选者。我们的方法在Framenet 1.5和1.7上实现了最先进的性能，在仅提供原始文本的场景中证明了其稳健性。此外，我们利用通过此方法获得的结构化表示，作为代理来增强将自然语言问题转化为SPARQL查询的任务中跨词汇变化的概括。]]></description>
      <guid>https://arxiv.org/abs/2502.12210</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS中的零令牌深思熟虑：通过循环改进解开现有参数的全部潜力</title>
      <link>https://arxiv.org/abs/2502.12214</link>
      <description><![CDATA[ARXIV：2502.12214V1公告类型：新 
摘要：资源限制通常会限制大语言模型（LLMS）的参数计数，从而阻碍其性能。尽管现有方法采用参数共享来重用固定预算下的相同参数集，但这种方法通常迫使每层效果扮演多个角色，并具有预定数量的迭代次数，从而限制了效率和适应性。在这项工作中，我们提出了零令牌变压器（ZTT），该变压器（ZTT）具有尾巴解耦参数循环方法。我们将第一（头）和最后一层（尾部）层从参数循环中解散，并且仅迭代地完善中间层。此外，我们引入了一种零token机制，一种内部体系结构组件而不是输入令牌，以指导特定于层的计算。在每个循环中，该模型从零token池中检索一个零令牌（具有训练键值），将其与注意机制中的常规令牌一起集成在一起。相应的注意力分数不仅反映了每一层的计算重要性，而且还可以使动态早期退出而无需牺牲整体模型精度。我们的方法在紧张的参数预算下实现了卓越的性能，通过早期出口有效地减少了计算开销，并且可以轻松地应用于现有的预训练模型，以提高效率和适应性。]]></description>
      <guid>https://arxiv.org/abs/2502.12214</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>景点：一种新颖的封闭式式变压器，用于有效的手语翻译</title>
      <link>https://arxiv.org/abs/2502.12223</link>
      <description><![CDATA[ARXIV：2502.12223V1公告类型：新 
摘要：机器翻译在减少语言障碍中起着至关重要的作用，但其对手语机器翻译（SLMT）的适应性较少。 SLMT上的现有作品主要使用变压器神经网络，该网络由于手语的动态性质而表现出较低的性能。在本文中，我们提出了一种新型的封闭式式变压器（斑点），该变压器（斑点）捕获了手语作为时间序列数据的长期时间依赖性。我们对磁带和变压器融合模型作为基线进行了全面评估，以符号到光文翻译。我们的结果表明，斑点始终优于所有指标的其他模型。这些发现强调了它的潜力，以应对聋人和听力社区所面临的沟通挑战。]]></description>
      <guid>https://arxiv.org/abs/2502.12223</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>InfoQuest：评估具有隐藏上下文的开放式对话的多转化对话代理</title>
      <link>https://arxiv.org/abs/2502.12257</link>
      <description><![CDATA[ARXIV：2502.12257V1公告类型：新 
摘要：虽然大型语言模型在遵循明确的说明方面表现出色，但他们经常会在用户要求的含糊或不完整的请求中挣扎，默认为冗长，通用响应，而不是寻求澄清。我们介绍了InfoQuest，这是一个多转弯聊天基准测试，旨在评估对话代理在开放式用户请求中如何处理隐藏的上下文。该基准提出了有意的歧义场景，这些场景需要模型在提供适当的回答之前通过澄清问题进行信息寻求对话。我们对开放式和封闭源模型的评估表明，尽管专有模型通常表现更好，但所有当前的助手都在有效地收集关键信息的情况下努力，通常需要多个转弯来推断用户意图，并且经常默认为通用响应，而无需正确澄清。我们提供了一种系统的方法，用于生成各种场景和评估模型寻求信息的能力，从而对语言模型的当前局限性提供有关通过多转交互作用来处理模棱两可请求的当前局限性的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.12257</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>故事语法语义匹配文学研究</title>
      <link>https://arxiv.org/abs/2502.12276</link>
      <description><![CDATA[Arxiv：2502.12276V1公告类型：新 
摘要：在自然语言处理（NLP）中，语义匹配算法传统上依赖于单词共存的特征来测量语义相似性。尽管这种特征方法在许多情况下已被证明具有价值，但其简单的性质限制了用来理解文学文本的分析和解释能力。为了解决这些局限性，我们提出了一种更透明的方法，该方法利用故事结构和相关元素。使用Bert语言模型管道，我们将散文和史诗诗用故事元素标签标记，并仅将这些标签视为特征来执行语义匹配。这种新方法，故事语法语义匹配，指导文学学者在文本之间引起典故和其他语义相似性，以表征模式和文学技巧。]]></description>
      <guid>https://arxiv.org/abs/2502.12276</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估逐步推理轨迹：调查</title>
      <link>https://arxiv.org/abs/2502.12289</link>
      <description><![CDATA[ARXIV：2502.12289V1公告类型：新 
摘要：逐步推理被广泛用于增强复杂问题中大语言模型（LLM）的推理能力。评估推理轨迹的质量对于理解和改善LLM推理至关重要。但是，评估标准仍然高度不合格，导致在制定指标和元评估基准方面的努力分散。为了解决这一差距，这项调查提供了逐步推理评估的全面概述，提出了具有四个顶级类别（接地，有效性，连贯性和实用程序）的评估标准的分类法。然后，我们根据指标的实现进行分类，调查哪些指标用于评估每个标准，并探索评估者模型是否可以跨不同标准传输。最后，我们确定了未来研究的关键方向。]]></description>
      <guid>https://arxiv.org/abs/2502.12289</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>