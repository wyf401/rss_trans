<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 06 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>IdeaBench：对大型语言模型进行基准测试，以生成研究想法</title>
      <link>https://arxiv.org/abs/2411.02429</link>
      <description><![CDATA[arXiv:2411.02429v1 公告类型：新
摘要：大型语言模型 (LLM) 改变了人们与人工智能 (AI) 系统交互的方式，在各种任务中取得了最先进的成果，包括科学发现和假设生成。然而，缺乏使用 LLM 生成研究想法的全面而系统的评估框架，这对理解和评估它们在科学发现中的生成能力构成了重大障碍。为了解决这一差距，我们提出了 IdeaBench，这是一个基准系统，包括一个全面的数据集和一个评估框架，用于标准化使用 LLM 生成研究想法的评估。我们的数据集包括来自各种有影响力的论文的标题和摘要，以及它们的参考文献。为了模拟人类产生研究想法的过程，我们将 LLM 描述为特定领域的研究人员，并将它们置于人类研究人员考虑的相同背景下。这最大限度地利用了 LLM 的参数知识来动态地产生新的研究想法。我们还引入了一个评估框架来评估生成的研究想法的质量。我们的评估框架分为两个阶段：首先，使用 GPT-4o 根据用户指定的质量指标（例如新颖性和可行性）对想法进行排名，从而实现可扩展的个性化；其次，计算基于相对排名的“洞察分数”以量化所选的质量指标。提议的基准系统将成为社区衡量和比较不同 LLM 的宝贵资产，最终推动科学发现过程的自动化。]]></description>
      <guid>https://arxiv.org/abs/2411.02429</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多模态对话中的生成情绪原因解释</title>
      <link>https://arxiv.org/abs/2411.02430</link>
      <description><![CDATA[arXiv:2411.02430v1 公告类型：新
摘要：多模态对话是人类交流的重要形式，承载着丰富的情感内容，探索其中的情感成因是一项重要的研究工作。然而，现有的关于情感成因的研究通常使用子句选择方法来定位原因话语，而没有提供情感成因的详细解释。在本文中，我们提出了一项新任务，\textbf{M}ultimodal \textbf{C}onversation \textbf{E}motion \textbf{C}ause \textbf{E}xplanation (MCECE)，旨在在多模态对话场景中对目标话语的情感成因进行详细解释。在 MELD 数据集的基础上，我们开发了一个新数据集 (ECEM)，将视频片段与人物情绪的详细解释相结合，有助于深入研究多模态对话中情绪表达背后的因果因素。我们还提出了一种新方法 FAME-Net，利用大型语言模型 (LLM) 的强大功能来分析视觉数据并准确解释视频中通过面部表情传达的情绪。通过利用面部情绪的传染效应，FAME-Net 有效地捕捉了参与对话的个人的情绪原因。我们在新构建的数据集上的实验结果表明，FAME-Net 明显优于几个优秀的大型语言模型基线。代码和数据集可在 \url{https://github.com/3222345200/ECEMdataset.git} 获得]]></description>
      <guid>https://arxiv.org/abs/2411.02430</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 能否在规定的痛苦和快乐状态之间做出权衡？</title>
      <link>https://arxiv.org/abs/2411.02432</link>
      <description><![CDATA[arXiv:2411.02432v1 公告类型：新
摘要：快乐和痛苦在人类决策中发挥着重要作用，因为它们为解决动机冲突提供了共同的货币。虽然大型语言模型 (LLM) 可以生成快乐和痛苦体验的详细描述，但 LLM 是否可以在选择场景中重现快乐和痛苦的动机力量仍是一个悬而未决的问题 - 这个问题可能与关于 LLM 感知的争论有关，LLM 感知被理解为具有价态的体验状态的能力。我们使用一个简单的游戏来探究这个问题，其中的既定目标是最大化积分，但据说积分最大化选项会招致痛苦惩罚，或者非积分最大化选项会带来快乐奖励，从而激励人们偏离积分最大化行为。通过改变痛苦惩罚和快乐奖励的强度，我们发现 Claude 3.5 Sonnet、Command R+、GPT-4o 和 GPT-4o mini 都表现出了至少一种权衡，即在达到规定的痛苦或快乐强度的临界阈值后，大多数反应从积分最大化转变为痛苦最小化或快乐最大化。LLaMa 3.1-405b 对规定的快乐奖励和痛苦惩罚表现出一定的分级敏感性。Gemini 1.5 Pro 和 PaLM 2 无论强度如何，都优先考虑避免痛苦而不是积分最大化，而无论强度如何，都倾向于优先考虑积分而不是快乐。我们讨论了这些发现对 LLM 感知可能性辩论的影响。]]></description>
      <guid>https://arxiv.org/abs/2411.02432</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SLED：自 Logits 进化解码，用于提高大型语言模型中的事实性</title>
      <link>https://arxiv.org/abs/2411.02433</link>
      <description><![CDATA[arXiv:2411.02433v1 公告类型：新
摘要：大型语言模型 (LLM) 已展示出卓越的能力，但它们的输出有时可能不可靠或事实不正确。为了解决这个问题，我们引入了自 Logits 进化解码 (SLED)，这是一种新颖的解码框架，可增强 LLM 的真实性，而无需依赖外部知识库或需要进一步微调。从优化的角度来看，我们的 SLED 框架通过将最后一层的输出 logit 与早期层的输出 logit 进行对比，利用嵌入在 LLM 中的潜在知识。然后，它利用近似梯度方法使潜在知识能够指导输出的自我改进，从而有效提高事实准确性。已经在各种模型系列（LLaMA 2、LLaMA 3、Gemma）和规模（从 2B 到 70B）的既定基准上进行了广泛的实验，包括更高级的架构配置，例如专家混合 (MoE)。我们的评估涵盖了多种任务，包括多项选择、开放生成和对思维链推理任务的适应性。结果表明，与现有解码方法相比，SLED 可以持续提高高达 20% 的事实准确性，同时保持自然语言流畅性和可忽略不计的延迟开销。此外，它可以灵活地与其他解码方法结合使用，以进一步提高其性能。]]></description>
      <guid>https://arxiv.org/abs/2411.02433</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用知识图谱增强大型语言模型对真实犯罪播客进行叙述分析</title>
      <link>https://arxiv.org/abs/2411.02435</link>
      <description><![CDATA[arXiv:2411.02435v1 公告类型：新
摘要：叙述数据涵盖所有学科，并为读者或观看者提供连贯的世界模型。机器学习和大型语言模型 (LLM) 的最新进展使自然语言分析取得了长足进步。然而，大型语言模型 (LLM) 仍然难以处理复杂的叙述弧以及包含冲突信息的叙述。最近的研究表明，通过外部知识库增强的 LLM 可以提高结果模型的准确性和可解释性。在这项工作中，我们分析了从经典自然语言处理 (NLP) 和 LLM 方法中应用知识图谱 (KG) 理解真实犯罪播客数据的有效性。我们直接将 KG 增强 LLM (KGLLM) 与经典的 KG 构建、主题建模和情感分析方法进行比较。此外，KGLLM 允许我们以自然语言查询知识库并测试其回答问题的能力。我们检查了模型对对抗性提示的鲁棒性，以测试模型处理冲突信息的能力。最后，我们应用经典方法来理解文本中更微妙的方面，例如在叙事结构中使用传闻和情绪，并提出未来的发展方向。我们的结果表明，KGLLM 在各种指标上都优于 LLM，对对抗性提示的鲁棒性更强，并且更有能力将文本概括为主题。]]></description>
      <guid>https://arxiv.org/abs/2411.02435</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TODO：通过三元偏好增强 LLM 对齐</title>
      <link>https://arxiv.org/abs/2411.02442</link>
      <description><![CDATA[arXiv:2411.02442v1 公告类型：新
摘要：将大型语言模型 (LLM) 与人类意图对齐对于提高其在各种任务中的性能至关重要。标准对齐技术（例如直接偏好优化 (DPO)）通常依赖于二元 Bradley-Terry (BT) 模型，该模型很难捕捉人类偏好的复杂性——尤其是在存在嘈杂或不一致的标签和频繁联系的情况下。为了解决这些限制，我们引入了 Tie-rank 导向 Bradley-Terry 模型 (TOBT)，这是 BT 模型的扩展，明确结合了联系，从而实现了更细致入微的偏好表示。在此基础上，我们提出了 Tie-rank 导向直接偏好优化 (TODO)，这是一种新颖的对齐算法，它利用 TOBT 的三元排名系统来改进偏好对齐。在对 Mistral-7B 和 Llama 3-8B 模型的评估中，TODO 在分布内和分布外数据集的建模偏好方面始终优于 DPO。使用 MT Bench 和 Piqa、ARC-c 和 MMLU 等基准进行的额外评估进一步证明了 TODO 的卓越对齐性能。值得注意的是，TODO 在二元偏好对齐方面也表现出色，凸显了其多功能性和更广泛地集成到 LLM 对齐中的潜力。实现细节可以在 https://github.com/XXares/TODO 中找到。]]></description>
      <guid>https://arxiv.org/abs/2411.02442</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评分、解释和引用 (REC)：大型语言模型在自动评估中增强解释和归因</title>
      <link>https://arxiv.org/abs/2411.02448</link>
      <description><![CDATA[arXiv:2411.02448v1 公告类型：新
摘要：LLM 在生成连贯且高质量的文本方面表现出令人印象深刻的能力，使其在一系列文本生成任务中具有价值。然而，对生成的内容进行严格评估至关重要，因为由于事实不准确和幻觉等持续存在的问题，确保其质量仍然是一项重大挑战。本文介绍了两个经过微调的通用 LLM 自动评估器，REC-12B 和 REC-70B，专门用于从几个维度评估生成的文本：忠实度、指令遵循、连贯性和完整性。这些模型不仅为这些指标提供评级，还提供详细的解释和可验证的引用，从而增强对内容的信任。此外，这些模型支持各种引用模式，可满足不同的延迟和粒度要求。对各种基准的广泛评估表明，我们的通用 LLM 自动评估器 REC-70B 的表现优于最先进的 LLM，通过提供更高质量的解释和引用以及最小的偏见，在内容评估方面表现出色。它在 RewardBench 排行榜上以模型名称 \texttt{TextEval-Llama3.1-70B} 获得了排名第一的生成模型\footnote{\url{https://huggingface.co/spaces/allenai/reward-bench}}。我们的 REC 数据集和模型在 \url{https://github.com/adelaidehsu/REC} 上发布。]]></description>
      <guid>https://arxiv.org/abs/2411.02448</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型集成实现高性能自动化摘要筛选</title>
      <link>https://arxiv.org/abs/2411.02451</link>
      <description><![CDATA[arXiv:2411.02451v1 公告类型：新
摘要：大型语言模型 (LLM) 在需要处理和解释输入文本的任务中表现出色。摘要筛选是系统评价中一项劳动密集型的工作，涉及对通过文献检索确定的大量研究重复应用纳入和排除标准。在这里，LLM（GPT-3.5 Turbo、GPT-4 Turbo、GPT-4o、Llama 3 70B、Gemini 1.5 Pro 和 Claude Sonnet 3.5）在 Cochrane Library 的完整期刊中的系统评价中进行了试用，以评估它们在摘要筛选的零样本二元分类中的准确性。对 800 条记录子集的试验确定了最佳提示策略，并证明了 LLM 在敏感度（LLMmax = 1.000，humanmax = 0.775）、精确度（LLMmax = 0.927，humanmax = 0.911）和平衡准确度（LLMmax = 0.904，humanmax = 0.865）方面优于人类研究人员。在每次重复的搜索结果（n = 119,691）中都试验了表现最佳的 LLM-prompt 组合，结果显示敏感度一致（范围 0.756-1.000），但精确度降低（范围 0.004-0.096）。66 个 LLM-human 和 LLM-LLM 集成表现出完美的敏感度，最大精确度为 0.458，在更大规模的试验中性能下降较少。在审查之间观察到了显著的性能差异，凸显了部署前进行领域特定验证的重要性。法学硕士可以减少系统评价的人力成本，同时保持或提高准确性和敏感性。系统评价是循证医学的基础，法学硕士有助于提高这种研究模式的效率和质量。]]></description>
      <guid>https://arxiv.org/abs/2411.02451</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图的大型语言模型置信度校准</title>
      <link>https://arxiv.org/abs/2411.02454</link>
      <description><![CDATA[arXiv:2411.02454v1 公告类型：新
摘要：提高大型语言模型 (LLM) 可靠性的一个重要方法是提供关于其答案正确性的准确置信度估计。然而，开发一个经过良好校准的置信度估计模型具有挑战性，因为 LLM 所犯的错误很难被发现。我们提出了一种新方法，将 LLM 的自洽性与标记数据相结合，并训练一个辅助模型来估计其对问题的回答的正确性。这个辅助模型仅根据一致的信息来预测答案的正确性。为了设置学习问题，我们使用加权图来表示 LLM 对一个问题的多个回答之间的一致性。根据这些回答与正确答案的相似性为它们分配正确性标签。然后，我们训练一个图神经网络来估计正确回答的概率。实验表明，在多个广泛采用的基准数据集的置信度校准中，所提出的方法大大优于几种最新的方法。此外，所提出的方法显著提高了域外（OOD）数据置信度校准的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2411.02454</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型助力高等教育课程评估探索</title>
      <link>https://arxiv.org/abs/2411.02455</link>
      <description><![CDATA[arXiv:2411.02455v1 公告类型：新 
摘要：课程评估是高等教育教学中的一个重要组成部分。它不仅可以识别现有课程设计的局限性并为课程创新提供基础，还可以为大学行政决策提供定量见解。传统的评估方法主要包括学生调查、教师自我评估和专家评审，经常遇到挑战，包括固有的主观性、反馈延迟、效率低下以及在解决创新教学方法方面的局限性。人工智能 (AI) 中大型语言模型 (LLM) 的最新进展为增强课程评估流程提供了有希望的新途径。本研究从多个角度探讨了 LLM 在自动化课程评估中的应用，并在中国一所主要大学的 100 门课程中进行了严格的实验。研究结果表明：（1）LLM 可以成为课程评估的有效工具；（2）它们的有效性取决于适当的微调和及时的工程； （3）LLM评估结果具有显著的合理性和可解释性。]]></description>
      <guid>https://arxiv.org/abs/2411.02455</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>能够模仿人物语言风格的多任务角色扮演代理</title>
      <link>https://arxiv.org/abs/2411.02457</link>
      <description><![CDATA[arXiv:2411.02457v1 公告类型：新
摘要：大型语言模型 (LLM) 的出现极大地推动了角色扮演代理 (RPA) 的发展。然而，目前的角色扮演代理主要关注模仿角色的基本属性，而忽略了语言风格的复制，并且在执行多轮对话以外的任务时无法有效地复制角色，从而导致生成的响应缺乏真实性。当前 RPA 缺乏这种能力的原因是由于现有角色数据集的性质，这些数据集缺乏角色引用的集合并且仅限于多轮对话任务，这限制了 RPA 在其他任务领域的性能并且无法模仿角色的语言风格。为了解决这一差距，我们开发了一个名为 MRstyle 的多任务角色扮演数据集，它包含大量真实个体及其引用，并涵盖七个不同的任务。在此基础上，我们开发了 StyleRPA，一个多任务角色扮演代理 (MRPA)，它在对话、词典、作文、故事生成、产品描述、音乐评论和开放式问答等 7 个任务上的表现显著优于最近的开源 LLM 和 RPA 基线。代码和数据即将发布。]]></description>
      <guid>https://arxiv.org/abs/2411.02457</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士课程中多语言迁移的代码转换课程学习</title>
      <link>https://arxiv.org/abs/2411.02460</link>
      <description><![CDATA[arXiv:2411.02460v1 公告类型：新
摘要：大型语言模型 (LLM) 现在在各种任务中表现出接近人类水平的性能，但由于预训练数据的不平衡，它们的性能在少数高资源语言之后急剧下降。受人类第二语言习得过程的启发，特别是代码转换（对话中的语言交替实践），我们提出了代码转换课程学习 (CSCL) 来增强 LLM 的跨语言迁移。CSCL 通过使用由 1) 标记级代码转换、2) 句子级代码转换和 3) 单语语料库组成的课程逐步训练模型来模仿人类语言学习的阶段。使用 Qwen 2 作为我们的基础模型，我们展示了 CSCL 在改善语言迁移到韩语方面的有效性，与单语持续预训练方法相比，取得了显着的性能提升。消融研究表明，标记和句子级别的代码转换都显著增强了跨语言迁移，而课程学习会放大这些影响。我们还将研究结果扩展到各种语言，包括日语（高资源）和印尼语（低资源），并使用了另外两个模型（Gemma 2 和 Phi 3.5）。我们进一步表明，CSCL 减轻了语言资源和安全一致性之间的虚假相关性，为 LLM 中更公平的语言迁移提供了一个强大而有效的框架。我们观察到，CSCL 适用于资源匮乏的环境，因为在这种环境中，高质量的单语语料库几乎不可用。]]></description>
      <guid>https://arxiv.org/abs/2411.02460</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过稀疏激活控制增强 LLM 中的多维度可信度</title>
      <link>https://arxiv.org/abs/2411.02461</link>
      <description><![CDATA[arXiv:2411.02461v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的开发和应用继续快速发展，提高其可信度并使其与人类偏好保持一致已成为一个关键的研究领域。传统方法严重依赖于大量数据来进行人类反馈强化学习 (RLHF)，但表示工程提供了一种新的、无需训练的方法。该技术利用语义特征来控制 LLM 中间隐藏状态的表示，使模型能够满足特定要求，例如提高诚实度或提高安全意识。然而，当试图同时满足多个要求时，就会出现一个重大挑战。事实证明，将各种语义内容（如诚实和安全）编码为单一的语义特征非常困难，这限制了它的实用性。在这项工作中，我们通过“稀疏激活控制”解决了这个问题。通过深入研究 LLM 的内在机制，我们设法识别并确定与模型内特定任务密切相关的组件，即注意力头。这些头部表现出稀疏特性，可以近乎独立地控制不同的任务。我们在开源 Llama 系列模型上进行的实验取得了令人鼓舞的结果。这些模型能够同时与人类在安全性、事实性和偏见问题上的偏好保持一致。]]></description>
      <guid>https://arxiv.org/abs/2411.02461</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>金融文本分类法学硕士 (LLM) 教学微调的比较分析</title>
      <link>https://arxiv.org/abs/2411.02476</link>
      <description><![CDATA[arXiv:2411.02476v1 公告类型：新
摘要：大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出令人印象深刻的能力，包括语言理解、推理和生成。然而，由于金融文本的技术性和专业性，通用领域的 LLM 通常在金融任务中遇到困难。本研究调查了指令微调小规模 LLM（包括 Mistral-7B、Llama3-8B 和 Phi3-mini）的有效性，以提高它们在金融文本分类任务中的性能。我们在四个金融分类任务中对指令调整和基础模型进行了微调，在任务特定性能方面取得了显着的改进。此外，我们在三个看不见的复杂金融任务上评估了这些微调模型的零样本能力，包括参数分类、交易完整性分类和因果分类。我们的结果表明，虽然基础模型微调导致更大的退化，但指令调整模型保持了更稳健的性能。为了解决这一问题，我们采用了模型合并技术，将单任务领域特定微调模型与基础模型集成。使用这种合并方法可以显著提高零样本性能，甚至在某些数据集上超过了原始模型的准确率。我们的研究结果强调了指令微调和模型合并对于将 LLM 适应专门的金融文本分类任务的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.02476</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>适合偏好数据注释的优秀法学硕士课程以及如何（不）找到它们</title>
      <link>https://arxiv.org/abs/2411.02481</link>
      <description><![CDATA[arXiv:2411.02481v1 公告类型：新
摘要：大型语言模型 (LLM) 的偏好调整依赖于高质量的人类偏好数据，而这些数据的收集通常既昂贵又耗时。虽然现有方法可以使用训练有素的奖励模型或专有模型作为偏好注释的判断标准，但它们存在明显的缺点：训练奖励模型仍然依赖于初始人类数据，而使用专有模型会施加许可限制，从而抑制商业使用。在本文中，我们引入了定制密度比 (CDR)，利用开源 LLM 进行数据注释，提供了一种可访问且有效的解决方案。我们的方法使用对齐良好的 LLM 和对齐程度较低的 LLM 之间的对数密度比作为奖励信号。我们探索了 221 种不同的 LLM 对，并通过经验证明，成对的 LLM 之间的性能差距增加与更好的奖励泛化相关。此外，我们表明，使用特定标准和偏好示例定制密度比奖励函数可以提高跨领域和目标区域内的性能。
在我们使用一对 Mistral-7B 模型的密度比进行的实验中，CDR 的 RewardBench 得分为 82.6，优于同类最佳的训练奖励函数，并且在安全 (91.0) 和推理 (88.0) 领域表现出与 SoTA 模型相当的竞争力。我们使用 CDR 注释一个基于策略的偏好数据集，我们利用该数据集使用 SimPO 对 Llama-3-8B-Instruct 进行偏好调整。最终模型在 ArenaHard 上的胜率为 37.4% (+15.1%)，在 Length-Controlled AlpacaEval 2.0 上的胜率为 40.7% (+17.8%)，在 MT-Bench 上的得分为 8.0。]]></description>
      <guid>https://arxiv.org/abs/2411.02481</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>