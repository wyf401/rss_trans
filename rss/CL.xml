<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 19 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>角色扮演是一把双刃剑：通过整合角色扮演和中性提示来增强零样本推理</title>
      <link>https://arxiv.org/abs/2408.08631</link>
      <description><![CDATA[arXiv:2408.08631v1 公告类型：新 
摘要：最近的研究表明，为 LLM 提示合适的角色扮演角色可以提高其推理能力。然而，分配合适的角色很困难，因为 LLM 的表现对分配的提示极为敏感；因此，角色有时会妨碍 LLM 并降低其推理能力。在本文中，我们提出了一个新颖的框架 Jekyll \&amp; Hyde，它集成了角色扮演和中性提示的结果，以消除通过单方面使用角色扮演提示的 LLM 导致的性能下降并增强 LLM 推理能力的稳健性。具体而言，Jekyll \&amp; Hyde 从角色扮演和中性提示中收集两个潜在解决方案，并通过 LLM 评估器进行交叉检查后选择一个更好的解决方案。然而，基于 LLM 的评估者在选择合适的解决方案时往往会受到提示中这些潜在解决方案的顺序的影响；因此，我们还提出了一个强大的 LLM 评估器来减轻位置偏差。实验分析表明，即使使用 GPT-4，角色扮演提示也会分散 LLM 的注意力，并在 12 个数据集中的 4 个数据集中降低其推理能力。此外，我们发现 Jekyll \&amp; Hyde 通过在 12 个广泛使用的推理数据集上的潜在解决方案中选择更好的选择来提高推理能力。我们进一步表明，我们提出的 LLM 评估器优于其他基线，证明 LLM 的位置偏差得到成功缓解。]]></description>
      <guid>https://arxiv.org/abs/2408.08631</guid>
      <pubDate>Mon, 19 Aug 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>自回归语言模型中三段论推理的机械解释</title>
      <link>https://arxiv.org/abs/2408.08590</link>
      <description><![CDATA[arXiv:2408.08590v1 公告类型：新
摘要：最近关于自回归语言模型 (LM) 中的逻辑推理的研究引发了一场争论，即此类模型是否可以在预训练期间学习系统推理原理，还是仅仅利用训练数据中的表面模式。本文提出了 LM 中三段论推理的机械解释，以进一步增强我们对内部动态的理解。具体来说，我们提出了一种电路发现方法，旨在将内容独立的推理机制与预训练期间获得的世界知识区分开来。通过两种不同的干预方法，我们发现了一个涉及中期抑制的充分和必要电路，阐明了 LM 如何传递信息以从前提中得出有效结论。此外，我们研究了信念偏见如何在三段论推理中表现出来，发现了来自负责编码常识和情境化知识的额外注意力头的部分污染证据。最后，我们探索了所发现的机制在各种三段论方案和模型大小中的泛化，发现所识别的电路对于模型实现高下游准确度（$\geq$ 60\%）的所有方案都是充分且必要的。总体而言，我们的研究结果表明，LM 确实学习了可转移的独立于内容的推理机制，但与此同时，这种机制不涉及可泛化和抽象的逻辑原语，容易受到在预训练期间获得的相同世界知识的污染。]]></description>
      <guid>https://arxiv.org/abs/2408.08590</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>RealMedQA：包含现实临床问题的试点生物医学问答数据集</title>
      <link>https://arxiv.org/abs/2408.08624</link>
      <description><![CDATA[arXiv:2408.08624v1 公告类型：新
摘要：临床问答系统有可能为临床医生提供相关且及时的答案。尽管取得了进展，但这些系统在临床环境中的采用仍然很缓慢。一个问题是缺乏反映医疗专业人员现实需求的问答数据集。在这项工作中，我们介绍了 RealMedQA，这是一个由人类和 LLM 生成的真实临床问题的数据集。我们描述了生成和验证 QA 对的过程，并评估了 BioASQ 和 RealMedQA 上的几个 QA 模型，以评估将答案与问题匹配的相对难度。我们表明 LLM 在生成“理想”QA 对方面更具成本效益。此外，根据结果，我们在问题和答案之间实现了比 BioASQ 更低的词汇相似度，这给前两个 QA 模型带来了额外的挑战。我们公开发布我们的代码和数据集以鼓励进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2408.08624</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>集成多视角分析：用于文本个性检测的多视角混合专家</title>
      <link>https://arxiv.org/abs/2408.08551</link>
      <description><![CDATA[arXiv:2408.08551v1 公告类型：新
摘要：文本个性检测旨在通过分析用户生成的内容来识别个性特征。为了有效地实现这一目标，必须从各个角度彻底检查用户生成的内容。然而，以前的研究一直在努力自动提取和有效地整合来自多个角度的信息，从而限制了它们在个性检测方面的表现。为了应对这些挑战，我们提出了用于文本个性检测的多视图混合专家模型 (MvP)。MvP 引入了一个多视图混合专家 (MoE) 网络，可以从各个角度自动分析用户帖子。此外，它采用用户一致性正则化来缓解不同观点之间的冲突并学习多视图通用用户表示。该模型的训练通过多任务联合学习策略进行优化，该策略平衡了监督个性检测和自监督用户一致性约束。在两个广泛使用的性格检测数据集上的实验结果证明了 MvP 模型的有效性以及从不同角度自动分析用户帖子进行文本性格检测的好处。]]></description>
      <guid>https://arxiv.org/abs/2408.08551</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>BioLaySumm 2024 生物医学研究论文通俗摘要共享任务概述</title>
      <link>https://arxiv.org/abs/2408.08566</link>
      <description><![CDATA[arXiv:2408.08566v1 公告类型：新 
摘要：本文介绍了在 ACL 2024 的 BioNLP 研讨会上举办的 BioLaySumm 共享任务“生物医学研究文章的平庸总结”的第二版的设置和结果。在此任务版本中，我们旨在通过进一步增加对这一重要任务的研究兴趣并鼓励参与者探索有助于推动最先进技术的新方法，在第一版成功的基础上再接再厉。令人鼓舞的是，我们发现对这项任务的研究兴趣很高，本版任务共吸引了 53 个参与团队，参与度比上一版显着提高。总体而言，我们的结果表明，任务参与者采用了广泛的创新方法，并且可以预见地转向使用大型语言模型 (LLM)。]]></description>
      <guid>https://arxiv.org/abs/2408.08566</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>SelectLLM：面向大型语言模型的查询感知高效选择算法</title>
      <link>https://arxiv.org/abs/2408.08545</link>
      <description><![CDATA[arXiv:2408.08545v1 公告类型：新
摘要：大型语言模型 (LLM) 因其在各种任务中取得的显著成功而越来越受欢迎，这导致了大量不同 LLM 的积极开发。然而，由于训练偏差、模型大小和使用的数据集等因素，单个 LLM 在应用于复杂任务时存在局限性。一种有前途的方法是有效地利用 LLM 的多样化功能来克服这些个体限制。为了实现这一目标，我们引入了一种名为 SelectLLM 的新型 LLM 选择算法。该算法将输入查询从大型池中引导到最合适的 LLM 子集，确保它们共同有效地提供正确的响应。SelectLLM 使用多标签分类器，利用分类器的预测和置信度分数来设计最佳策略，以选择最佳、查询感知和轻量级的 LLM 子集。我们的研究结果表明，与规模相似、计算成本高昂的顶级 LLM 子集相比，所提出的模型优于单个 LLM，并且实现了具有竞争力的性能。具体而言，使用规模相似的顶级 LLM 子集，我们在两个标准推理基准上实现了显著的延迟降低：GSM8K 的延迟降低了 13%，MMLU 的延迟降低了 70%。此外，我们还进行了全面的分析和消融研究，验证了所提出的模型的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2408.08545</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>CommunityKG-RAG：利用知识图谱中的社区结构进行事实核查中的高级检索增强生成</title>
      <link>https://arxiv.org/abs/2408.08535</link>
      <description><![CDATA[arXiv:2408.08535v1 公告类型：新 
摘要：尽管大型语言模型 (LLM) 和检索增强生成 (RAG) 系统取得了进步，但它们的有效性往往受到与实体关系和社区结构缺乏整合的阻碍，限制了它们为事实核查提供丰富而准确的信息检索的能力。我们引入了 CommunityKG-RAG（社区知识图谱检索增强生成），这是一种新颖的零样本框架，它将知识图谱 (KG) 中的社区结构与 RAG 系统相结合，以增强事实核查过程。CommunityKG-RAG 能够适应新领域和查询而无需额外培训，它利用 KG 内社区结构的多跳特性显着提高信息检索的准确性和相关性。我们的实验结果表明，CommunityKG-RAG 优于传统方法，通过提供强大、可扩展且高效的解决方案，代表了事实核查的重大进步。]]></description>
      <guid>https://arxiv.org/abs/2408.08535</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:56 GMT</pubDate>
    </item>
    <item>
      <title>标记化空间中的信号在哪里？</title>
      <link>https://arxiv.org/abs/2408.08541</link>
      <description><![CDATA[arXiv:2408.08541v1 公告类型：新
摘要：大型语言模型 (LLM) 通常附带标记器，标记器将文本确定性地编码为所谓的规范标记序列，LLM 为这些序列分配概率值。一个常见的假设是，一段文本的概率是其规范标记序列的概率。但是，字符串的标记化并不是唯一的：例如，Llama2 标记器将标记编码为 [Tok,ens]，但 [Tok,en,s] 也代表相同的文本。在本文中，我们研究非规范标记化。我们证明，给定一个字符串，在计算上很难找到自回归 LLM 最可能的标记化，也很难计算所有可能标记化的边际概率。然后我们展示了在大多数情况下，边际概率与规范概率是无法区分的。令人惊讶的是，我们随后通过经验证明了标记空间中隐藏着大量信号。值得注意的是，通过简单地汇总非规范标记的概率，我们实现了一系列 LLM 评估基准的改进，适用于各种架构，包括变压器和状态空间模型。]]></description>
      <guid>https://arxiv.org/abs/2408.08541</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:56 GMT</pubDate>
    </item>
    <item>
      <title>JPEG-LM：具有规范编解码器表示的图像生成器 LLM</title>
      <link>https://arxiv.org/abs/2408.08459</link>
      <description><![CDATA[arXiv:2408.08459v1 公告类型：新
摘要：图像和视频生成领域的最新研究一直采用自回归 LLM 架构，因为它具有通用性，并且可能易于集成到多模态系统中。将语言生成中的自回归训练应用于视觉生成的关键是离散化——将图像和视频等连续数据表示为离散标记。图像和视频离散化的常用方法包括对原始像素值进行建模（这些值非常长）或矢量量化（这需要复杂的预处理训练）。在这项工作中，我们建议直接将图像和视频建模为通过规范编解码器（例如 JPEG、AVC/H.264）保存在计算机上的压缩文件。使用默认的 Llama 架构（不进行任何视觉特定修改），我们从头开始预训练 JPEG-LM 以生成图像（并使用 AVC-LM 生成视频作为概念证明），通过直接输出 JPEG 和 AVC 格式的压缩文件字节。对图像生成的评估表明，这种简单直接的方法比基于像素的建模和复杂的矢量量化基线更有效（我们的方法使 FID 降低了 31%）。我们的分析表明，JPEG-LM 在生成长尾视觉元素方面比矢量量化模型具有特殊优势。总的来说，我们表明使用规范的编解码器表示可以帮助降低语言生成和视觉生成之间的障碍，促进未来对多模态语言/图像/视频 LLM 的研究。]]></description>
      <guid>https://arxiv.org/abs/2408.08459</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>例3：通过提取、精益求精和扩展自动写小说</title>
      <link>https://arxiv.org/abs/2408.08506</link>
      <description><![CDATA[arXiv:2408.08506v1 Announce Type: new 
摘要：利用人工智能生成小说等长篇文本一直是一个难题，常用的方法是利用大型语言模型（LLM）构建先规划后写作的层次化框架，生成的小说虽然篇幅足够长，但情节的逻辑连贯性和感染力较差，人物和事件刻画不足，影响了整体的叙事质量。本文提出了一种提取精益求精和扩展的方法。Ex3首先从原始小说数据中提取结构信息，结合结构信息和小说数据，精心制作一个指令跟随数据集，然后利用该数据集对LLM进行微调，以达到卓越的生成性能。最后采用树状扩展方法，实现任意长度的小说生成。与之前的方法相比，评估表明 Ex3 有能力创作更高质量的长篇小说。]]></description>
      <guid>https://arxiv.org/abs/2408.08506</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>W-RAG：面向开放域问答的 RAG 弱监督密集检索</title>
      <link>https://arxiv.org/abs/2408.08444</link>
      <description><![CDATA[arXiv:2408.08444v1 公告类型：新
摘要：在开放域问答 (OpenQA) 等知识密集型任务中，大型语言模型 (LLM) 通常难以仅依靠其内部（参数）知识来生成事实答案。为了解决这一限制，检索增强生成 (RAG) 系统通过从外部来源检索相关信息来增强 LLM，从而将检索器定位为关键组件。尽管密集检索表现出最先进的性能，但由于缺乏基本事实证据，其训练面临挑战，这主要归因于人工注释的高成本。在本文中，我们提出了 W-RAG，利用 LLM 的排名功能来创建弱标记数据以训练密集检索器。具体来说，我们通过评估 LLM 根据问题和每个段落生成正确答案的概率，对通过 BM25 检索到的前 $K$ 个段落进行重新排序。然后将排名最高的段落用作密集检索的正面训练示例。我们在四个公开的 OpenQA 数据集上进行的全面实验表明，与基线模型相比，我们的方法提高了检索和 OpenQA 性能。]]></description>
      <guid>https://arxiv.org/abs/2408.08444</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:54 GMT</pubDate>
    </item>
    <item>
      <title>零样本学习和关键点是自动事实核查所需的全部内容</title>
      <link>https://arxiv.org/abs/2408.08400</link>
      <description><![CDATA[arXiv:2408.08400v1 公告类型：新
摘要：自动事实核查是一项重要任务，因为在网上大量信息中确定所提主张的准确状态是一项关键挑战。这一挑战需要进行强有力的评估，以防止虚假信息的传播。现代大型语言模型 (LLM) 已证明在执行各种自然语言处理 (NLP) 任务方面具有很高的能力。通过利用适当的提示策略，它们由于理解大上下文大小和零样本学习能力而具有的多功能性使它们能够模拟人类解决问题的直觉，并朝着成为人类解决问题的替代品的方向发展。在这项工作中，我们引入了一个基于零样本学习和关键点 (ZSL-KeP) 的简单框架，用于自动事实核查，尽管它很简单，但在 AVeriTeC 共享任务数据集上表现良好，通过稳健地改进基线并取得第 10 名。]]></description>
      <guid>https://arxiv.org/abs/2408.08400</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>从替代视角看评分者凝聚力和质量</title>
      <link>https://arxiv.org/abs/2408.08411</link>
      <description><![CDATA[arXiv:2408.08411v1 公告类型：新
摘要：在存在分歧的领域（例如 AI 安全、内容审核或情绪分析），人类反馈对于构建以人为中心的 AI 系统至关重要。许多分歧，特别是在政治气氛浓厚的环境中，是因为评估者有相反的价值观或信仰而产生的。替代注释是一种通过询问评估者他们认为其他人会如何注释数据来打破分歧的方法。在本文中，我们探讨了使用替代注释和分析方法来缓和评估者分歧。我们使用评估者凝聚力指标来研究政治派别和人口统计背景对评估者犯罪看法的潜在影响。此外，我们利用 CrowdTruth 的评估者质量指标（该指标考虑了评估者的人口统计数据）来对评估者及其注释进行评分。我们研究评估者质量指标如何影响个人和替代层面的群体内和跨群体评估者凝聚力。]]></description>
      <guid>https://arxiv.org/abs/2408.08411</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>评估文本分类对词性对抗样本的鲁棒性</title>
      <link>https://arxiv.org/abs/2408.08374</link>
      <description><![CDATA[arXiv:2408.08374v1 公告类型：新
摘要：随着机器学习系统得到越来越广泛的应用，尤其是对于安全关键型应用，越来越需要确保这些系统能够按预期运行，即使面对对抗性示例也是如此。对抗性示例是旨在欺骗决策过程的输入，旨在使人类无法察觉。但是，对于基于文本的分类系统，对输入（一串文本）的更改始终是可以感知的。因此，基于文本的对抗性示例反而专注于尝试保留语义。不幸的是，最近的研究表明，这一目标往往无法实现。为了提高基于文本的对抗性示例的质量，我们需要知道输入文本的哪些元素值得关注。为了解决这个问题，在本文中，我们探讨了哪些词类对基于文本的分类器的影响最大。我们的实验强调了 CNN 算法对评论数据集中某些词类标记的明显偏见。这一发现凸显了 CNN 语言处理能力的严重脆弱性。]]></description>
      <guid>https://arxiv.org/abs/2408.08374</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:52 GMT</pubDate>
    </item>
    <item>
      <title>迈向逼真的合成用户生成内容：生成在线讨论的支架方法</title>
      <link>https://arxiv.org/abs/2408.08379</link>
      <description><![CDATA[arXiv:2408.08379v1 公告类型：新
摘要：合成数据的出现代表了现代机器学习的一个关键转变，它提供了一种解决方案来满足在真实数据稀缺、高度私密或难以获得的领域对大量数据的需求。我们研究了创建真实的、大规模的用户生成内容合成数据集的可行性，并注意到此类内容越来越普遍，并且是经常被搜索的信息来源。大型语言模型 (LLM) 为生成合成社交媒体讨论线程提供了一个起点，因为它们能够产生代表在线互动的多样化响应。然而，正如我们所展示的，直接应用 LLM 在捕捉在线讨论的复杂结构方面取得的成功有限，而标准的提示机制缺乏足够的控制。因此，我们提出了一个多步骤生成过程，该过程基于创建讨论线程的紧凑表示（称为支架）的想法。我们的框架是通用的，但可以适应特定社交媒体平台的独特特征。我们使用来自两个不同在线讨论平台的数据证明了其可行性。为了解决确保合成数据的代表性和真实性这一根本挑战，我们提出了一系列评估措施来比较我们框架的各种实例。]]></description>
      <guid>https://arxiv.org/abs/2408.08379</guid>
      <pubDate>Mon, 19 Aug 2024 06:20:52 GMT</pubDate>
    </item>
    </channel>
</rss>