<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 07 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>大型语言模型时代的小型语言模型综合调查：技术、增强、应用、与 LLM 的协作和可信度</title>
      <link>https://arxiv.org/abs/2411.03350</link>
      <description><![CDATA[arXiv:2411.03350v1 公告类型：新
摘要：大型语言模型 (LLM) 已在文本生成、问答和推理方面展现出新兴能力，有助于完成各种任务和领域。尽管它们在各种任务中都表现出色，但像 LaPM 540B 和 Llama-3.1 405B 这样的 LLM 因参数大小大和计算需求大而面临限制，通常需要使用云 API，这会引发隐私问题，限制边缘设备上的实时应用，并增加微调成本。此外，由于缺乏领域特定知识，LLM 通常在医疗保健和法律等专业领域表现不佳，需要专门的模型。因此，小型语言模型 (SLM) 因其推理延迟低、成本效益高、开发高效、易于定制和适应性而越来越受到青睐。这些模型特别适合资源有限的环境和领域知识获取，解决了 LLM 的挑战，并被证明是需要本地化数据处理以保护隐私、最小化推理延迟以提高效率以及通过轻量级微调获取领域知识的应用程序的理想选择。对 SLM 的不断增长的需求刺激了广泛的研究和开发。然而，仍然缺乏对与 SLM 的定义、获取、应用、增强和可靠性相关的问题的全面调查，这促使我们对这些主题进行详细调查。SLM 的定义差异很大，因此为了标准化，我们建议根据 SLM 执行专门任务的能力和对资源受限环境的适用性来定义 SLM，根据新兴能力的最小规模和资源约束下可持续的最大规模设定边界。对于其他方面，我们提供了相关模型/方法的分类，并为每个类别开发了通用框架，以有效地增强和利用 SLM。]]></description>
      <guid>https://arxiv.org/abs/2411.03350</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAUCE：用于多代理 LLM 交互的同步和异步用户可定制环境</title>
      <link>https://arxiv.org/abs/2411.03397</link>
      <description><![CDATA[arXiv:2411.03397v1 公告类型：新
摘要：许多人际互动，例如政治辩论，都是在群体环境中进行的，其中有任意多的参与者，每个参与者都有不同的观点和议程。为了探索这种复杂的社交环境，我们提出了 SAUCE：一个可定制的 Python 平台，允许研究人员即插即用各种 LLM 参与用户选择的任何主题的讨论。我们的平台负责实例化模型、安排他们的响应、管理讨论历史以及生成全面的输出日志，所有这些都可以通过配置文件进行定制，几乎不需要任何编码技能。SAUCE 的一个新功能是我们的异步通信功能，其中模型除了决定说什么之外还决定何时说话，从而模拟人类交流的一个重要方面。我们在两个初步实验中展示了 SAUCE 的吸引力，并邀请社区使用它来模拟各种群体模拟。]]></description>
      <guid>https://arxiv.org/abs/2411.03397</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为科学论文作者清单助手的实用性：NeurIPS'24 实验</title>
      <link>https://arxiv.org/abs/2411.03417</link>
      <description><![CDATA[arXiv:2411.03417v1 公告类型：新
摘要：大型语言模型 (LLM) 是一种有前途但有争议的辅助科学同行评审的工具。本研究评估了 LLM 在会议环境中作为根据提交标准审查论文提交的工具的实用性。我们在 2024 年神经信息处理系统 (NeurIPS) 会议上进行了一项实验，其中 234 篇论文自愿提交给“基于 LLM 的检查表助手”。该助手验证论文是否遵守 NeurIPS 使用的作者清单，其中包括确保符合研究和手稿准备标准的问题。NeurIPS 论文作者对助手的评估表明，基于 LLM 的助手通常有助于验证清单的完成情况。在使用后调查中，超过 70% 的作者认为该助手很有用，70% 的作者表示他们会根据其反馈修改论文或清单回复。虽然不能确定助理是否是导致这一结果的原因，但定性证据表明，LLM 有助于改进一些提交内容。调查回复和重新提交的分析表明，作者根据 LLM 的具体反馈对其提交内容进行了实质性修改。实验还强调了 LLM 的常见问题：不准确（20/52）和过于严格（14/52）是作者最常提到的问题。我们还进行了实验，以了解系统的潜在游戏性，结果表明，可以通过捏造的理由操纵助理来提高分数，这凸显了自动审查工具的潜在漏洞。]]></description>
      <guid>https://arxiv.org/abs/2411.03417</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>教育技术中使用大型语言模型自动生成数学问题提示</title>
      <link>https://arxiv.org/abs/2411.03495</link>
      <description><![CDATA[arXiv:2411.03495v1 公告类型：新 
摘要：智能辅导系统 (ITS) 中的大型语言模型 (LLM) 自动生成提示已显示出提高学生学习的潜力。然而，生成合理的教学提示以解决学生的误解并坚持特定的教育目标仍然具有挑战性。这项工作探索使用 LLM（GPT-4o 和 Llama-3-8B-instruct）作为教师，为通过 LLM（GPT-3.5-turbo、Llama-3-8B-Instruct 或 Mistral-7B-instruct-v0.3）模拟的学生生成有效的提示，解决为人类高中生设计的数学练习，并使用认知科学原理设计。我们在这里介绍了几个维度的研究：1）识别模拟学生在中学数学练习中的错误模式； 2) 为 GPT-4o 作为老师开发各种提示，并评估它们在生成提示方面的有效性，使模拟学生能够自我纠正；3) 以 Llama-3-8B-Instruct 作为老师，根据其产生相关提示和促进错误纠正的能力，测试表现最佳的提示，以便与 GPT-4o 进行性能比较。结果表明，模型错误会随着温度设置的升高而增加。值得注意的是，当 GPT-4o 生成提示时，最有效的提示包括针对特定错误定制的提示以及基于常见数学错误提供一般提示的提示。有趣的是，Llama-3-8B-Instruct 作为老师的整体表现优于 GPT-4o。此外，LLM 作为学生，尤其是 GPT-3.5-turbo，在收到提示后，解决问题和响应修订能力显著提高，尤其是在较低温度设置下。然而，像 Mistral-7B-Instruct 这样的模型表现出随着温度升高而下降的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.03495</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用（大型）语言模型对临床结果预测的不确定性进行量化</title>
      <link>https://arxiv.org/abs/2411.03497</link>
      <description><![CDATA[arXiv:2411.03497v1 公告类型：新
摘要：为了促进医疗保健服务，语言模型 (LM) 在使用电子健康记录 (EHR) 的临床预测任务中具有巨大潜力。然而，在这些高风险的应用中，不可靠的决策可能会因患者安全和道德问题受损而导致高昂的成本，从而增加了对自动临床预测的良好不确定性建模的需求。为了解决这个问题，我们考虑在白盒和黑盒环境中对 EHR 任务的 LM 的不确定性进行量化。我们首先量化白盒模型中的不确定性，我们可以在其中访问模型参数和输出对数。我们表明，通过在 EHR 中使用提出的多任务和集成方法可以有效减少模型不确定性。继续这个想法，我们将我们的方法扩展到黑盒设置，包括流行的专有 LM，例如 GPT-4。我们使用来自 10 个临床预测任务中 6,000 多名患者的纵向临床数据来验证我们的框架。结果表明，集成方法和多任务预测提示可减少不同场景中的不确定性。这些发现提高了模型在白盒和黑盒设置中的透明度，从而推动了可靠的 AI 医疗保健。]]></description>
      <guid>https://arxiv.org/abs/2411.03497</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>变化是唯一不变的：基于层冗余的动态 LLM 切片</title>
      <link>https://arxiv.org/abs/2411.03513</link>
      <description><![CDATA[arXiv:2411.03513v1 公告类型：新
摘要：本文介绍了一种通过大型语言模型 (LLM) 中的动态层特定修剪实现的新型模型压缩方法，增强了 SliceGPT 建立的传统方法。通过从恒定切片过渡到动态切片，我们的方法利用了新提出的层冗余 (LR) 分数，该分数通过测量输入与层输出的余弦相似性来评估每层对其输入的改变程度。我们使用此分数根据冗余度修剪各个层的部分，以使所有层的平均修剪百分比为固定值。我们在多个数据集上使用 Llama3-8B 和 Mistral-7B 等模型进行了广泛的实验，评估了不同的切片基础和百分比，以确定平衡效率和性能的最佳配置。我们的研究结果表明，与恒定切片方法建立的基线相比，我们的动态切片方法不仅保持了模型性能，而且在许多情况下提高了模型性能。例如，在多种设置中，我们看到性能比 SliceGPT 基线提高了 5%。此外，在多个基准测试中观察到困惑度降低了 7%，这证明了我们方法的有效性。代码、模型权重和数据集在 https://github.com/RazvanDu/DynamicSlicing 上开源。]]></description>
      <guid>https://arxiv.org/abs/2411.03513</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减轻最小贝叶斯风险解码中的度量偏差</title>
      <link>https://arxiv.org/abs/2411.03524</link>
      <description><![CDATA[arXiv:2411.03524v1 公告类型：新
摘要：虽然使用 COMET 或 MetricX 等指标的最小贝叶斯风险 (MBR) 解码优于贪婪或波束搜索等传统解码方法，但它引入了我们称为指标偏差的挑战。由于 MBR 解码旨在根据特定效用指标生成高分翻译，因此这一过程使得无法使用相同的指标进行解码和评估，因为改进可能仅仅是由于奖励黑客攻击，而不是反映真正的质量改进。在这项工作中，我们发现与人类评分相比，神经指标不仅在使用相同指标作为效用指标时高估了 MBR 解码的质量，而且还高估了使用其他神经效用指标的 MBR/QE 解码的质量。我们还表明，通过在 MBR 解码期间使用一组效用指标可以缓解度量偏差问题：人工评估表明，使用一组效用指标的 MBR 解码优于单一效用指标。]]></description>
      <guid>https://arxiv.org/abs/2411.03524</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索生成大型语言模型领域预训练对化学领域的好处</title>
      <link>https://arxiv.org/abs/2411.03542</link>
      <description><![CDATA[arXiv:2411.03542v1 公告类型：新
摘要：大型语言模型（GPT 系列、BLOOM、LLaMA 等）的激增正在推动多用途 AI 的新发展，用于各种任务，特别是自然语言处理 (NLP) 任务。这些模型在一系列任务上表现出色；然而，有证据表明，当应用于更小众或狭窄的领域时，它们会变得脆弱，在这些领域中，幻觉或流畅但不正确的反应会降低性能。鉴于科学领域的复杂性，明智的做法是研究利用现成的模型与更有针对性的基础模型在科学领域的权衡。在这项工作中，我们研究了域内预训练对给定科学领域（化学）的好处，并将它们与具有零样本和少量样本提示的开源现成模型进行比较。我们的结果表明，域内基础模型不仅在零样本设置的域内任务上表现得相当好，而且使用指令微调进行进一步调整可以在命名实体识别和分子式生成等化学特定任务上获得令人印象深刻的性能。]]></description>
      <guid>https://arxiv.org/abs/2411.03542</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学会理性写作：非母语人士文章中的信息如何分布</title>
      <link>https://arxiv.org/abs/2411.03550</link>
      <description><![CDATA[arXiv:2411.03550v1 公告类型：新
摘要：为了更好、更清晰地沟通，人们倾向于在语言表达中均匀分布信息。在本研究中，我们比较了具有不同母语（L1）背景的第二语言学习者所写的文章，以研究他们如何在非母语（L2）表达中分布信息。对意外性和熵率恒定性的分析表明，具有较高 L2 水平的作者可以降低语言表达的预期不确定性，同时仍能传达信息内容。然而，信息分布的均匀性在不同 L2 说话者群体中表现出较小的差异，这表明这一特征在 L2 文章写作中可能是普遍存在的，并且较少受到 L2 作者在 L1 背景和 L2 水平上的差异的影响。]]></description>
      <guid>https://arxiv.org/abs/2411.03550</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>美国手语知识图谱：将语言知识融入 ASL 模型</title>
      <link>https://arxiv.org/abs/2411.03568</link>
      <description><![CDATA[arXiv:2411.03568v1 公告类型：新
摘要：美国手语 (ASL) 的语言模型可以让那些手语使用者更容易获得语言技术。为了在孤立手势识别 (ISR) 和 ASL 到英语翻译等任务上训练模型，数据集提供了带注释的 ASL 手势视频示例。为了促进这些模型的通用性和可解释性，我们引入了美国手语知识图谱 (ASLKG)，它由十二个专家语言知识来源汇编而成。我们使用 ASLKG 训练神经符号模型以完成 3 个 ASL 理解任务，在 ISR 上实现了 91% 的准确率，在预测看不见的手势的语义特征方面实现了 14% 的准确率，在对 Youtube-ASL 视频主题进行分类方面实现了 36% 的准确率。]]></description>
      <guid>https://arxiv.org/abs/2411.03568</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从 Medprompt 到 o1：探索医学挑战问题及其他问题的运行时策略</title>
      <link>https://arxiv.org/abs/2411.03590</link>
      <description><![CDATA[arXiv:2411.03590v1 公告类型：新
摘要：像 Medprompt 这样的运行时指导策略对于指导大型语言模型 (LLM) 在具有挑战性的任务上取得最佳性能非常有价值。Medprompt 表明，通过使用提示来引出涉及思路推理和集成的运行时策略，通用 LLM 可以专注于在医学等专业领域提供最先进的性能。OpenAI 的 o1-preview 模型代表了一种新范式，其中模型旨在在生成最终响应之前进行运行时推理。我们试图了解 o1-preview 在各种医学挑战问题基准上的行为。继使用 GPT-4 进行 Medprompt 研究之后，我们在各种医学基准上系统地评估了 o1-preview 模型。值得注意的是，即使没有提示技术，o1-preview 的表现也远远优于使用 Medprompt 的 GPT-4 系列。我们进一步系统地研究了经典的提示工程策略（以 Medprompt 为代表）在新的推理模型范式中的有效性。我们发现，少量提示会阻碍 o1 的表现，这表明情境学习可能不再是推理原生模型的有效指导方法。虽然集成仍然可行，但它是资源密集型的，需要仔细的成本效益优化。我们对运行时策略的成本和准确性分析揭示了帕累托前沿，GPT-4o 代表了更实惠的选择，而 o1-preview 以更高的成本实现了最先进的性能。虽然 o1-preview 提供了顶级性能，但采用 Medprompt 等指导策略的 GPT-4o 在特定情况下仍具有价值。此外，我们注意到 o1-preview 模型在许多现有的医学基准上已经达到接近饱和，这凸显了对新的具有挑战性的基准的需求。最后，我们反思了使用 LLM 进行推理时间计算的一​​般方向。]]></description>
      <guid>https://arxiv.org/abs/2411.03590</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>部署具有大型语言模型的多任务在线服务器</title>
      <link>https://arxiv.org/abs/2411.03644</link>
      <description><![CDATA[arXiv:2411.03644v1 公告类型：新
摘要：在业界，许多任务都部署在线。传统方法通常通过自己的网络单独处理每个任务，这导致开发和扩展模型的成本过高，尤其是在大型语言模型的背景下。虽然多任务方法可以通过参数共享来节省成本，但它们在实际应用中往往难以超越单任务方法。为了应对这些挑战，我们提出了一个用于大型语言模型的三阶段多任务学习框架。它涉及任务过滤，然后对高资源任务进行微调，最后对所有任务进行微调。我们在单任务和多任务设置中进行了全面的实验。我们的方法在不同的基准上进行了示例，表明它能够实现与单任务方法相当的性能，同时将其开销降低高达 90.9% 。]]></description>
      <guid>https://arxiv.org/abs/2411.03644</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过多元化框架评估法学硕士的道德信仰</title>
      <link>https://arxiv.org/abs/2411.03665</link>
      <description><![CDATA[arXiv:2411.03665v1 公告类型：新
摘要：正确的道德信念是语言模型的基础，但评估这些信念是一项重大挑战。本研究引入了一个新颖的三模块框架来评估四个著名大型语言模型的道德信念。首先，我们构建了一个包含 472 个中文道德选择场景的数据集，这些场景源自道德词汇。这些场景中的模型的决策过程揭示了它们的道德原则偏好。通过对这些道德选择进行排序，我们可以辨别出不同语言模型持有的不同道德信念。此外，通过道德辩论，我们调查了这些模型对其道德选择的坚定性。我们的研究结果表明，英语语言模型 ChatGPT 和 Gemini 与中国大学生样本的道德决策非常相似，表现出对他们选择的强烈坚持和对个人主义道德信念的偏好。相比之下，Ernie 和 ChatGLM 等中国模型倾向于集体主义道德信念，在道德选择和辩论中表现出模糊性。本研究还揭示了所有受检语言模型的道德信念中都存在性别偏见。我们的方法提供了一种创新的方法来评估人工智能和人类智能中的道德信念，有助于比较不同文化之间的道德价值观。]]></description>
      <guid>https://arxiv.org/abs/2411.03665</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>QUILL：大型语言模型的引语生成增强</title>
      <link>https://arxiv.org/abs/2411.03675</link>
      <description><![CDATA[arXiv:2411.03675v1 公告类型：新
摘要：虽然大型语言模型 (LLM) 已经成为出色的写作助手，但它们在引语生成方面仍然举步维艰。这是因为它们要么在提供事实引语时产生幻觉，要么无法提供超出人类期望的引语。为了弥补这一差距，我们系统地研究了如何评估和提高 LLM 在引语生成任务中的表现。我们首先为引语生成任务建立了一个整体的自动评估系统，该系统由五个标准组成，每个标准都有相应的自动指标。为了提高 LLM 的引语生成能力，我们构建了一个范围广泛、维度丰富的双语知识库，包含多达 32,022 条引语。此外，在我们的标准的指导下，我们进一步设计了一个特定于引语的指标来对从知识库中检索到的引语进行重新排序。大量实验表明，我们的指标与人类偏好密切相关。现有的 LLM 很难生成所需的引文，但我们的引文知识库和重新排序指标有助于缩小这一差距。我们的数据集和代码可在 https://github.com/GraceXiaoo/QUILL 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2411.03675</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>根塑造果实：论一致语言模型中性别排斥危害的持续性</title>
      <link>https://arxiv.org/abs/2411.03700</link>
      <description><![CDATA[arXiv:2411.03700v1 公告类型：新
摘要：自然语言助手旨在为用户提供有用的响应，同时避免有害的输出，这主要通过与人类偏好保持一致来实现。然而，对于对齐技术是否会无意中延续甚至放大从其预先对齐的基础模型中继承的有害偏见，人们的理解有限。这个问题因流行的偏好微调模型中偏见评估基准的选择而加剧，这些模型主要关注占主导地位的社会类别，例如二元性别，从而限制了对影响代表性不足的群体的偏见的洞察。为了解决这一差距，我们以跨性别、非二元性别和其他性别多元化身份为中心，研究对齐程序如何与法学硕士中预先存在的性别多元化偏见相互作用。我们的主要贡献包括：1）全面调查领先的偏好微调法学硕士的偏见评估模式，强调性别多样性代表性方面的关键差距；2）系统评估 12 个模型中的性别多样性偏见，涵盖直接偏好优化 (DPO) 阶段，揭示流行偏见基准无法检测到的危害；3）灵活的框架，用于衡量适用于其他社会背景的隐性奖励信号中的有害偏见。我们的研究结果表明，与 DPO 对齐的模型对监督微调 (SFT) 特别敏感，并且可以从其基础模型中放大两种形式的现实世界性别多样性危害：污名化和性别非肯定语言。我们最后提出了针对 DPO 和更广泛的对齐实践的建议，倡导采用社区知情的偏见评估框架，以更有效地识别和解决法学硕士中代表性不足的危害。]]></description>
      <guid>https://arxiv.org/abs/2411.03700</guid>
      <pubDate>Thu, 07 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>