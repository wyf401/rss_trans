<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>语言模型能评估人类书写的文本吗？韩国学生教育写作案例研究</title>
      <link>https://arxiv.org/abs/2407.17022</link>
      <description><![CDATA[arXiv:2407.17022v1 公告类型：新
摘要：基于大型语言模型 (LLM) 的评估流程已证明其能够稳健地评估机器生成的文本。将这种方法扩展到评估人工书写的文本可以显著有益于教育环境，因为它可以提供直接反馈来提高写作技能，尽管这种应用并不简单。在本文中，我们研究了 LLM 是否可以有效地评估人工书写的文本以用于教育目的。我们从 32 名韩国学生那里收集了 15 种写作类型的 100 篇文本，并使用 GPT-4-Turbo 以语法、流畅性、连贯性、一致性和相关性为标准对其进行评估。我们的分析表明，LLM 评估者可以可靠地评估语法和流畅性，以及更客观的写作类型，尽管他们在其他标准和写作类型方面遇到困难。我们公开发布我们的数据集和反馈。]]></description>
      <guid>https://arxiv.org/abs/2407.17022</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>从语言模型的内部冲突到语境适应</title>
      <link>https://arxiv.org/abs/2407.17023</link>
      <description><![CDATA[arXiv:2407.17023v1 公告类型：新
摘要：知识密集型语言理解任务需要语言模型 (LM) 整合相关上下文，以减轻其固有的弱点，例如不完整或过时的知识。然而，研究表明，LM 通常会忽略提供的上下文，因为它可能与在预训练期间学习的现有 LM 记忆相冲突。此外，冲突的知识可能已经存在于 LM 的参数中，称为内存内冲突。现有研究仅孤立地研究了这两种类型的知识冲突。我们推测内存内冲突的程度反过来会影响 LM 对上下文内存冲突的处理。为了研究这一点，我们引入了 DYNAMICQA 数据集，其中包括具有时间动态性质的事实，其中事实可以随着时间频率的变化而变化，以及有争议的动态事实，这些事实可以根据观点而变化。 DYNAMICQA 是第一个包含现实世界知识冲突并提供背景来研究不同类型知识冲突之间联系的系统。借助建议的数据集，我们评估了使用不确定性来衡量记忆内冲突的效果，并引入了新颖的连贯说服 (CP) 分数来评估背景影响 LM 语义输出的能力。我们进行了广泛的实验，结果表明，相对于时间和有争议的事实，不太可能发生变化的静态事实更容易通过附加背景进行更新。]]></description>
      <guid>https://arxiv.org/abs/2407.17023</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>SAFETY-J：通过批评评估安全性</title>
      <link>https://arxiv.org/abs/2407.17075</link>
      <description><![CDATA[arXiv:2407.17075v2 公告类型：新
摘要：在内容生成中部署大型语言模型 (LLM) 引发了重大的安全问题，特别是关于内容评估的透明度和可解释性。当前的方法主要侧重于二元安全分类，缺乏详细批评的机制，限制了它们对模型改进和用户信任的效用。为了解决这些限制，我们推出了 SAFETY-J，这是一种基于批评判断的英语和中文双语生成安全评估器。SAFETY-J 利用包含各种对话和增强的查询-响应对的强大训练数据集来全面评估各种场景的安全性。我们建立了一个自动化的元评估基准，以最少的人为干预客观地评估批评的质量，促进可扩展和持续改进。此外，SAFETY-J 采用迭代偏好学习技术，根据元评估和批评动态细化安全评估。我们的评估表明，SAFETY-J 提供了更细致入微、更准确的安全评估，从而提高了复杂内容场景中的评论质量和预测可靠性。为了促进进一步的研究和应用，我们在 \url{https://github.com/GAIR-NLP/Safety-J} 开源了 SAFETY-J 的训练协议、数据集和代码。]]></description>
      <guid>https://arxiv.org/abs/2407.17075</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>迈向迁移学习：跨领域偏见缓解的实证证据</title>
      <link>https://arxiv.org/abs/2407.16951</link>
      <description><![CDATA[arXiv:2407.16951v1 公告类型：新
摘要：大型语言模型 (LLM) 通常会从大量训练语料库中继承偏见。传统的去偏见方法虽然在一定程度上有效，但并不能完全消除 LLM 中记忆的偏见和毒性。在本文中，我们研究了一种基于反学习的 LLM 去偏见方法，通过对针对少数群体的仇恨言论进行梯度上升，即最大限度地降低有偏见或有害内容的可能性。具体来说，我们提出了一种掩码语言建模反学习技术，它可以反学习文本中的有害部分。这种方法使 LLM 能够有选择地忘记和脱离有偏见和有害的内容。实验结果证明了我们的方法在减少偏见的同时保持语言建模能力的有效性。令人惊讶的是，结果还揭示了跨领域转移学习的意外潜力：消除一种偏见（例如性别）可能有助于减轻其他偏见（例如种族和宗教）。]]></description>
      <guid>https://arxiv.org/abs/2407.16951</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>将语言模型与文本反馈相结合</title>
      <link>https://arxiv.org/abs/2407.16970</link>
      <description><![CDATA[arXiv:2407.16970v1 公告类型：新
摘要：我们提出了 ALT（与文本反馈对齐），这是一种将语言模型与文本中表达的用户偏好对齐的方法。我们认为文本提供了更大的表现力，使用户能够提供比简单的比较偏好更丰富的反馈，这种更丰富的反馈可以带来更高效、更有效的对齐。ALT 通过根据文本反馈调整模型的生成来对齐模型。我们的方法完全依赖于语言建模技术，并且需要最少的超参数调整，但它仍然具有基于 RL 的对齐算法的主要优点，并且可以有效地从文本反馈中学习。我们探索了文本反馈在不同任务（例如毒性降低、总结和对话响应生成）中的有效性和效率。我们发现 ALT 在毒性降低任务中的表现优于 PPO，同时仅用 20% 的样本就能匹配其总结性能。我们还探讨了如何将 ALT 与现有 LLM 提供的反馈结合使用，其中我们探讨了提供受约束和不受约束的文本反馈的 LLM。我们还概述了未来将模型与自然语言反馈相结合的方向。]]></description>
      <guid>https://arxiv.org/abs/2407.16970</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>重温《谁是哈利·波特》：从因果干预的角度进行有针对性的忘却学习</title>
      <link>https://arxiv.org/abs/2407.16997</link>
      <description><![CDATA[arXiv:2407.16997v1 公告类型：新
摘要：本文研究了《谁是哈利波特》（WHP），这是一种开创性的但尚未得到充分理解的 LLM 反学习方法。我们分两个步骤进行探索。首先，我们引入了一个新的 LLM 目标反学习任务，给定一个反学习目标（例如，一个人）和一些反学习文档，我们的目标是只反学习有关目标的信息，而不是反学习文档中的所有内容。我们进一步认为，成功的反学习应该满足一些标准，例如不输出胡言乱语、不捏造关于反学习目标的事实、不在越狱攻击下泄露事实信息。其次，我们为目标反学习构建了一个因果干预框架，其中反学习目标的知识被建模为 LLM 输入和输出之间的混杂因素，反学习过程被建模为去混杂过程。该框架证明并扩展了 WHP，推导出一种简单的反学习算法，其中包括 WHP 作为特例。在现有和新数据集上进行的实验表明，我们的方法无需针对上述标准进行明确优化，在所有数据集上都实现了具有竞争力的性能。我们的代码可在 https://github.com/UCSB-NLP-Chang/causal_unlearn.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.16997</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>揭示情境学习：理解其工作机制的坐标系统</title>
      <link>https://arxiv.org/abs/2407.17011</link>
      <description><![CDATA[arXiv:2407.17011v1 公告类型：新
摘要：大型语言模型（LLM）表现出卓越的上下文学习（ICL）能力。然而，ICL 的底层工作机制仍然不太清楚。最近的研究对 ICL 提出了两种相互矛盾的观点：一种将其归因于 LLM 固有的任务识别能力，认为标签正确性和演示的镜头数量并不重要；另一种则强调演示中类似示例的影响，强调标签正确性和更多镜头的必要性。在这项工作中，我们提供了一个二维坐标系，将这两种观点统一为一个系统框架。该框架通过两个正交变量来解释 ICL 的行为：LLM 是否可以识别任务以及演示中是否呈现了类似的例子。我们提出了峰值逆秩度量来检测 LLM 的任务识别能力，并研究 LLM 对不同相似性定义的反应。在此基础上，我们进行了大量实验，以阐明 ICL 在多个代表性分类任务中如何在每个象限中发挥作用。最后，我们将分析扩展到生成任务，表明我们的坐标系也可用于有效地解释生成任务的 ICL。]]></description>
      <guid>https://arxiv.org/abs/2407.17011</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>限制生成比例可以减轻幻觉</title>
      <link>https://arxiv.org/abs/2407.16908</link>
      <description><![CDATA[arXiv:2407.16908v1 公告类型：新
摘要：解决大型语言模型 (LLM) 中的幻觉问题是一项关键挑战。由于幻觉的认知机制与记忆有关，我们在此探索通过显性记忆机制实现的 LLM 幻觉。我们通过经验证明，只需缩放限制记忆增强 LLM 解码器中生成的读出向量，就可以以无需训练的方式实现幻觉缓解。我们的方法受到几何启发，在生成类似维基百科的传记条目的任务上，无论是在生成质量还是运行时复杂度方面，都优于最先进的 LLM 编辑方法。]]></description>
      <guid>https://arxiv.org/abs/2407.16908</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>训练注意力：元学习在持续知识学习中应该关注的地方</title>
      <link>https://arxiv.org/abs/2407.16920</link>
      <description><![CDATA[arXiv:2407.16920v1 公告类型：新
摘要：先前对大型语言模型 (LLM) 中的持续知识学习 (CKL) 的研究主要集中在诸如正则化、架构修改和排练技术等方法上，以减轻灾难性遗忘。然而，这些方法天真地继承了标准训练程序的低效率，不加区分地对所有标记应用统一的权重，这可能导致不必要的参数更新和增加遗忘。为了解决这些缺点，我们提出了一种称为训练注意力增强语言模型 (TAALM) 的新型 CKL 方法，该方法通过根据标记的有用性动态预测和应用权重来提高学习效率。该方法采用元学习框架来优化标记重要性预测，促进有针对性的知识更新并最大限度地减少遗忘。此外，我们观察到现有基准没有清楚地展示学习和保留之间的权衡，因此我们提出了一个新的基准 \textsc{LAMA-ckl} 来解决此问题。通过对新推出和已建立的 CKL 基准进行的实验，TAALM 证明了基线上的最先进的性能，并且在与以前的 CKL 方法集成时也表现出协同兼容性。]]></description>
      <guid>https://arxiv.org/abs/2407.16920</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>ScholarChemQA：揭示语言模型在化学研究问答中的强大作用</title>
      <link>https://arxiv.org/abs/2407.16931</link>
      <description><![CDATA[arXiv:2407.16931v1 公告类型：新
摘要：问答 (QA) 有效地评估了语言模型的推理和知识深度。虽然 QA 数据集在通用领域和生物医学等领域非常丰富，但学术化学领域的探索较少。化学 QA 通过将复杂的化学信息有效地转化为易于理解的格式，在教育和研究中发挥着至关重要的作用。为了解决这一差距，我们推出了 ScholarChemQA，这是一个由化学论文构建的大规模 QA 数据集。该数据集反映了典型的现实世界挑战，包括不平衡的数据分布和大量可能有用的未标记数据。相应地，我们引入了一个 QAMatch 模型，专门设计用于通过充分利用我们收集的数据来有效地回答化学问题。我们首先通过根据每个类的逆频率重新加权实例损失来解决标签分布不平衡的问题，确保在优化过程中少数类不会被多数类所主导。接下来，我们利用未标记的数据来丰富学习过程，基于 SoftMix 操作生成各种增强，并确保它们的预测与同一目标（即伪标签）一致。为了确保伪标签的质量，我们提出了一种校准程序，旨在将单个样本的伪标签估计值与所需的基本事实分布紧密对齐。实验表明，我们的 QAMatch 不仅在我们的 ScholarChemQA 数据集上，而且在四个基准数据集上都明显优于最近的类似规模基线和大型语言模型 (LLM)。我们希望我们的基准和模型能够促进和推动对化学 QA 的更多研究。]]></description>
      <guid>https://arxiv.org/abs/2407.16931</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>具有增强可解释性的早期筛选潜在突破性技术：专利特定的分层注意力网络模型</title>
      <link>https://arxiv.org/abs/2407.16939</link>
      <description><![CDATA[arXiv:2407.16939v1 公告类型：新
摘要：尽管机器学习方法对于潜在突破性技术的早期筛选很有用，但它们的实用性往往受到不透明模型的阻碍。为了解决这个问题，我们提出了一种可解释的机器学习方法，使用专利特定的分层注意力网络 (PatentHAN) 模型从专利文本中预测未来的引用计数。这种方法的核心是 (1) 专利特定的预训练语言模型，捕捉专利权利要求中技术词汇的含义，(2) 分层网络结构，能够在权利要求级别进行详细分析，以及 (3) 权利要求方面的自注意力机制，在筛选过程中揭示关键权利要求。对 35,376 项制药专利的案例研究证明了我们的方法在确保可解释性的同时，在早期筛选潜在突破性技术方面的有效性。此外，我们使用不同的语言模型和权利要求类型进行额外分析，以检验该方法的稳健性。预计所提出的方法将加强专家与机器之间的协作，以识别突破性技术，并为从文本挖掘中获得的技术价值提供新的见解。]]></description>
      <guid>https://arxiv.org/abs/2407.16939</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>像人类学生一样培养法学硕士：结构感知领域知识注入</title>
      <link>https://arxiv.org/abs/2407.16724</link>
      <description><![CDATA[arXiv:2407.16724v1 公告类型：新
摘要：本文提出了一种开创性的方法，称为 StructTuning，可有效地将基础大型语言模型 (LLM) 转化为领域专家。它显著将训练语料库要求降至仅 0.3%，同时实现了传统知识注入性能的 50% 的惊人水平。我们的方法受到人类学生教育过程的启发，特别是如何吸收教科书中的结构化领域知识，然后通过特定练习应用于解决现实世界的挑战。基于此，我们提出了一种新颖的两阶段知识注入策略：结构感知持续预训练 (SCPT) 和结构感知监督微调 (SSFT)。在 SCPT 阶段，我们将训练数据组织成自动生成的领域知识分类法，使 LLM 能够有效地记忆与分类法架构内特定专业知识相关的文本片段。随后，在 SSFT 阶段，我们明确提示模型在其输出中揭示底层知识结构，利用这种结构化领域洞察力巧妙地解决实际问题。我们的最终方法已在模型架构和规模上进行了广泛的评估，使用 LongBench 和 MMedBench 数据集上的闭卷问答任务。值得注意的是，我们的方法与 MMedBench 上最先进的 MMedLM2 所显示的改进相差 50%，但训练语料库的数量仅为 0.3%。这一突破展示了扩展我们的 StructTuning 以用于更强大的特定领​​域 LLM 的潜力。代码将很快公开。]]></description>
      <guid>https://arxiv.org/abs/2407.16724</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>文本风格转换调查：应用和伦理影响</title>
      <link>https://arxiv.org/abs/2407.16737</link>
      <description><![CDATA[arXiv:2407.16737v1 公告类型：新
摘要：文本风格转换 (TST) 是可控文本生成中一项重要的任务，旨在控制语言使用的选定属性，例如礼貌、正式性或情感，而不改变文本的风格独立内容。近年来，该领域受到了广泛的研究关注，并已在多篇评论中进行了介绍，但重点主要放在开发新算法和从不同类型的数据（监督、无监督、域外等）中学习，而不是在应用方面。然而，与 TST 相关的技术正在逐渐达到生产和部署就绪的水平，因此，将应用视角纳入 TST 研究变得至关重要。同样，TST 技术中经常被忽视的道德考虑已成为一个紧迫的问题。本文全面回顾了多年来使用传统语言学方法和较新的深度学习方法研究的 TST 应用。我们讨论了 TST 在文本生成中的当前挑战、未来研究方向和伦理影响。通过全面概述 TST 应用的前景，我们希望能够促进进一步的研究，并有助于更好地理解与 TST 相关的潜力和伦理考虑。]]></description>
      <guid>https://arxiv.org/abs/2407.16737</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>检索增强生成还是长上下文法学硕士？综合研究与混合方法</title>
      <link>https://arxiv.org/abs/2407.16833</link>
      <description><![CDATA[arXiv:2407.16833v1 公告类型：新 
摘要：检索增强生成 (RAG) 一直是大型语言模型 (LLM) 有效处理过长上下文的强大工具。然而，最近的 LLM 如 Gemini-1.5 和 GPT-4 表现出直接理解长上下文的卓越能力。我们对 RAG 和长上下文 (LC) LLM 进行了全面比较，旨在利用两者的优势。我们使用三个最新的 LLM 在各种公共数据集上对 RAG 和 LC 进行基准测试。结果表明，在资源充足的情况下，LC 在平均性能方面始终优于 RAG。然而，RAG 显著降低的成本仍然是一个明显的优势。基于这一观察，我们提出了 Self-Route，这是一种简单而有效的方法，它基于模型自我反思将查询路由到 RAG 或 LC。Self-Route 显着降低了计算成本，同时保持了与 LC 相当的性能。我们的研究结果为使用 RAG 和 LC 的 LLM 的长期应用提供了指导。]]></description>
      <guid>https://arxiv.org/abs/2407.16833</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>$\textit{BenchIE}^{FL}$：基于事实的手动重新注释开放信息提取基准</title>
      <link>https://arxiv.org/abs/2407.16860</link>
      <description><![CDATA[arXiv:2407.16860v1 公告类型：新
摘要：开放信息提取 (OIE) 是自然语言处理的一个领域，旨在以一种允许组织、分析和反思的格式呈现文本信息。许多 OIE 系统被开发出来，声称性能不断提高，标志着对客观基准的需求。BenchIE 是我们所知的最新参考。尽管经过深思熟虑，但我们注意到一些我们认为有限制的问题。因此，我们提出了 $\textit{BenchIE}^{FL}$，这是一种新的 OIE 基准，它完全执行了 BenchIE 的原则，同时在候选事实与参考事实匹配时包含更少的错误、遗漏和缺点。$\textit{BenchIE}^{FL}$ 允许对 OIE 提取器的实际性能得出深刻的结论。]]></description>
      <guid>https://arxiv.org/abs/2407.16860</guid>
      <pubDate>Fri, 26 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>