<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 16 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>NERsocial：利用 RapidNER 构建用于人机交互的高效命名实体识别数据集</title>
      <link>https://arxiv.org/abs/2412.09634</link>
      <description><![CDATA[arXiv:2412.09634v1 公告类型：新
摘要：将命名实体识别 (NER) 方法应用于新领域带来了重大挑战。我们介绍了 RapidNER，这是一个旨在通过高效的数据集构建快速部署 NER 系统的框架。RapidNER 通过三个关键步骤进行操作：(1) 从一般知识图中提取特定领域的子图和三元组，(2) 收集和利用来自各种来源的文本来构建 NERsocial 数据集，该数据集侧重于人机交互中典型的实体，以及 (3) 使用 Elasticsearch (ES) 实施注释方案以提高效率。NERsocial 经过人类注释者的验证，包括六种实体类型、153K 个标记和 99.4K 个句子，展示了 RapidNER 加快数据集创建的能力。]]></description>
      <guid>https://arxiv.org/abs/2412.09634</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GReaTer：梯度推理使较小的语言模型变得强大，提示优化器</title>
      <link>https://arxiv.org/abs/2412.09722</link>
      <description><![CDATA[arXiv:2412.09722v1 公告类型：新
摘要：大型语言模型 (LLM) 的有效性与提示的设计密切相关，因此提示优化对于提高其在广泛任务中的性能至关重要。许多现有的自动化提示工程方法完全依赖于文本反馈，仅根据大型、计算成本高昂的 LLM 识别出的推理错误来改进提示。不幸的是，较小的模型难以生成高质量的反馈，导致完全依赖大型 LLM 判断。此外，由于纯粹在文本空间中操作，这些方法无法利用更直接、更细粒度的信息（例如梯度）。为此，我们引入了 GReaTer，这是一种新颖的提示优化技术，它直接结合梯度信息而不是特定于任务的推理。通过利用任务损失梯度，GReaTer 可以实现开源、轻量级语言模型的提示自我优化，而无需昂贵的闭源 LLM。这样就可以实现高性能的提示优化，而无需依赖大量 LLM，从而缩小小型模型与提示改进通常所需的复杂推理之间的差距。对包括 BBH、GSM8k 和 FOLIO 在内的各种推理任务进行的广泛评估表明，GReaTer 始终优于之前最先进的提示优化方法，即使是那些依赖强大 LLM 的方法。此外，GReaTer 优化的提示通常表现出更好的可转移性，在某些情况下，将任务性能提升到与大型语言模型相当或超过大型语言模型的水平，突出了由梯度引导的提示优化的有效性。GReaTer 的代码可在 https://github.com/psunlpgroup/GreaTer 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.09722</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>规模化的内存层</title>
      <link>https://arxiv.org/abs/2412.09764</link>
      <description><![CDATA[arXiv:2412.09764v1 公告类型：新
摘要：记忆层使用可训练的键值查找机制在不增加 FLOP 的情况下向模型添加额外参数。从概念上讲，稀疏激活的记忆层补充了计算密集的密集前馈层，提供了专用容量来廉价地存储和检索信息。这项工作使记忆层超越了概念验证，证明了它们在当代规模上的实用性。在下游任务中，使用我们改进的记忆层增强的语言模型在计算和参数匹配时优于计算预算两倍以上的密集模型以及专家混合模型。我们发现对于事实任务而言，收益尤其明显。我们提供了一个完全可并行化的记忆层实现，展示了具有多达 128B 内存参数的缩放定律，预训练到 1 万亿个标记，而基础模型的参数多达 8B。]]></description>
      <guid>https://arxiv.org/abs/2412.09764</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Semi-IIN：用于多模态情绪分析的半监督模态内交互学习网络</title>
      <link>https://arxiv.org/abs/2412.09784</link>
      <description><![CDATA[arXiv:2412.09784v1 公告类型：新
摘要：尽管多模态情绪分析是一个值得进一步研究的肥沃研究领域，但当前的方法占用了很高的注释成本，并且存在标签模糊性，不利于高质量标记数据获取。此外，选择正确的交互至关重要，因为不同样本中模态内或模态间交互的重要性可能不同。为此，我们提出了 Semi-IIN，一种用于多模态情绪分析的半监督模态内-模态间交互学习网络。Semi-IIN 集成了掩蔽注意和门控机制，在独立捕获模态内和模态间交互信息后实现有效的动态选择。结合自训练方法，Semi-IIN 充分利用了从未标记数据中学到的知识。在两个公共数据集 MOSI 和 MOSEI 上的实验结果证明了 Semi-IIN 的有效性，在多个指标上建立了新的最先进水平。代码可在https://github.com/flow-ljh/Semi-IIN获取。]]></description>
      <guid>https://arxiv.org/abs/2412.09784</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoPatent：用于自动生成专利的多代理框架</title>
      <link>https://arxiv.org/abs/2412.09796</link>
      <description><![CDATA[arXiv:2412.09796v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的功能不断进步，专利处理领域在自然语言处理社区中引起了越来越多的关注。然而，大多数研究都集中在分类任务上，例如专利分类和审查，或短文本生成任务，例如专利摘要和专利测验。在本文中，我们介绍了一项名为 Draft2Patent 的新颖实用任务及其相应的 D2P 基准，该基准要求 LLM 根据初稿生成平均 17K 个标记的全长专利。专利因其专业性、标准化术语和冗长篇幅而对 LLM 提出了重大挑战。我们提出了一个名为 AutoPatent 的多代理框架，该框架利用基于 LLM 的规划代理、作者代理和审查员代理以及 PGTree 和 RRAG 来生成冗长、复杂且高质量的完整专利文件。实验结果表明，我们的 AutoPatent 框架显著增强了跨各种 LLM 生成综合专利的能力。此外，我们发现，仅使用基于 Qwen2.5-7B 模型的 AutoPatent 框架生成的专利在客观指标和人工评估方面都优于由更大、更强大的 LLM（例如 GPT-4o、Qwen2.5-72B 和 LLAMA3.1-70B）生成的专利。我们将在 \url{https://github.com/QiYao-Wang/Au​​toPatent} 上提供数据和代码。]]></description>
      <guid>https://arxiv.org/abs/2412.09796</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 蒸馏技术助力高效小样本多项选择题解答</title>
      <link>https://arxiv.org/abs/2412.09807</link>
      <description><![CDATA[arXiv:2412.09807v1 公告类型：新 
摘要：多项选择题回答 (MCQA) 是众多实际应用（例如医学、法律和教育）中的一个重要问题。构建 MCQA 数据集的高成本使得小样本学习成为这一领域的关键。虽然大型语言模型 (LLM) 可以实现小样本学习，但它们在实际场景中的直接应用往往受到高计算成本的阻碍。为了应对这一挑战，我们提出了一种简单而有效的方法，即使用 LLM 进行数据生成和评分。我们的方法利用 LLM 创建包含问题和选项的 MCQA 数据，并为生成的选项分配概率分数。然后，我们使用生成的数据和 LLM 分配的分数，通过利用蒸馏损失来微调更小、更高效的仅编码器模型 DeBERTa-v3-base。在大规模多任务语言理解 (MMLU) 基准上进行的大量实验表明，我们的方法将准确率从 28.9% 提高到了 39.3%，与直接在 5 次样本上微调的基线相比，提升了 10% 以上。这证明了 LLM 驱动的数据生成和知识提炼对于少样本 MCQA 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.09807</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ScaleOT：使用动态 LayerReplace 和选择性等级压缩进行隐私实用可扩展的异地调整</title>
      <link>https://arxiv.org/abs/2412.09812</link>
      <description><![CDATA[arXiv:2412.09812v1 公告类型：新
摘要：异地调优是一种隐私保护方法，用于调整大型语言模型 (LLM)，通过与数据所有者共享来自 LLM 所有者的有损压缩模拟器来进行下游任务调整。这种方法保护了模型和数据所有者的隐私。然而，当前的异地调优方法通常会因统一删除 LLM 层或依赖昂贵的知识提炼而遭受适应性下降、计算成本高和保护强度有限的困扰。为了解决这些问题，我们提出了 ScaleOT，这是一种新颖的隐私效用可扩展的异地调优框架，可以有效地平衡隐私和效用。ScaleOT 引入了一种新颖的逐层有损压缩算法，该算法使用强化学习来获得每一层的重要性。它使用轻量级网络（称为协调器）来替换原始 LLM 层。通过以不同的比例组合重要的原始 LLM 层和协调器，ScaleOT 生成针对最佳性能量身定制的模拟器，具有各种模型规模，可增强隐私保护。此外，我们提出了一种降阶方法来进一步压缩原始的 LLM 层，从而显著增强了隐私性，而对实用性的影响却微不足道。综合实验表明，与完全微调相比，ScaleOT 可以实现几乎无损的异地调优性能，同时获得更好的模型隐私性。]]></description>
      <guid>https://arxiv.org/abs/2412.09812</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MERaLiON-AudioLLM：技术报告</title>
      <link>https://arxiv.org/abs/2412.09818</link>
      <description><![CDATA[arXiv:2412.09818v1 公告类型：新
摘要：我们推出了 MERaLiON-AudioLLM（多模态共情推理和学习网络），这是第一个针对新加坡多语言和多元文化环境量身定制的语音文本模型。MERaLiON-AudioLLM 是在新加坡国家大型语言模型资助计划下开发的，它集成了先进的语音和文本处理功能，以解决当地口音和方言的各种语言细微差别，增强了复杂多语言环境中的可访问性和可用性。我们的结果表明语音识别和特定任务理解都有所改进，将 MERaLiON-AudioLLM 定位为区域特定 AI 应用的先驱解决方案。我们设想这个版本将为未来的模型树立先例，这些模型旨在解决全球框架中的本地化语言和文化背景。]]></description>
      <guid>https://arxiv.org/abs/2412.09818</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于微调语言模型的低秩自适应和任务相关特征增强</title>
      <link>https://arxiv.org/abs/2412.09827</link>
      <description><![CDATA[arXiv:2412.09827v1 公告类型：新
摘要：以参数高效的方式对预训练的大型语言模型进行微调因其有效性和效率而受到广泛研究。LoRA 是最广泛使用的方法之一，它假设优化过程本质上是低维的。尽管 LoRA 表现出了令人称赞的性能，但在学习新任务时，LoRA 与完全微调之间仍然存在显着的性能差距。在这项工作中，我们提出了具有任务相关特征增强的低秩自适应 (LoRATRF)，以从编辑神经网络表示的角度增强任务相关特征。为了优先考虑与任务相关的特征，设计了一个任务感知过滤器，它可以选择性地从隐藏表示中提取有价值的知识以用于目标或当前任务。在 NLU、常识推理、数学推理任务等多种数据集上的实验表明，与 SOTA 低秩方法相比，我们的方法减少了 33.71% 的参数，并在多种数据集上取得了更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.09827</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>类人化具身 AI 面试官：在真实国际会议上使用 Android ERICA</title>
      <link>https://arxiv.org/abs/2412.09867</link>
      <description><![CDATA[arXiv:2412.09867v1 公告类型：新
摘要：本文介绍了一种类似人类的具身化人工智能面试官，它集成了配备先进对话功能的安卓机器人，包括专心聆听、对话修复和用户流畅度适应。此外，它可以在面试后分析和呈现结果。我们在 SIGDIAL 2024 上进行了一项真实案例研究，共有 42 名参与者，其中 69% 的人报告了积极的体验。这项研究证明了该系统在像人类一样进行面试方面的有效性，并标志着该系统在国际会议上的首次使用。演示视频可在 https://youtu.be/jCuw9g99KuE 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.09867</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Byte Latent Transformer：补丁比令牌更具可扩展性</title>
      <link>https://arxiv.org/abs/2412.09871</link>
      <description><![CDATA[arXiv:2412.09871v1 公告类型：新
摘要：我们引入了字节潜伏变压器 (BLT)，这是一种新的字节级 LLM 架构，它首次在规模上匹配基于标记化的 LLM 性能，同时显著提高了推理效率和鲁棒性。BLT 将字节编码为动态大小的补丁，作为计算的主要单位。补丁根据下一个字节的熵进行分段，在数据复杂性增加需要时分配更多的计算和模型容量。我们提出了第一个 FLOP 控制的字节级模型扩展研究，最多 8B 个参数和 4T 个训练字节。我们的结果证明了在没有固定词汇表的情况下扩展在原始字节上训练的模型的可行性。由于在数据可预测时动态选择长补丁，训练和推理效率都得到了提高，同时推理和长尾泛化也得到了定性改进。总体而言，对于固定的推理成本，BLT 通过同时增加补丁和模型大小，显示出比基于标记化的模型更好的扩展性。]]></description>
      <guid>https://arxiv.org/abs/2412.09871</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论语言模型作为规划形式化的局限性</title>
      <link>https://arxiv.org/abs/2412.09879</link>
      <description><![CDATA[arXiv:2412.09879v1 公告类型：新
摘要：大型语言模型已被证明无法在基础环境中创建可执行和可验证的计划。一项新兴的工作表明，使用 LLM 作为形式化器来生成规划域的形式表示（例如 PDDL）是成功的，可以确定性地解决该表示以找到计划。我们系统地评估了这种方法，同时弥补了一些主要差距。虽然以前的工作只在给定模板化且不切实际的环境描述的情况下生成部分 PDDL 表示，但我们会根据各种自然程度的描述生成完整的表示。在一系列对提高 LLM 的形式规划能力至关重要的观察中，我们注意到足够大的模型可以有效地将描述形式化为 PDDL，优于直接生成计划的模型，同时对词汇扰动具有鲁棒性。随着描述听起来越来越自然，我们观察到性能下降并提供详细的错误分析。]]></description>
      <guid>https://arxiv.org/abs/2412.09879</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在野外对表格理解进行基准测试</title>
      <link>https://arxiv.org/abs/2412.09884</link>
      <description><![CDATA[arXiv:2412.09884v1 公告类型：新 
摘要：大型语言模型 (LLM) 虽然在大量知识密集型活动中占据主导地位，但在理解冗长的表格文本混合（例如学术论文和财务报告）方面却收效甚微。长上下文 LLM 的最新进展为该领域开辟了新的可能性。尽管如此，我们还是发现了两个障碍：（1）先前的表格问答 (TableQA) 基准测试侧重于没有上下文的孤立表格，因此很难在现实场景中评估模型。（2）先前的基准测试侧重于一些狭窄的表格理解技能，例如表格识别、数据操作/计算、表格汇总等，而熟练的人会集体使用这些技能。在这项工作中，我们引入了 TableQuest，这是一种新的基准测试，旨在评估 LLM 在自然的表格丰富的财务报告环境中的整体表格理解能力。我们采用严格的数据处理和过滤程序，以确保问答对合乎逻辑、合理且多样化。我们试验了 7 个最先进的模型，发现尽管它们在定位事实方面具有合理的准确性，但在需要执行更复杂的推理或多步骤计算时，它们往往会失败。我们最后对故障模式进行了定性研究，并讨论了构建具有挑战性的基准的挑战。我们将评估数据、评判程序和这项研究的结果公开，以促进该领域的研究。]]></description>
      <guid>https://arxiv.org/abs/2412.09884</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过解决方案指导微调增强小型语言模型的推理能力</title>
      <link>https://arxiv.org/abs/2412.09906</link>
      <description><![CDATA[arXiv:2412.09906v1 公告类型：新
摘要：大型语言模型（LLM）在广泛的任务中表现出色。快速工程和微调技术的进步进一步增强了它们应对复杂推理挑战的能力。然而，这些高级功能通常仅限于超过 1000 亿个参数的模型。尽管已经针对较小的模型（100 亿个参数以下）探索了思路链（CoT）微调方法，但它们通常依赖于大量的 CoT 训练数据，这可能会引入不一致性并限制在低数据设置中的有效性。为了克服这些限制，本文介绍了一种新的推理策略解决方案指导（SG）和即插即用的训练范式解决方案指导微调（SGFT），以增强小型语言模型的推理能力。SG 专注于语义和逻辑层面的问题理解和分解，而不是特定的计算，这可以有效地提高 SLM 的泛化和推理能力。只需少量 SG 训练数据，SGFT 便可以对 SLM 进行微调，生成准确的问题解决指导，然后可以灵活地将其作为提示输入到任何 SLM，使其直接生成正确答案。实验结果表明，我们的方法显著提高了 SLM 在各种推理任务上的性能，提高了它们在资源受限环境中的实用性和效率。]]></description>
      <guid>https://arxiv.org/abs/2412.09906</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于类内和类间距离计算的低资源快速文本分类</title>
      <link>https://arxiv.org/abs/2412.09922</link>
      <description><![CDATA[arXiv:2412.09922v1 公告类型：新
摘要：近年来，基于神经网络和预训练模型的文本分类方法受到越来越多的关注并表现出色。然而，这些方法在实际应用中仍然存在一些局限性：（1）它们通常只关注句子之间的匹配相似性。然而，同一类的句子内部和不同类之间的句子中都存在隐含的高价值信息，这对于分类任务非常重要。（2）现有的方法，如预训练语言模型和基于图的方法，通常会消耗大量内存进行训练和文本图构建。（3）虽然一些低资源方法可以实现良好的性能，但它们通常会受到过长处理时间的影响。为了应对这些挑战，我们提出了一种低资源、快速的文本分类模型 LFTC。我们的方法首先为每个类构建一个压缩器列表，以充分挖掘类内数据中的规律信息。然后，我们删除与目标分类无关的冗余信息以减少处理时间。最后，我们计算文本对之间的相似性距离进行分类。我们在 9 个公开可用的基准数据集上对 LFTC 进行评估，结果表明性能和处理时间有显著提高，尤其是在有限的计算和数据资源下，凸显了其卓越的优势。]]></description>
      <guid>https://arxiv.org/abs/2412.09922</guid>
      <pubDate>Mon, 16 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>