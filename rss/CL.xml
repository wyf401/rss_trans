<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上进行更新。</description>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向现实的少样本关系提取：新的元数据集和评估</title>
      <link>https://arxiv.org/abs/2404.04445</link>
      <description><![CDATA[arXiv:2404.04445v1 公告类型：新
摘要：我们引入了用于少样本关系提取的元数据集，其中包括源自现有监督关系提取数据集 NYT29（Takanobu et al., 2019；Nayak and Ng, 2020）和 WIKIDATA（Sorokin and Gurevych, 2017）的两个数据集，如下所示：以及 TACRED 数据集的少数镜头形式（Sabo 等人，2021）。重要的是，所有这些小样本数据集都是在现实假设下生成的，例如：测试关系与模型之前可能见过的任何关系不同，训练数据有限，以及候选关系提及的优势与任何关系都不对应。利益关系。利用这个庞大的资源，我们对最近的六种小样本关系提取方法进行了综合评估，并观察到没有一种方法成为明显的赢家。此外，这项任务的整体表现较低，表明未来研究的需求很大。我们发布所有版本的数据，即监督数据和少量数据，以供未来研究使用。]]></description>
      <guid>https://arxiv.org/abs/2404.04445</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>协助人类进行复杂的比较：大规模自动信息比较</title>
      <link>https://arxiv.org/abs/2404.04351</link>
      <description><![CDATA[arXiv:2404.04351v1 公告类型：新
摘要：生成大型语言模型可以跨知识领域进行高效分析，在信息比较方面可与人类专家相媲美。然而，由于在大型上下文中维护信息和克服模型令牌限制的困难，法学硕士在信息比较方面的应用面临着可扩展性的挑战。为了应对这些挑战，我们开发了新颖的抽象总结\和标准驱动的比较端点（ASC$^2$End）系统来大规模自动化信息比较。我们的系统采用语义文本相似性比较来生成证据支持的分析。我们利用经过验证的数据处理策略（例如抽象摘要和检索增强生成）来克服标记限制并在模型推理过程中保留相关信息。提示是使用零样本策略设计的，将信息置于上下文中以改进模型推理。我们使用 ROUGE 评分评估抽象摘要，并使用调查回复评估生成的比较质量。在 ASC$^2$End 系统上评估的模型显示出理想的结果，提供了对系统预期性能的见解。 ASC$^2$End 是一种新颖的系统和工具，可以跨知识领域进行大规模的准确、自动化的信息比较，克服上下文长度和检索的限制。]]></description>
      <guid>https://arxiv.org/abs/2404.04351</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>用大型语言模型解读新闻中的政治实体情感：零镜头和少镜头策略</title>
      <link>https://arxiv.org/abs/2404.04361</link>
      <description><![CDATA[arXiv:2404.04361v1 公告类型：新
摘要：情感分析在理解公众舆论方面发挥着关键作用，特别是在政治领域，新闻文章中实体的描述会影响公众的看法。在本文中，我们研究了大型语言模型（LLM）在预测政治新闻文章中特定实体情绪方面的有效性。利用零样本和少样本策略，我们探索了法学硕士辨别新闻内容中对政治实体的情绪的能力。采用思想链（COT）方法，并在少镜头上下文学习中增强基本原理，我们评估该方法是否提高了情绪预测的准确性。我们对情感标记数据集的评估表明，LLM 在捕获特定实体情感方面优于微调的 BERT 模型。我们发现上下文学习显着提高了模型性能，而自我一致性机制增强了情绪预测的一致性。尽管结果令人鼓舞，但我们观察到 COT 提示方法的有效性不一致。总体而言，我们的研究结果强调了法学硕士在政治新闻领域以实体为中心的情感分析中的潜力，并强调了适当的提示策略和模型架构的重要性。]]></description>
      <guid>https://arxiv.org/abs/2404.04361</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>具有用于自动语音识别的发音感知嵌入的传感器</title>
      <link>https://arxiv.org/abs/2404.04295</link>
      <description><![CDATA[arXiv:2404.04295v1 公告类型：新
摘要：本文提出了具有发音感知嵌入（PET）的传感器。与传统 Transducer 不同标记的解码器嵌入是独立训练的不同，PET 模型的解码器嵌入合并了具有相同或相似发音的文本标记的共享组件。通过在中文和韩语的多个数据集中进行的实验，我们表明，与传统传感器相比，PET 模型持续提高了语音识别准确性。我们的调查还发现了一种称为错误链反应的现象。识别错误不是均匀分布在整个话语中，而是倾向于聚集在一起，随后的错误通常紧随先前的错误。我们的分析表明，PET 模型通过大幅降低模型在前一个错误之后产生额外错误的可能性，有效地缓解了这一问题。我们的实施将通过 NeMo 工具包进行开源。]]></description>
      <guid>https://arxiv.org/abs/2404.04295</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>CBR-RAG：法学硕士中基于案例的检索增强生成法律问答推理</title>
      <link>https://arxiv.org/abs/2404.04302</link>
      <description><![CDATA[arXiv:2404.04302v1 公告类型：新
摘要：检索增强生成（RAG）通过提供先验知识作为输入的上下文来增强大语言模型（LLM）的输出。这对于知识密集型和依赖专家的任务是有益的，包括法律问答，这需要证据来验证生成的文本输出。我们强调，基于案例的推理 (CBR) 为法学硕士 RAG 流程的一部分提供了结构化检索的关键机会。我们引入了 CBR-RAG，其中 CBR 循环的初始检索阶段、其索引词汇和相似性知识容器用于增强具有上下文相关案例的 LLM 查询。此集成增强了原始 LLM 查询，提供更丰富的提示。我们提出了 CBR-RAG 的评估，并检查了法律问答任务的不同表示（即一般和特定领域的嵌入）和比较方法（即内部、内部和混合相似性）。我们的结果表明，CBR 案例重用提供的上下文增强了问题的相关组成部分与证据库之间的相似性，从而显着提高了生成答案的质量。]]></description>
      <guid>https://arxiv.org/abs/2404.04302</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的范围歧义</title>
      <link>https://arxiv.org/abs/2404.04332</link>
      <description><![CDATA[arXiv:2404.04332v1 公告类型：新
摘要：包含多个范围重叠的语义运算符的句子通常会在解释中产生歧义，称为范围歧义。这些歧义为语言处理中语义结构和世界知识之间的相互作用提供了丰富的见解。尽管如此，关于现代大型语言模型如何处理它们的研究却很少。在本文中，我们研究了某些自回归语言模型的不同版本（GPT-2、GPT-3/3.5、Llama 2 和 GPT-4）如何处理范围歧义句子，并将其与人类判断进行比较。我们引入了新颖的数据集，其中总共包含近 1,000 个独特的范围模糊句子，包含一系列语义运算符之间的交互，并为人类判断进行了注释。使用这些数据集，我们发现有证据表明多个模型（i）对这些句子中的含义模糊性很敏感，其方式可以很好地与人类判断相匹配，并且（ii）可以成功地以高准确度识别人类偏好的阅读内容（在某些情况下超过 90%）。]]></description>
      <guid>https://arxiv.org/abs/2404.04332</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>CONFLARE：CONFormal大型语言模型检索</title>
      <link>https://arxiv.org/abs/2404.04287</link>
      <description><![CDATA[arXiv:2404.04287v1 公告类型：新
摘要：检索增强生成（RAG）框架使大型语言模型（LLM）能够从知识库中检索相关信息并将其合并到上下文中以生成响应。这可以减轻幻觉并允许更新知识而无需重新培训法学硕士。但是，如果检索无法将必要的信息识别为响应生成的上下文，RAG 不保证有效的响应。此外，如果存在矛盾的内容，RAG 响应可能仅反映两种可能响应之一。因此，量化检索过程中的不确定性对于确保 RAG 的可信度至关重要。在本报告中，我们介绍了一个四步框架，用于应用保形预测来量化 RAG 框架中的检索不确定性。首先，构建可从知识库回答的校准问题集。将每个问题的嵌入与文档嵌入进行比较，以识别包含答案的最相关的文档块并记录它们的相似性分数。给定用户指定的错误率（{\alpha}），然后分析这些相似性分数以确定相似性分数截止阈值。在推理过程中，检索所有相似度超过此阈值的块，为 LLM 提供上下文，确保在具有 (1-{\alpha}) 置信水平的上下文中捕获真实答案。我们提供了一个Python包，使用户能够实现我们工作中提出的整个工作流程，仅使用LLM并且无需人工干预。]]></description>
      <guid>https://arxiv.org/abs/2404.04287</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>通过外部规划器控制的大语言模型进行对话式疾病诊断</title>
      <link>https://arxiv.org/abs/2404.04292</link>
      <description><![CDATA[arXiv:2404.04292v1 公告类型：新
摘要：医疗人工智能（AI）的进步为对话式诊断的实现奠定了基础，其中人工智能系统模仿人类医生通过与患者对话来推断诊断。本研究引入了一种创新方法，使用外部规划器增强大型语言模型（LLM）来开发面向医疗任务的对话系统。该系统包括用于信息收集的政策模块、用于自然语言理解和生成的基于法学硕士的模块，解决了先前人工智能系统在这些领域的局限性。通过模拟医生疾病筛查和鉴别诊断的两阶段决策过程。我们设计了两个不同的规划器。第一个重点是收集患者症状以识别潜在的疾病，而第二个则深入研究具体询问以确认或排除这些疾病。利用法学硕士的强化学习和主动学习，我们培训这些规划人员有效地进行医学对话。我们对 MIMIC-IV 数据集的评估证明了该系统超越现有模型的能力，表明朝着实现自动化对话疾病诊断和提高医疗诊断的准确性和可访问性迈出了重要一步。]]></description>
      <guid>https://arxiv.org/abs/2404.04292</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>谬误推理：通过逻辑谬误理解增强大型语言模型的逻辑推理能力</title>
      <link>https://arxiv.org/abs/2404.04293</link>
      <description><![CDATA[arXiv:2404.04293v1 公告类型：新
摘要：大型语言模型（LLM）在许多推理任务中表现出了良好的性能，但它们仍然难以处理包括逻辑推理在内的一些复杂的推理任务。法学硕士在逻辑推理方面表现欠佳的一个不可忽视的原因是他们忽视了正确理解逻辑谬误。为了评估法学硕士的逻辑谬误理解（LFU）能力，我们在本文中从“什么”、“为什么”和“如何”三个认知维度提出了五个具体任务。针对这些 LFU 任务，我们在 GPT-4 的基础上成功构建了一个新的数据集 LFUD，并付出了一点人力。我们广泛的实验证明，我们的 LFUD 不仅可以用来评估 LLM 的 LFU 能力，还可以微调 LLM 以获得逻辑推理性能的显着增强。]]></description>
      <guid>https://arxiv.org/abs/2404.04293</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>MIMIR：领域专业知识中个性化代理调整的简化平台</title>
      <link>https://arxiv.org/abs/2404.04285</link>
      <description><![CDATA[arXiv:2404.04285v1 公告类型：新
摘要：最近，大型语言模型（LLM）已经发展成为交互式代理，精通各种任务的规划、工具使用和任务执行。然而，如果没有特定的代理调整，像 LLaMA 这样的开源模型目前很难与 GPT-4 的效率相匹配，特别是考虑到用于微调的代理调整数据集的稀缺。作为回应，我们推出了 \textsc{Mimir}：一个提供可定制管道的简化平台，使用户能够大规模利用私有知识和公开可用的、合法合规的数据集来进行 \textbf{个性化代理调整}。此外，\textsc{Mimir} 支持从同一输入生成通用指令调整数据集。这种双重能力确保了通过该平台开发的语言智能体同时拥有特定智能体能力和通用能力。 \textsc{Mimir} 将这些功能集成到一个有凝聚力的端到端平台中，方便从上传个性化文件到一键代理微调的一切。]]></description>
      <guid>https://arxiv.org/abs/2404.04285</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>语言模型演化：迭代学习视角</title>
      <link>https://arxiv.org/abs/2404.04286</link>
      <description><![CDATA[arXiv:2404.04286v1 公告类型：新
摘要：随着大型语言模型（LLM）的广泛采用，这些模型之间迭代交互的普遍性预计将会增加。值得注意的是，多轮自我改进方法的最新进展使法学硕士能够生成用于训练后续模型的新示例。与此同时，涉及代理之间自动交互的多代理法学硕士系统也越来越受到重视。因此，无论是短期还是长期，法学硕士都可以积极参与一个进化过程。我们将法学硕士的行为与人类文化的进化进行了比较，因为认知科学家几十年来已经对后者进行了广泛的研究。我们的方法涉及利用迭代学习（IL），这是一种贝叶斯框架，它阐明了人类文化进化过程中微妙的偏见是如何被放大的，来解释法学硕士的一些行为。本文概述了贝叶斯 IL 框架中代理行为的关键特征，包括由各种法学硕士实验验证支持的预测。这一理论框架可以帮助更有效地预测和指导法学硕士朝着期望的方向发展。]]></description>
      <guid>https://arxiv.org/abs/2404.04286</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 识别相似数据点：使用总结和隐藏状态洞察的人机交互策略</title>
      <link>https://arxiv.org/abs/2404.04281</link>
      <description><![CDATA[arXiv:2404.04281v1 公告类型：新
摘要：本研究介绍了一种简单而有效的方法，使用大型语言模型 (LLM) 识别非自由文本域（例如表格和图像数据）中的相似数据点。我们的两步方法涉及数据点汇总和隐藏状态提取。最初，使用法学硕士通过摘要来压缩数据，降低复杂性并突出句子中的基本信息。随后，摘要句子通过另一个 LLM 来提取隐藏状态，作为紧凑、特征丰富的表示。这种方法利用了法学硕士的先进理解和生成能力，为跨不同数据集的相似性识别提供了可扩展且高效的策略。我们证明了我们的方法在识别多个数据集上的相似数据点方面的有效性。此外，我们的方法使非技术领域专家（例如欺诈调查人员或营销运营商）能够快速识别针对特定场景定制的类似数据点，展示其在实际应用中的实用性。总的来说，我们的结果为在各个领域利用法学硕士进行数据分析开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2404.04281</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:03 GMT</pubDate>
    </item>
    <item>
      <title>评估用于抑郁症检测的 ML 分类算法和 NLP 技术：实验案例研究</title>
      <link>https://arxiv.org/abs/2404.04284</link>
      <description><![CDATA[arXiv:2404.04284v1 公告类型：新
摘要：抑郁症影响着全球数百万人，已成为最常见的精神障碍之一。早期精神障碍检测可以降低​​公共卫生机构的成本并预防其他主要合并症。此外，专业人员的短缺也非常令人担忧，因为抑郁症的诊断高度依赖于专业人士，而且非常耗时。最近的研究证明，机器学习 (ML) 和自然语言处理 (NLP) 工具和技术显着有利于抑郁症的诊断。然而，在存在创伤后应激障碍（PTSD）等其他情况的抑郁症检测方法的评估中仍然存在一些挑战。这些挑战包括评估数据清理和预处理技术、特征选择和适当的机器学习分类算法方面的替代方案。本文基于比较不同机器学习分类器的案例研究来解决此类评估，特别是在数据清理和预处理、特征选择、参数设置和模型选择方面。该案例研究基于痛苦分析访谈语料库 - Wizard-of-Oz (DAIC-WOZ) 数据集，该数据集旨在支持抑郁、焦虑和 PTSD 等精神障碍的诊断。除了对替代技术的评估之外，我们还能够使用随机森林和 XGBoost 模型构建准确率约为 84% 的模型，这明显高于可比较文献中的结果，后者提出了 SVM 模型的准确率 72% 。]]></description>
      <guid>https://arxiv.org/abs/2404.04284</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:03 GMT</pubDate>
    </item>
    <item>
      <title>一种基于 BERT 的新型分类器，可根据标题检测 YouTube 视频的政治倾向</title>
      <link>https://arxiv.org/abs/2404.04261</link>
      <description><![CDATA[arXiv:2404.04261v1 公告类型：新
摘要：四分之一的美国成年人定期从 YouTube 获取新闻。然而，尽管该平台上提供了大量政治内容，但迄今为止尚未提出任何分类器来识别 YouTube 视频的政治倾向。为了填补这一空白，我们提出了一种基于 Bert（谷歌的语言模型）的新型分类器，仅根据标题将 YouTube 视频分为六类，即：远左、左、中、反唤醒、右、和极右翼。我们使用包含 1000 万个 YouTube 视频标题（不同类别）的公共数据集来训练和验证所提出的分类器。我们将分类器与在同一数据集上训练的几种替代方案进行比较，结果表明我们的分类器实现了最高的准确率 (75%) 和最高的 F1 分数 (77%)。为了进一步验证分类性能，我们从众多著名新闻机构（例如福克斯新闻和纽约时报）的 YouTube 频道收集视频，这些新闻机构的政治倾向众所周知，并将我们的分类器应用到他们的视频标题中。在绝大多数情况下，预测的政治倾向与新闻机构的政治倾向相符。]]></description>
      <guid>https://arxiv.org/abs/2404.04261</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>当亚伯杀死该隐时：机器翻译无法捕捉到的东西</title>
      <link>https://arxiv.org/abs/2404.04279</link>
      <description><![CDATA[arXiv:2404.04279v1 公告类型：新
摘要：本文旨在从结构的角度确定基于人工智能的自动翻译器无法完全捕获的内容。它重点关注机器的错误，以试图解释其原因。选择该因和亚伯的圣经故事是因为它具有丰富的解释和批评传统，而且还因为它的语义困难。为了翻译该文本，调查从语言对和界面的观察开始由最著名的机器翻译服务（谷歌翻译、DeepL）提供。然后建立了最常见翻译错误的类型。最后，比较当代翻译，以强调每个翻译的独特贡献。总之，本文建议翻译理论的修订，以及人工智能、翻译、限制、解释、比较、相对性地重新表述其有关文化文本的技术。]]></description>
      <guid>https://arxiv.org/abs/2404.04279</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:02 GMT</pubDate>
    </item>
    </channel>
</rss>