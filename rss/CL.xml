<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 10 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>自然语言规划：对自然语言规划法学硕士进行基准测试</title>
      <link>https://arxiv.org/abs/2406.04520</link>
      <description><![CDATA[arXiv:2406.04520v1 公告类型：新 
摘要：我们引入了 NATURAL PLAN，这是一个自然语言中的现实规划基准，包含 3 个关键任务：旅行规划、会议规划和日历安排。我们将评估重点放在 LLM 的规划能力上，并提供关于任务的完整信息，方法是将 Google 航班、Google 地图和 Google 日历等工具的输出作为模型的上下文。这消除了对规划 LLM 进行评估的工具使用环境的需求。我们观察到 NATURAL PLAN 是最先进的模型的一个具有挑战性的基准。例如，在旅行规划中，GPT-4 和 Gemini 1.5 Pro 分别只能实现 31.1% 和 34.8% 的解决率。我们发现，随着问题复杂性的增加，模型性能急剧下降：当有 10 个城市时，所有模型的性能都低于 5%，这突显了 SoTA LLM 在自然语言规划方面存在显著差距。我们还对 NATURAL PLAN 进行了广泛的消融研究，以进一步阐明自我校正、小样本泛化和长上下文规划等方法对改进 LLM 规划的有效性（不有效性）。]]></description>
      <guid>https://arxiv.org/abs/2406.04520</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>校对：一键修复所有错误</title>
      <link>https://arxiv.org/abs/2406.04523</link>
      <description><![CDATA[arXiv:2406.04523v1 公告类型：新
摘要：大型语言模型 (LLM) 的强大功能为重新构想用户的打字体验提供了强大的方法。本文展示了 Proofread，这是 Gboard 的一项新颖的功能，由 Gboard 中的服务器端 LLM 提供支持，只需单击即可实现无缝的句子级和段落级更正。我们在本文中描述了完整的系统，从数据生成、指标设计到模型调整和部署。为了获得具有足够质量的模型，我们实施了针对在线用例量身定制的精心数据合成管道，设计了多方面的指标，采用两阶段调整方法来获取该功能的专用 LLM：监督微调 (SFT) 用于基础质量，然后是强化学习 (RL) 调整方法用于有针对性的细化。具体而言，我们发现对重写和校对任务进行顺序调整在 SFT 阶段可获得最佳质量，并在 RL 调整阶段提出全局和直接奖励以寻求进一步改进。在人工标记的黄金集上进行的大量实验表明，我们调整后的 PaLM2-XS 模型实现了 85.56% 的良好率。我们通过在 Google Cloud 的 TPU v5 上为模型提供服务，向 Pixel 8 设备推出了该功能，每天有数千名活跃用户。通过量化、桶推理、文本分割和推测解码，服务延迟显著降低。我们的演示可以在 \href{https://youtu.be/4ZdcuiwFU7I}{Youtube} 中看到。]]></description>
      <guid>https://arxiv.org/abs/2406.04523</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>通过高效微调进行时间敏感知识编辑</title>
      <link>https://arxiv.org/abs/2406.04496</link>
      <description><![CDATA[arXiv:2406.04496v1 公告类型：新
摘要：大型语言模型 (LLM) 在不同任务中表现出令人印象深刻的能力，并为许多领域带来了变革性的变化。然而，在预训练完成后，保持 LLM 中的知识最新仍然是一个挑战。因此，设计有效的方法来更新过时的知识并将新知识引入 LLM 至关重要。现有的定位和编辑知识编辑 (KE) 方法存在两个限制。首先，通过此类方法编辑的后编辑 LLM 通常在回答需要多跳推理的复杂查询方面能力较差。其次，这种定位和编辑方法执行知识编辑的运行时间长，使其在实践中不适用于大规模 KE。在本文中，我们探索参数高效微调 (PEFT) 技术作为 KE 的替代方案。我们策划了一个更全面的时间 KE 数据集，其中包含知识更新和知识注入示例，用于 KE 性能基准测试。我们进一步探究了微调 LLM 中一系列层对多跳 QA 任务的影响。我们发现，对于时间敏感的知识编辑，PEFT 的表现优于定位和编辑技术。]]></description>
      <guid>https://arxiv.org/abs/2406.04496</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>蒸馏还是不蒸馏？关于稳健知识蒸馏的稳健性</title>
      <link>https://arxiv.org/abs/2406.04512</link>
      <description><![CDATA[arXiv:2406.04512v1 公告类型：新
摘要：众所周知，阿拉伯语对自动语音识别 (ASR) 提出了独特的挑战。一方面，其丰富的语言多样性和广泛的方言使开发稳健、包容的模型变得复杂。另一方面，当前的多语言 ASR 模型是计算密集型的，缺乏适当的综合评估。鉴于这些挑战，我们将知识从大型教师模型中提炼成效率更高的小型学生变体。我们还引入了一个新颖的人工注释数据集，涵盖五种代表性不足的阿拉伯方言以供评估。我们进一步根据标准可用基准和我们的新方言数据评估我们的模型和现有的 SoTA 多语言模型。我们的最佳提炼模型的整体性能 ($45.0$\% WER) 超过了其两倍大小的 SoTA 模型 (SeamlessM4T-large-v2，WER=$47.0$\%) 及其教师模型 (Whisper-large-v2，WER=$55.1$\%)，并且其在我们的新方言数据上的平均性能 ($56.9$\% WER) 优于所有其他模型。为了更深入地了解这些模型在方言数据上表现不佳的原因，我们进行了错误分析并报告了不同模型容易犯的主要错误类型。该项目的 GitHub 存储库位于 \url{https://github.com/UBC-NLP/distill-whisper-ar}。]]></description>
      <guid>https://arxiv.org/abs/2406.04512</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>PromptFix：通过对抗性提示调整来移除少量后门</title>
      <link>https://arxiv.org/abs/2406.04478</link>
      <description><![CDATA[arXiv:2406.04478v1 公告类型：新 
摘要：预训练语言模型 (PLM) 以其无与伦比的性能在过去几年中引起了极大的关注。同时，训练 PLM 的成本飙升以及其惊人的泛化能力共同促成了少样本微调和提示成为自然语言处理 (NLP) 模型最流行的训练范式。然而，现有研究表明，这些 NLP 模型可以被后门攻击，从而当出现触发标记时模型行为会被操纵。在本文中，我们提出了 PromptFix，这是一种通过少样本设置中的对抗性提示调整为 NLP 模型的新型后门缓解策略。与依赖于精确触发器反转和随后的模型微调的现有 NLP 后门删除方法不同，PromptFix 保持模型参数完整，并且仅使用两组额外的软标记，分别近似触发器和抵消触发器。软令牌和对抗优化的使用消除了枚举可能的后门配置的需要，并能够在触发器查找和性能保留之间实现自适应平衡。对各种后门攻击的实验验证了所提方法的有效性，并且存在域转移时的性能进一步表明 PromptFix 适用于在未知数据源上预训练的模型，这是快速调整场景中的常见情况。]]></description>
      <guid>https://arxiv.org/abs/2406.04478</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 自动检测基于 LLM 的文本游戏中的错误</title>
      <link>https://arxiv.org/abs/2406.04482</link>
      <description><![CDATA[arXiv:2406.04482v1 公告类型：新
摘要：大型语言模型 (LLM) 的进步正在彻底改变交互式游戏设计，使玩家和非玩家角色 (NPC) 之间的动态情节和互动成为可能。然而，LLM 可能会出现幻觉、健忘或误解提示等缺陷，导致逻辑不一致和与预期设计的意外偏差。检测此类游戏错误的自动技术仍然缺乏。为了解决这个问题，我们提出了一种基于 LLM 的系统方法，用于从玩家游戏日志中自动识别此类错误，从而无需收集游戏后调查等额外数据。应用于基于文本的游戏 DejaBoom！，我们的方法有效地识别了 LLM 驱动的交互式游戏中固有的错误，超越了非结构化的 LLM 驱动的错误捕捉方法，填补了自动检测逻辑和设计缺陷的空白。]]></description>
      <guid>https://arxiv.org/abs/2406.04482</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 评估文本生成中属性强度的平滑控制</title>
      <link>https://arxiv.org/abs/2406.04460</link>
      <description><![CDATA[arXiv:2406.04460v1 公告类型：新
摘要：控制文本生成的属性强度对于各种场景都至关重要（例如，写作简洁、聊天情绪和解释清晰度）。大型语言模型 (LLM) 的卓越功能彻底改变了文本生成，促使我们探索 LLM 生成的这种 \emph{平滑控制}。具体来说，我们提出了一些指标来评估生成的文本的属性强度随控制值变化的范围、校准和一致性，以及它与预期上下文的相关性。为了量化属性强度和上下文相关性，我们提出了一个有效的评估框架，利用 Elo 评分系统和 GPT4，这两者都以与人类判断的稳健一致性而闻名。我们研究了两种可行的无需训练的方法来实现 LLM 的平滑控制：（1）使用语义转换器提示，以及（2）修改内部模型表示。这两种方法的评估是在 5 个不同属性上使用各种模型进行的。我们的代码和数据集可以从 \url{https://github.com/ShangDataLab/Smooth-Control} 获取。]]></description>
      <guid>https://arxiv.org/abs/2406.04460</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>用于隐性语篇关系识别的多标签分类</title>
      <link>https://arxiv.org/abs/2406.04461</link>
      <description><![CDATA[arXiv:2406.04461v1 公告类型：新
摘要：话语关系在建立文本内容的连贯性、将句子和从句统一为一个连贯的叙述方面起着关键作用。宾夕法尼亚话语树库 (PDTB) 是该领域使用最广泛的数据集之一。在 PDTB-3 中，当注释者认为存在多种关系时，他们可以为一个示例分配多个标签。先前的话语关系识别研究在训练期间将这些实例视为单独的示例，并且只有一个示例的标签需要正确预测才能判断该实例是正确的。然而，这种方法是不够的，因为它无法解释现实世界中标签的相互依赖性，也无法区分只有一种意义关系成立的情况和多种关系同时成立的情况。在我们的工作中，我们通过探索各种多标签分类框架来处理隐式话语关系识别，以应对这一挑战。我们表明，多标签分类方法不会降低单标签预测的性能。此外，我们对结果和数据进行了全面的分析。我们的工作有助于促进话语关系的理解和应用，并为未来的研究奠定基础]]></description>
      <guid>https://arxiv.org/abs/2406.04461</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>MAIRA-2：接地放射学报告生成</title>
      <link>https://arxiv.org/abs/2406.04449</link>
      <description><![CDATA[arXiv:2406.04449v1 公告类型：新
摘要：放射学报告是一项复杂的任务，需要详细的图像理解、多种输入的整合（包括与之前成像的比较）以及精确的语言生成。这使其成为生成性多模态模型的开发和使用的理想选择。在这里，我们扩展了报告生成，以包括对图像上单个发现的定位 - 我们称之为扎根报告生成的任务。先前的研究表明，扎根对于澄清图像理解和解释 AI 生成的文本非常重要。因此，扎根报告可以提高自动报告起草的实用性和透明度。为了评估扎根报告，我们提出了一种新颖的评估框架 - RadFact - 利用大型语言模型 (LLM) 的推理能力。RadFact 评估单个生成句子的真实性，以及生成的空间定位的正确性（如果存在）。我们引入了 MAIRA-2，这是一个大型多模态模型，结合了放射学专用图像编码器和 LLM，并针对胸部 X 光片扎根报告生成这一新任务进行了训练。 MAIRA-2 使用的输入比以前探索的更全面：当前正面图像、当前侧面图像、先前正面图像和先前报告，以及当前报告的指示、技术和比较部分。我们证明这些附加功能显著提高了报告质量并减少了幻觉，在 MIMIC-CXR 上建立了发现生成（无基础）的新水平，同时证明了基础报告作为一项新颖且更丰富的任务的可行性。]]></description>
      <guid>https://arxiv.org/abs/2406.04449</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>MoralBench：法学硕士的道德评估</title>
      <link>https://arxiv.org/abs/2406.04428</link>
      <description><![CDATA[arXiv:2406.04428v1 公告类型：新
摘要：在快速发展的人工智能领域，大型语言模型 (LLM) 已成为从自然语言处理到决策支持系统等无数应用的强大工具。然而，随着这些模型越来越多地融入社会框架，确保它们在道德和道德界限内运作的必要性从未如此重要。本文介绍了一种旨在衡量和比较 LLM 道德推理能力的新基准。我们提出了第一个专门用于探究 LLM 输出的道德维度的综合数据集，解决了反映现实世界复杂性的各种道德困境和情景。
这项工作的主要贡献在于开发了用于评估 LLM 道德身份的基准数据集和指标，这些数据集和指标考虑了细微差别、语境敏感性以及与人类道德标准的一致性。我们的方法涉及多方面的方法，将定量分析与伦理学者的定性见解相结合，以确保对模型性能进行全面评估。通过将我们的基准应用于几个领先的法学硕士 (LLM)，我们发现不同模型的道德推理能力存在显著差异。这些发现强调了在法学硕士 (LLM) 的开发和评估中考虑道德推理的重要性，以及需要持续研究以解决我们在研究中发现的偏见和局限性。我们在 https://drive.google.com/drive/u/0/folders/1k93YZJserYc2CkqP8d4B3M3sgd3kA8W7 公开发布了基准，并在 https://github.com/agiresearch/MoralBench 开源代码。]]></description>
      <guid>https://arxiv.org/abs/2406.04428</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:12 GMT</pubDate>
    </item>
    <item>
      <title>TexIm FAST：使用 Transformers 进行文本到图像表示的语义相似度评估</title>
      <link>https://arxiv.org/abs/2406.04438</link>
      <description><![CDATA[arXiv:2406.04438v1 公告类型：新
摘要：自然语言处理 (NLP) 的主要目标之一是从文本生成有意义的表示。提高表示的信息量导致维数和内存占用量大幅增加。它导致级联效应，通过增加下游模型的参数来放大其复杂性。可用的技术不能应用于文本到图像等跨模态应用。为了改善这些问题，本文提出了一种新颖的文本到图像方法，用于通过自监督变分自动编码器 (VAE) 生成固定长度的表示，以应用变换器 (TexIm FAST) 进行语义评估。图形表示允许无意识推理，同时保留语言的复杂性，并且在跨模态应用中非常有效。TexIm FAST 处理可变长度序列并生成固定长度的表示，内存占用量减少 75% 以上。它通过减少参数来提高模型在下游任务中的效率。TexIm FAST 的有效性已在 MSRPC、CNN/Daily Mail 和 XSum 数据集上针对语义文本相似性 (STS) 任务进行了广泛分析。结果显示，与基线相比，准确率提高了 6%，并展示了其将文本等不同长度序列与其摘要进行比较的卓越能力。]]></description>
      <guid>https://arxiv.org/abs/2406.04438</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:12 GMT</pubDate>
    </item>
    <item>
      <title>通过黑盒访问进行大型语言模型置信度估计</title>
      <link>https://arxiv.org/abs/2406.04370</link>
      <description><![CDATA[arXiv:2406.04370v1 公告类型：新
摘要：评估模型响应的不确定性或置信度不仅对于评估响应的信任度很重要，而且对于评估整个模型的信任度也很重要。在本文中，我们探讨了仅使用黑盒或查询访问大型语言模型 (LLM) 的响应置信度的问题。我们提出了一个简单且可扩展的框架，在该框架中，我们设计新特征并在这些特征上训练（可解释的）模型（即逻辑回归）以估计置信度。我们通过经验证明，我们的简单框架可以有效地评估 flan-ul2、llama-13b 和 mistral-7b 的置信度，并且它在基准数据集（例如 TriviaQA、SQuAD、CoQA 和 Natural Questions）上的表现始终优于现有的黑盒置信度估计方法，在某些情况下甚至超过 $10\%$（在 AUROC 上）。此外，我们的可解释方法提供了对预测置信度的特征的洞察，从而带来了有趣且有用的发现：我们为一个 LLM 构建的置信度模型可以在给定数据集上将零样本推广到其他模型。]]></description>
      <guid>https://arxiv.org/abs/2406.04370</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的分阶段指令微调</title>
      <link>https://arxiv.org/abs/2406.04371</link>
      <description><![CDATA[arXiv:2406.04371v1 公告类型：新 
摘要：指令微调是一种增强预训练语言模型能力的方法，从单纯的下一个单词预测到复杂的指令跟踪，通常采用对不同指令数据集的一次性训练方法。然而，由于同时处理不同的指令复杂性，这种方法可能无法有效增强模型对指令的遵守。为了解决这个问题，我们提出了一种新的分阶段指令微调 (Phased IFT) 方法，该方法基于渐进对齐的假设，该方法假定预训练语言模型从简单的下一个单词预测到复杂的指令跟踪的过渡是一个渐进的学习过程。具体来说，我们通过 GPT-4 获得每条指令的难度分数，将指令数据分层为难度不断增加的子集，并使用标准监督损失对这些子集进行顺序训练。通过使用 52K Alpaca 指令数据对预训练模型 Llama-2 7B/13B 和 Mistral-7B 进行大量实验，我们证明了 Phased IFT 在胜率方面显著超越了传统的一次性指令微调 (One-off IFT) 方法，从而实证验证了渐进对齐假设。我们的研究结果表明，Phased IFT 提供了一种简单而有效的途径来提升预训练语言模型的指令遵循能力。我们实验中的模型和数据集可在 https://github.com/xubuvd/PhasedSFT 免费获取。]]></description>
      <guid>https://arxiv.org/abs/2406.04371</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>探索最新的 LLM 排行榜提取方法</title>
      <link>https://arxiv.org/abs/2406.04383</link>
      <description><![CDATA[arXiv:2406.04383v1 公告类型：新 
摘要：大型语言模型 (LLM) 的快速发展为自动化 AI 研究中的复杂任务开辟了新途径。本文研究了不同的 LLM-Mistral 7B、Llama-2、GPT-4-Turbo 和 GPT-4.o 在从实证 AI 研究文章中提取排行榜信息方面的功效。我们探讨了模型的三种上下文输入类型：DocTAET（文档标题、摘要、实验设置和表格信息）、DocREC（结果、实验和结论）和 DocFULL（整个文档）。我们的综合研究评估了这些模型在从研究论文中生成（任务、数据集、指标、分数）四重奏方面的表现。研究结果揭示了对每种模型和上下文类型的优势和局限性的重要见解，为未来的 AI 研究自动化工作提供了宝贵的指导。]]></description>
      <guid>https://arxiv.org/abs/2406.04383</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>SocialNLP Fake-EmoReact 2021 挑战赛概述：根据回复和 GIF 预测虚假推文</title>
      <link>https://arxiv.org/abs/2406.04368</link>
      <description><![CDATA[arXiv:2406.04368v1 公告类型：新
摘要：本文概述了在第九届 SocialNLP 研讨会上与 NAACL 2021 联合举办的 Fake-EmoReact 2021 挑战赛。该挑战赛要求使用来自 EmotionGIF 数据集的回复上下文和增强 GIF 类别来预测推文的真实性。我们提供超过 453k 的 Fake-EmoReact 数据集作为实验材料，其中每条推文都标有真实性。24 支队伍注册参加本次挑战赛，其中 5 支队伍在评估阶段成功提交了结果。最佳团队在 Fake-EmoReact 2021 数据集上使用 F1 分数获得 93.9 分。此外，我们展示了共享任务的定义、数据收集以及参加此挑战赛的团队的表现及其方法。]]></description>
      <guid>https://arxiv.org/abs/2406.04368</guid>
      <pubDate>Mon, 10 Jun 2024 06:20:10 GMT</pubDate>
    </item>
    </channel>
</rss>