<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 29 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用 LLM 和目标导向 ASP 构建的可靠常识推理社交机器人</title>
      <link>https://arxiv.org/abs/2407.18498</link>
      <description><![CDATA[arXiv:2407.18498v1 公告类型：新
摘要：大型语言模型 (LLM)（例如 GPT）的发展使得构建多个社交机器人（例如 ChatGPT）成为可能，这些机器人因其模拟人类对话的能力而备受关注。但是，对话不受目标引导，难以控制。此外，由于 LLM 更多地依赖于模式识别而不是演绎推理，因此它们可能会给出令人困惑的答案，并且难以将多个主题整合成一个有凝聚力的响应。这些限制通常会导致 LLM 偏离主要主题以保持对话的趣味性。我们提出了 AutoCompanion，这是一个社交机器人，它使用 LLM 模型将自然语言转换为谓词（反之亦然），并使用基于答案集编程 (ASP) 的常识推理与人类进行社交对话。特别是，我们依赖 s(CASP)，这是 ASP 的目标导向实现作为后端。本文介绍了框架设计以及如何使用 LLM 解析用户消息并从 s(CASP) 引擎输出生成响应。为了验证我们的提议，我们描述了（真实的）对话，其中聊天机器人的目标是通过谈论电影和书籍来让用户感到愉悦，而 s(CASP) 确保 (i) 答案的正确性、(ii) 对话过程中的连贯性（和准确性），它会动态调节以实现其特定目的，以及 (iii) 不偏离主要话题。]]></description>
      <guid>https://arxiv.org/abs/2407.18498</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>早期语音习得中感知空间的形成：跨语言建模方法</title>
      <link>https://arxiv.org/abs/2407.18501</link>
      <description><![CDATA[arXiv:2407.18501v1 公告类型：新
摘要：本研究通过在两个关键方面推进先前的研究，研究学习者如何在早期语音习得中组织感知空间。首先，它检查学习到的隐藏表示的形状及其对语音类别进行分类的能力。其次，它探索训练模型对不涉及上下文线索的上下文无关声学信息对语音习得的影响，密切模仿早期语言学习阶段。使用跨语言建模方法，自动编码器模型在英语和普通话上进行训练，并在母语和非母语条件下进行评估，遵循婴儿语言感知研究中使用的实验条件。结果表明，对上下文无关声学信息进行无监督的自下而上的训练会导致在英语和普通话的母语和非母语条件下学习到的感知空间表示相当，类似于婴儿普遍聆听的早期阶段。这些发现为早期语音习得过程中感知空间的组织提供了见解，并有助于我们理解语音类别的形成和表征。]]></description>
      <guid>https://arxiv.org/abs/2407.18501</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>规模越大就越好吗？评估和推动用于非生成性医疗任务的大型语言模型</title>
      <link>https://arxiv.org/abs/2407.18525</link>
      <description><![CDATA[arXiv:2407.18525v1 公告类型：新 
摘要：大型语言模型 (LLM) 在医学中的使用正在增长，但它们处理结构化电子健康记录 (EHR) 数据和非结构化临床笔记的能力尚未得到充分研究。本研究利用知名数据集对各种模型进行了基准测试，包括基于 GPT 的 LLM、基于 BERT 的模型和传统的临床预测模型，用于非生成性医疗任务。我们使用 MIMIC 数据集（ICU 患者记录）和 TJH 数据集（早期 COVID-19 EHR 数据）评估了 14 种语言模型（9 种基于 GPT 和 5 种基于 BERT）和 7 种传统预测模型，重点关注死亡率和再入院预测、疾病层次重建和生物医学句子匹配等任务，比较了零样本和微调性能。结果表明，当使用精心设计的提示策略时，LLM 在结构化 EHR 数据上表现出强大的零样本预测能力，经常超越传统模型。然而，对于非结构化的医学文本，LLM 的表现并不优于微调的 BERT 模型，后者在监督和无监督任务中均表现出色。因此，虽然 LLM 适用于结构化数据的零样本学习，但微调的 BERT 模型更适合非结构化文本，这凸显了根据特定任务需求和数据特征选择模型的重要性，以优化 NLP 技术在医疗保健领域的应用。]]></description>
      <guid>https://arxiv.org/abs/2407.18525</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>基于风格区分的眼科咨询角色引导大型语言模型</title>
      <link>https://arxiv.org/abs/2407.18483</link>
      <description><![CDATA[arXiv:2407.18483v1 公告类型：新
摘要：眼科问诊对于诊断、治疗和预防眼部疾病至关重要。然而，问诊需求的不断增长超过了眼科医生的可用人数。通过利用大型预训练语言模型，我们可以为特定场景设计有效的对话，帮助问诊。传统的问答任务微调策略由于模型规模的增加和问诊过程中经常忽略患者-医生角色的功能而不切实际。在本文中，我们提出了 EyeDoctor，一种眼科医学问答大型语言模型，通过引导医生-患者角色感知和增强知识库与外部疾病信息来提高准确性。实验结果表明，EyeDoctor 在眼科问诊中实现了更高的问答精度。值得注意的是，与排名第二的模型 ChatGPT 相比，EyeDoctor 在多轮数据集上的 Rouge-1 得分提高了 7.25%，F1 得分提高了 10.16%，凸显了医生与患者角色区分和动态知识库扩展对于智能医疗咨询的重要性。EyeDoc 也是一种免费的基于 Web 的服务，源代码可在 https://github.com/sperfu/EyeDoc 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.18483</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>通过结合高级 NLP、基于 Transformer 的网络和语言学方法，更准确地预测文本和多轮对话中的人类同理心和情感</title>
      <link>https://arxiv.org/abs/2407.18496</link>
      <description><![CDATA[arXiv:2407.18496v1 公告类型：新
摘要：基于 WASSA 2022 共情检测和情绪分类共享任务，我们预测了文章中表现出的共情关注和个人困扰的程度。在该项目的第一阶段，我们使用句子级嵌入作为特征实现了前馈神经网络。我们尝试了四种不同的嵌入模型来生成神经网络的输入。后续阶段以之前的工作为基础，我们实施了三种类型的修订。第一次修订侧重于模型架构和训练方法的增强。第二次修订侧重于使用分层数据采样处理类别不平衡。第三次修订侧重于利用词汇资源，我们应用四种不同的资源来丰富与数据集相关的特征。在该项目的最后阶段，我们使用一组模型来修改主要任务性能，为主要任务创建了最终的端到端系统。此外，作为最后阶段的一部分，这些方法已适应 WASSA 2023 交互中的同理心情感和人格检测共享任务，其中预测了二元文本对话中的同理心关注、情感极性和情感强度。]]></description>
      <guid>https://arxiv.org/abs/2407.18496</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>构建 CORD-19 疫苗数据集</title>
      <link>https://arxiv.org/abs/2407.18471</link>
      <description><![CDATA[arXiv:2407.18471v1 公告类型：新 
摘要：我们引入了新的数据集“CORD-19-Vaccination”，以满足专门研究 COVID-19 疫苗相关研究的科学家的需求。该数据集是从 CORD-19 数据集 [Wang et al., 2020] 中提取的，并增加了语言详细信息、作者人口统计、关键字和每篇论文主题的新列。Facebook 的 fastText 模型用于识别语言 [Joulin et al., 2016]。为了建立作者人口统计（作者隶属关系、实验室/机构位置和实验室/机构国家列），我们处理了每篇论文的 JSON 文件，然后使用 Google 的搜索 API 进一步增强以确定国家/地区值。&#39;Yake&#39; 用于从每篇论文的标题、摘要和正文中提取关键字，并使用 LDA（潜在狄利克雷分配）算法添加主题信息 [Campos et al., 2020, 2018a,b]。为了评估数据集，我们演示了一个问答任务，类似于 CORD-19 Kaggle 挑战赛中使用的任务 [Goldbloom et al., 2022]。为了进一步评估，使用 Dernoncourt et al. [2016] 的模型对每篇论文的摘要进行了顺序句子分类。我们对训练数据集进行了部分手动注释，并使用了预先训练的 BERT-PubMed 层。&#39;CORD-19-Vaccination&#39; 包含 30,000 篇研究论文，对于 NLP 研究（例如文本挖掘、信息提取和问答）非常有价值，具体到 COVID-19 疫苗研究领域。]]></description>
      <guid>https://arxiv.org/abs/2407.18471</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>使用常识增强语言模型进行多轮响应选择</title>
      <link>https://arxiv.org/abs/2407.18479</link>
      <description><![CDATA[arXiv:2407.18479v1 公告类型：新
摘要：作为高级人工智能的一个分支，对话系统正在蓬勃发展。多轮响应选择是对话系统中普遍的研究问题。在背景信息和预训练语言模型的帮助下，最先进的方法在这个问题上的表现得到了令人瞩目的提升。然而，现有的研究忽视了外部常识知识的重要性。因此，我们设计了一个孪生网络，其中预训练的语言模型与图神经网络 (SinLG) 合并。SinLG 利用预训练语言模型 (PLM) 来捕捉上下文和响应候选中的单词相关性，并利用图神经网络 (GNN) 从外部知识图中推理有用的常识。GNN 旨在协助 PLM 进行微调，并唤醒其相关记忆以获得更好的性能。具体而言，我们首先从外部知识图中提取相关概念作为节点，为每个样本构建一个以上下文响应对为超节点的子图。接下来，我们通过 PLM 和 GNN 学习上下文响应对的两个表示。利用两个表示之间的相似性损失将常识知识从 GNN 转移到 PLM。然后只使用 PLM 进行在线推理，这样可以保证效率。最后，我们对 PERSONA-CHAT 数据集的两个变体进行了广泛的实验，证明了我们的解决方案不仅可以提高 PLM 的性能，还可以实现高效的推理。]]></description>
      <guid>https://arxiv.org/abs/2407.18479</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>基于引导的命名实体识别专用领域快速数据增强</title>
      <link>https://arxiv.org/abs/2407.18442</link>
      <description><![CDATA[arXiv:2407.18442v1 公告类型：新
摘要：虽然众多领域丰富而庞大的数据集促进了自然语言处理的进步，但需要专门数据类型的部门仍然面临着寻找优质数据的挑战。我们的研究引入了一种新颖的指导数据增强技术，利用抽象的上下文和句子结构来生成不同的句子，同时保持上下文-实体关系，解决数据稀缺的挑战。通过建立上下文、句子结构和实体角色之间的更紧密关系，我们的方法提高了数据增强的有效性。因此，通过展示与实体相关的词汇和整体句子结构的多样化，同时提高命名实体识别任务的训练性能。]]></description>
      <guid>https://arxiv.org/abs/2407.18442</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>语言模型中的公平性定义解析</title>
      <link>https://arxiv.org/abs/2407.18454</link>
      <description><![CDATA[arXiv:2407.18454v1 公告类型：新
摘要：语言模型 (LM) 在各种自然语言处理 (NLP) 任务中表现出色。尽管取得了这些进步，但 LM 可以继承和放大与性别和种族等敏感属性相关的社会偏见，从而限制其在现实世界中的应用。因此，公平性在 LM 中得到了广泛的探索，从而提出了各种公平概念。然而，由于缺乏明确的共识，无法确定在特定情况下应应用哪种公平性定义（\textit{例如} 中型 LM 与大型 LM），而且理解这些定义之间的区别很复杂，可能会造成混乱并阻碍进一步的进展。为此，本文提出了一项系统调查，阐明公平性在 LM 中的应用。具体来说，我们首先简要介绍一下语言模型和语言模型中的公平性，然后全面、最新地概述语言模型中现有的公平性概念，并介绍一种新颖的分类法，该分类法根据这些概念的基本原理和操作区别对这些概念进行分类。我们通过实验进一步说明每个定义，展示它们的实际意义和结果。最后，我们讨论当前的研究挑战和未解决的问题，旨在培养创新思想并推动该领域的发展。实现和其他资源可在 https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2407.18454</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>拒绝的艺术：大型语言模型中的弃权调查</title>
      <link>https://arxiv.org/abs/2407.18418</link>
      <description><![CDATA[arXiv:2407.18418v1 公告类型：新
摘要：弃权，即大型语言模型 (LLM) 拒绝提供答案，因其在构建 LLM 系统中减轻幻觉和增强安全性的潜力而日益受到认可。在本次调查中，我们引入了一个框架，从三个角度检查弃权行为：查询、模型和人类价值观。我们回顾了关于弃权方法（根据 LLM 的开发阶段分类）、基准和评估指标的文献，并讨论了先前工作的优点和局限性。我们进一步确定并激励未来研究的领域，例如鼓励将弃权作为跨任务的元能力进行研究，并根据上下文定制弃权能力。通过这样做，我们旨在扩大人工智能系统中弃权方法的范围和影响。]]></description>
      <guid>https://arxiv.org/abs/2407.18418</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>自主合成对话与修订技术报告</title>
      <link>https://arxiv.org/abs/2407.18421</link>
      <description><![CDATA[arXiv:2407.18421v1 公告类型：新
摘要：合成数据已成为语言模型微调以遵循指令和解决复杂问题的重要工具。然而，迄今为止的大多数开放数据通常缺乏多轮数据并且是在封闭模型上收集的，这限制了开放微调方法的进展。我们引入了自导合成对话 (SDSD)，这是一个实验数据集，由语言模型自言自语的引导对话组成。该数据集包括使用 DBRX、Llama 2 70B 和 Mistral Large 生成的多轮对话，所有对话均指示遵循对话前生成的对话计划。我们还探索包括 Constitutional AI 和其他相关作品中的原则，通过对最终对话轮次的修订来创建合成偏好数据。我们希望这项工作能够鼓励进一步探索多轮数据并使用开放模型来扩大合成数据的影响。]]></description>
      <guid>https://arxiv.org/abs/2407.18421</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>通过事实检测进行可靠的声明验证</title>
      <link>https://arxiv.org/abs/2407.18367</link>
      <description><![CDATA[arXiv:2407.18367v1 公告类型：新
摘要：索赔验证可能是一项具有挑战性的任务。在本文中，我们提出了一种通过从证据中提取简短事实来增强自动索赔验证的稳健性和推理能力的方法。我们的新方法 FactDetect 利用大型语言模型 (LLM) 从证据中生成简洁的事实陈述，并根据这些事实与索赔和证据的语义相关性对这些事实进行标记。然后将生成的事实与索赔和证据相结合。为了训练轻量级监督模型，我们将事实检测任务纳入索赔验证过程，作为一种多任务方法，以提高性能和可解释性。我们还表明，在索赔验证提示中增强 FactDetect 可提高使用 LLM 进行零样本索赔验证的性能。当针对具有挑战性的科学索赔验证数据集进行评估时，我们的方法在监督索赔验证模型中表现出竞争力，F1 分数提高了 15%。我们还证明了 FactDetect 可以通过 LLM 中的零样本提示 (AugFactDetect) 来增强声明和证据，以进行判决预测。我们表明，AugFactDetect 在三个具有挑战性的科学声明验证数据集上的表现优于基线，与表现最佳的基线相比，平均性能提高了 17.3%。]]></description>
      <guid>https://arxiv.org/abs/2407.18367</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:44 GMT</pubDate>
    </item>
    <item>
      <title>PersonaGym：评估角色代理和法学硕士</title>
      <link>https://arxiv.org/abs/2407.18416</link>
      <description><![CDATA[arXiv:2407.18416v1 公告类型：新
摘要：角色代理是根据指定角色行事的 LLM 代理，已在各种应用中展示了令人印象深刻的上下文响应能力。这些角色代理在教育、医疗保健和娱乐等不同领域提供了显着的增强，模型开发人员可以根据不同的用户需求调整代理响应，从而扩大代理应用的范围。然而，评估角色代理的性能非常具有挑战性，因为在与每个角色代理相关的各种环境中评估自由形式交互中角色依从性的复杂性。我们介绍了 PersonaGym，这是第一个用于评估角色代理的动态评估框架，以及 PersonaScore，这是第一个基于决策理论的自动化人机对齐指标，用于对角色代理进行全面的大规模评估。我们对 6 个开源和闭源 LLM 进行了评估，使用包含 200 个角色和 10,000 个问题的基准，揭示了在最先进模型中角色代理能力的重大提升机会。例如，尽管 Claude 3.5 Sonnet 是一个更先进的模型，但它的 PersonaScore 仅比 GPT 3.5 提高了 2.97%。重要的是，我们发现模型大小和复杂性的增加并不一定意味着角色代理能力的增强，从而凸显了对算法和架构创新的迫切需求，以实现忠实而高性能的角色代理。]]></description>
      <guid>https://arxiv.org/abs/2407.18416</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:44 GMT</pubDate>
    </item>
    <item>
      <title>揭秘评分流程：剖析法学硕士与人工评分者在自动评分方面的差异</title>
      <link>https://arxiv.org/abs/2407.18328</link>
      <description><![CDATA[arXiv:2407.18328v1 公告类型：新
摘要：大型语言模型 (LLM) 在自动评分结构化反应评估方面表现出巨大潜力。虽然人类评分的结构化反应通常基于给定的评分标准，但 LLM 分配分数的方法仍然很大程度上不清楚。还不确定人工智能的评分过程与人类的评分过程有多接近，或者它是否遵循相同的评分标准。为了解决这一差距，本文揭示了 LLM 用于对学生对科学任务的书面回答进行评分的评分标准及其与人类分数的一致性。我们还研究了增强一致性是否可以提高评分准确性。具体来说，我们提示 LLM 生成他们用来分配分数的分析标准，并研究与人类评分标准的一致性差距。基于对 LLM 设置进行各种配置的一系列实验，我们发现人类和 LLM 评分者之间存在明显的一致性差距。虽然 LLM 可以快速适应评分任务，但它们经常走捷径，绕过人工评分中预期的更深层次的逻辑推理。我们发现，采用旨在反映人类评分逻辑的高质量分析评分标准可以弥补这一差距并提高 LLM 的评分准确性。这些结果警告不要将 LLM 简单应用于科学教育，并强调将 LLM 输出与人类期望保持一致以确保高效准确的自动评分的重要性。]]></description>
      <guid>https://arxiv.org/abs/2407.18328</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:43 GMT</pubDate>
    </item>
    <item>
      <title>医疗安全关键环境中对大型语言模型护栏的需求：药物警戒生态系统中的人工智能应用</title>
      <link>https://arxiv.org/abs/2407.18322</link>
      <description><![CDATA[arXiv:2407.18322v1 公告类型：新
摘要：大型语言模型 (LLM) 是一种有用的工具，能够以有效的规模执行特定类型的知识工作。然而，在高风险和安全关键领域部署 LLM 带来了独特的挑战，特别是“幻觉”问题，LLM 可以生成虚假信息。这在药品安全等环境中尤其令人担忧，因为不准确的信息可能会导致患者受到伤害。为了减轻这些风险，我们开发并展示了一套概念验证护栏，专门用于减轻某些类型的幻觉和药品安全错误，并可能适用于其他医疗安全关键环境。这些护栏包括检测异常文档的机制，以防止摄入不适当的数据，识别不正确的药品名称或不良事件术语，并在生成的内容中传达不确定性。我们将这些护栏与针对文本到文本任务进行微调的 LLM 集成在一起，该任务涉及将不良事件报告中的结构化和非结构化数据转换为自然语言。该方法已应用于翻译个案安全报告，证明了其在药物警戒处理任务中的有效应用。我们的护栏框架提供了一套适用于各个领域的广泛工具，通过消除关键错误的发生（包括生成不正确的药物警戒相关术语），确保 LLM 可以在高风险情况下安全使用，从而遵守医疗安全关键环境中严格的监管和质量标准。]]></description>
      <guid>https://arxiv.org/abs/2407.18322</guid>
      <pubDate>Mon, 29 Jul 2024 06:20:42 GMT</pubDate>
    </item>
    </channel>
</rss>