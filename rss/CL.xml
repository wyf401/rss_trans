<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>人工智能辅助自杀风险总结公式</title>
      <link>https://arxiv.org/abs/2412.10388</link>
      <description><![CDATA[arXiv:2412.10388v1 公告类型：新
摘要：背景：与自杀风险评估相关的制定是一个个性化的过程，旨在了解个人问题的特殊性质和发展。审核电子健康记录 (EHR) 上的临床文档具有挑战性，因为它需要资源密集型的手动工作来识别特定表格相关部分中的关键字。此外，临床医生和医疗保健专业人员通常不使用关键字；他们的临床语言可能差异很大，可能包含各种行话和首字母缩略词。此外，相关信息可能记录在其他地方。本研究描述了我们如何开发先进的自然语言处理 (NLP) 算法（人工智能 (AI) 的一个分支）来自动分析 EHR 数据。方法：使用先进的光学字符识别技术来处理非结构化数据集，例如便携式文档格式 (pdf) 文件。使用自由文本规范化技术清理和预处理自由文本数据。我们开发了算法和工具来统一自由文本。最后，使用 NLP 驱动的语义匹配技术根据相似性检查公式中是否存在每个概念。结果：我们提取了公式的指示信息并对其进行评估以涵盖相关概念。这是使用加权分数获得置信度来实现的。结论：公式完成的严谨性对于有效使用 EHR、确保正确及时地识别、参与和干预至关重要，这可能会避免许多自杀企图和自杀。]]></description>
      <guid>https://arxiv.org/abs/2412.10388</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>强化学习增强法学硕士：一项调查</title>
      <link>https://arxiv.org/abs/2412.10400</link>
      <description><![CDATA[arXiv:2412.10400v2 公告类型：新 
摘要：本文调查了使用强化学习 (RL) 增强大型语言模型 (LLM) 这一快速发展领域的研究，强化学习是一种使 LLM 能够通过根据其输出质量以奖励形式接收反馈来提高其性能的技术，从而使它们能够生成更准确、连贯和更符合上下文的响应。在这项工作中，我们系统地回顾了有关 RL 增强型 LLM 的最新知识状态，试图整合和分析该领域快速发展的研究，帮助研究人员了解当前的挑战和进步。具体来说，我们 (1) 详细介绍了 RL 的基础知识；(2) 介绍流行的 RL 增强型 LLM；(3) 回顾了两种广泛使用的基于奖励模型的 RL 技术的研究：从人类反馈中进行强化学习 (RLHF) 和从 AI 反馈中进行强化学习 (RLAIF)； (4) 探索直接偏好优化 (DPO)，这是一组绕过奖励模型直接使用人类偏好数据来使 LLM 输出与人类期望保持一致的方法。我们还将指出现有方法的当前挑战和不足，并提出一些进一步改进的途径。这项工作的项目页面可以在以下位置找到：\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}。]]></description>
      <guid>https://arxiv.org/abs/2412.10400</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估 LLM 在危机相关微博上跨事件、信息类型和语言特征的稳健性</title>
      <link>https://arxiv.org/abs/2412.10413</link>
      <description><![CDATA[arXiv:2412.10413v1 公告类型：新
摘要：灾难期间广泛使用 X（以前称为 Twitter）等微博平台为政府和响应机构提供实时信息。然而，来自这些平台的数据往往很嘈杂，需要自动化方法来过滤相关信息。传统上，人们使用监督机器学习模型，但它们缺乏通用性。相比之下，大型语言模型 (LLM) 在理解和处理开箱即用的自然语言方面表现出更好的能力。本文详细分析了六个著名的 LLM 在处理来自大量现实世界事件的灾难相关社交媒体数据方面的表现。我们的研究结果表明，虽然 LLM，特别是 GPT-4o 和 GPT-4，在不同灾难和信息类型中提供了更好的通用性，但大多数 LLM 在处理与洪水相关的数据方面面临挑战，尽管提供了示例（即镜头），但改进幅度很小，并且难以识别关键信息类别，如紧急请求和需求。此外，我们研究了各种语言特征如何影响模型性能，并强调了 LLM 在某些特征（如拼写错误）方面的弱点。最后，我们提供了零样本和小样本设置下所有事件的基准测试结果，并观察到专有模型在所有任务中的表现都优于开源模型。]]></description>
      <guid>https://arxiv.org/abs/2412.10413</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过使用可解释的 LLM 对社交媒体数据进行分类来探索复杂的心理健康症状</title>
      <link>https://arxiv.org/abs/2412.10414</link>
      <description><![CDATA[arXiv:2412.10414v1 公告类型：新
摘要：我们提出了一种洞察复杂疾病的管道，通过在具有挑战性的社交媒体文本数据分类任务上训练 LLM，获得分类输出的解释，并对解释进行定性和定量分析。我们报告了对报告莱姆病问题的人的心理健康问题预测报告的解释进行预测、解释和系统化的初步结果。我们报告了对报告焦虑症问题的人未来 ADHD 问题的预测初步结果，并展示了可视化预测有焦虑问题的人将来会有 ADHD 问题的解释的初步结果。]]></description>
      <guid>https://arxiv.org/abs/2412.10414</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成对抗性评论：当法学硕士成为批评者时</title>
      <link>https://arxiv.org/abs/2412.10415</link>
      <description><![CDATA[arXiv:2412.10415v1 公告类型：新
摘要：同行评审过程是科学进步的基础，决定了哪些论文符合出版的质量标准。然而，学术成果的快速增长和知识领域的日益专业化给传统的科学反馈机制带来了压力。鉴于此，我们引入了生成代理审阅者 (GAR)，利用 LLM 授权的代理来模拟忠实的同行评审者。为了启用生成审阅者，我们设计了一个架构，该架构扩展了具有记忆功能的大型语言模型，并为代理配备了来自历史数据的审阅者角色。这种方法的核心是基于图形的手稿表示，浓缩内容并逻辑地组织信息 - 将想法与证据和技术细节联系起来。GAR 的审查过程利用外部知识来评估论文的新颖性，然后使用图形表示和多轮评估进行详细评估。最后，元审阅者汇总个人评论以预测接受决定。我们的实验表明，GAR 在提供详细反馈和预测论文结果方面的表现与人类审阅者相当。除了单纯的绩效比较外，我们还开展了富有洞察力的实验，例如评估审稿人专业知识的影响以及检查评论的公平性。通过提供早期专家级反馈（通常仅限于有限的一组研究人员），GAR 使透明和深入的评估变得民主化。]]></description>
      <guid>https://arxiv.org/abs/2412.10415</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SUPERMERGE：一种基于梯度的模型合并方法</title>
      <link>https://arxiv.org/abs/2412.10416</link>
      <description><![CDATA[arXiv:2412.10416v1 公告类型：新
摘要：大型语言模型，例如 ChatGPT、Claude 或 LLaMA，是巨大的、单片的，并且具有同时支持数千个任务的超能力。然而，高吞吐量应用程序通常更喜欢较小的任务特定模型，因为它们的延迟和成本较低。使用任务特定模型的一个挑战是，在模型已经部署到现有任务后，需要逐步解决新任务。一种简单的解决方案需要针对现有任务和新任务再次微调模型，这在计算上是昂贵的并且耗时的。为了解决这个问题，我们提出了一种基于模型合并的方法，称为 SUPERMERGE。SUPERMERGE 是一种基于梯度的方法，用于系统地合并在现有任务和新任务上训练的几个微调模型。SUPERMERGE 的设计轻量级且快速，合并后的模型在所有任务上都实现了与完全微调模型类似的性能。此外，我们提出了一种分层模型合并策略，以减少峰值空间需求而不牺牲合并模型的性能。我们通过实验证明，SUPERMERGE 在常见的自然语言处理和计算机视觉任务上优于现有的模型合并方法。]]></description>
      <guid>https://arxiv.org/abs/2412.10416</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用音频和文本模式促进心理健康：法学硕士绩效研究</title>
      <link>https://arxiv.org/abs/2412.10417</link>
      <description><![CDATA[arXiv:2412.10417v1 公告类型：新
摘要：精神健康障碍在世界范围内日益普遍，迫切需要创新工具来支持早期诊断和干预。本研究探讨了大型语言模型 (LLM) 在多模态心理健康诊断中的潜力，特别是通过文本和音频模式检测抑郁症和创伤后应激障碍。使用 E-DAIC 数据集，我们比较文本和音频模式，以调查 LLM 在音频输入下是否能表现得同样好或更好。我们进一步研究了两种模式的整合，以确定这是否可以提高诊断准确性，这通常会提高性能指标。我们的分析特别利用了定制的指标；模式优势分数和分歧解决分数来评估组合模式如何影响模型性能。 Gemini 1.5 Pro 模型在使用组合模态时在二元抑郁分类中取得了最高分，在整个数据集中评估的 F1 得分为 0.67，平衡准确度 (BA) 为 77.4%。这些结果比其在文本模态下的表现提高了 3.1%，比音频模态提高了 2.7%，凸显了整合模态以提高诊断准确性的有效性。值得注意的是，所有结果都是在零样本推理中获得的，凸显了模型的稳健性，无需针对特定任务进行微调。为了探索不同配置对模型性能的影响，我们使用零样本和少量样本提示执行二元、严重程度和多类任务，检查提示变化对性能的影响。结果表明，文本和音频模态中的 Gemini 1.5 Pro 和文本模态中的 GPT-4o mini 等模型在多个任务的平衡准确度和 F1 得分方面往往超越其他模型。]]></description>
      <guid>https://arxiv.org/abs/2412.10417</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有推测前瞻的约束解码</title>
      <link>https://arxiv.org/abs/2412.10418</link>
      <description><![CDATA[arXiv:2412.10418v1 公告类型：新
摘要：带前瞻启发式的约束解码 (CDLH) 是一种将 LLM 生成与人类偏好相结合的高效方法。然而，每个生成的 token 的大量前瞻转出操作使得 CDLH 成本过高，导致实践中采用率低。相比之下，贪婪解码等常见解码策略效率极高，但实现的约束满足率非常低。我们提出了带推测前瞻的约束解码 (CDSL)，该技术显著提高了 CDLH 的推理效率，而不会出现贪婪解码所经历的急剧性能下降。CDSL 的动机是最近提出的推测解码的想法，它使用更小的草稿 LLM 进行生成，使用更大的目标 LLM 进行验证。在 CDSL 中，草稿模型用于生成前瞻，并通过目标 LLM 和特定于任务的奖励函数的组合进行验证。此过程通过减少计算负担来加速解码，同时保持强大的性能。我们使用三个 LLM 系列在两个约束解码任务中对 CDSL 进行评估，与 CDLH 相比，其加速性能提高了 2.2 倍到 12.15 倍，且性能没有显著降低。]]></description>
      <guid>https://arxiv.org/abs/2412.10418</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoPrep：使用多智能体框架进行自然语言问题感知数据准备</title>
      <link>https://arxiv.org/abs/2412.10422</link>
      <description><![CDATA[arXiv:2412.10422v1 公告类型：新
摘要：回答有关表格的自然语言 (NL) 问题（称为表格问答 (TQA)）非常重要，因为它使用户能够从结构化数据中快速有效地提取有意义的见解，从而弥合人类语言和机器可读格式之间的差距。这些表格中的许多都来自网络来源或现实世界场景，需要仔细的数据准备（或简称数据准备）以确保答案准确。然而，与传统的数据准备不同，问题感知数据准备引入了新的要求，其中包括给定问题的列增强和过滤以及问题感知值规范化或转换等任务。由于上述每个任务都是独一无二的，因此单个模型（或代理）可能无法在所有场景中有效执行。在本文中，我们提出了 AUTOPREP，这是一个基于大型语言模型 (LLM) 的多代理框架，它利用多个代理的优势，每个代理专门从事某种类型的数据准备，确保更准确和上下文相关的响应。给定一个表格上的 NL 问题，AUTOPREP 通过三个关键组件执行数据准备。规划器：确定逻辑计划，概述一系列高级操作。程序员：通过生成相应的低级代码将此逻辑计划转换为物理计划。执行器：迭代执行和调试生成的代码以确保正确的结果。为了支持这个多智能体框架，我们设计了一种新颖的思路链推理机制来提供高级操作建议，以及一种工具增强的低级代码生成方法。在现实世界的 TQA 数据集上进行的大量实验表明，AUTOPREP 可以通过问题感知数据准备显著改进 SOTA TQA 解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.10422</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>三思而后行：通过 GuidelineLLM 增强对有害内容的关注和警惕</title>
      <link>https://arxiv.org/abs/2412.10423</link>
      <description><![CDATA[arXiv:2412.10423v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 拥有对齐机制，但它们越来越容易受到新兴越狱攻击，这些攻击可能会破坏其对齐机制。这种漏洞对现实世界的应用构成了重大风险。现有工作在训练效率和泛化能力方面都面临挑战（即从人类反馈和红队中进行强化学习）。制定有效的策略以使 LLM 能够抵御不断发展的越狱尝试是一项重大挑战。为了应对这一挑战，我们提出了一种名为 GuidelineLLM 的新型防御范式，它可以帮助 LLM 识别可能包含有害内容的查询。在 LLM 响应查询之前，GuidelineLLM 首先识别与查询相关的潜在风险，将这些风险总结为指南建议，然后将这些指南提供给响应的 LLM。重要的是，我们的方法消除了对 LLM 本身进行额外安全微调的必要性；只有 GuidelineLLM 需要微调。这一特性增强了 GuidelineLLM 在各种 LLM 中的通用适用性。实验结果表明，GuidelineLLM 可以显著降低针对 LLM 的攻击成功率 (ASR)（平均降低 34.17% ASR），同时保持 LLM 在处理良性查询方面的有用性。代码可在 https://github.com/sqzhang-lazy/GuidelineLLM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.10423</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM-AS-AN-INTERVIEWER：通过动态 LLM 评估超越静态测试</title>
      <link>https://arxiv.org/abs/2412.10424</link>
      <description><![CDATA[arXiv:2412.10424v1 公告类型：新
摘要：我们引入了一种用于大型语言模型 (LLM) 的新型评估范式，即 LLM-as-an-Interviewer。此方法由两个阶段组成，旨在评估 LLM 的真正能力：首先，修改基准数据集以生成初始查询，其次，通过反馈和后续问题与 LLM 进行交互。与现有的评估方法（例如 LLM as a Judge）相比，我们的框架解决了几个限制，包括数据污染、冗长偏见和自我增强偏见。此外，我们表明，我们的多轮评估流程为 LLM 在现实场景中的表现提供了宝贵的见解，包括其对反馈的适应性以及处理后续问题（包括澄清或要求更多知识）的能力。最后，我们提出了面试报告，该报告全面反映了 LLM 的优势和劣势，并通过面试过程中的具体示例进行了说明。本报告提供了 LLM 功能的快照，详细介绍了其实际性能。]]></description>
      <guid>https://arxiv.org/abs/2412.10424</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自组织多 LLM 系统的主动推理：贝叶斯热力学适应方法</title>
      <link>https://arxiv.org/abs/2412.10425</link>
      <description><![CDATA[arXiv:2412.10425v1 公告类型：新
摘要：本文介绍了一种通过将主动推理与大型语言模型 (LLM) 相结合来创建自适应语言代理的新方法。虽然 LLM 表现出非凡的能力，但它们对静态提示的依赖限制了对新信息和不断变化的环境的适应。我们通过实现一个主动推理框架来解决这个问题，该框架充当基于 LLM 的代理之上的认知层，通过原则性的信息搜索行为动态调整提示和搜索策略。我们的框架使用三个状态因素（提示、搜索和信息状态）对环境进行建模，七种观察模式捕获质量指标。通过自由能原理构建代理的学习，我们可以系统地探索提示组合和搜索策略。实验结果证明了这种方法的有效性，代理开发了准确的环境动态模型，这由观察矩阵中出现的结构证明。动作选择模式揭示了复杂的探索-利用行为，从最初的信息收集过渡到有针对性的提示测试。热力学原理与语言模型功能的结合为创建稳健、适应性强的代理提供了一个原则框架，将主动推理从传统的低维控制问题扩展到高维、语言驱动的环境。]]></description>
      <guid>https://arxiv.org/abs/2412.10425</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过激活工程识别和操纵法学硕士中的性格特征</title>
      <link>https://arxiv.org/abs/2412.10427</link>
      <description><![CDATA[arXiv:2412.10427v1 公告类型：新
摘要：近年来，大型语言模型 (LLM) 领域发展迅速，这得益于人们对更高效率、可解释性和安全使用的需求。本研究基于“激活工程”这一新方法，探索了 LLM 中的个性修改，灵感来自《LLM 中的拒绝由单一方向介导》（arXiv:2406.11717）和《通过对比激活添加引导骆驼 2》（arXiv:2312.06681）等研究。我们利用激活工程开发了一种识别和调整与个性特征相关的激活方向的方法，这可能允许动态 LLM 个性微调。这项工作旨在进一步加深我们对 LLM 可解释性的理解，同时研究此类发展的伦理影响。]]></description>
      <guid>https://arxiv.org/abs/2412.10427</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检测前模仿：对齐机器风格偏好以进行机器修改文本检测</title>
      <link>https://arxiv.org/abs/2412.10432</link>
      <description><![CDATA[arXiv:2412.10432v1 公告类型：新
摘要：大型语言模型 (LLM) 彻底改变了文本生成，使得检测机器生成的文本变得越来越具有挑战性。尽管过去的方法在检测纯机器生成的文本方面取得了良好的性能，但这些检测器在区分机器修改的文本（重写、扩展和润色）方面性能不佳，这些文本与其原始的人类提示只有很小的变化。由于文本的内容可能源自人类提示，因此检测机器修改的文本通常涉及识别独特的机器风格，例如 LLM 偏爱的措辞。然而，现有的方法很难检测到隐藏在人类贡献的内容中的机器风格措辞。我们提出了“先模仿后检测”（ImBD）方法，该方法首先模仿机器风格的标记分布，然后将要测试的文本的分布与机器风格的分布进行比较，以确定文本是否经过机器修改。为此，我们引入了风格偏好优化 (SPO)，将评分 LLM 模型与机器生成的文本风格偏好对齐。然后使用对齐的评分模型计算风格条件概率曲率 (Style-CPC)，量化原始文本和条件采样文本之间的对数概率差异，以实现有效检测。我们在各种场景中进行了广泛的比较，涵盖了六个 LLM、四个不同的文本域和三种机器修订类型的文本修订。与现有的最先进方法相比，我们的方法在检测开源 LLM 修订的文本时 AUC 提高了 13%，在检测 GPT-3.5 和 GPT-4o 修订文本时的性能分别提高了 5% 和 19%。值得注意的是，我们的方法仅用 1,000 美元的样本和五分钟的 SPO 就超越了商业训练的 GPT-Zero，证明了其效率和有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.10432</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NAT-NL2GQL：一种将自然语言转化为图形查询语言的新型多智能体框架</title>
      <link>https://arxiv.org/abs/2412.10434</link>
      <description><![CDATA[arXiv:2412.10434v1 公告类型：新 
摘要：大型语言模型（LLM）的出现彻底改变了许多领域，不仅仅是传统的自然语言处理（NLP）任务。最近，将LLM应用于数据库领域的研究蓬勃发展，作为典型的非关系型数据库，LLM在图数据库研究中的使用自然受到了极大的关注。最近的努力越来越多地集中在利用LLM将自然语言转换为图查询语言（NL2GQL）。虽然取得了一些进展，但这些方法有明显的局限性，例如它们依赖于精简的流程，而这往往忽视了LLM在应对复杂NL2GQL挑战时自主规划和与其他LLM协作的潜力。为了解决这一差距，我们提出了NAT-NL2GQL，一种用于将自然语言转换为图查询语言的新型多代理框架。具体来说，我们的框架由三个协同代理组成：预处理器代理、生成器代理和精炼器代理。预处理器代理将数据处理作为上下文进行管理，包括名称实体识别、查询重写、路径链接和查询相关模式的提取等任务。生成器代理是一个在 NL-GQL 数据上训练的经过微调的 LLM，负责根据查询及其相关模式生成相应的 GQL 语句。精炼器代理的任务是使用从 GQL 执行结果中获得的错误信息来精炼 GQL 或上下文。鉴于基于 nGQL 语法的高质量开源 NL2GQL 数据集的稀缺，我们开发了 StockGQL，这是一个从金融市场图形数据库构建的数据集。它可以从以下网址获得：https://github.com/leonyuancode/StockGQL。 StockGQL 和 SpCQL 数据集上的实验结果表明，我们的方法明显优于基线方法，凸显了其推动 NL2GQL 研究的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.10434</guid>
      <pubDate>Tue, 17 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>