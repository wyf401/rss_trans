<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 18 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用DeepSeek-R1解释情感分析：表现，效率和少数学习</title>
      <link>https://arxiv.org/abs/2503.11655</link>
      <description><![CDATA[ARXIV：2503.11655V1公告类型：新 
摘要：大语言模型（LLM）的最新进展具有显着增强的情感分析功能。但是，模型性能，效率和某些最新模型的解释性之间的权衡仍然没有得到充实的态度。这项研究介绍了对DeepSeek-R1系列模型的首次全面评估，即推理开源LLM，以进行情感分析，并将其与OpenAI的GPT-4和GPT-4-MINI进行了比较。我们系统地分析了它们在几次弹性条件下的性能，最多扩展50张配置，以评估在上下文中的学习有效性。我们的实验表明，DeepSeek-R1证明了竞争精度，尤其是在多类情感任务中，同时通过其详细的推理过程提供了增强的可解释性。此外，我们强调了增加几乎没有示例对模型性能的影响，并讨论解释性和计算效率之间的关键权衡。]]></description>
      <guid>https://arxiv.org/abs/2503.11655</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>真相衰减：量化语言模型中的多弯曲</title>
      <link>https://arxiv.org/abs/2503.11656</link>
      <description><![CDATA[ARXIV：2503.11656V1公告类型：新 
摘要：大语言模型的快速改进已经揭示了人类互动中的关键挑战：粘粘一下。在这种情况下，摇摇欲坠是指模型过度同意或夸张的用户的趋势，通常是以事实准确性为代价。虽然先前的研究主要在单转交互中分析了这种行为，但在多步对话中的持久性和演变基本上仍未开发。我们介绍了Truth Decay，这是一种专门旨在评估扩展对话中的无粘合剂的基准，语言模型必须导致迭代用户反馈，挑战和说服力。我们促使模型引起四种类型的scophantic偏见。然后，我们提出和测试减少糊状策略，评估其在单步相互作用之外的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.11656</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大语言模型代理和知识图自动化数学证明生成</title>
      <link>https://arxiv.org/abs/2503.11657</link>
      <description><![CDATA[ARXIV：2503.11657V1公告类型：新 
摘要：大型语言模型在自然语言处理任务中表现出了显着的功能，包括需要多步逻辑推理的数学解决问题。但是，挑战持续存在自动化关键数学概念的识别，理解它们的相互关系以及在严格的框架内形式上证明的挑战。我们提出了一个新颖的框架，该框架利用知识图来增强LLM，以构建和形式化数学证明。我们的结果表明，在多个数据集中，使用知识图的性能得到了重大改进，在O1-Mini上的芥末酱数据集上达到了34％的成功率，并且在不同模型上始终超过2-11％的基线方法。我们展示了这种方法如何弥合自然语言理解与正式逻辑证明系统之间的差距，并在基线上取得了基础模型的提高结果。]]></description>
      <guid>https://arxiv.org/abs/2503.11657</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>logitlens4llms：将Logit Lens分析扩展到现代大型语言模型</title>
      <link>https://arxiv.org/abs/2503.11667</link>
      <description><![CDATA[ARXIV：2503.11667V1公告类型：新 
摘要：本文介绍了Logitlens4llms，该工具包将Logit Lens技术扩展到现代大型语言模型。尽管Logit Lens是了解语言模型内部表示的关键方法，但它以前仅限于早期的模型体系结构。我们的工作克服了现有实施的局限性，使该技术能够应用于最先进的体系结构（例如QWEN-2.5和LLAMA-3.1），同时自动化关键的分析工作流程。通过开发特定于组件的挂钩以捕获注意机制和MLP输出，我们的实现可以与Huggingface Transfereser库完全兼容，同时保持低推理开销。该工具包提供交互式探索和批处理处理功能，从而支持大规模的层分析。通过开源实施，我们旨在促进对大规模语言模型的内部机制进行更深入的研究。该工具包可在https://github.com/zhenyu-02/logitlens4llms上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2503.11667</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>rewwordbench：通过转换投入的基准测试和提高奖励模型的鲁棒性</title>
      <link>https://arxiv.org/abs/2503.11751</link>
      <description><![CDATA[ARXIV：2503.11751V1公告类型：新 
摘要：奖励模型已成为现代NLP中的主食，不仅是可扩展的文本评估器，而且是许多对齐配方和推理时间算法中必不可少的组件。但是，尽管最近的奖励模型提高了标准基准的性能，但这可能部分是由于过度拟合的效果，这会使人们对其真正能力的理解感到困惑。在这项工作中，我们仔细检查了奖励模型的鲁棒性以及这种过度拟合的程度。我们构建** rewwordBench **，该**以含义或排名的方式有系统地转换奖励模型输入。我们表明，即使进行较小的输入转换，最新的奖励模型即使具有较小的输入转换也遭受了巨大的性能降解，有时会降至最低的准确性，表明脆弱性。为了提高奖励模型的鲁棒性，我们建议明确训练它们，以将类似的分数分配给释义，并发现这种方法还提高了对其他不同类型的转型的鲁棒性。例如，我们可靠的奖励模型将这种退化减少了大约一半的奖励式聊天子集。此外，在对齐中使用时，我们的强大奖励模型表现出更好的实用性和提高质量更高的产出，最多可在59％的实例中与经过标准训练的RM赢得。]]></description>
      <guid>https://arxiv.org/abs/2503.11751</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>钥匙，价值，压缩：KV缓存压缩技术的系统探索</title>
      <link>https://arxiv.org/abs/2503.11816</link>
      <description><![CDATA[ARXIV：2503.11816V1公告类型：新 
摘要：大型语言模型（LLMS）在生成文本，图像和视频内容方面表现出了非凡的功能。但是，随着上下文长度的增长，注意力的计算成本随着令牌数量的数量而倍增，带来了巨大的效率挑战。本文介绍了对各种键值（KV）缓存压缩策略的分析，提供了全面的分类法，该分类法通过其基本原则和实施技术对这些方法进行了分类。此外，我们评估了它们对绩效和推理潜伏期的影响，从而为其有效性提供了重要的见解。我们的发现强调了KV缓存压缩涉及的权衡及其对处理长篇小说方案的影响，为更有效的LLM实施铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2503.11816</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>桥接LLM可访问性鸿沟？自动论文评分的闭合与开放LLM的表现，公平和成本</title>
      <link>https://arxiv.org/abs/2503.11827</link>
      <description><![CDATA[ARXIV：2503.11827V1公告类型：新 
摘要：封闭的大型语言模型（LLM）（例如GPT-4）已在多个NLP任务中设定了最新的结果，并已成为NLP和机器学习（ML）驱动的解决方案的核心。封闭的LLMS的性能和广泛的采用引发了有关其可及性，成本和透明度的可及性的巨大争论。在这项研究中，我们跨文本评估和与自动论文评分有关的文本评估和发电任务进行了九个领先的LLM，跨越封闭，开放和开源LLM生态系统的严格比较分析。我们的发现表明，对于对人类生成的论文的几次基于学习的评估，诸如Llama 3和Qwen2.5的开放性LLM在预测绩效方面与GPT-4相当地表现，在考虑与年龄相关的公平性时，不同的影响分数没有显着差异。此外，Llama 3具有可观的成本优势，其成本效益高37倍。对于生成任务，我们发现，在其语义组成/嵌入和ML评估的分数方面，由顶级开放LLM生成的论文与封闭的LLM相当。我们的发现挑战了封闭的LLM的主导地位，并强调了开放LLM的民主化潜力，这表明它们可以有效地弥合可访问性分歧，同时保持竞争性表现和公平性。]]></description>
      <guid>https://arxiv.org/abs/2503.11827</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上自动反馈生成的转移学习</title>
      <link>https://arxiv.org/abs/2503.11836</link>
      <description><![CDATA[ARXIV：2503.11836V1公告类型：新 
摘要：反馈是学习过程非常重要的部分。但是，在依靠人类标记时及时，准确的反馈是一个挑战。这是自动反馈生成试图解决的挑战。在本文中，提出了一种在非常长序列的非常小的数据集上训练这样的系统的技术。这两个属性都使这是一个非常具有挑战性的任务，但是，可以使用三阶段转移学习管道的最先进的结果，可以通过定性准确但unuman的声音结果实现。还讨论了在现实世界中使用自动论文评分和自动反馈生成系统的使用。]]></description>
      <guid>https://arxiv.org/abs/2503.11836</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一个基于变压器和原型的可解释模型，用于上下文讽刺检测</title>
      <link>https://arxiv.org/abs/2503.11838</link>
      <description><![CDATA[ARXIV：2503.11838V1公告类型：新 
摘要：具有象征性的讽刺检测对旨在执行情感分析的情感系统提出了独特的挑战。尽管这些系统通常在识别直接表达情感方面表现良好，但它们与讽刺性的字面意见和预期情感之间的固有矛盾斗争。由于基于变压器的语言模型（LMS）以其捕获上下文含义的有效能力而闻名，因此我们提出了一种利用LMS和基于原型网络的方法，通过情感嵌入来增强了可解释的讽刺检测。我们的方法本质上是可以解释的，而没有额外的事后可解释性技术。我们在三个公共基准数据集上测试我们的模型，并表明我们的模型的表现优于当前最新设备。同时，原型层通过在参考时间中通过类似示例生成解释来增强模型的固有解释性。此外，我们在消融研究中证明了不一致丧失的有效性，我们使用情感原型来构建该研究。]]></description>
      <guid>https://arxiv.org/abs/2503.11838</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenLGauge：可解释的NLG评估的指标</title>
      <link>https://arxiv.org/abs/2503.11858</link>
      <description><![CDATA[ARXIV：2503.11858V1公告类型：新 
摘要：大型语言模型（LLMS）作为NLG系统的评估者表现出巨大的潜力，可以进行高质量，无参考和多样性评估。但是，现有的基于LLM的指标遭受了两个主要缺点：依靠专有模型来生成培训数据或进行评估，以及缺乏细粒度的解释性反馈。在本文中，我们介绍了OpenLGauge，这是一种完全开源的，无参考的NLG评估度量标准，可根据误差跨度提供准确的解释。 OpenLGauge可作为较大的开放式LLM的两阶段集合，或作为一个小型的微调评估模型，具有确认的通用性，可以看不见任务，域和方面。我们广泛的元评估表明，OpenLGauge与人类判断达到了竞争性的相关性，在某些任务上表现优于最先进的模型，同时保持完全可重复性，并提供了准确的两倍以上的解释。]]></description>
      <guid>https://arxiv.org/abs/2503.11858</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT的破坏性和骆驼的内容：基于关键字的LLM中的情感表示一致性</title>
      <link>https://arxiv.org/abs/2503.11881</link>
      <description><![CDATA[ARXIV：2503.11881V1公告类型：新 
摘要：在使用大语言模型（LLM）的受控文本生成中，语言模型的解释和人类期望之间会出现差距。我们查看GPT-4和Llama-3中基于关键字的句子生成中控制情绪的问题。我们选择了四个情感表示：单词，词性 - 宽松占主导地位（VAD）维度，以词汇和数字形式和表情符号表示。我们的人类评估研究了每种表示的人类llm对齐方式，以及生成句子的准确性和现实主义。尽管诸如VAD之类的表示形式将情绪融合为易于计算的组件，但我们的发现表明，人们更多地同意LLMS在英语单词（例如“愤怒”）而不是VAD量表时的生成。将数字VAD与单词进行比较时，这种差异尤其明显。但是，我们发现将最初数量的VAD量表转换为词汇量表（例如，+4.0变为“高”）大大改善了一致性。此外，对生成句子的感知的感知高度取决于LLM，代表类型以及它是哪种情感。]]></description>
      <guid>https://arxiv.org/abs/2503.11881</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过迭代和邻居辅助模型编辑解决Budtered＆Overedit</title>
      <link>https://arxiv.org/abs/2503.11895</link>
      <description><![CDATA[ARXIV：2503.11895V1公告类型：新 
摘要：大型语言模型（LLMS）用于各种下游语言任务，这对于保持其知识的最新性至关重要，但是对模型进行了重新调整和微调可能是昂贵的。模型编辑仅通过单个更新提供了一个模型参数的关键子集，提供了有效的替代方案。在高效的同时，这些方法并不完美。有时，知识编辑是不成功的，即基金会或编辑污染的邻近知识，应保持不变，即覆盖。为了解决这些局限性，我们提出了迭代模型编辑，基于我们的假设，即单个参数更新通常不足以减轻跨越的模型编辑，并且在编辑过程中结合了相邻的知识以最大程度地减少过度编辑。广泛的实验表明，在多个模型编辑算法，LLMS和基准数据集中，我们的方法有效地将底部的设计有效地降低了38个百分点，并将其覆盖高达6个百分点。]]></description>
      <guid>https://arxiv.org/abs/2503.11895</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>翻译的LLM：历史，低资源的语言和现代AI模型</title>
      <link>https://arxiv.org/abs/2503.11898</link>
      <description><![CDATA[ARXIV：2503.11898V1公告类型：新 
摘要：大型语言模型（LLMS）在执行各种任务（包括机器翻译（MT））的情况下表现出显着的适应性，而无需明确的培训。诸如OpenAI的GPT-4和Google的双子座之类的模型经常在翻译基准上进行评估，并由于其高性能而被用作翻译工具。本文研究了双子座在将18世纪奥斯曼帝国手稿，《异教徒的囚徒：Timisoara的Osman Agha的回忆录》翻译成英文时的表现。手稿讲述了奥斯曼·阿哈（Osman Agha）的经历，奥斯曼（Osman Agha）的经历在奥地利度过了11年的战俘，其中包括他对战争和暴力的说法。我们的分析表明，双子座的安全机制在手稿的14％至23％之间被标记为有害，导致未翻译的通道。这些安全环境虽然有效地减轻潜在的伤害，但仍阻碍了模型提供历史文本的完整和准确翻译的能力。通过真实的历史示例，本研究突出了当前LLM安全实现在处理敏感和上下文富含材料中的固有挑战和局限性。这些现实世界中的实例强调了LLM在当代翻译方案中的潜在失败，在当代的翻译情景中，准确而全面的翻译是至关重要的例子，将现代战争受害者的法律程序或人道主义文献的叙述翻译了。]]></description>
      <guid>https://arxiv.org/abs/2503.11898</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>lag-mmlu：在拉脱维亚和giriama的基准测试前沿LLM理解</title>
      <link>https://arxiv.org/abs/2503.11911</link>
      <description><![CDATA[ARXIV：2503.11911V1公告类型：新 
摘要：随着大型语言模型（LLMS）迅速发展，评估其性能至关重要。 LLM经过多语言数据的培训，但其推理能力主要使用英语数据集评估。因此，使用高质量的非英语数据集需要强大的评估框架，尤其是低资源语言（LRLS）。这项研究使用大量的多任务语言理解（MMLU）子集评估了八个最先进的LLM（SOTA）在拉脱维亚和Giriama上的LLM，该语言和以语言和文化相关性为例。 Giriama首次进行基准测试。我们的评估表明，OpenAI的O1模型在所有语言上都优于其他语言，英语得分为92.8 \％，在Latvian中为88.8 \％，在0-Shot Tasks上的Giriama中为70.8 \％。在拉脱维亚人和吉里亚玛（Latvian and Giriama）的Mistral-Large（35.6 \％）和Llama-70B IT（41 \％）的性能较弱。我们的结果强调了在推进文化AI情境化方面对局部基准和人类评估的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.11911</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>恢复：具有自然语言批评和叙述的数据集和基准</title>
      <link>https://arxiv.org/abs/2503.11924</link>
      <description><![CDATA[ARXIV：2503.11924V1公告类型：新 
摘要：本文介绍了一种新颖的数据集重新（用生成叙事增强的评论），旨在基于推荐的大语言模型（LLMS）的对话能力，从而解决了主要关注顺序项目预测的现有数据集的局限性。 Regen通过介绍两个关键的自然语言特征来扩展亚马逊产品评论数据集：（1）用户评论，代表用户“转向”查询，导致选择后续项目的选择，以及（2）叙述，富裕的文本输出与每个推荐项目相关的文本输出。叙述包括产品认可，购买说明以及用户偏好的摘要。
  此外，我们为对话推荐的任务建立了端到端的建模基准，在该任务中，培训了模型以生成建议和相应的叙述，并以用户历史记录（项目和评论）为条件。对于这项联合任务，我们介绍了一个建模框架Lumen（基于LLM的统一多任务模型，具有批评，建议和叙述），该模型使用LLM作为批评，检索和发电的骨干。我们还使用标准自动评估技术评估了数据集的质量，并通过培训传统和基于LLM的建议模型来基准测试。我们的结果表明，通过使建议者了解语言理解并将其与建议信号集成在一起，可以提高建议质量。此外，在我们的数据集中培训的LLM有效地产生了建议和上下文叙述，实现了与最先进的建议者和语言模型相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2503.11924</guid>
      <pubDate>Tue, 18 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>