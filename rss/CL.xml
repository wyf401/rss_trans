<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>CHATGPT或无处不在的助手：大语言模型的调查</title>
      <link>https://arxiv.org/abs/2503.17403</link>
      <description><![CDATA[arxiv：2503.17403v1公告类型：新 
摘要：大型语言模型（LLMS）已将自然语言处理自然语言处理（NLP）撤销，聊天生成的预训练的变压器（CHATGPT）是其高级功能和广泛应用的著名审查。这项调查提供了对Chatgpt的全面分析，探索其架构，培训过程和功能。我们研究了它与客户服务，教育，医疗保健和娱乐等行业之间的各个领域的集成。与其他LLMS的比较分析突出了Chatgpt的独特功能和性能指标。关于基准，本文研究了Chatgpt与其他LLM的比较性能，并讨论了潜在的风险，例如错误信息，偏见和数据隐私问题。此外，我们提供了许多数字和表格，概述了讨论的背景，文章的主要思想，众多LLM模型，用于预训练，微调和评估的数据集列表，以及特定的LLM应用程序。最后，我们确定了未来的研究方向和技术进步，强调了LLM的不断发展的景观及其对人工智能人工智能（AI）和社会的深远影响。]]></description>
      <guid>https://arxiv.org/abs/2503.17403</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对长上下文语言建模的全面调查</title>
      <link>https://arxiv.org/abs/2503.17407</link>
      <description><![CDATA[ARXIV：2503.17407V1公告类型：新 
摘要：在自然语言处理中，有效地处理长篇小说一直是一种持续的追求。随着越来越多的文档，对话和其他文本数据的数量，开发长上下文语言模型（LCLM）很重要，这些语言模型（LCLM）可以以有效而有效的方式处理和分析广泛的输入。在本文中，我们介绍了一项有关大语模型长篇小说建模的最新进展的综合调查。我们的调查围绕三个关键方面进行了结构：如何获得有效，有效的LCLM，如何有效培训和部署LCLM，以及如何全面评估和分析LCLM。对于第一个方面，我们讨论了以长上下文处理为导向的数据策略，架构设计和工作流程方法。在第二方面，我们对LCLM培训和推理所需的基础设施进行了详细的检查。对于第三方面，我们提出了长篇文化理解和长期产生以及LCLM的行为分析和机制的评估范例。除了这三个关键方面，我们还彻底探讨了已经部署现有LCLM的各种应用程序方案并概述了有希望的未来开发方向。这项调查提供了有关长篇文献LLM的文献的最新审查，我们希望为研究人员和工程师提供宝贵的资源。收集最新论文和存储库的一个相关的GitHub存储库可在以下网址获得：\ href {https://github.com/lclm-horizo​​n/a-comprehand-survey-survey-survey-for-long--context-context-context-language-modeling} {\ color [rgb]]]></description>
      <guid>https://arxiv.org/abs/2503.17407</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越否定检测：临床NLP的全面断言检测模型</title>
      <link>https://arxiv.org/abs/2503.17425</link>
      <description><![CDATA[ARXIV：2503.17425V1公告类型：新 
摘要：断言状态检测是临床NLP的关键但经常被忽略的组成部分，对于准确归因于提取的医学事实至关重要。过去的研究狭窄地专注于否定检测，导致诸如AWS医学理解，Azure AI文本分析和GPT-4O等表现不佳的商业解决方案，因为它们的域名有限。为了解决这一差距，我们开发了最新的断言检测模型，包括精细调整的LLM，基于变压器的分类器，少量分类器和深度学习（DL）方法。我们根据基于云的商业API解决方案，基于旧规则的NEGEX方法和GPT-4O评估了这些模型。我们的微调LLM达到了最高的总体精度（0.962），表现优于GPT-4O（0.901）和商业API，并具有明显的差距，尤其是当前（+4.2％），缺乏（+8.4％），以及假设（+23.4％）的主张。我们的基于DL的模型超过条件（+5.3％）和相关的someone-Else（+10.1％）类别的商业解决方案，而少数拍摄的分类器提供了一种轻巧但高度竞争的替代方案（0.929），使其非常适合资源受限环境。集成在Spark NLP中，我们的模型始终胜过黑盒商业解决方案，同时可以与医学NER，关系提取和术语解决方案实现可扩展的推理和无缝集成。这些结果增强了针对域的适应性，透明和可定制的NLP解决方案的重要性，而不是通用LLM和专有API的重要性。]]></description>
      <guid>https://arxiv.org/abs/2503.17425</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言特定的神经元不会促进跨语性转移</title>
      <link>https://arxiv.org/abs/2503.17456</link>
      <description><![CDATA[ARXIV：2503.17456V1公告类型：新 
摘要：多语言大语言模型（LLM）旨在跨不同语言的自然语言理解，但其表现在低资源语言上大大降低。这项工作探讨了是否可以利用识别特定语言神经元的现有技术来增强低位分子语言的跨语性任务性能。我们进行了详细的实验，涵盖了现有的语言特异性神经元识别技术（例如语言激活概率熵和基于激活概率的阈值）和神经元特异性的Lora微调，并使用Llama 3.1和Missral Nemo等模型进行了微调。我们发现，这种特定于神经元的干预措施不足以对下游任务（XNLI，Xquad）进行低分子语言的跨语性改进。这项研究强调了实现跨语性概括的挑战，并为多语言LLM提供了关键的见解。]]></description>
      <guid>https://arxiv.org/abs/2503.17456</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Convogen：使用合成数据增强对话AI：一种多代理方法</title>
      <link>https://arxiv.org/abs/2503.17460</link>
      <description><![CDATA[ARXIV：2503.17460V1公告类型：新 
摘要：在本文中，我们介绍了Convogen：一种创新的框架，用于使用多代理系统生成合成对话数据。我们的方法利用了很少的学习学习，并从动态更新的几个弹药中心引入了迭代采样，以创建多样化和现实的对话场景。生成的数据具有许多应用程序，包括培训和评估对话式AI模型，并为对话意图分类或对话摘要等任务增强现有数据集。我们的实验证明了该方法在产生高质量的合成对话数据方面的有效性，突出了其增强对话AI系统的开发和评估的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.17460</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>沙于文化：用于评估大型语言模型文化能力的基准</title>
      <link>https://arxiv.org/abs/2503.17485</link>
      <description><![CDATA[ARXIV：2503.17485V1公告类型：新 
摘要：大型语言模型（LLMS）在自然语言处理中表现出了显着的功能；但是，他们经常难以准确捕捉和反映文化细微差别。这项研究通过关注沙特阿拉伯，这是一个以多种方言和丰富的文化传统为特征的国家来解决这一挑战。我们介绍了一项新颖的基准，旨在评估LLM在沙特阿拉伯独特的地理和文化背景下的文化能力。沙于文化是一个综合的数据集，涵盖了五个主要地理区域，例如西，东，南，北部和中心，以及在所有地区都适用的一般问题。数据集涵盖了许多文化领域，包括食物，服装，娱乐，庆祝活动和手工艺品。为了确保严格的评估，沙特文化包括不同复杂性的问题，例如开放式，单选项和多项选择格式，有些需要多个正确的答案。此外，数据集还区分了常见的文化知识和专业的区域方面。我们对五个LLM进行了广泛的评估，例如GPT-4，Llama 3.3，Fanar，Jais和Acegpt，分析了它们在不同问题类型和文化背景的绩效。我们的发现表明，当面对高度专业化或特定于地区的问题，尤其是那些需要多个正确回答的问题时，所有模型都会均均有显着的性能下降。此外，某些文化类别比其他文化类别更容易识别，这进一步强调了LLMS文化理解的不一致。这些结果强调了将特定区域知识纳入LLMS培训以增强其文化能力的重要性。]]></description>
      <guid>https://arxiv.org/abs/2503.17485</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>判断任何事情：MLLM作为任何方式的法官</title>
      <link>https://arxiv.org/abs/2503.17489</link>
      <description><![CDATA[ARXIV：2503.17489V1公告类型：新 
摘要：评估有关开放式多模式理解（MMU）和生成（MMG）任务（例如，图像，音频，视频）的生成基础模型，由于跨模式相互作用的复杂性，构成了重大挑战。为此，随着自动化法官的出现，利用多模式LLM（MLLM）的想法已经出现，从而令人鼓舞地评估视力语言理解任务。 Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks.具体而言，Taskything评估了15个对任何一种模式类别的MMU和MMG功能，采用了1,500个从建立的基准测试中策划的查询。此外，审判从对比较和得分评估的角度评估了5个高级（例如GPT-4O和Gemini-2.0-Flash）的评判功能，从而提供了一个标准化的测试台，并提供了人类判断和详细的专栏。 Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues.为了解决这个问题，我们提出了Omniarena，这是一个自动化平台，用于评估Omni模型和多模式奖励模型。我们的工作强调了需要更公平的评估协议，并且与人类偏好更加一致。源代码和数据集可公开可用：https：//urrealhero.github.io/judgeanythingweb/。]]></description>
      <guid>https://arxiv.org/abs/2503.17489</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>后续问题生成增强的患者提供对话</title>
      <link>https://arxiv.org/abs/2503.17509</link>
      <description><![CDATA[ARXIV：2503.17509V1公告类型：新 
摘要：后续问题生成是对话系统的重要特征，因为它可以减少对话歧义并增强建模复杂的相互作用。会话环境通常会带来核心NLP挑战，例如（i）提取埋在零散数据源中的相关信息，以及（ii）对平行思维过程进行建模。这两个挑战经常出现在医学对话中，因为医生不仅基于患者的话语，而且还提出了以前的EHR数据和当前诊断假设的问题。在异步对话中提出医疗问题会使这些问题加剧了这些问题，因为医生只能依靠静态EHR信息来激发后续问题。
  为了应对这些挑战，我们介绍了后续行动，这是一种增强异步医疗对话的新型框架。后续行动是一个多代理框架，可处理患者消息和EHR数据以产生个性化的后续问题，从而阐明患者报告的医疗状况。后续行动将必要的提供商的后续通信减少34％。它还分别在实际和合成数据上提高了17％和5％的性能。我们还发布了第一个与链接的EHR数据的异步医疗信息的公共数据集以及临床专家为更广泛的NLP研究社区编写的2300个后续问题。]]></description>
      <guid>https://arxiv.org/abs/2503.17509</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型可能会逐字完成，但没有明确培训</title>
      <link>https://arxiv.org/abs/2503.17514</link>
      <description><![CDATA[ARXIV：2503.17514V1公告类型：新 
摘要：当今的一个重要问题是，是否使用给定文本训练大型语言模型（LLM）。经常采用A \ Emph {postemion}测试：检查LLM是否完成了足够复杂的文本。但是，这需要对成员的基础定义。最常见的是，它根据目标文本和数据集中的任何文本之间的$ n $ gram重叠定义为成员。在这项工作中，我们证明了基于$ n $ gram的会员定义可以有效地进行认可。我们研究给定$ n $的序列\ emph {nonembers}的方案，我们发现完成测试仍然成功。我们发现了许多自然案例，通过删除所有完成的训练样本后，通过从头开始重新研究LLM。这些案例包括精确的重复，近乎解复器，甚至是短的重叠。他们展示了很难为会员定义找到单一可行的$ n $选择。使用这些见解，我们设计了可以导致给定目标序列完成的对抗数据集，而无需包含它，以选择$ n $的任何合理选择。我们的发现突出了$ n $ gram会员资格的不足，这表明会员定义无法解决培训算法可用的辅助信息。]]></description>
      <guid>https://arxiv.org/abs/2503.17514</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯教学在大语言模型中启用概率推理</title>
      <link>https://arxiv.org/abs/2503.17523</link>
      <description><![CDATA[ARXIV：2503.17523V1公告类型：新 
摘要：基于大语言模型（LLM）的人工智能系统越来越多地用作与用户和世界互动的代理。为此，LLMS需要成功地构建世界内部表示，并就这些表示形式形成概率的信念。为了为用户提供个性化的建议，例如，LLM需要在多个交互过程中逐渐推断用户的喜好。为了评估当代LLM是否能够做到这一点，我们使用概率理论的贝叶斯推理框架，该理论列出了在收到新信息时更新代理商信念的最佳方法。我们首先表明LLM并未像贝叶斯框架那样更新他们的信念，因此，随着更多信息的可用信息，他们的预测并没有改善，甚至比我们发现的人类所发现的要少。为了解决这个问题，我们通过训练来模仿最佳贝叶斯模型的预测来教LLMS以贝叶斯的方式进行推理。我们发现，这种方法不仅可以显着提高LLM在接受培训的特定建议任务上的性能，而且还可以对其他任务进行概括。这表明该方法赋予LLM具有更广泛的贝叶斯推理技能。更普遍地，我们的结果表明，LLM可以有效地学习推理策略并将这些技能推广到新领域，这部分解释了LLMS的经验成功。]]></description>
      <guid>https://arxiv.org/abs/2503.17523</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用人类生产解释不对称测试LLM认知合理性</title>
      <link>https://arxiv.org/abs/2503.17579</link>
      <description><![CDATA[ARXIV：2503.17579V1公告类型：新 
摘要：大型语言模型（LLMS）是否与人类相似的语言是许多理论和实用辩论的主题。我们通过在人类句子处理中发现的生产解释区别的角度来研究这个问题，并评估指导调节的LLMS在多大程度上复制这种区别。将人类的生产与解释之间的经验文献记录在隐式因果关系动词中作为测试床，我们发现某些LLMS在定量上和定性地反映了生产和解释之间的类似人类的不对称性。我们证明，这种行为是否取决于两个模型大小 - 更大的模型更有可能反映人类样模式以及用于引起行为的元语言提示的选择。]]></description>
      <guid>https://arxiv.org/abs/2503.17579</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GPBENCH：作为一般从业者评估大型语言模型的全面且细粒度的基准</title>
      <link>https://arxiv.org/abs/2503.17599</link>
      <description><![CDATA[ARXIV：2503.17599V1公告类型：新 
摘要：通过提供持续和全面的医疗服务，全科医生（GPS）是主要医疗保健系统的基石。但是，由于其实践，不平衡的培训和资源差距的性质，全科医生之间的临床水平在各个地区和医疗机构之间可能会有很大差异。目前，大型语言模型（LLMS）在临床和医疗应用中表现出巨大的潜力，使其成为支持通用实践的有前途的工具。但是，大多数现有的基准测试框架和评估框架都集中在考试式评估上 - 典型的多项选择问题 - 插件综合评估集，这些评估集准确地反映了GPS遇到的真实情况。为了评估LLM在GP的日常工作中如何有效地做出决策，我们设计了GPBench，其中包括临床实践和新颖的评估框架中的测试问题。该测试集包括评估一般实践的基本知识以及基于场景的问题的多项选择问题。所有问题均由专家精心注释，并结合了与临床管理有关的丰富细粒度信息。拟议的LLM评估框架基于通用实践的能力模型，提供了评估现实世界中LLM性能的全面方法。作为针对GP决策情况的第一个大型模型评估集，GPBench使我们能够评估当前的主流LLM。专家评估和评估表明，在疾病分期，并发症识别，治疗细节和用药等领域，这些模型至少表现出十个主要缺点。总体而言，现有的LLM尚不适合在没有人类监督的情况下在现实世界中的GP工作场景中独立使用。]]></description>
      <guid>https://arxiv.org/abs/2503.17599</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用角色敏感的对比学习增强LLMS角色扮演角色扮演的角色一致性</title>
      <link>https://arxiv.org/abs/2503.17662</link>
      <description><![CDATA[ARXIV：2503.17662V1公告类型：新 
摘要：近年来，大型语言模型（LLMS）在许多对话生成任务中取得了突破性的进步。但是，他们缺乏情感和精细的角色意识限制了模型进一步提供个性化和多样化互动的能力。当前的方法在为角色扮演等场景和传统人类一致性方法收集高质量注释的数据时面临高昂的成本，由于在角色扮演场景中模型行为的固有多样性，很难部署。 Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named \textbf{\underline{P}}ersona-Aware \textbf{\underline{C}}ontrastive \ textbf {\ underline {l}} restning（pcl）在角色扮演过程中与LLMS的行为保持一致，从而增强了模型的角色一致性。具体而言，我们首先设计了一种角色链方法，以鼓励模型根据角色特征和对话环境来调整人格一致性。然后，我们通过在角色特征的使用（而不是）之间进行迭代对比学习进一步增强了模型的角色扮演策略。在Black-Box和White-Box LLMS上进行的实验表明，在自动评估方法（Chareval \＆GPT-4）和人类专家评估下，配备PCL的LLMS明显胜过了PCL的LLM。]]></description>
      <guid>https://arxiv.org/abs/2503.17662</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS可以自动化事实核对文章写作吗？</title>
      <link>https://arxiv.org/abs/2503.17684</link>
      <description><![CDATA[ARXIV：2503.17684V1公告类型：新 
摘要：自动事实检查旨在通过提供可以帮助加快手动事实检查的工具来支持专业事实检查器。然而，现有框架无法解决生产适合广泛传播给公众的输出的关键步骤：当人类事实检查者通过事实检查文章传达其发现，而自动化系统通常很少或根本没有理由进行评估。在这里，我们的目标是弥合这一差距。我们主张需要通过自动生成完整的事实检查文章来扩展典型的自动事实检查管道。我们首先通过对领先事实检查组织的专家进行的一系列访谈来确定此类文章的关键Desiderata。然后，我们开发了基于LLM的代理框架Qraft，模仿了人类事实检查者的写作工作流程。最后，我们通过通过专业事实检查者的人类评估来评估QRAFT的实际实用性。我们的评估表明，尽管Qraft的表现优于先前提出的文本生成方法，但它远远落后于专家写的文章。我们希望我们的工作能够以这个新的重要方向进行进一步的研究。]]></description>
      <guid>https://arxiv.org/abs/2503.17684</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用合成数据和错误注入增强阿拉伯语自动论文评分</title>
      <link>https://arxiv.org/abs/2503.17739</link>
      <description><![CDATA[ARXIV：2503.17739V1公告类型：新 
摘要：自动化论文评分（AES）在评估语言学习者的写作质量，降低等级工作量并提供实时反馈方面起着至关重要的作用。阿拉伯语AES系统尤其受到带注释的论文数据集的挑战。本文提出了一个新的框架，利用大型语言模型（LLM）和变形金刚生成了AES的合成阿拉伯论文数据集。我们促使LLM跨CEFR生成论文水平水平，并使用微型标准的阿拉伯BERT模型引入控制错误注入，以进行错误类型预测。我们的方法产生了类似人类的文章，贡献了3,040个注释论文的数据集。此外，我们开发了一个基于BERT的自动标记系统，以进行准确且可扩展的阿拉伯论文评估。实验结果证明了我们框架在改善阿拉伯AES性能方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.17739</guid>
      <pubDate>Tue, 25 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>