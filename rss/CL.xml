<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 05 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>CycleResearcher：通过自动审查改进自动化研究</title>
      <link>https://arxiv.org/abs/2411.00816</link>
      <description><![CDATA[arXiv:2411.00816v1 公告类型：新
摘要：科学发现的自动化一直是研究界的一个长期目标，其驱动力是加速知识创造的潜力。虽然使用商业大型语言模型 (LLM) 作为研究助手或创意生成器已经取得了重大进展，但使用开源 LLM 自动化整个研究过程的可能性仍未得到充分探索。本文探讨了使用开源后训练 LLM 作为自主代理的可行性，这些代理能够执行从文献综述和手稿准备到同行评审和论文修订的全周期自动化研究和审查。我们的迭代偏好训练框架包括执行研究任务的 CycleResearcher 和模拟同行评审过程的 CycleReviewer，通过强化学习提供迭代反馈。为了训练这些模型，我们开发了两个新的数据集，Review-5k 和 Research-14k，反映了现实世界的机器学习研究和同行评审动态。我们的结果表明，CycleReviewer 在预测论文分数方面的平均绝对误差 (MAE) 比单个人类审阅者提高了 26.89%，这表明 LLM 在研究评估方面的表现可以超越专家级。在研究中，CycleResearcher 模型生成的论文在模拟同行评审中获得了 5.36 分，超过了人类专家的预印本水平 5.24，接近已接受论文的水平 5.69。这项工作代表着朝着完全自动化的科学探究迈出了重要一步，提供了道德保障并提高了人工智能驱动的研究能力。代码、数据集和模型权重在 \url{http://github/minjun-zhu/Researcher} 上发布。]]></description>
      <guid>https://arxiv.org/abs/2411.00816</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型中的文化意识调查：文本及其他</title>
      <link>https://arxiv.org/abs/2411.00860</link>
      <description><![CDATA[arXiv:2411.00860v1 公告类型：新
摘要：在聊天机器人和虚拟助手等各种应用中大规模部署大型语言模型 (LLM) 需要 LLM 对用户具有文化敏感性，以确保包容性。文化在心理学和人类学中得到了广泛的研究，最近出现了大量关于如何使 LLM 在 LLM 中更具文化包容性的研究，这些研究超越了多语言性，并以心理学和人类学的研究成果为基础。在本文中，我们调查了将文化意识纳入基于文本和多模式的 LLM 的努力。我们首先定义 LLM 中的文化意识，以人类学和心理学中对文化的定义为出发点。然后，我们研究了用于创建跨文化数据集的方法、下游任务中的文化包容性策略以及用于对 LLM 中的文化意识进行基准测试的方法。此外，我们讨论了文化契合的伦理影响、人机交互在推动法学硕士文化包容性方面的作用以及文化契合在推动社会科学研究中的作用。最后，我们根据对文献空白的发现，为未来的研究提供了指引。]]></description>
      <guid>https://arxiv.org/abs/2411.00860</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>下一个标记预测任务假设 LLM 训练在证明生成中采用最佳数据排序</title>
      <link>https://arxiv.org/abs/2411.00863</link>
      <description><![CDATA[arXiv:2411.00863v1 公告类型：新
摘要：在基于大型语言模型 (LLM) 的证明生成领域，尽管这些模型在 OpenWebMath 和 Arxiv 等大量语料库上进行了训练，但它们在中等难度的证明任务上仍然表现不佳。我们认为，这部分是由于训练中使用的每个证明数据的顺序不是最优的。已发布的证明通常遵循纯逻辑顺序，其中每个步骤都基于演绎规则从前面的步骤逻辑地进行。然而，这种顺序旨在促进对证明的合理性的验证，而不是帮助人们和模型学习证明的发现过程。在证明生成中，我们认为，当证明中特定证明步骤的相关中间监督始终位于该证明步骤的左侧时，一个训练数据样本的最佳顺序就会发生。我们将这种顺序称为直观顺序。我们使用两个任务来验证我们的主张：直觉命题逻辑定理证明和数字乘法。我们的实验验证了顺序效应并为我们的解释提供了支持。我们证明，当证明按照直观的顺序进行时，训练是最有效的。此外，顺序效应和在不同数据顺序上训练的模型之间的性能差距是巨大的——在命题逻辑定理证明任务中，与以最差顺序训练的模型相比，以最佳顺序训练的模型的证明成功率提高了 11%。]]></description>
      <guid>https://arxiv.org/abs/2411.00863</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索知识不匹配假设：小模型中的幻觉倾向根据大模型的数据进行微调</title>
      <link>https://arxiv.org/abs/2411.00878</link>
      <description><![CDATA[arXiv:2411.00878v1 公告类型：新
摘要：最近，通过使用来自较大模型的数据进行微调而创建的大型语言模型数量激增。这些小型模型能够产生与大型模型在质量上相似的输出。然而，这些模型的一个关键限制是它们比大型模型更容易产生幻觉。具体来说，据观察，它们会产生连贯的输出，这些输出涉及事实上不正确的信息，并传播错误信息、毒性和刻板印象。幻觉有很多潜在的原因，其中一种假设是，对大型模型产生的数据进行微调会导致知识不匹配，从而导致幻觉。具体来说，假设输入到模型中进行微调的知识与图中已经存在的知识之间存在不匹配。对具有这种不匹配的数据进行模型微调可能会导致幻觉倾向增加。我们表明，在看不见的测试集上，与基于小模型创建的数据进行微调的模型相比，基于较大模型生成的数据进行微调的较小模型产生了更多的错误答案，这证实了这一假设。]]></description>
      <guid>https://arxiv.org/abs/2411.00878</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考规模：精细化开源法学硕士在大规模可重复社会科学研究中的功效</title>
      <link>https://arxiv.org/abs/2411.00890</link>
      <description><![CDATA[arXiv:2411.00890v1 公告类型：新
摘要：大型语言模型 (LLM) 以其架构为特征，这决定了它们的参数大小和性能能力。社会科学家越来越多地采用 LLM 进行文本分类任务，而这些任务很难通过人类编码员进行扩展。虽然非常大的闭源模型通常提供卓越的性能，但它们的使用存在重大风险。这些风险包括缺乏透明度、敏感数据的潜在暴露、可复制性的挑战以及对专有系统的依赖。此外，它们的高成本使它们不适用于大型研究项目。
相比之下，开源模型虽然有各种尺寸，但如果不进一步微调，其性能可能会低于商业替代方案。然而，开源模型具有明显的优势：它们可以在本地运行（确保数据隐私）、针对特定任务进行微调、在研究社区内共享以及集成到可重现的工作流程中。
本研究表明，小型、经过微调的开源 LLM 可以实现与 ChatGPT-4 等模型相同或更优异的性能。我们进一步探讨了开源模型中训练集大小与微调效果之间的关系。最后，我们提出了一种混合工作流程，充分利用开放和封闭模型的优势，为性能、透明度和可重复性提供平衡的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.00890</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过人工智能反馈强化学习增强大型语言模型的中医能力</title>
      <link>https://arxiv.org/abs/2411.00897</link>
      <description><![CDATA[arXiv:2411.00897v1 公告类型：新
摘要：虽然大型语言模型在理解和响应用户意图方面表现良好，但由于缺乏专业知识，它们在传统中医 (TCM) 等专业领域的表现仍然有限。此外，与 TCM 相关的高质量数据稀缺且难以获取，使得大型语言模型在处理 TCM 任务时无效。在这项工作中，我们提出了一个框架，仅使用少量数据即可提高大型语言模型在 TCM 任务中的表现。首先，我们使用医疗案例数据对大型模型进行监督微调，使其最初能够执行 TCM 任务。随后，我们使用来自 AI 反馈的强化学习 (RLAIF) 进一步优化模型的性能，使其与偏好数据保持一致。消融研究还表明，性能提升归因于监督微调和直接策略优化。实验结果表明，使用少量数据训练的模型在代表性 TCM 任务上实现了显着的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2411.00897</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LIBMoE：大型语言模型专家混合综合基准测试库</title>
      <link>https://arxiv.org/abs/2411.00918</link>
      <description><![CDATA[arXiv:2411.00918v1 公告类型：新
摘要：混合专家 (MoE) 在开发更高效、更有效的大型语言模型 (LLM) 中发挥着重要作用。由于巨大的资源需求，许多研究人员仍然无法研究大规模 MoE 算法。这项工作开发了 \emph{LibMoE}，这是一个全面的模块化框架，用于简化 MoE 算法的研究、训练和评估。基于三个核心原则：(i) 模块化设计、(ii) 高效训练；(iii) 全面评估，LibMoE 通过标准化训练和评估流程，使 LLM 中的 MoE 更容易被广泛的研究人员使用。使用 LibMoE，我们在零样本设置下对三个不同的 LLM 和 11 个数据集上的五种最先进的 MoE 算法进行了广泛的基准测试。结果表明，尽管具有独特的特性，但所有 MoE 算法在广泛任务的平均表现大致相同。凭借模块化设计和广泛评估，我们相信 LibMoE 将对研究人员在下一代 MoE 和 LLM 方面取得有意义的进展具有无价的价值。项目页面：\url{https://fsoft-aic.github.io/fsoft-LibMoE.github.io}。]]></description>
      <guid>https://arxiv.org/abs/2411.00918</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReSpAct：协调推理、说话和行动，构建基于大型语言模型的对话式 AI 代理</title>
      <link>https://arxiv.org/abs/2411.00927</link>
      <description><![CDATA[arXiv:2411.00927v1 公告类型：新
摘要：基于大型语言模型 (LLM) 的代理越来越多地用于与外部环境（例如游戏、API 等）交互并解决任务。然而，当前的框架不允许这些代理与用户合作并与他们交互以协调他们的任务细节并实现用户定义的目标；相反，在模棱两可的情况下，这些代理可能会根据假设做出决策。这项工作引入了 ReSpAct（推理、说话和行动），这是一个新颖的框架，它协同结合了构建面向任务的“对话”代理的基本技能。ReSpAct 满足了对代理的这种需求，扩展了 ReAct 方法。ReSpAct 框架使代理能够解释用户指令、推理复杂任务、执行适当的操作并进行动态对话以寻求指导、澄清歧义、了解用户偏好、解决问题，并使用用户的中间反馈和响应来更新他们的计划。我们在支持用户交互的环境中评估了 ReSpAct，例如面向任务的对话 (MultiWOZ) 和交互式决策 (AlfWorld、WebShop)。ReSpAct 足够灵活，可以纳入动态用户反馈，并解决错误传播和代理陷入推理循环等常见问题。与仅依赖推理轨迹相比，这会产生更易于解释、更像人类的任务解决轨迹。在两个交互式决策基准测试 AlfWorld 和 WebShop 中，ReSpAct 的绝对成功率分别比仅使用强大推理的方法 ReAct 高出 6% 和 4%。在面向任务的对话基准测试 MultiWOZ 中，ReSpAct 分别将 Inform 和 Success 得分提高了 5.5% 和 3%。]]></description>
      <guid>https://arxiv.org/abs/2411.00927</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于嵌入的通用词典，用于透明且可重现的文本评分</title>
      <link>https://arxiv.org/abs/2411.00964</link>
      <description><![CDATA[arXiv:2411.00964v1 公告类型：新
摘要：随着文本分析工具在过去十年中变得越来越复杂，研究人员现在面临着一个决定：是否使用最先进的模型，这些模型提供高性能，但其操作可能非常不透明，并且运行时计算量很大。另一种选择通常是依赖较旧的、手工制作的文本评分工具，这些工具透明且易于应用，但性能有限。我提出了一种结合两者优势的替代方案：使用最少的研究人员输入从通用（预训练）词嵌入创建的词典。通过展示从 FastText 和 GloVe (6B) 词向量表示生成的许多概念词典，我认为基于嵌入的词典满足了对透明但高性能的文本测量工具的需求。]]></description>
      <guid>https://arxiv.org/abs/2411.00964</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强电子健康环境中构音障碍说话者的 AAC 软件：使用 TORGO 进行评估</title>
      <link>https://arxiv.org/abs/2411.00980</link>
      <description><![CDATA[arXiv:2411.00980v1 公告类型：新
摘要：患有脑瘫 (CP) 和肌萎缩侧索硬化症 (ALS) 的人经常面临发音困难，导致构音障碍并导致非典型言语模式。在医疗保健环境中，沟通障碍会降低护理质量。在构建增强和替代沟通 (AAC) 工具以实现流畅沟通时，我们发现最先进的 (SOTA) 自动语音识别 (ASR) 技术（如 Whisper 和 Wav2vec2.0）在很大程度上由于缺乏训练数据而使非典型说话者边缘化。我们的工作旨在利用 SOTA ASR，然后进行特定领域的错误纠正。英语构音障碍 ASR 性能通常在 TORGO 数据集上进行评估。提示重叠是此数据集的一个众所周知的问题，其中训练和测试说话者之间的短语重叠。我们的工作提出了一种打破这种提示重叠的算法。减少提示重叠后，SOTA ASR 模型的结果显示，对于患有轻度和重度构音障碍的说话者，单词错误率极高。此外，为了改进 ASR，我们的工作研究了 n-gram 语言模型和基于大语言模型 (LLM) 的多模态生成纠错算法（如 Whispering-LLaMA）对第二遍 ASR 的影响。我们的工作强调了还需要做多少工作来改进非典型说话者的 ASR，以便在面对面和电子医疗环境中实现公平的医疗保健。]]></description>
      <guid>https://arxiv.org/abs/2411.00980</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FedDTPT：针对黑盒大型语言模型的联合离散和可转移提示调整</title>
      <link>https://arxiv.org/abs/2411.00985</link>
      <description><![CDATA[arXiv:2411.00985v1 公告类型：新
摘要：近年来，大型语言模型（LLM）极大地推动了自然语言处理（NLP）领域的发展。通过使用特定场景的数据对LLM进行微调，这些基础模型可以更好地适应各种下游任务。然而，微调过程存在隐私泄露风险，尤其是在集中式数据处理场景中。为了解决用户隐私问题，引入了联邦学习（FL）来减轻从多个来源集中收集数据所带来的风险。然而，LLM本身的隐私同样至关重要，因为潜在的恶意攻击会挑战它们的安全性，而这个问题在当前的研究中受到的关注有限。因此，建立一个可信的多方模型微调环境至关重要。此外，大型LLM的本地部署会产生大量的存储成本和很高的计算需求。为了应对这些挑战，我们首次提出了一种用于黑盒大型语言模型的联邦离散可转移快速调优方法，即FedDTPT。在客户端优化阶段，我们采用 token 级离散提示优化方法，利用基于预测准确度的反馈循环通过 MLM API 驱动无梯度提示优化。对于服务器优化，我们采用基于语义相似性的注意机制来过滤所有本地提示 token，并使用嵌入距离肘部检测和 DBSCAN 聚类策略来增强过滤过程。实验结果表明，与最先进的方法相比，我们的方法在黑盒设置中实现了更高的准确度、更低的通信开销和对非 iid 数据的鲁棒性。此外，优化后的提示是可迁移的。]]></description>
      <guid>https://arxiv.org/abs/2411.00985</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Provenance：用于检索增强型 LLM 生成输出的轻量级事实检查器</title>
      <link>https://arxiv.org/abs/2411.01022</link>
      <description><![CDATA[arXiv:2411.01022v1 公告类型：新
摘要：我们提出了一种轻量级方法来检测检索增强生成 (RAG) 中的非事实输出。给定一个上下文和假定的输出，我们计算一个事实性分数，该分数可以进行阈值化以产生二元决策，以检查基于 LLM 的问答、摘要或其他系统的结果。与依赖 LLM 的事实性检查器不同，我们使用紧凑的开源自然语言推理 (NLI) 模型，这些模型可以产生一个可自由访问的解决方案，运行时具有低延迟和低成本，并且无需 LLM 微调。该方法还可以通过将幻觉追溯到特定的上下文块来实现下游的缓解和纠正。我们的实验显示，在广泛的相关开源数据集中，ROC 曲线下面积 (AUC) 很高，表明我们的方法对 RAG 输出进行事实核查的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.01022</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Birdie：利用奖励驱动的目标和课程推进状态空间模型</title>
      <link>https://arxiv.org/abs/2411.01030</link>
      <description><![CDATA[arXiv:2411.01030v2 公告类型：新
摘要：高效的状态空间模型 (SSM)，例如线性循环神经网络和线性注意变体，与 Transformer 相比具有计算优势，但在需要长距离上下文检索（如文本复制、联想回忆和长上下文问答）的任务中却举步维艰。以前为解决这些挑战所做的努力主要集中在架构修改上，这通常会重新引入计算效率低下的问题。在本文中，我们提出了一种新颖的训练程序 Birdie，它可以显着增强 SSM 的上下文检索能力，而无需改变其架构。我们的方法将双向输入处理与专门的预训练目标的动态混合相结合，并通过强化学习进行优化。我们引入了一种新的双向 SSM 架构，可以无缝地从双向上下文处理过渡到因果生成。实验评估表明，Birdie 显著提高了检索密集型任务（例如多号码电话簿查找、长段落问答和填充）的性能。这缩小了与 Transformers 的性能差距，同时保持了计算效率。我们的研究结果强调了训练程序在利用 SSM 的固定状态容量方面的重要性，为提升其功能提供了新的方向。所有代码和预训练模型均可在 https://www.github.com/samblouir/birdie 上找到，并支持 JAX 和 PyTorch。]]></description>
      <guid>https://arxiv.org/abs/2411.01030</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中推测解码的隐私风险</title>
      <link>https://arxiv.org/abs/2411.01076</link>
      <description><![CDATA[arXiv:2411.01076v2 公告类型：新
摘要：大型语言模型 (LLM) 中的推测解码通过廉价地推测预测多个标记并并行验证它们来加速标记生成，并且已被广泛部署。在本文中，我们提供了第一份展示推测解码隐私风险的研究。我们观察到，输入相关的正确和错误预测模式可能会泄露给监视标记生成时间和数据包大小的对手，从而导致隐私泄露。通过观察正确和错误推测的标记模式，我们表明恶意对手可以通过三种不同的推测解码技术（REST（准确度接近 100%）、LADE（准确度高达 92%）和 BiLD（准确度高达 95%）对查询进行指纹识别并以超过 90% 的准确度学习私人用户输入。我们表明，攻击者还可以泄露用于设计这些技术的机密知识产权，例如用于预测的数据存储数据（在 REST 中），速度超过每秒 25 个令牌，甚至用于预测的超参数（在 LADE 中）。我们还讨论了缓解策略，例如在多次迭代中聚合令牌并用额外的字节填充数据包，以避免此类隐私或机密性泄露。]]></description>
      <guid>https://arxiv.org/abs/2411.01076</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>表情符号攻击：一种误导法官的方法 安全风险检测法学硕士</title>
      <link>https://arxiv.org/abs/2411.01077</link>
      <description><![CDATA[arXiv:2411.01077v1 公告类型：新
摘要：越狱攻击展示了大型语言模型 (LLM) 如何被恶意提示诱骗生成有害输出。为了防止这些攻击，其他 LLM 通常被用作判断者来评估生成内容的有害性。然而，依赖 LLM 作为判断者可能会在检测过程中引入偏见，进而损害评估的有效性。在本文中，我们表明 Judge LLM 与其他 LLM 一样，也受到 token 分割偏见的影响。当 token 被分割成更小的子 token 时，就会发生这种偏见，从而改变它们的嵌入。这使得模型更难检测有害内容。具体而言，这种偏见可能导致子 token 与嵌入空间中的原始 token 有很大不同，从而导致对有害内容的“安全”预测不正确。为了利用 Judge LLM 中的这种偏见，我们引入了表情符号攻击——一种将表情符号放在 token 中以增加子 token 与其原始 token 之间的嵌入差异的方法。这些表情符号会创建新的 token，从而进一步扭曲 token 嵌入，加剧偏见。为了应对表情符号攻击，我们设计了提示来帮助 LLM 过滤掉不寻常的字符。但是，仍然可以通过使用表情符号和其他字符的混合来绕过这种防御。表情符号攻击还可以与使用少量学习的现有越狱提示相结合，这使 LLM 能够使用表情符号生成有害响应。这些响应经常被 Judge LLM 错误地标记为“安全”，从而让攻击得以溜走。我们对六台最先进的 Judge LLM 进行的实验表明，表情符号攻击允许 25% 的有害响应绕过 Llama Guard 和 Llama Guard 2 的检测，以及高达 75% 的 ShieldLM。这些结果凸显了对更强大的法官法学硕士 (LLM) 的需求，以解决这一弱点。]]></description>
      <guid>https://arxiv.org/abs/2411.01077</guid>
      <pubDate>Tue, 05 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>