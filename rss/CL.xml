<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title></title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description></description>
    <lastBuildDate>Fri, 07 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03552</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03552</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在Babel中对Babble进行排序：评估Openalex数据库中语言检测算法的性能</title>
      <link>https://arxiv.org/abs/2502.03627</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03627</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03643</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03643</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>寻找内在音乐：探究LLM对文学风格的理解</title>
      <link>https://arxiv.org/abs/2502.03647</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03647</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在大型语言模型中推进推理：有希望的方法和方法</title>
      <link>https://arxiv.org/abs/2502.03671</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03671</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03678</link>
      <description><![CDATA[ARXIV：2502.03678V1公告类型：新 
摘要：大型语言模型（LLMS）中文本生成的自动解码虽然缺乏内置机制来执行精炼和/或校正生成的内容，但本质上是次优的。在本文中，当共同考虑所有令牌同时考虑所有令牌时，我们从产生的响应的关节概率上考虑最佳性。从理论上讲，我们表征了自动加压力生成的响应与其全球最佳响应的潜在偏差，该响应的长度相同。我们的分析表明，当文本生成期间出现明显的不确定性时，我们需要保持谨慎，这可能表明了一代历史的次要性。为了解决文本生成自回旋解码的陷阱，我们提出了一种包含滑动反射窗口和暂停标准的方法，以便可以在解码过程中互换进行细化和生成。我们的选择性改进框架达到了效率和最佳性之间的平衡，我们广泛的实验结果证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.03678</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03685</link>
      <description><![CDATA[ARXIV：2502.03685V1公告类型：新 
摘要：受控文本生成允许对大型语言模型输出的用户定义的约束，这是越来越重要的领域，因为LLMS在日常生活中变得更加普遍。一种常见的方法使用基于能量的解码，该解码通过能量函数定义目标分布，将多个约束结合到加权平均值中。但是，即使对能量函数的系数进行了广泛的调整，这些方法通常也很难平衡流利性和限制满意度。在本文中，我们确定了这种次优的平衡是由在连续空间中进行采样而不是文本令牌的自然离散空间。为了解决这个问题，我们提出了离散自动回归偏置，这是一种受控的解码算法，在完全在离散文本域中运行时利用梯度。具体而言，我们通过在生成的序列和辅助偏置序列上定义联合分布来引入一种新的公式，以实现受控文本生成。为了有效地从该联合分布中采样，我们提出了使用基于梯度的离散MCMC的langevin-gibbs采样算法。我们的方法显着提高了限制满意度，同时保持可比或更好的流利度，所有这些都以较低的计算成本。我们证明了我们受控解码方法在情感控制，语言排毒和关键字引导生成方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.03685</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03688</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03688</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM对齐作为检索器优化：信息检索透视图</title>
      <link>https://arxiv.org/abs/2502.03699</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03699</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03708</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03708</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03711</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03711</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03748</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03748</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03766</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03766</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03793</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03793</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2502.03799</link>
      <description><![CDATA[]]></description>
      <guid>https://arxiv.org/abs/2502.03799</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>