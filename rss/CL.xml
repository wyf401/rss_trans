<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Satyrn：分析增强生成平台</title>
      <link>https://arxiv.org/abs/2406.12069</link>
      <description><![CDATA[arXiv:2406.12069v1 公告类型：新
摘要：大型语言模型 (LLM) 能够生成文档，检索增强生成 (RAG) 已被证明是一种在不牺牲流畅性的情况下提高准确性的强大方法。但是，并非所有信息都可以从文本中检索出来。我们提出了一种方法，该方法使用结构化数据的分析来生成事实集，这些事实集用于指导生成，其方式与 RAG 中使用检索到的文档的方式大致相同。这种分析增强生成 (AAG) 方法支持利用标准分析技术生成事实的能力，然后将其转换为文本并传递给 LLM。我们提出了一个神经符号平台 Satyrn，它利用 AAG 生成基于大型数据库的准确、流畅和连贯的报告。在我们的实验中，我们发现，即使使用 Mistral-7B 等较小的语言模型，Satyrn 生成的报告也能达到 86% 以上的准确率，同时保持较高的流畅性和连贯性，而 GPT-4 代码解释器中只有 57% 的声明是准确的。]]></description>
      <guid>https://arxiv.org/abs/2406.12069</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>不是消除而是聚合：事后控制专家混合来解决自然语言理解中的捷径转变</title>
      <link>https://arxiv.org/abs/2406.12060</link>
      <description><![CDATA[arXiv:2406.12060v1 公告类型：新
摘要：最近的自然语言理解模型倾向于利用数据集中的简单模式，通常称为捷径。这些捷径取决于训练数据中存在的标签和潜在特征之间的虚假相关性。在推理时，依赖捷径的模型很可能在分布发生变化的情况下产生错误的预测，特别是当某些潜在特征不再与标签相关时。为了避免这种情况，以前的研究已经训练模型以消除对捷径的依赖。在本研究中，我们探索了一个不同的方向：悲观地汇总专家混合的预测，假设每个专家都捕获相对不同的潜在特征。实验结果表明，我们对专家的事后控制显着增强了模型对捷径分布变化的鲁棒性。此外，我们表明我们的方法具有一些实际优势。我们还分析了我们的模型并提供支持该假设的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.12060</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>在生物医学基准测试中，语言模型对药物名称的识别出奇地脆弱</title>
      <link>https://arxiv.org/abs/2406.12066</link>
      <description><![CDATA[arXiv:2406.12066v1 公告类型：新
摘要：医学知识依赖于上下文，需要在语义等效短语的各种自然语言表达中进行一致的推理。这对于药物名称尤其重要，患者经常使用 Advil 或 Tylenol 等品牌名称而不是通用名称。为了研究这一点，我们创建了一个新的稳健性数据集 RABBITS，以评估使用医生专家注释交换品牌和通用药物名称后在医学基准上的性能差异。
我们在 MedQA 和 MedMCQA 上评估了开源和基于 API 的 LLM，发现性能持续下降 1-10%。此外，我们发现这种脆弱性的潜在来源是广泛使用的预训练数据集中测试数据的污染。所有代码都可以在 https://github.com/BittermanLab/RABBITS 上访问，HuggingFace 排行榜可以在 https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.12066</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>UniGLM：为文本属性图训练一个统一的语言模型</title>
      <link>https://arxiv.org/abs/2406.12052</link>
      <description><![CDATA[arXiv:2406.12052v1 公告类型：新 
摘要：文本属性图 (TAG) 上的表示学习对于文本和关系知识系统以及推荐系统至关重要，其中节点由文本描述表示。目前，TAG 的最先进的嵌入方法主要侧重于使用结构感知训练信号微调语言模型（例如 BERT）。虽然有效，但这些方法是针对单个 TAG 量身定制的，无法推广到各种图形场景。考虑到共享的文本空间，利用多个 TAG 进行联合微调，从不同方面对齐文本和图形结构将更有益。受此启发，我们引入了一种新颖的统一图形语言模型 (UniGLM) 框架，这是第一个可以很好地推广到域内和跨域 TAG 的图形嵌入模型。具体而言，UniGLM 使用自监督对比学习在具有不同域和规模的多个 TAG 上进行训练。 UniGLM 包括一种自适应正样本选择技术，用于识别结构相似的节点，以及一个惰性对比模块，旨在通过最小化重复编码计算来加速训练。9 个基准 TAG 的大量实证结果表明，UniGLM 在泛化（各种下游任务和主干）和迁移学习（域内和域外场景）方面优于领先的嵌入基线。代码可在 https://github.com/NYUSHCS/UniGLM 获得。]]></description>
      <guid>https://arxiv.org/abs/2406.12052</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>InternalInspector $I^2$：通过内部状态进行 LLM 中的稳健置信度估计</title>
      <link>https://arxiv.org/abs/2406.12053</link>
      <description><![CDATA[arXiv:2406.12053v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 功能强大，但它们通常难以生成可靠的输出，经常产生高置信度不准确性，即幻觉。为了应对这一挑战，我们的研究引入了 InternalInspector，这是一个新颖的框架，旨在通过利用内部状态（包括注意力状态、前馈状态和所有层的激活状态）的对比学习来增强 LLM 中的置信度估计。与主要关注最终激活状态的现有方法不同，InternalInspector 对每一层的所有内部状态进行全面分析，以准确识别正确和不正确的预测过程。通过将 InternalInspector 与各种自然语言理解和生成任务（包括事实问题回答、常识推理和阅读理解）中的现有置信度估计方法进行基准测试，InternalInspector 在将估计的置信度分数与 LLM 预测的正确性对齐方面实现了更高的准确度，并降低了校准误差。此外，InternalInspector 在幻觉检测基准 HaluEval 中表现出色，在该任务中优于其他基于内部的置信度估计方法。]]></description>
      <guid>https://arxiv.org/abs/2406.12053</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的软提示</title>
      <link>https://arxiv.org/abs/2406.12038</link>
      <description><![CDATA[arXiv:2406.12038v1 公告类型：新
摘要：大型语言模型 (LLM) 的广泛流行，部分原因是它们具有独特的上下文学习能力，这也揭示了在部署这些预训练模型时道德和安全考虑的重要性。在这项工作中，我们专注于研究受数据保护法规驱动的 LLM 的机器学习。与越来越多的关于实现学习的微调方法的文献相比，我们专注于一种相对轻量级的替代方法，称为软提示，以实现对训练数据子集的学习。通过旨在强制遗忘和效用保存的损失，我们的框架 \textbf{S}oft \textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) 学习可以附加到任意查询的提示标记，以在推理时诱导对特定示例的学习，而无需更新 LLM 参数。我们对所提出的方法进行了严格的评估，结果表明 SPUL 可以在使用 LLM 进行文本分类的情况下显著改善效用与遗忘之间的权衡。我们使用多个 LLM 进一步验证了我们的方法，以突出我们框架的可扩展性，并提供有关超参数选择和反学习数据大小影响的详细见解。我们的实现可在 \url{https://github.com/karuna-bhaila/llm_unlearning} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.12038</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>超越答案的学习：使用反射训练语言模型进行数学推理</title>
      <link>https://arxiv.org/abs/2406.12050</link>
      <description><![CDATA[arXiv:2406.12050v1 公告类型：新
摘要：监督微调增强了语言模型在各种数学推理任务中解决问题的能力。为了最大限度地发挥这些优势，现有的研究侧重于使用各种数据增强技术来扩大训练集，这对于标准的单轮问答设置非常有效。我们的工作引入了一种新技术，旨在培养对手头训练问题的更深入理解，不仅在标准设置中提高性能，而且在需要反思性思维的更复杂场景中也提高性能。具体来说，我们提出了反思增强，一种将问题反思嵌入到每个训练实例中的方法。它训练模型考虑替代观点并参与抽象和类比，从而通过反思推理促进彻底的理解。大量的实验验证了我们目标的实现，强调了我们的方法的独特优势及其相对于现有增强技术的互补性。]]></description>
      <guid>https://arxiv.org/abs/2406.12050</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>自 MoE：面向具有自专业专家的组合大型语言模型</title>
      <link>https://arxiv.org/abs/2406.12034</link>
      <description><![CDATA[arXiv:2406.12034v1 公告类型：新
摘要：我们提出了 Self-MoE，这是一种将单片 LLM 转换为自专业专家的组合式模块化系统的方法，称为 MiXSE（自专业专家的混合体）。我们的方法利用自专业化，使用自生成的合成数据构建专家模块，每个模块都配备共享的基础 LLM 并结合自优化路由。这允许对各种目标任务进行动态和特定于能力的处理，从而增强整体能力，而无需大量人工标记的数据和添加的参数。我们的实证结果表明，专业化的 LLM 可能会在非专业任务上表现出潜在的性能权衡。另一方面，我们的 Self-MoE 在知识、推理、数学和编码等各种基准上都比基础 LLM 有了显着的改进。它还始终优于其他方法，包括实例合并和权重合并，同时通过语义专家和路由的设计提供更好的灵活性和可解释性。我们的研究结果强调了模块化在实现高效、可扩展和适应性系统方面的重要作用以及自我改进的潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.12034</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>MedCalc-Bench：评估用于医学计算的大型语言模型</title>
      <link>https://arxiv.org/abs/2406.12036</link>
      <description><![CDATA[arXiv:2406.12036v1 公告类型：新
摘要：与评估计算和基于逻辑的推理相反，当前评估医学大型语言模型 (LLM) 的基准主要集中在涉及领域知识和描述性推理的问答上。虽然这种定性能力对于医学诊断至关重要，但在现实世界中，医生经常使用遵循定量方程和基于规则的推理范式的临床计算器来进行基于证据的决策支持。为此，我们提出了 MedCalc-Bench，这是一个首创的数据集，专注于评估 LLM 的医学计算能力。MedCalc-Bench 包含来自 55 种不同医学计算任务的 1000 多个手动审查实例的评估集。MedCalc-Bench 中的每个实例都包含一份患者记录、一个要求计算特定医疗值的问题、一个基本事实答案以及一个逐步说明如何获得答案的说明。虽然我们的评估结果显示了 LLM 在这一领域的潜力，但它们都不足以用于临床环境。常见问题包括提取错误的实体、未使用正确的方程式或规则进行计算任务，或错误地执行计算算术。我们希望我们的研究能够凸显 LLM 在医疗环境中的定量知识和推理方面的差距，鼓励未来改进 LLM 在各种临床计算任务中的应用。]]></description>
      <guid>https://arxiv.org/abs/2406.12036</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>LiLiuM：eBay 的电子商务大型语言模型</title>
      <link>https://arxiv.org/abs/2406.12023</link>
      <description><![CDATA[arXiv:2406.12023v1 公告类型：新
摘要：我们推出了 LiLiuM 系列大型语言模型 (LLM)：1B、7B 和 13B 参数模型，这些模型 100% 内部开发，以满足 eBay 在电子商务领域的特定需求。这使 eBay 能够完全控制模型的所有方面，包括许可证、数据、词汇表和架构。我们希望这些模型被用作微调和指令调整的基础，从而消除对外部模型的依赖。
LiLiuM LLM 已在来自通用和电子商务领域的 3 万亿个多语言文本标记上进行了训练。它们在英语自然语言理解 (NLU) 基准上的表现与流行的 LLaMA-2 模型相似。同时，我们在非英语 NLU 任务、机器翻译和电子商务特定的下游任务上的表现优于 LLaMA-2。
作为数据混合的一部分，我们利用新发布的 RedPajama-V2 数据集进行训练，并分享了有关数据过滤和重复数据删除的见解。我们还详细讨论了如何序列化结构化数据以用于自回归语言建模。我们提供了有关在预训练中包含代码和并行机器翻译数据的效果的见解。此外，我们开发了自己的标记器和模型词汇表，针对电子商务进行了定制。这样，与 LLaMA-2 相比，我们可以在 eBay 特定的下游任务上将文本生成速度提高 34%。
最后，关于 LLM 预训练，我们表明检查点平均可以进一步改善最佳单个模型检查点。]]></description>
      <guid>https://arxiv.org/abs/2406.12023</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:20 GMT</pubDate>
    </item>
    <item>
      <title>利用大型语言模型揭示和减轻心理健康分析中的偏见</title>
      <link>https://arxiv.org/abs/2406.12033</link>
      <description><![CDATA[arXiv:2406.12033v1 公告类型：新
摘要：大型语言模型 (LLM) 的进步已在包括心理健康分析在内的各种应用中展现出强大的能力。然而，现有的研究主要集中在预测性能上，而对公平性这一关键问题的研究不足，对弱势群体构成了重大风险。尽管承认存在潜在的偏见，但以前的研究缺乏对这些偏见及其影响的彻底调查。为了解决这一差距，我们使用十个具有不同提示方法的 LLM 在八个不同的心理健康数据集上系统地评估了七个社会因素（例如性别、年龄、宗教）的偏见。我们的结果表明，GPT-4 在 LLM 中实现了性能和公平性的最佳整体平衡，尽管在某些情况下它仍然落后于 MentalRoBERTa 等领域特定模型。此外，我们量身定制的公平意识提示可以有效减轻心理健康预测中的偏见，凸显了公平分析在该领域的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.12033</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:20 GMT</pubDate>
    </item>
    <item>
      <title>FinTruthQA：评估财务信息披露质量的基准数据集</title>
      <link>https://arxiv.org/abs/2406.12009</link>
      <description><![CDATA[arXiv:2406.12009v1 公告类型：新 
摘要：准确、透明的财务信息披露在会计和财务领域至关重要，可以确保市场效率和投资者信心。在众多信息披露平台中，中国证券交易所的投资者互动平台为上市公司通过在线问答 (Q&amp;A) 形式披露投资者感兴趣的信息提供了一种新颖的互动方式。然而，上市公司回答问题时通常只提供有限或没有实质性信息，而自动评估大量问答对的财务信息披露质量是一项挑战。本文构建了一个基准 FinTruthQA，它可以评估先进的自然语言处理 (NLP) 技术对财务问答数据中信息披露质量的自动评估。FinTruthQA 包含 6,000 个现实世界的财务问答条目，每个问答都基于会计的四个概念维度进行手动注释。我们在 FinTruthQA 上对各种 NLP 技术进行了基准测试，包括统计机器学习模型、预训练语言模型及其微调版本，以及大型语言模型 GPT-4。实验表明，现有的 NLP 模型对真实问题识别和问题相关性任务具有很强的预测能力，但对于答案相关性和答案可读性任务则表现不佳。通过建立这一基准，我们为信息披露的自动评估提供了坚实的基础，大大提高了财务报告的透明度和质量。审计师、监管机构和财务分析师可以使用 FinTruthQA 进行实时监控和数据驱动的决策，研究人员也可以使用 FinTruthQA 进行会计和金融的高级研究，最终提高金融市场的信任度和效率。]]></description>
      <guid>https://arxiv.org/abs/2406.12009</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:19 GMT</pubDate>
    </item>
    <item>
      <title>CItruS：用于长序列建模的分块指令感知状态驱逐</title>
      <link>https://arxiv.org/abs/2406.12018</link>
      <description><![CDATA[arXiv:2406.12018v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的不断发展，长序列建模引起了广泛关注。最近的研究发现，Transformer 模型的键值缓存中的大部分隐藏状态可以被丢弃（也称为逐出），而不会影响生成长序列的困惑度性能。然而，我们表明，这些方法尽管保留了困惑度性能，但经常会丢失对解决下游任务很重要的信息，我们称之为信息忽视的问题。为了解决这个问题，我们引入了分块指令感知状态逐出 (CItruS)，这是一种新颖的建模技术，它将对下游任务有用的注意力偏好集成到隐藏状态的逐出过程中。此外，我们设计了一种分块序列处理方法，以进一步提高效率。我们的免训练方法在相同的内存预算下，在几个强基线上的长序列理解和检索任务上表现出卓越的性能，同时保留了语言建模困惑度。]]></description>
      <guid>https://arxiv.org/abs/2406.12018</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:19 GMT</pubDate>
    </item>
    <item>
      <title>使用基于视觉的语法归纳模型将语言引导重新定义为联合推理</title>
      <link>https://arxiv.org/abs/2406.11977</link>
      <description><![CDATA[arXiv:2406.11977v1 公告类型：新
摘要：语义和句法引导假设儿童利用他们对一个语言领域的先前知识（例如句法关系）来帮助以后获得另一个语言领域，例如新词的含义。支持这两种理论的实证结果可能会诱使我们相信这些是不同的学习策略，其中一种可能先于另一种。在这里，我们认为它们都取决于一种更普遍的语言习得学习策略：联合学习。使用一系列神经视觉基础语法诱导模型，我们证明当同时学习语法和语义时，句法和语义引导效果最强。联合学习可以带来更好的语法诱导、现实的词汇类别学习以及对新句子和动词含义的更好解释。联合学习通过相互约束句法和语义的假设空间，使学习者更容易习得语言。研究多种输入源和模态的联合推理动态代表了认知科学和人工智能领域语言建模和学习研究的一个重要的新方向，因为它可以帮助我们解释如何在更受限的学习环境中获得语言。]]></description>
      <guid>https://arxiv.org/abs/2406.11977</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:18 GMT</pubDate>
    </item>
    <item>
      <title>对话动作标记：使用多轮规划器在目标导向对话中引导语言模型</title>
      <link>https://arxiv.org/abs/2406.11978</link>
      <description><![CDATA[arXiv:2406.11978v1 公告类型：新
摘要：我们提出了一种称为对话动作标记 (DAT) 的方法，该方法可调整语言模型代理来规划目标导向的对话。核心思想是将每个话语视为一个动作，从而将对话转换为可以应用强化学习等现有方法的游戏。具体来说，我们冻结预训练的语言模型并训练一个小型规划器模型，该模型预测连续动作向量，用于每轮的控制生成。这种设计避免了奖励优化下的语言退化问题。在 Sotopia 平台上进行社交模拟评估时，DAT 引导的 LLaMA 模型超越了 GPT-4 的性能。我们还应用 DAT 在新颖的多轮红队设置中引导攻击者语言模型，揭示了潜在的新攻击面。]]></description>
      <guid>https://arxiv.org/abs/2406.11978</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:18 GMT</pubDate>
    </item>
    </channel>
</rss>