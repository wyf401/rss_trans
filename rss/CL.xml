<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上进行更新。</description>
    <lastBuildDate>Thu, 18 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>论情感分析的因果性质</title>
      <link>https://arxiv.org/abs/2404.11055</link>
      <description><![CDATA[arXiv:2404.11055v1 公告类型：新
摘要：情感分析（SA）旨在识别文本中表达的情感，例如产品评论。给定评论和与之相关的情绪，本文将 SA 表述为两个任务的组合：（1）因果发现任务，区分评论是“启动”情绪（因果假设 C1）还是情绪“启动”审查（因果假设 C2）； (2) 传统的预测任务，使用评论作为输入来建模情绪。使用心理学中的峰值-结束规则，如果样本的总体情感得分接近评论中所有句子级情感的平均值，则将样本分类为 C1；如果总体情感得分接近峰值和最终情感的平均值，则将样本分类为 C2 。对于预测任务，我们使用样本背后发现的因果机制来提高 LLM 的性能，方法是提出因果提示，为模型提供底层因果图的归纳偏差，从而在零上实现高达 32.13 F1 点的实质性改进。射五级SA。我们的代码位于 https://github.com/cogito233/causal-sa]]></description>
      <guid>https://arxiv.org/abs/2404.11055</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士基于不确定性的弃权提高了安全性并减少了幻觉</title>
      <link>https://arxiv.org/abs/2404.10960</link>
      <description><![CDATA[arXiv:2404.10960v1 公告类型：新
摘要：大型语言模型（LLM）实际部署的一个主要障碍是它们缺乏可靠性。这种情况尤其明显的三种情况是正确性、给出无法回答的问题时的幻觉以及安全性。在这三种情况下，理想情况下模型应该避免做出回应，就像人类一样，人类理解不确定性的能力使我们避免回答我们不知道的问题。受类似分类方法的启发，本研究探讨了法学硕士在问答领域中不确定时弃权的可行性和有效性。我们研究了两种不确定性：统计不确定性指标和一种独特的言语测量，称为对话中不确定性（InDU）。将这些不确定性度量与带有或不带有人类反馈的强化学习（RLHF）的模型相结合，我们表明，在所有三种情况下，基于正确类型的不确定性度量的弃权可以提高法学硕士的可靠性。通过仅牺牲一些高度不确定的样本，我们可以将正确性提高 2% 至 8%，通过正确识别无法回答的问题来避免 50% 的幻觉，并将安全性提高 70% 至 99%，而几乎没有额外的计算开销。]]></description>
      <guid>https://arxiv.org/abs/2404.10960</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>用于评估人类和语言模型中的道德推理的程序困境生成</title>
      <link>https://arxiv.org/abs/2404.10975</link>
      <description><![CDATA[arXiv:2404.10975v1 公告类型：新
摘要：随着语言模型等人工智能系统越来越多地融入到影响人们生活的决策过程中，确保这些系统具有健全的道德推理至关重要。为了测试它们是否有效，我们需要进行系统评估。我们提供了一个框架，使用语言模型将捕获道德困境关键方面的因果图转换为提示模板。通过这个框架，我们在程序上生成了一组大量且多样化的道德困境——OffTheRails 基准——由 50 个场景和 400 个独特的测试项目组成。我们收集了人类参与者对我们项目子集的道德许可性和意图判断，并将这些判断与八种条件下的两种语言模型（GPT-4 和 Claude-2）的判断进行了比较。我们发现，在道德困境中，伤害是一种必要手段（与副作用相比），导致参与者和语言模型的允许性较低，意图评级较高。对于可避免的有害结果与不可避免的有害结果也观察到相同的模式。然而，对于伤害是由代理人的行为还是由于不采取行动造成的，尚无明确的影响。我们讨论了即时生成管道的局限性以及改进场景以增加实验效果强度的机会。]]></description>
      <guid>https://arxiv.org/abs/2404.10975</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的偏移遗忘</title>
      <link>https://arxiv.org/abs/2404.11045</link>
      <description><![CDATA[arXiv:2404.11045v1 公告类型：新
摘要：尽管大型语言模型（LLM）从训练语料库中获取知识的能力很强，但对语料库中敏感信息（例如受版权保护的、有害的和私人内容）的记忆引起了伦理和法律问题。为了应对这些挑战，忘记学习已成为受有问题的培训数据影响的法学硕士的一种潜在补救措施。然而，以前的遗忘技术要么由于需要访问模型内​​部权重而不适用于黑盒法学硕士，要么因保留敏感数据以进行推理时间校正而违反了数据保护原则。我们提出$\delta$-unlearning，一种用于黑盒法学硕士的抵消学习框架。 $\delta$-unlearning 不是调整黑盒 LLM 本身，而是通过对比一对较小模型的 logit 来学习 unlearning 所需的 logit 偏移量。实验表明，$\delta$-unlearning 可以有效地忘却目标数据，同时在一般的遗忘范围任务上保持相似甚至更强的性能。 $\delta$-unlearning还有效地结合了不同的unlearning算法，使我们的方法成为一种通用的解决方案，可以将各种现有的unlearning算法适应黑盒LLM。]]></description>
      <guid>https://arxiv.org/abs/2404.11045</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>Binder：通过二进制向量的顺序嵌入进行分层概念表示</title>
      <link>https://arxiv.org/abs/2404.10924</link>
      <description><![CDATA[arXiv:2404.10924v1 公告类型：新
摘要：对于自然语言理解和生成，使用基于顺序的表示嵌入概念是一项基本任务。与传统的基于点向量的表示不同，基于顺序的表示对表示向量施加几何约束，以明确捕获一对概念之间可能存在的各种语义关系。在现有文献中，已经提出了几种基于顺序的嵌入方法，主要侧重于捕获层次关系；示例包括欧几里得空间、复数、双曲线、阶数和盒嵌入中的向量。框嵌入创建了基于区域的丰富概念表示，但在此过程中它牺牲了简单性，需要定制的优化方案来学习表示。双曲嵌入通过利用双曲空间不断扩展的特性来提高嵌入质量，但它也遭受了与盒嵌入相同的命运，因为梯度下降之类的优化在双曲空间中并不简单。在这项工作中，我们提出了 Binder，一种基于顺序表示的新颖方法。 Binder 使用二进制向量进行嵌入，因此嵌入向量非常紧凑，占用空间比其他方法小一个数量级。 Binder 使用简单高效的优化方案来学习具有线性时间复杂度的表示向量。我们的综合实验结果表明，Binder 非常准确，在表示任务上产生了有竞争力的结果。但 Binder 在传递闭包链接预测任务上从竞争对手中脱颖而出，因为它可以仅从直接边缘学习概念嵌入，而所有现有的基于顺序的方法都依赖于间接边缘。]]></description>
      <guid>https://arxiv.org/abs/2404.10924</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>为语言提供更多空间：研究检索对语言模型的影响</title>
      <link>https://arxiv.org/abs/2404.10939</link>
      <description><![CDATA[arXiv:2404.10939v1 公告类型：新
摘要：检索增强语言模型为标准语言建模提供了一种有前景的替代方案。在预训练期间，这些模型在文档语料库中搜索有助于实现语言建模目标的上下文相关信息。我们引入了一种“理想检索”方法来在完全可控的环境中研究这些模型。我们进行了广泛的评估，以检查检索增强如何影响底层语言模型的行为。除此之外，我们观察到这些模型：i）在权重中保存的世界知识要少得多，ii）更好地理解本地上下文和单词间依赖关系，但 iii）在理解全局上下文方面较差。]]></description>
      <guid>https://arxiv.org/abs/2404.10939</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>语言模型可以解决奥林匹克编程问题吗？</title>
      <link>https://arxiv.org/abs/2404.10952</link>
      <description><![CDATA[arXiv:2404.10952v1 公告类型：新
摘要：计算机奥林匹克竞赛包含一些对人类来说最具挑战性的问题，除了生成高效的代码之外，还需要复杂的算法推理、谜题解决。然而，它作为评估语言模型 (LM) 的领域尚未得到充分研究。在本文中，我们介绍了 USACO 基准测试，包含来自美国计算机奥林匹克竞赛的 307 个问题，以及每个问题的高质量单元测试、参考代码和官方分析。这些资源使我们能够首次构建和测试一系列用于竞争性编程的 LM 推理方法。我们发现 GPT-4 在零样本思维链提示下仅达到 8.7% 的 pass@1 准确率，而我们最好的推理方法通过结合自我反思和情景知识检索将其提高到 20.2%。然而，这还远未解决基准问题。为了更好地理解剩下的挑战，我们设计了一项新颖的人机循环研究，并令人惊讶地发现少量有针对性的提示使 GPT-4 能够解决以前任何模型和方法都无法解决的 15 个问题中的 13 个。我们的基准、基线方法、定量结果和定性分析是迈向具有基础、创造性和算法推理的 LM 的第一步。]]></description>
      <guid>https://arxiv.org/abs/2404.10952</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>我应该回答哪些问题？好奇问题的显着性预测</title>
      <link>https://arxiv.org/abs/2404.10917</link>
      <description><![CDATA[arXiv:2404.10917v1 公告类型：新
摘要：好奇问题——人们在阅读时提出的开放式、好奇心驱动的问题——是话语处理（Kehler 和 Rohde，2017；Onea，2016）和理解（Prince，2004）不可或缺的一部分。 NLP 领域的最新工作利用了法学硕士的问题生成功能来增强广泛的应用。但好奇问题的空间是巨大的：许多问题可以从给定的背景中引发出来。那么应该优先考虑其中哪一个来寻找答案呢？不幸的是，语言学理论尚未提供这个问题的答案。本文提出了 QSALIENCE，一种好奇问题的显着性预测器。 QSALIENCE 是根据我们的语言学家注释的 1,766 个（上下文、问题）对的显着性分数数据集进行指令调整的。如果回答一个问题能够极大地增强对文本的理解，那么该问题的显着性得分就很高（Van Rooy，2003）。我们表明，根据经验，高度显着的问题更有可能在同一篇文章中得到回答，从而将潜在问题（Onea，2016）与正在讨论的问题（Roberts，2012）联系起来。我们进一步验证了我们的发现，表明回答突出问题是新闻摘要质量的指标。]]></description>
      <guid>https://arxiv.org/abs/2404.10917</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>通过多指令训练教授多语言大语言模型以理解多语言语音</title>
      <link>https://arxiv.org/abs/2404.10922</link>
      <description><![CDATA[arXiv:2404.10922v1 公告类型：新
摘要：语言建模的最新进展导致了能够执行各种自然语言处理任务的大型语言模型（LLM）的出现。尽管法学硕士在基于文本的任务中取得了成功，但将法学硕士应用于语音领域仍然有限且具有挑战性。本文提出了 BLOOMZMMS，这是一种将多语言 LLM 与多语言语音编码器集成在一起的新颖模型，旨在利用 LLM 进行语音识别及其他方面的功能。利用多教学训练方法，我们展示了语言知识从文本到语音模态的可迁移性。我们对来自 139 种语言的 1900 个小时的转录数据进行了实验，结果表明可以有效地学习多语言语音表示，并与多语言法学硕士保持一致。虽然这种学习的表示最初显示出任务泛化的局限性，但我们通过以多指令风格生成合成目标来解决这个问题。我们的零样本评估结果证实了我们的方法在多个任务中的稳健性，包括语音翻译和多语言口语理解，从而为在语音领域应用法学硕士开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2404.10922</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>仅使用法学硕士即可按照用户指示孵化文本分类器</title>
      <link>https://arxiv.org/abs/2404.10877</link>
      <description><![CDATA[arXiv:2404.10877v1 公告类型：新
摘要：在本文中，我们的目标是在给定任意类定义（即用户指令）的情况下生成文本分类数据，因此可以在没有任何人工注释或原始语料库的情况下训练小型文本分类器。与先驱尝试相比，我们提出的孵化器是第一个可以处理复杂甚至相互依赖的类的框架（例如，“教育者给出的 TED 演讲”和“其他”）。具体来说，Incubator 是一个法学硕士，首先根据我们从 HuggingFace 上的分类数据集和描述以及 GPT-4 的上下文增强中获得的指令到数据映射进行调整。然后，我们通过学习语义文本嵌入的聚类中心来完善孵化器，以强调世代的一致性和语义多样性。我们将 Incubator 的各种分类任务与强大的基线进行比较，例如基于 LLM 的直接推理和通过即时工程生成训练数据。实验表明，Incubator 能够 (1) 在传统基准测试上表现良好，(2) 考虑标签依赖性和用户偏好，(3) 通过孵化多个分类器来实现逻辑文本挖掘。]]></description>
      <guid>https://arxiv.org/abs/2404.10877</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>超越查询的搜索：通过强化学习训练较小的网络交互语言模型</title>
      <link>https://arxiv.org/abs/2404.10887</link>
      <description><![CDATA[arXiv:2404.10887v1 公告类型：新
摘要：传统的搜索系统专注于查询制定以获得有效的结果，但在产品搜索等场景中面临挑战，在用户访问特定产品页面之前，关键的产品细节（例如尺寸、颜色）仍然被隐藏。这凸显了对能够根据用户的高级意图制定查询和导航网页的智能网络导航代理的需求。为了满足这一需求，这项工作引入了一种用于智能 Web 交互的基础语言代理，称为 GLAINTEL。利用语言建模和强化学习方面的进步，GLAINTEL 研究了基于 Transformer 的模型在增强交互式网络环境的搜索能力方面的功效。考虑到网络导航中每个状态的动态动作空间，GLAINTEL 采用 Flan-T5 架构，并结合了语言建模和价值估计头。这项工作的重点是在各种场景中训练较小的语言模型作为代理，系统地评估人类演示对训练过程的影响。具体来说，我们调查无法进行人类演示的场景，并随后评估此类演示的有效利用。我们还探索了针对演示仅限于特定领域的情况的无监督领域适应。跨不同设置的实验评估证明了在无监督环境中训练代理的有效性，优于基于上下文学习的方法，这些方法采用具有多达 5400 亿个参数的更大模型。令人惊讶的是，直接使用人类演示的基于行为克隆的方法并不优于基于无监督学习的方法。此外，将人类演示与基于强化学习的训练相结合所产生的结果可与使用 GPT-4 的模型相媲美。]]></description>
      <guid>https://arxiv.org/abs/2404.10887</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>基于 LayoutLMv3 的模型，用于增强视觉丰富文档中的关系提取</title>
      <link>https://arxiv.org/abs/2404.10848</link>
      <description><![CDATA[arXiv:2404.10848v1 公告类型：新
摘要：文档理解是自然语言处理（NLP）中一个不断发展的领域。特别是，除了原始文本本身之外，视觉和空间特征也很重要，因此，在视觉文档理解（VDU）领域开发了几种多模态模型。然而，虽然研究主要集中在关键信息提取（KIE）上，但已识别实体之间的关系提取（RE）仍然没有得到充分研究。例如，RE 对于重新组合实体或获取文档中数据的全面层次结构至关重要。在本文中，我们提出了一个模型，该模型从 LayoutLMv3 初始化，可以匹配或优于应用于 FUNSD 和 CORD 数据集上的视觉丰富文档（VRD）的 RE 中当前最先进的结果，而无需任何特定的预处理。训练并且参数较少。我们还报告了在 FUNSD 上进行的广泛消融研究，强调了某些功能和建模选择对性能的巨大影响。]]></description>
      <guid>https://arxiv.org/abs/2404.10848</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>D3CODE：消除跨文化中关于攻击性检测和评估的数据分歧</title>
      <link>https://arxiv.org/abs/2404.10857</link>
      <description><![CDATA[arXiv:2404.10857v1 公告类型：新
摘要：虽然人类注释在语言技术中发挥着至关重要的作用，但注释者的主观性在数据收集中长期以来一直被忽视。最近对这个问题进行批判性研究的研究通常是在西方背景下进行的，并且仅记录了年龄、性别或种族群体之间的差异。因此，NLP 对主观性的研究忽视了这样一个事实，即人口群体中的个体可能持有不同的价值观，这可能会影响他们超出群体规范的看法。为了有效地将这些考虑因素纳入 NLP 流程中，我们需要具有来自不同社会和文化群体的广泛并行注释的数据集。在本文中，我们介绍了 \dataset 数据集：一个大规模跨文化数据集，包含超过 4500 个句子中的冒犯性语言并行注释，由超过 4000 个注释者池注释，在性别和年龄上保持平衡，来自 21 个国家/地区，代表八个地理文化区域。该数据集包含注释者的道德价值观，涵盖六个道德基础：关怀、平等、相称、权威、忠诚和纯洁。我们的分析揭示了注释者的认知存在巨大的地区差异，这些差异是由个人道德价值观塑造的，为构建多元化、文化敏感的 NLP 模型提供了重要的见解。]]></description>
      <guid>https://arxiv.org/abs/2404.10857</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>迫使分散分布脱离语言模型</title>
      <link>https://arxiv.org/abs/2404.10859</link>
      <description><![CDATA[arXiv:2404.10859v1 公告类型：新
摘要：尽管经过专门训练以遵循用户指令，但当今的语言模型在被指示生成随机输出时表现不佳。例如，当提示您统一选择 1 到 10 之间的数字时，Llama-2-13B-chat 不成比例地倾向于数字 5，并且当任务是随机选择名字时，Mistral-7B-Instruct 选择 Avery 的频率是其选择 Avery 的 40 倍。我们根据美国人口进行预期。当这些语言模型用于输出多样性至关重要的现实世界任务时，例如语言模型辅助数据集构建，它们无法在有效选择上产生分散分布是一个主要障碍。在这项工作中，我们提出了一种微调方法，鼓励语言模型输出在有效结果上扩散的分布。我们介绍的方法可以推广到各种任务和分布，并使大型语言模型适用于合成数据集生成，而无需人工干预。]]></description>
      <guid>https://arxiv.org/abs/2404.10859</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>更少的截断改善了语言建模</title>
      <link>https://arxiv.org/abs/2404.10830</link>
      <description><![CDATA[arXiv:2404.10830v1 公告类型：新
摘要：在大型语言模型训练中，输入文档通常连接在一起，然后分成相等长度的序列以避免填充标记。尽管串联方法效率很高，但它会损害数据完整性——它不可避免地将许多文档分成不完整的部分，导致过度截断，从而阻碍模型学习基于完整上下文构建逻辑连贯且事实上一致的内容。为了解决这个问题，我们提出了最佳拟合打包，这是一种可扩展且高效的方法，通过长度感知组合优化将文档打包到训练序列中。我们的方法完全消除了不必要的截断，同时保持与串联相同的训练效率。文本和代码预训练的实证结果表明，我们的方法实现了卓越的性能（例如，阅读理解相对+4.7%；上下文跟踪+16.8%；程序合成+9.2%），并减少了闭域幻觉有效提升高达 58.3%。]]></description>
      <guid>https://arxiv.org/abs/2404.10830</guid>
      <pubDate>Thu, 18 Apr 2024 06:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>