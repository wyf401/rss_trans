<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 06 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>NUMCoT：使用大型语言模型进行思路链推理中的数字和测量单位</title>
      <link>https://arxiv.org/abs/2406.02864</link>
      <description><![CDATA[arXiv:2406.02864v1 公告类型：新
摘要：数系与计量单位是人类活动中两个紧密相连的话题，与表达它们的语言相互影响。目前，大型语言模型（LLM）的评估通常涉及数学推理，但很少关注数字或单位的微小变化如何显著改变问题的复杂性和LLM的性能。在本文中，我们通过构建带有扰动的数据集来审视现有的LLM对数字和计量单位的处理。我们首先将数学应用题的推理分解为不同的子过程，例如从语言到数字的数字转换和基于单位的计量转换。然后，我们进一步注释了中国古代算术著作中在数字和计量单位方面具有挑战性的数学应用题。在扰动数据集上的实验表明，LLM在处理数字和计量转换方面仍然存在困难。]]></description>
      <guid>https://arxiv.org/abs/2406.02864</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>使用低秩矩阵完成算法进行高效最小贝叶斯风险解码</title>
      <link>https://arxiv.org/abs/2406.02832</link>
      <description><![CDATA[arXiv:2406.02832v1 公告类型：新
摘要：最小贝叶斯风险 (MBR) 解码是一种功能强大的解码策略，广泛用于文本生成任务，但其二次计算复杂度限制了其实际应用。本文提出了一种使用矩阵完成技术近似 MBR 解码的新方法，重点关注机器翻译任务。我们将 MBR 解码表述为矩阵完成问题，其中候选假设和伪参考翻译之间的效用度量分数形成一个低秩矩阵。首先，我们通过经验证明分数矩阵确实具有低秩结构。然后，我们通过仅计算分数的随机子集来利用这一点，并通过应用交替最小二乘 (ALS) 算法有效地恢复矩阵中缺失的条目，从而实现对 MBR 解码过程的快速近似。我们在机器翻译任务上的实验结果表明，与 vanilla MBR 解码相比，所提出的方法需要 1/16 的效用度量计算，同时在 WMT22 数据集（ende 和 enru）上实现 COMET22 测量的同等翻译质量。我们还将我们的方法与其他近似方法进行了基准测试，与它们相比，我们发现质量有所提高。]]></description>
      <guid>https://arxiv.org/abs/2406.02832</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>Xmodel-LM 技术报告</title>
      <link>https://arxiv.org/abs/2406.02856</link>
      <description><![CDATA[arXiv:2406.02856v1 公告类型：新
摘要：我们介绍了 Xmodel-LM，这是一个紧凑而高效的 1.1B 语言模型，已在超过 2 万亿个 token 上进行了预训练。Xmodel-LM 在我们自建的数据集 (Xdata) 上进行训练，该数据集基于下游任务优化平衡了中文和英文语料库，尽管规模较小，但性能却非常出色。它明显超越了现有类似规模的开源语言模型。我们的模型检查点和代码可在 GitHub 上公开访问，网址为 https://github.com/XiaoduoAILab/XmodelLM。]]></description>
      <guid>https://arxiv.org/abs/2406.02856</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>LLM 作为评分者：输出顺序对对话评价的影响</title>
      <link>https://arxiv.org/abs/2406.02863</link>
      <description><![CDATA[arXiv:2406.02863v1 公告类型：新
摘要：本研究使用大型语言模型 (LLM) 研究提示设计对对话评估的影响。虽然 LLM 越来越多地用于对各种输入进行评分，但由于对话评估中的模型敏感性和主观性，创建有效的对话评估提示仍然具有挑战性。我们的研究尝试了不同的提示结构，改变了输出指令的顺序并包括解释性原因。我们发现，呈现原因和分数的顺序会显著影响 LLM 的评分，而“原因优先”的方法会产生更全面的评估。这一见解对于提高基于 LLM 的评估的准确性和一致性至关重要。]]></description>
      <guid>https://arxiv.org/abs/2406.02863</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>探索医患对话摘要的稳健性：对域外 SOAP 注释的分析</title>
      <link>https://arxiv.org/abs/2406.02826</link>
      <description><![CDATA[arXiv:2406.02826v1 公告类型：新
摘要：由于专业领域和收集领域内训练数据的困难，总结医疗对话带来了独特的挑战。在本研究中，我们研究了最先进的医患对话生成摘要模型在领域外数据上的性能。我们将医患对话的摘要模型分为两种配置：（1）通用模型，不指定主观（S）、客观（O）和评估（A）和计划（P）注释；（2）面向 SOAP 的模型，生成带有 SOAP 部分的摘要。我们分析了基于微调语言模型的方法和 GPT 在这两种配置上的局限性和优势。我们还进行了语言查询和字数统计分析，以比较来自不同数据集的 SOAP 注释。结果显示，不同数据集的参考注释具有很强的相关性，表明格式不匹配（即单词分布不一致）不是域外数据性能下降的主要原因。最后，本文还对 SOAP 注释进行了详细分析，以深入了解模型引入的缺失信息和幻觉。]]></description>
      <guid>https://arxiv.org/abs/2406.02826</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>大而不倒：较大的语言模型对痴呆症相关语言异常的诱发具有不成比例的适应力</title>
      <link>https://arxiv.org/abs/2406.02830</link>
      <description><![CDATA[arXiv:2406.02830v1 公告类型：新
摘要：随着人工神经网络变得越来越复杂，了解其内部工作原理变得越来越具有挑战性，这在医疗保健应用中尤为重要。自回归神经语言模型 (NLM) 的内在评估指标困惑度 (PPL) 可以反映 NLM 模型对新输入的“惊讶”程度。PPL 已被广泛用于理解 NLM 的行为。先前的研究结果表明，在预训练的基于 Transformer 的 NLM 中掩盖注意力层时 PPL 的变化反映了与阿尔茨海默病痴呆相关的语言异常。在此基础上，我们探索了一种新颖的双向注意力头部消融方法，该方法表现出归因于人类大脑研究中认知和大脑储备概念的特性，这些特性假设大脑中神经元更多、处理效率更高的人对神经退行性疾病的抵抗力更强。我们的结果表明，较大的 GPT-2 模型需要遮盖/消融更大比例的注意力头，才能显示出与较小模型中遮盖类似程度的退化。这些结果表明，Transformer 模型中的注意力机制可能类似于认知和大脑储备的概念，并可能用于模拟神经退行性疾病和衰老进展的某些方面。]]></description>
      <guid>https://arxiv.org/abs/2406.02830</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>解开逻辑：上下文在大型语言模型推理能力中的作用</title>
      <link>https://arxiv.org/abs/2406.02787</link>
      <description><![CDATA[arXiv:2406.02787v1 公告类型：新
摘要：本研究旨在通过研究来自一系列综合领域的抽象和情境化逻辑问题的对比，系统地解开纯逻辑推理和文本理解。我们探索当底层逻辑结构保持不变时，LLM 是否在各个领域表现出真正的推理能力。我们关注两个主要问题 (1) 抽象逻辑问题是否可以准确地在现实世界场景中对 LLM 的推理能力进行基准测试，脱离实际环境中的上下文支持？(2) 对抽象逻辑问题进行微调的 LLM 是否可以推广到情境化逻辑问题，反之亦然？为了研究这些问题，我们专注于标准命题逻辑，特别是命题演绎和溯因逻辑推理。特别是，我们构建了具有 4 个难度级别的演绎和溯因推理的实例化数据集，涵盖了基于维基百科分类的 12 个不同类别或领域。我们的实验旨在深入了解逻辑推理中的上下文解开以及 LLM 的真正推理能力及其泛化潜力。代码和数据集可在以下网址获取：https://github.com/agiresearch/ContextHub。]]></description>
      <guid>https://arxiv.org/abs/2406.02787</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>代理链：大型语言模型协作完成长上下文任务</title>
      <link>https://arxiv.org/abs/2406.02818</link>
      <description><![CDATA[arXiv:2406.02818v1 公告类型：新
摘要：解决有效处理长上下文的挑战已成为大型语言模型 (LLM) 的关键问题。出现了两种常见策略：1) 减少输入长度，例如通过检索增强生成 (RAG) 检索相关块，以及 2) 扩展 LLM 的上下文窗口限制。然而，这两种策略都有缺点：输入减少不能保证覆盖所需信息的部分，而窗口扩展则难以专注于解决任务的相关信息。为了缓解这些限制，我们提出了 Chain-of-Agents (CoA)，这是一种新颖的框架，它通过自然语言利用多代理协作，实现跨长上下文任务的各种 LLM 的信息聚合和上下文推理。CoA 由多个工作代理组成，它们依次通信以处理文本的不同分段部分，然后是一个管理代理，它将这些贡献合成一个连贯的最终输出。 CoA 通过交替阅读和推理来处理整个输入，并通过为每个代理分配一个短上下文来缓解长上下文焦点问题。我们在问答、总结和代码完成等各种长上下文任务上对 CoA 进行了全面评估，结果表明，与 RAG、全上下文和多代理 LLM 的强大基线相比，CoA 的性能显著提升了 10%。]]></description>
      <guid>https://arxiv.org/abs/2406.02818</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>RATT：连贯且正确的法学推理的思维结构</title>
      <link>https://arxiv.org/abs/2406.02746</link>
      <description><![CDATA[arXiv:2406.02746v1 公告类型：新
摘要：大型语言模型（LLM）从思维结构中获得了实质性的推理和决策能力。然而，现有的方法如思维树和检索增强思维往往因对事实知识的局部检索不足和全局策略选择不足而无法在复杂任务中发挥作用。这些限制使得这些方法难以有效地平衡事实准确性和综合逻辑优化。为了解决这些限制，我们引入了检索增强思维树（RATT），这是一种新颖的思维结构，它在思维过程的每一步都考虑整体的逻辑合理性和事实的正确性。具体而言，在思维分支的每个点，RATT 都会进行规划和前瞻，以探索和评估多个潜在的推理步骤，并将检索增强生成（RAG）的事实核查能力与 LLM 评估整体策略的能力相结合。通过这种事实知识与战略可行性的结合，RATT 调整并整合思维树结构，在搜索空间中寻找最有前途的分支。这种思维结构显著增强了模型在逻辑推理中的连贯性和决策的效率，从而提高了 LLM 基于思维结构生成可靠推理和决策的能力极限。对不同类型的任务进行的大量实验表明，RATT 结构在事实正确性和逻辑连贯性方面明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2406.02746</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>通过细粒度监督对齐大型语言模型</title>
      <link>https://arxiv.org/abs/2406.02756</link>
      <description><![CDATA[arXiv:2406.02756v1 公告类型：新
摘要：预训练的大规模语言模型 (LLM) 擅长生成连贯的文章，但它们的输出可能不真实、有害或不符合用户期望。当前的方法侧重于使用强化学习和人类反馈 (RLHF) 来改进模型对齐，其工作原理是将 LLM 输出的粗略人类偏好转化为指导模型学习过程的反馈信号。但是，由于这种方法在序列级反馈上运行，因此它缺乏识别影响用户偏好的输出的确切部分的精度。为了解决这一差距，我们提出了一种通过细粒度标记级监督来增强 LLM 对齐的方法。具体来说，我们要求注释者在标准奖励建模数据集中对不太受欢迎的响应进行最低限度的编辑，以使其更受欢迎，确保仅在必要时进行更改，同时保留大部分原始内容。精炼后的数据集用于训练标记级奖励模型，然后用于训练我们的细粒度近端策略优化 (PPO) 模型。我们的实验结果表明，与传统的 PPO 模型相比，该方法在 LLM 性能方面（相对于参考模型的胜率）可实现高达 $5.1\%$ 的绝对提升。]]></description>
      <guid>https://arxiv.org/abs/2406.02756</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>通过将后缀梯度压缩到前缀控制器来实现 LLM 行为的自我控制</title>
      <link>https://arxiv.org/abs/2406.02721</link>
      <description><![CDATA[arXiv:2406.02721v1 公告类型：新
摘要：我们提出了一种新方法 Self-Control，利用后缀梯度来控制大型语言模型 (LLM) 的行为，而无需明确的人工注释。给定一个以后缀字符串表示的指南和模型的自我评估，Self-Control 计算有关模型隐藏状态的自我判断的梯度，直接影响自回归生成过程以实现期望的行为。为了提高效率，我们引入了 Self-Control_{prefix}，这是一个紧凑的模块，它将从后缀梯度中学习到的表示封装到前缀控制器中，从而促进对各种 LLM 行为的推理时间控制。我们的实验证明了 Self-Control 在多个领域的有效性，包括情绪调节、确保无害和增强复杂推理。特别是，Self-Control_{prefix} 可以实现即插即用控制并联合控制多个属性，从而无需改变模型参数或增加推理时间成本即可改善模型输出。]]></description>
      <guid>https://arxiv.org/abs/2406.02721</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>具有自监督蒸馏功能的无文本声学模型，用于抗噪富有表现力的语音到语音翻译</title>
      <link>https://arxiv.org/abs/2406.02733</link>
      <description><![CDATA[arXiv:2406.02733v1 公告类型：新
摘要：在本文中，我们提出了一种无文本声学模型，该模型具有自监督蒸馏策略，用于抗噪的富有表现力的语音到语音翻译（S2ST）。最近提出的富有表现力的 S2ST 系统通过将单元到语音（U2S）生成器级联到语音到单元翻译模型，实现了令人印象深刻的表达力保持性能。然而，这些系统容易受到输入语音中存在噪声的影响，这是现实世界翻译场景中的假设。为了解决这一限制，我们提出了一种 U2S 生成器，它将无标签蒸馏（DINO）自监督训练策略结合到其预训练过程中。由于所提出的方法捕​​获了与噪声无关的表达力表示，因此即使在嘈杂的环境中也可以生成合格的语音。客观和主观评估结果验证了所提出的方法显着提高了富有表现力的 S2ST 系统在嘈杂环境中的性能，同时在干净的环境中保持了有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.02733</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>跨模式安全协调：您所需要的只是文本忘却吗？</title>
      <link>https://arxiv.org/abs/2406.02575</link>
      <description><![CDATA[arXiv:2406.02575v1 公告类型：新
摘要：最近的研究表明，将新模态集成到大型语言模型 (LLM)（例如视觉语言模型 (VLM)）中，会创建一个新的攻击面，从而绕过现有的安全训练技术，如监督微调 (SFT) 和带人工反馈的强化学习 (RLHF)。虽然可以在多模态设置中进行进一步的基于 SFT 和 RLHF 的安全训练，但收集多模态训练数据集是一项重大挑战。受最近多模态模型结构设计的启发，无论输入模态的组合如何，所有输入最终都会融合到语言空间中，我们旨在探索仅在文本域中进行反学习是否可以有效地实现跨模态安全对齐。我们对六个数据集的评估从经验上证明了其可迁移性——VLM 中的文本反学习将攻击成功率 (ASR) 显著降低至 8% 以下，在某些情况下，甚至低至近 2%，无论是基于文本的攻击还是基于视觉文本的攻击，同时保留了实用性。此外，我们的实验表明，使用多模态数据集进行反学习不会带来任何潜在好处，但会显著增加计算需求，可能高达 6 倍。]]></description>
      <guid>https://arxiv.org/abs/2406.02575</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 语言模型可以被黑客入侵吗？</title>
      <link>https://arxiv.org/abs/2406.02577</link>
      <description><![CDATA[arXiv:2406.02577v1 公告类型：新
摘要：已经提出了许多算法来消除不良行为。然而，与非常大的状态空间和创建适当的奖励函数相关的挑战往往会导致各种越狱。我们的论文旨在研究在受控的积极情绪语言生成环境中奖励的这种影响。我们没有使用基于人类反馈的奖励模型进行在线训练，而是使用静态学习的情绪分类器。我们还考虑了一种设置，在训练后，我们的模型的权重和激活会暴露给最终用户。我们在应用近端策略优化 (PPO) 来促进积极情绪反应之前和之后，通过机械可解释性的视角来检查预训练的 GPT-2。利用这些见解，我们 (1) 尝试“破解”PPO 模型以生成负面情绪反应，以及 (2) 在奖励函数中添加一个项来尝试改变“负面”权重。]]></description>
      <guid>https://arxiv.org/abs/2406.02577</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:06 GMT</pubDate>
    </item>
    <item>
      <title>Block Transformer：用于快速推理的全局到局部语言建模</title>
      <link>https://arxiv.org/abs/2406.02657</link>
      <description><![CDATA[arXiv:2406.02657v1 公告类型：新
摘要：本文介绍了 Block Transformer 架构，该架构采用分层全局到局部建模来对自回归变压器进行建模，以缓解自注意力的推理瓶颈。要应用自注意力，必须在每个解码步骤中从内存中检索所有先前序列的键值 (KV) 缓存。因此，此 KV 缓存 IO 成为批量推理中的重要瓶颈。我们注意到这些成本源于将自注意力应用于全局上下文，因此我们将全局建模的昂贵瓶颈隔离到较低层，并在较高层应用快速局部建模。为了减轻较低层的剩余成本，我们将输入标记聚合成固定大小的块，然后在这个粗略级别应用自注意力。上下文信息被聚合到单个嵌入中，以使上层能够解码下一个标记块，而无需全局注意力。没有全局注意力瓶颈，上层可以充分利用计算硬件来最大化推理吞吐量。通过利用全局和局部模块，Block Transformer 架构与具有同等困惑度的 vanilla Transformer 相比，推理吞吐量提高了 10-20 倍。我们的工作引入了一种新方法，通过新颖的全局到局部建模应用来优化语言模型推理。代码可在 https://github.com/itsnamgyu/block-transformer 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.02657</guid>
      <pubDate>Thu, 06 Jun 2024 06:19:06 GMT</pubDate>
    </item>
    </channel>
</rss>