<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CL 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 30 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>反向图像检索提示多模态法学硕士中的参数记忆</title>
      <link>https://arxiv.org/abs/2405.18740</link>
      <description><![CDATA[arXiv:2405.18740v1 公告类型：新 
摘要：尽管最近的多模态大型语言模型 (MLLM) 取得了令人瞩目的进步，但来自 GPT-4 套件等最先进的模型仍然难以完成知识密集型任务。为了解决这个问题，我们考虑使用反向图像检索 (RIR) 增强生成，这是一种简单而有效的策略，可以通过网络规模的反向图像搜索结果增强 MLLM。在开放式 VQA 评估指标方面，RIR 可显著提高 GPT-4V 的知识密集型视觉问答 (VQA) 37-43%，GPT-4 Turbo 提高 25-27%，GPT-4o 提高 18-20%。令我们惊讶的是，我们发现 RIR 有助于模型更好地访问自己的世界知识。具体来说，我们的实验表明，RIR 增强有助于提供进一步的视觉和文本提示，而不一定包含对查询的直接答案。此外，我们阐明了 RIR 可能损害性能的情况并进行了人工评估。最后，我们发现使用 RIR 的整体优势使得可以选择使用 RIR 的代理很难比以 RIR 为默认设置的方法表现更好。]]></description>
      <guid>https://arxiv.org/abs/2405.18740</guid>
      <pubDate>Thu, 30 May 2024 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>Genshin：具有大型语言模型的自然语言处理的通用盾牌</title>
      <link>https://arxiv.org/abs/2405.18741</link>
      <description><![CDATA[arXiv:2405.18741v1 公告类型：新
摘要：大型语言模型 (LLM)（如 ChatGPT、Gemini 或 LLaMA）最近非常流行，在无数领域展示了相当大的进步和可推广性。然而，LLM 创建了一个更大的黑匣子，加剧了不透明性，可解释性仅限于少数方法。LLM 的本质所包含的不确定性和不透明性限制了它们在金融欺诈、网络钓鱼等高风险领域的应用。当前的方法主要依赖于具有后验可解释算法的传统文本分类，攻击者可能会创建多种对抗样本来突破系统的防御，迫使用户在效率和稳健性之间做出权衡。为了解决这个问题，我们提出了一种名为 Genshin（用于大型语言模型的自然语言处理的通用盾牌）的新型级联框架，利用 LLM 作为防御性一次性插件。与大多数试图将文本转换为新内容或结构内容的 LLM 应用不同，Genshin 使用 LLM 将文本恢复到其原始状态。Genshin 旨在结合 LLM 的通用性、中值模型的区分度和简单模型的可解释性。我们在情感分析和垃圾邮件检测任务上的实验表明，当前中值模型存在致命缺陷，而 LLM 的恢复能力令人振奋，证明了 Genshin 既有效又高效。在我们的消融研究中，我们发现了一些有趣的观察结果。利用 LLM 防御者（一种源自第四范式的工具），我们在 NLP 的第三范式中重现了 BERT 的 15% 最佳掩码率结果。此外，当使用 LLM 作为潜在的对抗工具时，攻击者能够执行几乎语义无损的有效攻击。]]></description>
      <guid>https://arxiv.org/abs/2405.18741</guid>
      <pubDate>Thu, 30 May 2024 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>上下文位置编码：学习计算重要的事情</title>
      <link>https://arxiv.org/abs/2405.18719</link>
      <description><![CDATA[arXiv:2405.18719v1 公告类型：新
摘要：注意力机制是大型语言模型 (LLM) 的一个关键组件，它允许序列中的标记相互交互，但顺序不变。结合位置编码 (PE) 可以按位置寻址，例如关注第 i 个标记。但是，当前的 PE 方法使用标记计数来推导位置，因此无法推广到更高的抽象级别，例如关注第 i 个句子。在本文中，我们提出了一种新的位置编码方法，即上下文位置编码 (CoPE)，它允许通过仅在模型确定的某些标记上增加位置来根据上下文调整位置。这允许更一般的位置寻址，例如关注第 $i$ 个特定单词、名词或句子。我们表明，CoPE 可以解决流行位置嵌入失败的选择性复制、计数和触发器任务，并改善语言建模和编码任务的困惑度。]]></description>
      <guid>https://arxiv.org/abs/2405.18719</guid>
      <pubDate>Thu, 30 May 2024 06:19:01 GMT</pubDate>
    </item>
    <item>
      <title>CtrlA：通过探针引导控制实现自适应检索增强生成</title>
      <link>https://arxiv.org/abs/2405.18727</link>
      <description><![CDATA[arXiv:2405.18727v1 公告类型：新
摘要：检索增强生成 (RAG) 已成为一种有前途的解决方案，可用于缓解使用检索的外部知识的大型语言模型 (LLM) 的幻觉。自适应 RAG 通过动态评估检索必要性来增强这种方法，旨在平衡外部和内部知识的使用。然而，现有的自适应 RAG 方法主要依靠 LLM 的表面言语化或概率反馈来实现按需检索，或通过精心制作的数据集直接微调 LLM，导致检索必要性决策不可靠、额外成本高昂和响应生成不理想。我们首次尝试深入研究 LLM 的内部状态以缓解此类问题，方法是引入一种有效的探测引导自适应 RAG 框架，称为 CtrlA。具体来说，CtrlA 使用诚实探测器来控制 LLM 的行为，通过操纵其表示来提高诚实度，并使用置信度探测器来监控 LLM 的内部状态并评估置信度，从而确定生成过程中的检索必要性。实验表明，CtrlA 在多种任务上都优于现有的自适应 RAG 方法，诚实控制可以有效地使 LLM 更加诚实，置信度监控被证明是检索触发的一个有希望的指标。我们的代码可在 https://github.com/HSLiu-Initial/CtrlA.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2405.18727</guid>
      <pubDate>Thu, 30 May 2024 06:19:01 GMT</pubDate>
    </item>
    <item>
      <title>理解大型语言模型中的内在社会经济偏见</title>
      <link>https://arxiv.org/abs/2405.18662</link>
      <description><![CDATA[arXiv:2405.18662v1 公告类型：新 
摘要：大型语言模型 (LLM) 越来越多地被整合到关键的决策过程中，例如贷款审批和签证申请，其中固有的偏见可能导致歧视性结果。在本文中，我们研究了 LLM 中人口统计属性与社会经济偏见之间的细微关系，这是 LLM 公平性的一个重要但研究不足的领域。我们引入了一个包含一百万个英语句子的新数据集，以系统地量化不同人口群体的社会经济偏见。我们的研究结果揭示了 GPT-2 等成熟模型和 Llama 2 和 Falcon 等最先进模型中普遍存在的社会经济偏见。我们证明，当考虑交叉性时，这些偏见会被显着放大，LLM 表现出从姓名中提取多个人口统计属性然后将它们与特定的社会经济偏见相关联的非凡能力。这项研究强调了在关键的现实应用中部署这些强大的模型时，迫切需要主动和强大的偏见缓解技术来防止出现歧视性的结果。]]></description>
      <guid>https://arxiv.org/abs/2405.18662</guid>
      <pubDate>Thu, 30 May 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT 能否重新定义医学理解？评估 GPT 在生物医学机器阅读理解方面的表现</title>
      <link>https://arxiv.org/abs/2405.18682</link>
      <description><![CDATA[arXiv:2405.18682v1 公告类型：新
摘要：大型语言模型 (LLM) 在不同领域的许多任务上表现出色。然而，它们在闭卷生物医学机器阅读理解 (MRC) 中的表现尚未得到深入评估。在这项工作中，我们在四个闭卷生物医学 MRC 基准上评估了 GPT。我们尝试了不同的传统提示技术，并介绍了我们自己的新提示方法。为了解决 LLM 固有的一些检索问题，我们提出了一种名为隐式检索增强生成 (RAG) 的提示策略，该策略减轻了在传统 RAG 设置中使用向量数据库检索重要块的需要。此外，我们报告了对我们方法的自然语言生成输出的定性评估。结果表明，我们的新提示技术能够在四个数据集中的两个中获得最佳性能，并在其余数据集中排名第二。实验表明，即使在零样本设置下，GPT 等现代 LLM 也能胜过监督模型，从而在两个基准上取得新的最先进 (SoTA) 结果。]]></description>
      <guid>https://arxiv.org/abs/2405.18682</guid>
      <pubDate>Thu, 30 May 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>通过贝叶斯说服实现高效的模型无关对齐</title>
      <link>https://arxiv.org/abs/2405.18718</link>
      <description><![CDATA[arXiv:2405.18718v1 公告类型：新
摘要：随着大型语言模型 (LLM) 的最新进展，对齐已成为一种保持 LLM 与人类意图一致的有效技术。当前的方法主要涉及通过监督微调 (SFT) 或从人类反馈中强化学习 (RLHF) 进行直接训练，这两者都需要大量的计算资源和大量的地面实况数据。本文探讨了一种使用较小模型对齐黑盒大型模型的有效方法，引入了一个与模型无关的轻量级贝叶斯说服对齐框架。我们将这个问题形式化为从小模型的角度对信号策略的优化。在说服过程中，小模型 (Advisor) 观察信息项 (即状态) 并说服大模型 (Receiver) 以引起改进的响应。然后，接收器根据输入、来自 Advisor 的信号及其对信息项的更新信念生成响应。通过使用我们的框架进行训练，我们证明了顾问可以显著提高各种接收者在一系列任务中的表现。我们从理论上分析了我们的说服框架，并为顾问的遗憾提供了上限，证实了其在学习最佳信号策略方面的有效性。我们的实证结果表明，GPT-2 可以显著提高各种模型的性能，数学推理能力平均提高了 16.1%，代码生成能力平均提高了 13.7%。我们希望我们的工作可以为从贝叶斯说服的角度重新思考对齐框架迈出第一步。]]></description>
      <guid>https://arxiv.org/abs/2405.18718</guid>
      <pubDate>Thu, 30 May 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>训练 LLM 以更好地自我调试和解释代码</title>
      <link>https://arxiv.org/abs/2405.18649</link>
      <description><![CDATA[arXiv:2405.18649v1 公告类型：新
摘要：在代码生成领域，自调试至关重要。它允许 LLM 根据执行反馈改进其生成的代码。这尤其重要，因为对于复杂的任务来说，一次性生成正确的解决方案是一项挑战。之前关于自调试的研究主要集中在通过为 LLM 提供少量示例来提示方法，这些方法在小型开源 LLM 上效果不佳。在这项工作中，我们提出了一个训练框架，可以显着提高 LLM 的自调试能力。直观地，我们观察到对错误代码进行一系列解释，然后进行代码改进，有助于 LLM 更好地分析错误代码并进行改进。因此，我们提出了一种自动化管道，通过生成大量解释和改进轨迹并通过执行验证进行过滤，来收集高质量的代码解释和改进数据集。我们对成功和失败轨迹执行监督微调 (SFT) 和进一步强化学习 (RL)，并采用一种考虑代码解释和细化质量的新颖奖励设计。在四个基准测试中，SFT 将 pass@1 提高了 15.92%，将 pass@10 提高了 9.30%。RL 训练将 pass@1 提高了 3.54%，将 pass@10 提高了 2.55%。经过训练的 LLM 表现出迭代细化能力，可以持续细化代码。最后，我们的人工评估表明，使用我们的框架训练的 LLM 可以生成更有用的代码解释，并帮助开发人员更好地理解源代码中的错误。]]></description>
      <guid>https://arxiv.org/abs/2405.18649</guid>
      <pubDate>Thu, 30 May 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>基于基础语言模型的持续学习的最新进展：一项综述</title>
      <link>https://arxiv.org/abs/2405.18653</link>
      <description><![CDATA[arXiv:2405.18653v1 公告类型：新
摘要：最近，基础语言模型 (LM) 在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了重大成就。与传统的神经网络模型不同，基础语言模型通过对大量参数的大量无监督数据集进行预训练，获得丰富的常识性知识，从而获得了强大的迁移学习能力。然而，由于灾难性遗忘，它们仍然无法模拟类似人类的持续学习。因此，已经开发了各种基于持续学习 (CL) 的方法来改进 LM，使它们能够适应新任务而不会忘记以前的知识。然而，现有方法的系统分类和它们的性能比较仍然缺乏，这正是我们的调查旨在填补的空白。我们深入研究了基于 CL 的方法应用于基础语言模型（例如预训练语言模型 (PLM)、大型语言模型 (LLM) 和视觉语言模型 (VLM)）的现有文献的全面回顾、总结和分类。我们将这些研究分为离线 CL 和在线 CL，包括传统方法、基于参数高效的方法、基于指令调整的方法和持续预训练方法。离线 CL 包括领域增量学习、任务增量学习和类增量学习，而在线 CL 细分为硬任务边界和模糊任务边界设置。此外，我们概述了 CL 研究中使用的典型数据集和指标，并对基于 LM 的持续学习的挑战和未来工作进行了详细分析。]]></description>
      <guid>https://arxiv.org/abs/2405.18653</guid>
      <pubDate>Thu, 30 May 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>GLOCON 数据库：设计决策和用户手册 (v1.0)</title>
      <link>https://arxiv.org/abs/2405.18613</link>
      <description><![CDATA[arXiv:2405.18613v1 公告类型：新
摘要：GLOCON 是一个有争议事件的数据库，它自动从来自不同国家的国家新闻来源中提取多种语言的内容。利用国家新闻来源，处理完整的新闻档案，为每个来源创建一个事件列表。自动化是使用从完整新闻档案中随机抽取的黄金标准语料库 (Y\&quot;or\&quot;uk et al. 2022) 实现的，并且至少由两位领域专家根据 Duru\c{s}an et al. (2022) 中提供的事件定义进行注释。]]></description>
      <guid>https://arxiv.org/abs/2405.18613</guid>
      <pubDate>Thu, 30 May 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>ConSiDERS-人类评估框架：重新思考生成大型语言模型的人类评估</title>
      <link>https://arxiv.org/abs/2405.18638</link>
      <description><![CDATA[arXiv:2405.18638v1 公告类型：新
摘要：在本立场文件中，我们认为，对生成式大型语言模型 (LLM) 的人工评估应该是一项多学科的工作，它借鉴用户体验研究和人类行为心理学等学科的见解，以确保实验设计和结果可靠。因此，这些评估的结论必须考虑可用性、美学和认知偏差等因素。我们强调了认知偏差如何将流畅的信息和真实性混为一谈，以及认知不确定性如何影响 Likert 等评分的可靠性。此外，评估应该区分日益强大的大型语言模型的能力和弱点——这需要有效的测试集。人工评估的可扩展性对于更广泛的采用也至关重要。因此，为了在生成式 NLP 时代设计一个有效的人工评估系统，我们提出了 ConSiDERS-The-Human 评估框架，该框架由 6 个支柱组成——一致性、评分标准、差异化、用户体验、责任和可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2405.18638</guid>
      <pubDate>Thu, 30 May 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>学习对大型语言模型的多样化攻击，以实现强大的红队和安全调整</title>
      <link>https://arxiv.org/abs/2405.18540</link>
      <description><![CDATA[arXiv:2405.18540v1 公告类型：新
摘要：红队演练或识别引发有害响应的提示是确保安全且负责任地部署大型语言模型 (LLM) 的关键步骤。开发针对多种攻击提示模式的有效保护需要发现各种攻击。自动红队演练通常使用强化学习来微调攻击者语言模型，以生成引发目标 LLM 不良响应的提示，例如通过辅助毒性分类器进行测量。我们表明，即使使用显式正则化来支持新颖性和多样性，现有方法也会遭受模式崩溃或无法产生有效攻击。作为一种灵活且符合概率原则的替代方案，我们建议使用 GFlowNet 微调，然后进行二次平滑阶段，以训练攻击者模型以生成多样化且有效的攻击提示。我们发现，我们的方法生成的攻击对各种目标 LLM 都有效，无论是否进行安全调整，并且可以在目标 LLM 之间很好地转移。最后，我们证明，使用我们的方法生成的红队提示数据集进行安全调整的模型对于其他基于 RL 的红队方法的攻击具有很强的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2405.18540</guid>
      <pubDate>Thu, 30 May 2024 06:18:57 GMT</pubDate>
    </item>
    <item>
      <title>基于 BioBERT 的深度学习和合并 ChemProt-DrugProt 用于增强生物医学关系提取</title>
      <link>https://arxiv.org/abs/2405.18605</link>
      <description><![CDATA[arXiv:2405.18605v1 公告类型：新
摘要：本文介绍了一种增强生物医学文本关系提取的方法，特别关注化学-基因相互作用。利用 BioBERT 模型和多层全连接网络架构，我们的方法使用新颖的合并策略集成了 ChemProt 和 DrugProt 数据集。通过大量实验，我们展示了显着的性能改进，特别是在数据集之间共享的 CPR 组中。研究结果强调了数据集合并在增加样本数量和提高模型准确性方面的重要性。此外，该研究还强调了自动信息提取在生物医学研究和临床实践中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.18605</guid>
      <pubDate>Thu, 30 May 2024 06:18:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 CamemBERT-bio 对临床叙述中的数字进行多目标表示</title>
      <link>https://arxiv.org/abs/2405.18448</link>
      <description><![CDATA[arXiv:2405.18448v1 公告类型：新
摘要：本研究旨在使用 CamemBERT-bio 对从七个不同生理类别的医学文档中提取的数值进行分类。先前的研究表明，基于 Transformer 的模型在此类任务中的表现可能不如传统的 NLP 模型。为了提高 CamemBERT-bio 的性能，我们引入了两项主要创新：将关键字嵌入集成到模型中，并通过从文本中排除所有数值数据来采用数字不可知策略。标签嵌入技术的实施改进了注意力机制，而使用“数字盲”数据集的技术旨在支持以上下文为中心的学习。我们研究的另一个关键组成部分是确定提取的数值数据的关键性。为此，我们采用了一种简单的方法，即验证值是否在既定的标准范围内。我们的研究结果令人鼓舞，表明 CamemBERT-bio 的有效性有了显着提高，F1 得分为 0.89，超过了传统方法。这比传统方法的 0.73 $F_1$ 得分提高了 20% 以上，比最先进方法的 0.82 $F_1$ 得分提高了 9% 以上。尽管使用了小而平衡的训练数据集，但仍然取得了这些成就。]]></description>
      <guid>https://arxiv.org/abs/2405.18448</guid>
      <pubDate>Thu, 30 May 2024 06:18:56 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士和记忆：关于版权合规的质量和特殊性</title>
      <link>https://arxiv.org/abs/2405.18492</link>
      <description><![CDATA[arXiv:2405.18492v1 公告类型：新
摘要：大型语言模型 (LLM) 中的记忆是一个日益严重的问题。事实证明，LLM 可以轻松复制其部分训练数据，包括受版权保护的作品。这是一个需要解决的重要问题，因为它可能违反现有的版权法以及《欧洲人工智能法案》。在这项工作中，我们提出了一种系统分析，以欧洲法律为例，量化 LLM 中潜在版权侵权的程度。与以前的工作不同，我们在现实的最终用户场景中评估指令微调模型。我们的分析建立在 160 个字符的建议阈值上，我们借鉴了《德国版权服务提供商法》和模糊文本匹配算法来识别可能侵犯版权的文本复制品。通过比较模型在受版权保护和公共领域数据上的行为来分析针对版权侵权的对策的特异性。我们调查了模型除了生成受保护的文本之外还表现出哪些行为（例如拒绝或幻觉），并对这些行为进行了初步法律评估。我们发现，热门 LLM 在版权合规性、特异性和适当拒绝方面存在巨大差异。在我们的比较中，Alpaca、GPT 4、GPT 3.5 和 Luminous 表现最佳，而 OpenGPT-X、Alpaca 和 Luminous 产生的潜在版权侵权绝对数量特别低。代码将很快发布。]]></description>
      <guid>https://arxiv.org/abs/2405.18492</guid>
      <pubDate>Thu, 30 May 2024 06:18:56 GMT</pubDate>
    </item>
    </channel>
</rss>